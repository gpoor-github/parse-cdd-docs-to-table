"Section"	"section_id"	"req_id"	"full_key"	"key_as_number"	"requirement"	"Test Availability"	"search_roots"	"search_terms"	"manual_search_terms"	"not_search_terms"	"not_files"	"max_matches"	"class_defs"	"methods"	"modules"	"protected"	"class_def"	"method"	"module"	"file_name"	"matched_files"	"methods_string"	"urls"	"method_text"	"matched_terms"	"qualified_method"	"Annotation?"	"New Req for S?"	"New CTS for S?"	"Comment(internal) e.g. why a test is not possible"	"CTS Bug Id"	"CDD Bug Id"	"Area"	"Shortened"	"Test Level"
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.video.CameraVideoActivity"	"exists"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/video/CameraVideoActivity.java"	""	"public void test/*
 *.
 */
package com.android.cts.verifier.camera.video;

import android.app.AlertDialog;
import android.content.Context;
import android.content.DialogInterface;
import android.graphics.Matrix;
import android.graphics.SurfaceTexture;
import android.hardware.Camera;
import android.hardware.Camera.CameraInfo;
import android.hardware.Camera.Size;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraManager;
import android.hardware.cts.helpers.CameraUtils;
import android.media.CamcorderProfile;
import android.media.MediaPlayer;
import android.media.MediaRecorder;
import android.os.Bundle;
import android.os.Environment;
import android.os.Handler;
import android.text.method.ScrollingMovementMethod;
import android.util.Log;
import android.view.Surface;
import android.view.TextureView;
import android.view.View;
import android.widget.AdapterView;
import android.widget.ArrayAdapter;
import android.widget.Button;
import android.widget.ImageButton;
import android.widget.Spinner;
import android.widget.TextView;
import android.widget.Toast;
import android.widget.VideoView;

import com.android.cts.verifier.PassFailButtons;
import com.android.cts.verifier.R;

import java.io.File;
import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.Date;
import java.util.List;
import java.util.Optional;
import java.util.TreeSet;


/**
 * Tests for manual verification of camera video capture
 */
public class CameraVideoActivity extends PassFailButtons.Activity
        implements TextureView.SurfaceTextureListener {

    private static final String TAG = ""CtsCameraVideo"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);
    private static final int MEDIA_TYPE_IMAGE = 1;
    private static final int MEDIA_TYPE_VIDEO = 2;
    private static final int VIDEO_LENGTH = 3000; // in ms

    private TextureView mPreviewView;
    private SurfaceTexture mPreviewTexture;
    private int mPreviewTexWidth;
    private int mPreviewTexHeight;
    private int mPreviewRotation;
    private int mVideoRotation;

    private VideoView mPlaybackView;

    private Spinner mCameraSpinner;
    private Spinner mResolutionSpinner;

    private int mCurrentCameraId = -1;
    private Camera mCamera;
    private boolean mIsExternalCamera;
    private int mVideoFrameRate = -1;

    private MediaRecorder mMediaRecorder;

    private List<Size> mPreviewSizes;
    private Size mNextPreviewSize;
    private Size mPreviewSize;
    private List<Integer> mVideoSizeIds;
    private List<String> mVideoSizeNames;
    private int mCurrentVideoSizeId;
    private String mCurrentVideoSizeName;

    private boolean isRecording = false;
    private boolean isPlayingBack = false;
    private Button captureButton;
    private ImageButton mPassButton;
    private ImageButton mFailButton;

    private TextView mStatusLabel;

    private TreeSet<CameraCombination> mTestedCombinations = new TreeSet<>(COMPARATOR);
    private TreeSet<CameraCombination> mUntestedCombinations = new TreeSet<>(COMPARATOR);
    private TreeSet<String> mUntestedCameras = new TreeSet<>();

    private File outputVideoFile;

    private class CameraCombination {
        private final int mCameraIndex;
        private final int mVideoSizeIdIndex;
        private final String mVideoSizeName;

        private CameraCombination(
            int cameraIndex, int videoSizeIdIndex, String videoSizeName) {
            this.mCameraIndex = cameraIndex;
            this.mVideoSizeIdIndex = videoSizeIdIndex;
            this.mVideoSizeName = videoSizeName;
        }

        @Override
        public String toString() {
            return String.format(""Camera %d, %s"", mCameraIndex, mVideoSizeName);
        }
    }

    private static final Comparator<CameraCombination> COMPARATOR =
        Comparator.<CameraCombination, Integer>comparing(c -> c.mCameraIndex)
            .thenComparing(c -> c.mVideoSizeIdIndex);

    /**
     * @see #MEDIA_TYPE_IMAGE
     * @see #MEDIA_TYPE_VIDEO
     */
    private File getOutputMediaFile(int type) {
        File mediaStorageDir = new File(getExternalFilesDir(null), TAG);
        if (mediaStorageDir == null) {
            Log.e(TAG, ""failed to retrieve external files directory"");
            return null;
        }

        if (!mediaStorageDir.exists()) {
            if (!mediaStorageDir.mkdirs()) {
                Log.d(TAG, ""failed to create directory"");
                return null;
            }
        }

        String timeStamp = new SimpleDateFormat(""yyyyMMdd_HHmmss"").format(new Date());
        File mediaFile;
        if (type == MEDIA_TYPE_IMAGE) {
            mediaFile = new File(mediaStorageDir.getPath() + File.separator +
                    ""IMG_"" + timeStamp + "".jpg"");
        } else if (type == MEDIA_TYPE_VIDEO) {
            mediaFile = new File(mediaStorageDir.getPath() + File.separator +
                    ""VID_"" + timeStamp + "".mp4"");
            if (VERBOSE) {
                Log.v(TAG, ""getOutputMediaFile: output file "" + mediaFile.getPath());
            }
        } else {
            return null;
        }

        return mediaFile;
    }

    private static final int BIT_RATE_720P = 8000000;
    private static final int BIT_RATE_MIN = 64000;
    private static final int BIT_RATE_MAX = BIT_RATE_720P;

    private int getVideoBitRate(Camera.Size sz) {
        int rate = BIT_RATE_720P;
        float scaleFactor = sz.height * sz.width / (float)(1280 * 720);
        rate = (int)(rate * scaleFactor);

        // Clamp to the MIN, MAX range.
        return Math.max(BIT_RATE_MIN, Math.min(BIT_RATE_MAX, rate));
    }

    private int getVideoFrameRate() {
        return mVideoFrameRate;
    }

    private void setVideoFrameRate(int videoFrameRate) {
        mVideoFrameRate = videoFrameRate;
    }

    private boolean prepareVideoRecorder() {

        mMediaRecorder = new MediaRecorder();

        // Step 1: unlock and set camera to MediaRecorder
        mCamera.unlock();
        mMediaRecorder.setCamera(mCamera);

        // Step 2: set sources
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);
        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);

        // Step 3: set a CamcorderProfile
        if (mIsExternalCamera) {
            Camera.Size recordSize = null;
            switch (mCurrentVideoSizeId) {
                case CamcorderProfile.QUALITY_QCIF:
                    recordSize = mCamera.new Size(176, 144);
                break;
                case CamcorderProfile.QUALITY_QVGA:
                    recordSize = mCamera.new Size(320, 240);
                break;
                case CamcorderProfile.QUALITY_CIF:
                    recordSize = mCamera.new Size(352, 288);
                break;
                case CamcorderProfile.QUALITY_480P:
                    recordSize = mCamera.new Size(720, 480);
                break;
                case CamcorderProfile.QUALITY_720P:
                    recordSize = mCamera.new Size(1280, 720);
                break;
                default:
                    String msg = ""Unknown CamcorderProfile: "" + mCurrentVideoSizeId;
                    Log.e(TAG, msg);
                    releaseMediaRecorder();
                    throw new AssertionError(msg);
            }

            mMediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.DEFAULT);
            mMediaRecorder.setVideoEncoder(MediaRecorder.VideoEncoder.DEFAULT);
            mMediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.DEFAULT);
            mMediaRecorder.setVideoEncodingBitRate(getVideoBitRate(recordSize));
            mMediaRecorder.setVideoSize(recordSize.width, recordSize.height);
            mMediaRecorder.setVideoFrameRate(getVideoFrameRate());
        } else {
            mMediaRecorder.setProfile(CamcorderProfile.get(mCurrentCameraId, mCurrentVideoSizeId));
        }

        // Step 4: set output file
        outputVideoFile = getOutputMediaFile(MEDIA_TYPE_VIDEO);
        mMediaRecorder.setOutputFile(outputVideoFile.toString());

        // Step 5: set preview output
        // This is not necessary since preview has been taken care of

        // Step 6: set orientation hint
        mMediaRecorder.setOrientationHint(mVideoRotation);

        // Step 7: prepare configured MediaRecorder
        try {
            mMediaRecorder.prepare();
        } catch (IOException e) {
            Log.e(TAG, ""IOException preparing MediaRecorder: "", e);
            releaseMediaRecorder();
            throw new AssertionError(e);
        }

        mMediaRecorder.setOnErrorListener(
                new MediaRecorder.OnErrorListener() {
                    @Override
                    public void onError(MediaRecorder mr, int what, int extra) {
                        if (what == MediaRecorder.MEDIA_RECORDER_ERROR_UNKNOWN) {
                            Log.e(TAG, ""unknown error in media recorder, error: "" + extra);
                        } else {
                            Log.e(TAG, ""media recorder server died, error: "" + extra);
                        }

                        failTest(""Media recorder error."");
                    }
                });

        if (VERBOSE) {
            Log.v(TAG, ""prepareVideoRecorder: prepared configured MediaRecorder"");
        }

        return true;
    }

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        setContentView(R.layout.camera_video);
        setPassFailButtonClickListeners();
        setInfoResources(R.string.camera_video, R.string.video_info, /*viewId*/-1);

        mPreviewView = (TextureView) findViewById(R.id.video_capture);
        mPlaybackView = (VideoView) findViewById(R.id.video_playback);
        mPlaybackView.setOnCompletionListener(mPlaybackViewListener);

        captureButton = (Button) findViewById(R.id.record_button);
        mPassButton = (ImageButton) findViewById(R.id.pass_button);
        mFailButton = (ImageButton) findViewById(R.id.fail_button);
        mPassButton.setEnabled(false);
        mFailButton.setEnabled(true);

        mPreviewView.setSurfaceTextureListener(this);

        int numCameras = Camera.getNumberOfCameras();
        String[] cameraNames = new String[numCameras];
        for (int i = 0; i < numCameras; i++) {
            cameraNames[i] = ""Camera "" + i;
            mUntestedCameras.add(""All combinations for Camera "" + i + ""\n"");
        }
        if (VERBOSE) {
            Log.v(TAG, ""onCreate: number of cameras="" + numCameras);
        }
        mCameraSpinner = (Spinner) findViewById(R.id.cameras_selection);
        mCameraSpinner.setAdapter(
            new ArrayAdapter<String>(
                this, R.layout.camera_list_item, cameraNames));
        mCameraSpinner.setOnItemSelectedListener(mCameraSpinnerListener);

        mResolutionSpinner = (Spinner) findViewById(R.id.resolution_selection);
        mResolutionSpinner.setOnItemSelectedListener(mResolutionSelectedListener);

        mStatusLabel = (TextView) findViewById(R.id.status_label);

        Button mNextButton = (Button) findViewById(R.id.next_button);
        mNextButton.setOnClickListener(v -> {
            setUntestedCombination();
            if (VERBOSE) {
                Log.v(TAG, ""onClick: mCurrentVideoSizeId = "" +
                    mCurrentVideoSizeId + "" "" + mCurrentVideoSizeName);
                Log.v(TAG, ""onClick: setting preview size ""
                    + mNextPreviewSize.width + ""x"" + mNextPreviewSize.height);
            }

            startPreview();
            if (VERBOSE) {
                Log.v(TAG, ""onClick: started new preview"");
            }
            captureButton.performClick();
        });
    }

    /**
     * Set an untested combination of the current camera and video size.
     * Triggered by next button click.
     */
    private void setUntestedCombination() {
        Optional<CameraCombination> combination = mUntestedCombinations.stream().filter(
            c -> c.mCameraIndex == mCurrentCameraId).findFirst();
        if (!combination.isPresent()) {
            Toast.makeText(this, ""All Camera "" + mCurrentCameraId + "" tests are done."",
                Toast.LENGTH_SHORT).show();
            return;
        }

        // There is untested combination for the current camera, set the next untested combination.
        int mNextVideoSizeIdIndex = combination.get().mVideoSizeIdIndex;

        mCurrentVideoSizeId = mVideoSizeIds.get(mNextVideoSizeIdIndex);
        mCurrentVideoSizeName = mVideoSizeNames.get(mNextVideoSizeIdIndex);
        mNextPreviewSize = matchPreviewRecordSize();
        mResolutionSpinner.setSelection(mNextVideoSizeIdIndex);
    }

    @Override
    public void onResume() {
        super.onResume();

        setUpCamera(mCameraSpinner.getSelectedItemPosition());
        if (VERBOSE) {
            Log.v(TAG, ""onResume: camera has been setup"");
        }

        setUpCaptureButton();
        if (VERBOSE) {
            Log.v(TAG, ""onResume: captureButton has been setup"");
        }

    }

    @Override
    public void onPause() {
        super.onPause();

        releaseMediaRecorder();
        shutdownCamera();
        mPreviewTexture = null;
    }

    private MediaPlayer.OnCompletionListener mPlaybackViewListener =
            new MediaPlayer.OnCompletionListener() {

                @Override
                public void onCompletion(MediaPlayer mp) {
                    isPlayingBack = false;
                    mPlaybackView.stopPlayback();
                    captureButton.setEnabled(true);

                    mStatusLabel.setMovementMethod(new ScrollingMovementMethod());
                    StringBuilder progress = new StringBuilder();
                    progress.append(getResources().getString(R.string.status_ready));
                    progress.append(""\n---- Progress ----\n"");
                    progress.append(getTestDetails());
                    mStatusLabel.setText(progress.toString());
                }

    };

    private void releaseMediaRecorder() {
        if (mMediaRecorder != null) {
            mMediaRecorder.reset();
            mMediaRecorder.release();
            mMediaRecorder = null;
            mCamera.lock(); // check here, lock camera for later use
        }
    }

    @Override
    public String getTestDetails() {
        StringBuilder reportBuilder = new StringBuilder();
        reportBuilder.append(""Tested combinations:\n"");
        for (CameraCombination combination: mTestedCombinations) {
            reportBuilder.append(combination);
            reportBuilder.append(""\n"");
        }
        reportBuilder.append(""Untested combinations:\n"");
        for (String untestedCam : mUntestedCameras) {
            reportBuilder.append(untestedCam);
        }
        for (CameraCombination combination: mUntestedCombinations) {
            reportBuilder.append(combination);
            reportBuilder.append(""\n"");
        }
        return reportBuilder.toString();
    }

    @Override
    public void onSurfaceTextureAvailable(SurfaceTexture surface,
            int width, int height) {
        mPreviewTexture = surface;
        mPreviewTexWidth = width;
        mPreviewTexHeight = height;
        if (mCamera != null) {
            startPreview();
        }
    }

    @Override
    public void onSurfaceTextureSizeChanged(SurfaceTexture surface, int width, int height) {
        // Ignored, Camera does all the work for us
    }

    @Override
    public boolean onSurfaceTextureDestroyed(SurfaceTexture surface) {
        return true;
    }


    @Override
    public void onSurfaceTextureUpdated(SurfaceTexture surface) {
        // Invoked every time there's a new Camera preview frame
    }

    private AdapterView.OnItemSelectedListener mCameraSpinnerListener =
            new AdapterView.OnItemSelectedListener() {
                @Override
                public void onItemSelected(AdapterView<?> parent,
                        View view, int pos, long id) {
                    if (mCurrentCameraId != pos) {
                        setUpCamera(pos);
                    }
                }

                @Override
                public void onNothingSelected(AdapterView<?> parent) {
                    // Intentionally left blank
                }

            };

    private AdapterView.OnItemSelectedListener mResolutionSelectedListener =
            new AdapterView.OnItemSelectedListener() {
                @Override
                public void onItemSelected(AdapterView<?> parent,
                        View view, int position, long id) {
                    if (mVideoSizeIds.get(position) != mCurrentVideoSizeId) {
                        mCurrentVideoSizeId = mVideoSizeIds.get(position);
                        mCurrentVideoSizeName = mVideoSizeNames.get(position);
                        if (VERBOSE) {
                            Log.v(TAG, ""onItemSelected: mCurrentVideoSizeId = "" +
                                    mCurrentVideoSizeId + "" "" + mCurrentVideoSizeName);
                        }
                        mNextPreviewSize = matchPreviewRecordSize();
                        if (VERBOSE) {
                            Log.v(TAG, ""onItemSelected: setting preview size ""
                                    + mNextPreviewSize.width + ""x"" + mNextPreviewSize.height);
                        }

                        startPreview();
                        if (VERBOSE) {
                            Log.v(TAG, ""onItemSelected: started new preview"");
                        }
                    }
                }

                @Override
                public void onNothingSelected(AdapterView<?> parent) {
                    // Intentionally left blank
                }

            };


    private void setUpCaptureButton() {
        captureButton.setOnClickListener (
                new View.OnClickListener() {
                    @Override
                    public void onClick(View V) {
                        if ((!isRecording) && (!isPlayingBack)) {
                            if (prepareVideoRecorder()) {
                                mMediaRecorder.start();
                                if (VERBOSE) {
                                    Log.v(TAG, ""onClick: started mMediaRecorder"");
                                }
                                isRecording = true;
                                captureButton.setEnabled(false);
                                mStatusLabel.setText(getResources()
                                        .getString(R.string.status_recording));
                            } else {
                                releaseMediaRecorder();
                                Log.e(TAG, ""media recorder cannot be set up"");
                                failTest(""Unable to set up media recorder."");
                            }
                            Handler h = new Handler();
                            Runnable mDelayedPreview = new Runnable() {
                                @Override
                                public void run() {
                                    mMediaRecorder.stop();
                                    releaseMediaRecorder();

                                    mPlaybackView.setVideoPath(outputVideoFile.getPath());
                                    mPlaybackView.start();
                                    isRecording = false;
                                    isPlayingBack = true;
                                    mStatusLabel.setText(getResources()
                                            .getString(R.string.status_playback));

                                    int resIdx = mResolutionSpinner.getSelectedItemPosition();
                                    CameraCombination combination = new CameraCombination(
                                            mCurrentCameraId, resIdx,
                                            mVideoSizeNames.get(resIdx));

                                    mUntestedCombinations.remove(combination);
                                    mTestedCombinations.add(combination);

                                    if (mUntestedCombinations.isEmpty() &&
                                            mUntestedCameras.isEmpty()) {
                                        mPassButton.setEnabled(true);
                                        if (VERBOSE) {
                                            Log.v(TAG, ""run: test success"");
                                        }
                                    }
                                }
                            };
                            h.postDelayed(mDelayedPreview, VIDEO_LENGTH);
                        }

                    }
                }
        );
    }

    private class VideoSizeNamePair {
        private int sizeId;
        private String sizeName;

        public VideoSizeNamePair(int id, String name) {
            sizeId = id;
            sizeName = name;
        }

        public int getSizeId() {
            return sizeId;
        }

        public String getSizeName() {
            return sizeName;
        }
    }

    private ArrayList<VideoSizeNamePair> getVideoSizeNamePairs(int cameraId) {
        int[] qualityArray = {
                CamcorderProfile.QUALITY_LOW,
                CamcorderProfile.QUALITY_HIGH,
                CamcorderProfile.QUALITY_QCIF,  // 176x144
                CamcorderProfile.QUALITY_QVGA,  // 320x240
                CamcorderProfile.QUALITY_CIF,   // 352x288
                CamcorderProfile.QUALITY_480P,  // 720x480
                CamcorderProfile.QUALITY_720P,  // 1280x720
                CamcorderProfile.QUALITY_1080P, // 1920x1080 or 1920x1088
                CamcorderProfile.QUALITY_2160P
        };

        final Camera.Size skip = mCamera.new Size(-1, -1);
        Camera.Size[] videoSizeArray = {
                skip,
                skip,
                mCamera.new Size(176, 144),
                mCamera.new Size(320, 240),
                mCamera.new Size(352, 288),
                mCamera.new Size(720, 480),
                mCamera.new Size(1280, 720),
                skip,
                skip
        };

        String[] nameArray = {
                ""LOW"",
                ""HIGH"",
                ""QCIF"",
                ""QVGA"",
                ""CIF"",
                ""480P"",
                ""720P"",
                ""1080P"",
                ""2160P""
        };

        ArrayList<VideoSizeNamePair> availableSizes =
                new ArrayList<VideoSizeNamePair> ();

        Camera.Parameters p = mCamera.getParameters();
        List<Camera.Size> supportedVideoSizes = p.getSupportedVideoSizes();
        for (int i = 0; i < qualityArray.length; i++) {
            if (mIsExternalCamera) {
                Camera.Size videoSz = videoSizeArray[i];
                if (videoSz.equals(skip)) {
                    continue;
                }
                if (supportedVideoSizes.contains(videoSz)) {
                    VideoSizeNamePair pair = new VideoSizeNamePair(qualityArray[i], nameArray[i]);
                    availableSizes.add(pair);
                }
            } else {
                if (CamcorderProfile.hasProfile(cameraId, qualityArray[i])) {
                    VideoSizeNamePair pair = new VideoSizeNamePair(qualityArray[i], nameArray[i]);
                    availableSizes.add(pair);
                }
            }
        }
        return availableSizes;
    }

    static class ResolutionQuality {
        private int videoSizeId;
        private int width;
        private int height;

        public ResolutionQuality() {
            // intentionally left blank
        }
        public ResolutionQuality(int newSizeId, int newWidth, int newHeight) {
            videoSizeId = newSizeId;
            width = newWidth;
            height = newHeight;
        }
    }

    private Size findRecordSize(int cameraId) {
        int[] possibleQuality = {
                CamcorderProfile.QUALITY_LOW,
                CamcorderProfile.QUALITY_HIGH,
                CamcorderProfile.QUALITY_QCIF,
                CamcorderProfile.QUALITY_QVGA,
                CamcorderProfile.QUALITY_CIF,
                CamcorderProfile.QUALITY_480P,
                CamcorderProfile.QUALITY_720P,
                CamcorderProfile.QUALITY_1080P,
                CamcorderProfile.QUALITY_2160P
        };

        final Camera.Size skip = mCamera.new Size(-1, -1);
        Camera.Size[] videoSizeArray = {
                skip,
                skip,
                mCamera.new Size(176, 144),
                mCamera.new Size(320, 240),
                mCamera.new Size(352, 288),
                mCamera.new Size(720, 480),
                mCamera.new Size(1280, 720),
                skip,
                skip
        };

        ArrayList<ResolutionQuality> qualityList = new ArrayList<ResolutionQuality>();
        Camera.Parameters p = mCamera.getParameters();
        List<Camera.Size> supportedVideoSizes = p.getSupportedVideoSizes();
        for (int i = 0; i < possibleQuality.length; i++) {
            if (mIsExternalCamera) {
                Camera.Size videoSz = videoSizeArray[i];
                if (videoSz.equals(skip)) {
                    continue;
                }
                if (supportedVideoSizes.contains(videoSz)) {
                    qualityList.add(new ResolutionQuality(possibleQuality[i],
                            videoSz.width, videoSz.height));
                }
            } else {
                if (CamcorderProfile.hasProfile(cameraId, possibleQuality[i])) {
                    CamcorderProfile profile = CamcorderProfile.get(cameraId, possibleQuality[i]);
                    qualityList.add(new ResolutionQuality(possibleQuality[i],
                            profile.videoFrameWidth, profile.videoFrameHeight));
                }
            }
        }

        Size recordSize = null;
        for (int i = 0; i < qualityList.size(); i++) {
            if (mCurrentVideoSizeId == qualityList.get(i).videoSizeId) {
                recordSize = mCamera.new Size(qualityList.get(i).width,
                        qualityList.get(i).height);
                break;
            }
        }

        if (recordSize == null) {
            Log.e(TAG, ""findRecordSize: did not find a match"");
            failTest(""Cannot find video size"");
        }
        return recordSize;
    }

    // Match preview size with current recording size mCurrentVideoSizeId
    private Size matchPreviewRecordSize() {
        Size recordSize = findRecordSize(mCurrentCameraId);

        Size matchedSize = null;
        // First try to find exact match in size
        for (int i = 0; i < mPreviewSizes.size(); i++) {
            if (mPreviewSizes.get(i).equals(recordSize)) {
                matchedSize = mCamera.new Size(recordSize.width, recordSize.height);
                break;
            }
        }
        // Second try to find same ratio in size
        if (matchedSize == null) {
            for (int i = mPreviewSizes.size() - 1; i >= 0; i--) {
                if (mPreviewSizes.get(i).width * recordSize.height ==
                        mPreviewSizes.get(i).height * recordSize.width) {
                    matchedSize = mCamera.new Size(mPreviewSizes.get(i).width,
                            mPreviewSizes.get(i).height);
                    break;
                }
            }
        }
        //Third try to find one with similar if not the same apect ratio
        if (matchedSize == null) {
            for (int i = mPreviewSizes.size() - 1; i >= 0; i--) {
                if (Math.abs((float)mPreviewSizes.get(i).width * recordSize.height /
                        mPreviewSizes.get(i).height / recordSize.width - 1) < 0.12) {
                    matchedSize = mCamera.new Size(mPreviewSizes.get(i).width,
                            mPreviewSizes.get(i).height);
                    break;
                }
            }
        }
        // Last resort, just use the first preview size
        if (matchedSize == null) {
            matchedSize = mCamera.new Size(mPreviewSizes.get(0).width,
                    mPreviewSizes.get(0).height);
        }

        if (VERBOSE) {
            Log.v(TAG, ""matchPreviewRecordSize "" + matchedSize.width + ""x"" + matchedSize.height);
        }

        return matchedSize;
    }

    private void setUpCamera(int id) {
        shutdownCamera();

        mCurrentCameraId = id;
        mIsExternalCamera = isExternalCamera(id);
        try {
            mCamera = Camera.open(id);
        }
        catch (Exception e) {
            Log.e(TAG, ""camera is not available"", e);
            failTest(""camera not available"" + e.getMessage());
            return;
        }

        Camera.Parameters p = mCamera.getParameters();
        if (VERBOSE) {
            Log.v(TAG, ""setUpCamera: setUpCamera got camera parameters"");
        }

        // Get preview resolutions
        List<Size> unsortedSizes = p.getSupportedPreviewSizes();

        class SizeCompare implements Comparator<Size> {
            @Override
            public int compare(Size lhs, Size rhs) {
                if (lhs.width < rhs.width) return -1;
                if (lhs.width > rhs.width) return 1;
                if (lhs.height < rhs.height) return -1;
                if (lhs.height > rhs.height) return 1;
                return 0;
            }
        };

        if (mIsExternalCamera) {
            setVideoFrameRate(p.getPreviewFrameRate());
        }

        SizeCompare s = new SizeCompare();
        TreeSet<Size> sortedResolutions = new TreeSet<Size>(s);
        sortedResolutions.addAll(unsortedSizes);

        mPreviewSizes = new ArrayList<Size>(sortedResolutions);

        ArrayList<VideoSizeNamePair> availableVideoSizes = getVideoSizeNamePairs(id);
        String[] availableVideoSizeNames = new String[availableVideoSizes.size()];
        mVideoSizeIds = new ArrayList<Integer>();
        mVideoSizeNames = new ArrayList<String>();
        for (int i = 0; i < availableVideoSizes.size(); i++) {
            availableVideoSizeNames[i] = availableVideoSizes.get(i).getSizeName();
            mVideoSizeIds.add(availableVideoSizes.get(i).getSizeId());
            mVideoSizeNames.add(availableVideoSizeNames[i]);
        }

        mResolutionSpinner.setAdapter(
            new ArrayAdapter<String>(
                this, R.layout.camera_list_item, availableVideoSizeNames));

        // Update untested
        mUntestedCameras.remove(""All combinations for Camera "" + id + ""\n"");

        for (int videoSizeIdIndex = 0;
                videoSizeIdIndex < mVideoSizeIds.size(); videoSizeIdIndex++) {
            CameraCombination combination = new CameraCombination(
                id, videoSizeIdIndex, mVideoSizeNames.get(videoSizeIdIndex));

            if (!mTestedCombinations.contains(combination)) {
                mUntestedCombinations.add(combination);
            }
        }

        // Set initial values
        mCurrentVideoSizeId = mVideoSizeIds.get(0);
        mCurrentVideoSizeName = mVideoSizeNames.get(0);
        mNextPreviewSize = matchPreviewRecordSize();
        mResolutionSpinner.setSelection(0);

        // Set up correct display orientation
        CameraInfo info = new CameraInfo();
        Camera.getCameraInfo(id, info);
        int rotation = getWindowManager().getDefaultDisplay().getRotation();
        int degrees = 0;
        switch (rotation) {
            case Surface.ROTATION_0: degrees = 0; break;
            case Surface.ROTATION_90: degrees = 90; break;
            case Surface.ROTATION_180: degrees = 180; break;
            case Surface.ROTATION_270: degrees = 270; break;
        }

        if (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {
            mVideoRotation = (info.orientation + degrees) % 360;
            mPreviewRotation = (360 - mVideoRotation) % 360;  // compensate the mirror
        } else {  // back-facing
            mVideoRotation = (info.orientation - degrees + 360) % 360;
            mPreviewRotation = mVideoRotation;
        }
        if (mPreviewRotation != 0 && mPreviewRotation != 180) {
            Log.w(TAG,
                ""Display orientation correction is not 0 or 180, as expected!"");
        }

        mCamera.setDisplayOrientation(mPreviewRotation);

        // Start up preview if display is ready
        if (mPreviewTexture != null) {
            startPreview();
        }
    }

    private void shutdownCamera() {
        if (mCamera != null) {
            mCamera.setPreviewCallback(null);
            mCamera.stopPreview();
            mCamera.release();
            mCamera = null;
        }
    }

    /**
     * starts capturing and drawing frames on screen
     */
    private void startPreview() {

        mCamera.stopPreview();

        Matrix transform = new Matrix();
        float widthRatio = mNextPreviewSize.width / (float)mPreviewTexWidth;
        float heightRatio = mNextPreviewSize.height / (float)mPreviewTexHeight;
        if (VERBOSE) {
            Log.v(TAG, ""startPreview: widthRatio="" + widthRatio + "" "" + ""heightRatio="" +
                    heightRatio);
        }

        if (heightRatio < widthRatio) {
            transform.setScale(1, heightRatio / widthRatio);
            transform.postTranslate(0,
                    mPreviewTexHeight * (1 - heightRatio / widthRatio) / 2);
            if (VERBOSE) {
                Log.v(TAG, ""startPreview: shrink vertical by "" + heightRatio / widthRatio);
            }
        } else {
            transform.setScale(widthRatio / heightRatio, 1);
            transform.postTranslate(mPreviewTexWidth * (1 - widthRatio / heightRatio) / 2, 0);
            if (VERBOSE) {
                Log.v(TAG, ""startPreview: shrink horizontal by "" + widthRatio / heightRatio);
            }
        }

        mPreviewView.setTransform(transform);

        mPreviewSize = mNextPreviewSize;

        Camera.Parameters p = mCamera.getParameters();
        p.setPreviewSize(mPreviewSize.width, mPreviewSize.height);
        mCamera.setParameters(p);

        try {
            mCamera.setPreviewTexture(mPreviewTexture);
            if (mPreviewTexture == null) {
                Log.e(TAG, ""preview texture is null."");
            }
            if (VERBOSE) {
                Log.v(TAG, ""startPreview: set preview texture in startPreview"");
            }
            mCamera.startPreview();
            if (VERBOSE) {
                Log.v(TAG, ""startPreview: started preview in startPreview"");
            }
        } catch (IOException ioe) {
            Log.e(TAG, ""Unable to start up preview"", ioe);
            // Show a dialog box to tell user test failed
            failTest(""Unable to start preview."");
        }
    }

    private void failTest(String failMessage) {
        DialogInterface.OnClickListener dialogClickListener =
                new DialogInterface.OnClickListener() {
                    @Override
                    public void onClick(DialogInterface dialog, int which) {
                        switch (which) {
                            case DialogInterface.BUTTON_POSITIVE:
                                setTestResultAndFinish(/* passed */false);
                                break;
                            case DialogInterface.BUTTON_NEGATIVE:
                                break;
                        }
                    }
                };

        AlertDialog.Builder builder = new AlertDialog.Builder(CameraVideoActivity.this);
        builder.setMessage(getString(R.string.dialog_fail_test) + "". "" + failMessage)
                .setPositiveButton(R.string.fail_quit, dialogClickListener)
                .setNegativeButton(R.string.cancel, dialogClickListener)
                .show();
    }

    private boolean isExternalCamera(int cameraId) {
        try {
            return CameraUtils.isExternal(this, cameraId);
        } catch (Exception e) {
            Toast.makeText(this, ""Could not access camera "" + cameraId +
                    "": "" + e.getMessage(), Toast.LENGTH_LONG).show();
        }
        return false;
    }

}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.ImageReaderDecoderTest"	"testHwAVCDecode360pForFlexibleYuv"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/ImageReaderDecoderTest.java"	""	"public void testHwAVCDecode360pForFlexibleYuv() throws Exception {
        Decoder[] decoders = other(new MediaAssets(
                MediaFormat.MIMETYPE_VIDEO_AVC,
                new MediaAsset(
                        ""video_480x360_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                        480 /* width */, 360 /* height */)));

        decodeTest(decoders, MODE_IMAGEREADER, false /* checkSwirl */);
    }

    /**
     * Test ImageReader with 480x360 google (SW) AVC decoding for flexible yuv format
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.ImageReaderDecoderTest"	"testSwAVCDecode360pForFlexibleYuv"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/ImageReaderDecoderTest.java"	""	"public void testSwAVCDecode360pForFlexibleYuv() throws Exception {
        Decoder[] decoders = goog(new MediaAssets(
                MediaFormat.MIMETYPE_VIDEO_AVC,
                new MediaAsset(
                        ""video_480x360_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                        480 /* width */, 360 /* height */)));

        decodeTest(decoders, MODE_IMAGEREADER, false /* checkSwirl */);
    }

    private void swirlTest(Decoder[] decoders, int mode) {
        decodeTest(decoders, mode, true /* checkSwirl */);
    }

    private void decodeTest(Decoder[] decoders, int mode, boolean checkSwirl) {
        try {
            boolean skipped = true;
            for (Decoder codec : decoders) {
                if (codec.videoDecode(mode, checkSwirl)) {
                    skipped = false;
                }
            }
            if (skipped) {
                MediaUtils.skipTest(""decoder does not any of the input files"");
            }
        } finally {
            closeImageReader();
        }
    }

    private static class ImageListener implements ImageReader.OnImageAvailableListener {
        private final LinkedBlockingQueue<Image> mQueue =
                new LinkedBlockingQueue<Image>();

        @Override
        public void onImageAvailable(ImageReader reader) {
            try {
                mQueue.put(reader.acquireNextImage());
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onImageAvailable"");
            }
        }

        /**
         * Get an image from the image reader.
         *
         * @param timeout Timeout value for the wait.
         * @return The image from the image reader.
         */
        public Image getImage(long timeout) throws InterruptedException {
            Image image = mQueue.poll(timeout, TimeUnit.MILLISECONDS);
            assertNotNull(""Wait for an image timed out in "" + timeout + ""ms"", image);
            return image;
        }
    }

    /**
     * Decode video frames to image reader.
     */
    private void decodeFramesToImage(
            MediaCodec decoder, MediaExtractor extractor, MediaFormat mediaFormat,
            int width, int height, int imageFormat, int mode, boolean checkSwirl)
            throws InterruptedException {
        ByteBuffer[] decoderInputBuffers;
        ByteBuffer[] decoderOutputBuffers;

        // Configure decoder.
        if (VERBOSE) Log.v(TAG, ""stream format: "" + mediaFormat);
        if (mode == MODE_IMAGEREADER) {
            createImageReader(width, height, imageFormat, MAX_NUM_IMAGES, mImageListener);
            decoder.configure(mediaFormat, mReaderSurface, null /* crypto */, 0 /* flags */);
        } else {
            assertEquals(mode, MODE_IMAGE);
            decoder.configure(mediaFormat, null /* surface */, null /* crypto */, 0 /* flags */);
        }

        decoder.start();
        decoderInputBuffers = decoder.getInputBuffers();
        decoderOutputBuffers = decoder.getOutputBuffers();
        extractor.selectTrack(0);

        // Start decoding and get Image, only test the first NUM_FRAME_DECODED frames.
        MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
        boolean sawInputEOS = false;
        boolean sawOutputEOS = false;
        int outputFrameCount = 0;
        while (!sawOutputEOS && outputFrameCount < NUM_FRAME_DECODED) {
            if (VERBOSE) Log.v(TAG, ""loop:"" + outputFrameCount);
            // Feed input frame.
            if (!sawInputEOS) {
                int inputBufIndex = decoder.dequeueInputBuffer(DEFAULT_TIMEOUT_US);
                if (inputBufIndex >= 0) {
                    ByteBuffer dstBuf = decoderInputBuffers[inputBufIndex];
                    int sampleSize =
                        extractor.readSampleData(dstBuf, 0 /* offset */);

                    if (VERBOSE) Log.v(TAG, ""queue a input buffer, idx/size: ""
                        + inputBufIndex + ""/"" + sampleSize);

                    long presentationTimeUs = 0;

                    if (sampleSize < 0) {
                        if (VERBOSE) Log.v(TAG, ""saw input EOS."");
                        sawInputEOS = true;
                        sampleSize = 0;
                    } else {
                        presentationTimeUs = extractor.getSampleTime();
                    }

                    decoder.queueInputBuffer(
                            inputBufIndex,
                            0 /* offset */,
                            sampleSize,
                            presentationTimeUs,
                            sawInputEOS ? MediaCodec.BUFFER_FLAG_END_OF_STREAM : 0);

                    if (!sawInputEOS) {
                        extractor.advance();
                    }
                }
            }

            // Get output frame
            int res = decoder.dequeueOutputBuffer(info, DEFAULT_TIMEOUT_US);
            if (VERBOSE) Log.v(TAG, ""got a buffer: "" + info.size + ""/"" + res);
            if (res == MediaCodec.INFO_TRY_AGAIN_LATER) {
                // no output available yet
                if (VERBOSE) Log.v(TAG, ""no output frame available"");
            } else if (res == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                // decoder output buffers changed, need update.
                if (VERBOSE) Log.v(TAG, ""decoder output buffers changed"");
                decoderOutputBuffers = decoder.getOutputBuffers();
            } else if (res == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                // this happens before the first frame is returned.
                MediaFormat outFormat = decoder.getOutputFormat();
                if (VERBOSE) Log.v(TAG, ""decoder output format changed: "" + outFormat);
            } else if (res < 0) {
                // Should be decoding error.
                fail(""unexpected result from deocder.dequeueOutputBuffer: "" + res);
            } else {
                if ((info.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                    sawOutputEOS = true;
                }

                // res >= 0: normal decoding case, copy the output buffer.
                // Will use it as reference to valid the ImageReader output
                // Some decoders output a 0-sized buffer at the end. Ignore those.
                boolean doRender = (info.size != 0);

                if (doRender) {
                    outputFrameCount++;
                    String fileName = DEBUG_FILE_NAME_BASE + MediaUtils.getTestName()
                            + (mode == MODE_IMAGE ? ""_image_"" : ""_reader_"")
                            + width + ""x"" + height + ""_"" + outputFrameCount + "".yuv"";

                    Image image = null;
                    try {
                        if (mode == MODE_IMAGE) {
                            image = decoder.getOutputImage(res);
                        } else {
                            decoder.releaseOutputBuffer(res, doRender);
                            res = -1;
                            // Read image and verify
                            image = mImageListener.getImage(WAIT_FOR_IMAGE_TIMEOUT_MS);
                        }
                        validateImage(image, width, height, imageFormat, fileName);

                        if (checkSwirl) {
                            try {
                                validateSwirl(image);
                            } catch (Throwable e) {
                                dumpFile(fileName, getDataFromImage(image));
                                throw e;
                            }
                        }
                    } finally {
                        if (image != null) {
                            image.close();
                        }
                    }
                }

                if (res >= 0) {
                    decoder.releaseOutputBuffer(res, false /* render */);
                }
            }
        }
    }

    /**
     * Validate image based on format and size.
     *
     * @param image The image to be validated.
     * @param width The image width.
     * @param height The image height.
     * @param format The image format.
     * @param filePath The debug dump file path, null if don't want to dump to file.
     */
    public static void validateImage(
            Image image, int width, int height, int format, String filePath) {
        if (VERBOSE) {
            Plane[] imagePlanes = image.getPlanes();
            Log.v(TAG, ""Image "" + filePath + "" Info:"");
            Log.v(TAG, ""first plane pixelstride "" + imagePlanes[0].getPixelStride());
            Log.v(TAG, ""first plane rowstride "" + imagePlanes[0].getRowStride());
            Log.v(TAG, ""Image timestamp:"" + image.getTimestamp());
        }

        assertNotNull(""Input image is invalid"", image);
        assertEquals(""Format doesn't match"", format, image.getFormat());
        assertEquals(""Width doesn't match"", width, image.getCropRect().width());
        assertEquals(""Height doesn't match"", height, image.getCropRect().height());

        if(VERBOSE) Log.v(TAG, ""validating Image"");
        byte[] data = getDataFromImage(image);
        assertTrue(""Invalid image data"", data != null && data.length > 0);

        validateYuvData(data, width, height, format, image.getTimestamp());

        if (VERBOSE && filePath != null) {
            dumpFile(filePath, data);
        }
    }

    private static void validateSwirl(Image image) {
        Rect crop = image.getCropRect();
        final int NUM_SIDES = 4;
        final int step = 8;      // the width of the layers
        long[][] rawStats = new long[NUM_SIDES][10];
        int[][] colors = new int[][] {
            { 111, 96, 204 }, { 178, 27, 174 }, { 100, 192, 92 }, { 106, 117, 62 }
        };

        // successively accumulate statistics for each layer of the swirl
        // by using overlapping rectangles, and the observation that
        // layer_i = rectangle_i - rectangle_(i+1)
        int lastLayer = 0;
        int layer = 0;
        boolean lastLayerValid = false;
        for (int pos = 0; ; pos += step) {
            Rect area = new Rect(pos - step, pos, crop.width() / 2, crop.height() + 2 * step - pos);
            if (area.isEmpty()) {
                break;
            }
            area.offset(crop.left, crop.top);
            area.intersect(crop);
            for (int lr = 0; lr < 2; ++lr) {
                long[] oneStat = CodecUtils.getRawStats(image, area);
                if (VERBOSE) Log.v(TAG, ""area="" + area + "", layer="" + layer + "", last=""
                                    + lastLayer + "": "" + Arrays.toString(oneStat));
                for (int i = 0; i < oneStat.length; i++) {
                    rawStats[layer][i] += oneStat[i];
                    if (lastLayerValid) {
                        rawStats[lastLayer][i] -= oneStat[i];
                    }
                }
                if (VERBOSE && lastLayerValid) {
                    Log.v(TAG, ""layer-"" + lastLayer + "": "" + Arrays.toString(rawStats[lastLayer]));
                    Log.v(TAG, Arrays.toString(CodecUtils.Raw2YUVStats(rawStats[lastLayer])));
                }
                // switch to the opposite side
                layer ^= 2;      // NUM_SIDES / 2
                lastLayer ^= 2;  // NUM_SIDES / 2
                area.offset(crop.centerX() - area.left, 2 * (crop.centerY() - area.centerY()));
            }

            lastLayer = layer;
            lastLayerValid = true;
            layer = (layer + 1) % NUM_SIDES;
        }

        for (layer = 0; layer < NUM_SIDES; ++layer) {
            float[] stats = CodecUtils.Raw2YUVStats(rawStats[layer]);
            if (DEBUG) Log.d(TAG, ""layer-"" + layer + "": "" + Arrays.toString(stats));
            if (VERBOSE) Log.v(TAG, Arrays.toString(rawStats[layer]));

            // check layer uniformity
            for (int i = 0; i < 3; i++) {
                assertTrue(""color of layer-"" + layer + "" is not uniform: ""
                        + Arrays.toString(stats),
                        stats[3 + i] < COLOR_STDEV_ALLOWANCE);
            }

            // check layer color
            for (int i = 0; i < 3; i++) {
                assertTrue(""color of layer-"" + layer + "" mismatches target ""
                        + Arrays.toString(colors[layer]) + "" vs ""
                        + Arrays.toString(Arrays.copyOf(stats, 3)),
                        Math.abs(stats[i] - colors[layer][i]) < COLOR_DELTA_ALLOWANCE);
            }
        }
    }

    private static void validateYuvData(byte[] yuvData, int width, int height, int format,
            long ts) {

        assertTrue(""YUV format must be one of the YUV_420_888, NV21, or YV12"",
                format == ImageFormat.YUV_420_888 ||
                format == ImageFormat.NV21 ||
                format == ImageFormat.YV12);

        if (VERBOSE) Log.v(TAG, ""Validating YUV data"");
        int expectedSize = width * height * ImageFormat.getBitsPerPixel(format) / 8;
        assertEquals(""Yuv data doesn't match"", expectedSize, yuvData.length);
    }

    private static void checkYuvFormat(int format) {
        if ((format != ImageFormat.YUV_420_888) &&
                (format != ImageFormat.NV21) &&
                (format != ImageFormat.YV12)) {
            fail(""Wrong formats: "" + format);
        }
    }
    /**
     * <p>Check android image format validity for an image, only support below formats:</p>
     *
     * <p>Valid formats are YUV_420_888/NV21/YV12 for video decoder</p>
     */
    private static void checkAndroidImageFormat(Image image) {
        int format = image.getFormat();
        Plane[] planes = image.getPlanes();
        switch (format) {
            case ImageFormat.YUV_420_888:
            case ImageFormat.NV21:
            case ImageFormat.YV12:
                assertEquals(""YUV420 format Images should have 3 planes"", 3, planes.length);
                break;
            default:
                fail(""Unsupported Image Format: "" + format);
        }
    }

    /**
     * Get a byte array image data from an Image object.
     * <p>
     * Read data from all planes of an Image into a contiguous unpadded,
     * unpacked 1-D linear byte array, such that it can be write into disk, or
     * accessed by software conveniently. It supports YUV_420_888/NV21/YV12
     * input Image format.
     * </p>
     * <p>
     * For YUV_420_888/NV21/YV12/Y8/Y16, it returns a byte array that contains
     * the Y plane data first, followed by U(Cb), V(Cr) planes if there is any
     * (xstride = width, ystride = height for chroma and luma components).
     * </p>
     */
    private static byte[] getDataFromImage(Image image) {
        assertNotNull(""Invalid image:"", image);
        Rect crop = image.getCropRect();
        int format = image.getFormat();
        int width = crop.width();
        int height = crop.height();
        int rowStride, pixelStride;
        byte[] data = null;

        // Read image data
        Plane[] planes = image.getPlanes();
        assertTrue(""Fail to get image planes"", planes != null && planes.length > 0);

        // Check image validity
        checkAndroidImageFormat(image);

        ByteBuffer buffer = null;

        int offset = 0;
        data = new byte[width * height * ImageFormat.getBitsPerPixel(format) / 8];
        byte[] rowData = new byte[planes[0].getRowStride()];
        if(VERBOSE) Log.v(TAG, ""get data from "" + planes.length + "" planes"");
        for (int i = 0; i < planes.length; i++) {
            int shift = (i == 0) ? 0 : 1;
            buffer = planes[i].getBuffer();
            assertNotNull(""Fail to get bytebuffer from plane"", buffer);
            rowStride = planes[i].getRowStride();
            pixelStride = planes[i].getPixelStride();
            assertTrue(""pixel stride "" + pixelStride + "" is invalid"", pixelStride > 0);
            if (VERBOSE) {
                Log.v(TAG, ""pixelStride "" + pixelStride);
                Log.v(TAG, ""rowStride "" + rowStride);
                Log.v(TAG, ""width "" + width);
                Log.v(TAG, ""height "" + height);
            }
            // For multi-planar yuv images, assuming yuv420 with 2x2 chroma subsampling.
            int w = crop.width() >> shift;
            int h = crop.height() >> shift;
            buffer.position(rowStride * (crop.top >> shift) + pixelStride * (crop.left >> shift));
            assertTrue(""rowStride "" + rowStride + "" should be >= width "" + w , rowStride >= w);
            for (int row = 0; row < h; row++) {
                int bytesPerPixel = ImageFormat.getBitsPerPixel(format) / 8;
                int length;
                if (pixelStride == bytesPerPixel) {
                    // Special case: optimized read of the entire row
                    length = w * bytesPerPixel;
                    buffer.get(data, offset, length);
                    offset += length;
                } else {
                    // Generic case: should work for any pixelStride but slower.
                    // Use intermediate buffer to avoid read byte-by-byte from
                    // DirectByteBuffer, which is very bad for performance
                    length = (w - 1) * pixelStride + bytesPerPixel;
                    buffer.get(rowData, 0, length);
                    for (int col = 0; col < w; col++) {
                        data[offset++] = rowData[col * pixelStride];
                    }
                }
                // Advance buffer the remainder of the row stride
                if (row < h - 1) {
                    buffer.position(buffer.position() + rowStride - length);
                }
            }
            if (VERBOSE) Log.v(TAG, ""Finished reading data from plane "" + i);
        }
        return data;
    }

    private static void dumpFile(String fileName, byte[] data) {
        assertNotNull(""fileName must not be null"", fileName);
        assertNotNull(""data must not be null"", data);

        FileOutputStream outStream;
        try {
            Log.v(TAG, ""output will be saved as "" + fileName);
            outStream = new FileOutputStream(fileName);
        } catch (IOException ioe) {
            throw new RuntimeException(""Unable to create debug output file "" + fileName, ioe);
        }

        try {
            outStream.write(data);
            outStream.close();
        } catch (IOException ioe) {
            throw new RuntimeException(""failed writing data to file "" + fileName, ioe);
        }
    }

    private void createImageReader(
            int width, int height, int format, int maxNumImages,
            ImageReader.OnImageAvailableListener listener)  {
        closeImageReader();

        mReader = ImageReader.newInstance(width, height, format, maxNumImages);
        mReaderSurface = mReader.getSurface();
        mReader.setOnImageAvailableListener(listener, mHandler);
        if (VERBOSE) {
            Log.v(TAG, String.format(""Created ImageReader size (%dx%d), format %d"", width, height,
                    format));
        }
    }

    /**
     * Close the pending images then close current active {@link ImageReader} object.
     */
    private void closeImageReader() {
        if (mReader != null) {
            try {
                // Close all possible pending images first.
                Image image = mReader.acquireLatestImage();
                if (image != null) {
                    image.close();
                }
            } finally {
                mReader.close();
                mReader = null;
            }
        }
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.helpers.CameraSessionUtils"	"getMockCaptureListener"	""	"/home/gpoor/cts-12-source/cts/tests/camera/utils/src/android/hardware/camera2/cts/helpers/CameraSessionUtils.java"	""	"public void test/*
 *.
 */
package android.hardware.camera2.cts.helpers;

import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CaptureFailure;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.TotalCaptureResult;
import android.hardware.camera2.cts.CameraTestUtils;
import android.os.Handler;
import android.util.Log;
import android.util.Pair;
import android.view.Surface;

import com.android.ex.camera2.blocking.BlockingCaptureCallback;
import com.android.ex.camera2.blocking.BlockingSessionCallback;
import com.android.ex.camera2.exceptions.TimeoutRuntimeException;

import junit.framework.Assert;

import org.mockito.Mockito;

import java.util.List;
import java.util.concurrent.LinkedBlockingQueue;

import static android.hardware.camera2.cts.helpers.Preconditions.*;
import static org.mockito.Mockito.*;

/**
 * A utility class with common functions setting up sessions and capturing.
 */
public class CameraSessionUtils extends Assert {
    private static final String TAG = ""CameraSessionUtils"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);

    /**
     * A blocking listener class for synchronously opening and configuring sessions.
     */
    public static class SessionListener extends BlockingSessionCallback {
        private final LinkedBlockingQueue<CameraCaptureSession> mSessionQueue =
                new LinkedBlockingQueue<>();

        /**
         * Get a new configured {@link CameraCaptureSession}.
         *
         * <p>
         * This method is blocking, and will time out after
         * {@link CameraTestUtils#SESSION_CONFIGURE_TIMEOUT_MS}.
         * </p>
         *
         * @param device the {@link CameraDevice} to open a session for.
         * @param outputs the {@link Surface} outputs to configure.
         * @param handler the {@link Handler} to use for callbacks.
         * @return a configured {@link CameraCaptureSession}.
         *
         * @throws CameraAccessException if any of the {@link CameraDevice} methods fail.
         * @throws TimeoutRuntimeException if no result was received before the timeout.
         */
        public synchronized CameraCaptureSession getConfiguredSession(CameraDevice device,
                                                                      List<Surface> outputs,
                                                                      Handler handler)
                throws CameraAccessException {
            device.createCaptureSession(outputs, this, handler);
            getStateWaiter().waitForState(SESSION_CONFIGURED,
                    CameraTestUtils.SESSION_CONFIGURE_TIMEOUT_MS);
            return mSessionQueue.poll();
        }

        @Override
        public void onConfigured(CameraCaptureSession session) {
            mSessionQueue.offer(session);
            super.onConfigured(session);
        }
    }

    /**
     * A blocking listener class for synchronously capturing and results with a session.
     */
    public static class CaptureCallback extends BlockingCaptureCallback {
        private final LinkedBlockingQueue<TotalCaptureResult> mResultQueue =
                new LinkedBlockingQueue<>();
        private final LinkedBlockingQueue<Long> mCaptureTimeQueue =
                new LinkedBlockingQueue<>();

        /**
         * Capture a new result with the given {@link CameraCaptureSession}.
         *
         * <p>
         * This method is blocking, and will time out after
         * {@link CameraTestUtils#CAPTURE_RESULT_TIMEOUT_MS}.
         * </p>
         *
         * @param session the {@link CameraCaptureSession} to use.
         * @param request the {@link CaptureRequest} to capture with.
         * @param handler the {@link Handler} to use for callbacks.
         * @return a {@link Pair} containing the capture result and capture time.
         *
         * @throws CameraAccessException if any of the {@link CameraDevice} methods fail.
         * @throws TimeoutRuntimeException if no result was received before the timeout.
         */
        public synchronized Pair<TotalCaptureResult, Long> getCapturedResult(
                CameraCaptureSession session, CaptureRequest request, Handler handler)
                throws CameraAccessException {
            session.capture(request, this, handler);
            getStateWaiter().waitForState(CAPTURE_COMPLETED,
                    CameraTestUtils.CAPTURE_RESULT_TIMEOUT_MS);
            return new Pair<>(mResultQueue.poll(), mCaptureTimeQueue.poll());
        }

        @Override
        public void onCaptureStarted(CameraCaptureSession session, CaptureRequest request,
                                     long timestamp, long frameNumber) {
            mCaptureTimeQueue.offer(timestamp);
            super.onCaptureStarted(session, request, timestamp, frameNumber);
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                                       TotalCaptureResult result) {
            mResultQueue.offer(result);
            super.onCaptureCompleted(session, request, result);
        }
    }

    /**
     * Get a mocked {@link CaptureCallback}.
     */
    public static CaptureCallback getMockCaptureListener() {
        return spy(new CaptureCallback());
    }

    /**
     * Get a mocked {@link CaptureCallback}.
     */
    public static SessionListener getMockSessionListener() {
        return spy(new SessionListener());
    }

    /**
     * Configure and return a new {@link CameraCaptureSession}.
     *
     * <p>
     * This will verify that the correct session callbacks are called if a mocked listener is
     * passed as the {@code listener} argument. This method is blocking, and will time out after
     * {@link CameraTestUtils#SESSION_CONFIGURE_TIMEOUT_MS}.
     * </p>
     *
     * @param listener a {@link SessionListener} to use for callbacks.
     * @param device the {@link CameraDevice} to use.
     * @param outputs the {@link Surface} outputs to configure.
     * @param handler the {@link Handler} to call callbacks on.
     * @return a configured {@link CameraCaptureSession}.
     *
     * @throws CameraAccessException if any of the {@link CameraDevice} methods fail.
     * @throws TimeoutRuntimeException if no result was received before the timeout.
     */
    public static CameraCaptureSession configureAndVerifySession(SessionListener listener,
                                                                 CameraDevice device,
                                                                 List<Surface> outputs,
                                                                 Handler handler)
            throws CameraAccessException {
        checkNotNull(listener);
        checkNotNull(device);
        checkNotNull(handler);
        checkCollectionNotEmpty(outputs, ""outputs"");
        checkCollectionElementsNotNull(outputs, ""outputs"");

        CameraCaptureSession session = listener.getConfiguredSession(device, outputs, handler);
        if (Mockito.mockingDetails(listener).isMock()) {
            verify(listener, never()).onConfigureFailed(any(CameraCaptureSession.class));
            verify(listener, never()).onClosed(eq(session));
            verify(listener, atLeastOnce()).onConfigured(eq(session));
        }

        checkNotNull(session);
        return session;
    }

    /**
     * Capture and return a new {@link TotalCaptureResult}.
     *
     * <p>
     * This will verify that the correct capture callbacks are called if a mocked listener is
     * passed as the {@code listener} argument. This method is blocking, and will time out after
     * {@link CameraTestUtils#CAPTURE_RESULT_TIMEOUT_MS}.
     * </p>
     *
     * @param listener a {@link CaptureCallback} to use for callbacks.
     * @param session the {@link CameraCaptureSession} to use.
     * @param request the {@link CaptureRequest} to capture with.
     * @param handler the {@link Handler} to call callbacks on.
     * @return a {@link Pair} containing the capture result and capture time.
     *
     * @throws CameraAccessException if any of the {@link CameraDevice} methods fail.
     * @throws TimeoutRuntimeException if no result was received before the timeout.
     */
    public static Pair<TotalCaptureResult, Long> captureAndVerifyResult(CaptureCallback listener,
            CameraCaptureSession session, CaptureRequest request, Handler handler)
            throws CameraAccessException {
        checkNotNull(listener);
        checkNotNull(session);
        checkNotNull(request);
        checkNotNull(handler);

        Pair<TotalCaptureResult, Long> result = listener.getCapturedResult(session, request,
                handler);
        if (Mockito.mockingDetails(listener).isMock()) {
            verify(listener, never()).onCaptureFailed(any(CameraCaptureSession.class),
                    any(CaptureRequest.class), any(CaptureFailure.class));
            verify(listener, atLeastOnce()).onCaptureStarted(eq(session), eq(request),
                    anyLong(), anyLong());
            verify(listener, atLeastOnce()).onCaptureCompleted(eq(session), eq(request),
                    eq(result.first));
        }

        checkNotNull(result);
        return result;
    }

    // Suppress default constructor for noninstantiability
    private CameraSessionUtils() { throw new AssertionError(); }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.testcases.Camera2SurfaceViewTestCase"	"isInstantApp"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/testcases/Camera2SurfaceViewTestCase.java"	""	"public void test/*
 *.
 */

package android.hardware.camera2.cts.testcases;

import static android.hardware.camera2.cts.CameraTestUtils.*;

import static com.android.ex.camera2.blocking.BlockingStateCallback.STATE_CLOSED;
import androidx.test.InstrumentationRegistry;
import android.app.UiAutomation;

import android.content.Context;
import android.graphics.ImageFormat;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraCaptureSession.CaptureCallback;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CameraMetadata;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.cts.Camera2SurfaceViewCtsActivity;
import android.hardware.camera2.cts.Camera2ParameterizedTestCase;
import android.hardware.camera2.cts.CameraTestUtils;
import android.hardware.camera2.cts.CameraTestUtils.SimpleCaptureCallback;
import android.hardware.camera2.cts.helpers.CameraErrorCollector;
import android.hardware.camera2.cts.helpers.StaticMetadata;
import android.hardware.camera2.cts.helpers.StaticMetadata.CheckLevel;
import android.media.ImageReader;
import android.os.Handler;
import android.os.HandlerThread;
import android.os.Looper;
import android.util.Log;
import android.util.Range;
import android.util.Size;
import android.view.Surface;
import android.view.SurfaceHolder;
import android.view.View;
import android.view.WindowManager;

import androidx.test.rule.ActivityTestRule;

import com.android.ex.camera2.blocking.BlockingSessionCallback;
import com.android.ex.camera2.blocking.BlockingStateCallback;
import com.android.ex.camera2.exceptions.TimeoutRuntimeException;

import org.junit.After;
import org.junit.Before;
import org.junit.Ignore;
import org.junit.Rule;

import java.io.File;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.junit.runners.Parameterized;
import org.junit.runners.Parameterized.Parameter;
import org.junit.runners.Parameterized.Parameters;


/**
 * Camera2 Preview test case base class by using SurfaceView as rendering target.
 *
 * <p>This class encapsulates the SurfaceView based preview common functionalities.
 * The setup and teardown of CameraManager, test HandlerThread, Activity, Camera IDs
 * and CameraStateCallback are handled in this class. Some basic preview related utility
 * functions are provided to facilitate the derived preview-based test classes.
 * </p>
 */

public class Camera2SurfaceViewTestCase extends Camera2ParameterizedTestCase {
    private static final String TAG = ""SurfaceViewTestCase"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);
    private static final int WAIT_FOR_SURFACE_CHANGE_TIMEOUT_MS = 1000;

    protected static final int WAIT_FOR_RESULT_TIMEOUT_MS = 3000;
    protected static final float FRAME_DURATION_ERROR_MARGIN = 0.01f; // 1 percent error margin.
    protected static final int NUM_RESULTS_WAIT_TIMEOUT = 100;
    protected static final int NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY = 8;
    protected static final int MIN_FRAME_DURATION_ERROR_MARGIN = 100; // ns

    protected HandlerThread mHandlerThread;
    protected Handler mHandler;
    protected BlockingStateCallback mCameraListener;
    protected BlockingSessionCallback mSessionListener;
    protected CameraErrorCollector mCollector;
    protected HashMap<String, StaticMetadata> mAllStaticInfo;
    // Per device fields:
    protected StaticMetadata mStaticInfo;
    protected CameraDevice mCamera;
    protected CameraCaptureSession mSession;
    protected ImageReader mReader;
    protected Surface mReaderSurface;
    protected Surface mPreviewSurface;
    protected SurfaceHolder mPreviewHolder;
    protected Size mPreviewSize;
    protected List<Size> mOrderedPreviewSizes; // In descending order.
    protected List<Size> m1080pBoundedOrderedPreviewSizes; // In descending order.
    protected List<Size> mOrderedVideoSizes; // In descending order.
    protected List<Size> mOrderedStillSizes; // In descending order.
    protected HashMap<Size, Long> mMinPreviewFrameDurationMap;
    protected String mDebugFileNameBase;

    protected WindowManager mWindowManager;

    @Rule
    public ActivityTestRule<Camera2SurfaceViewCtsActivity> mActivityRule =
            new ActivityTestRule<>(Camera2SurfaceViewCtsActivity.class);

    @Before
    public void setUp() throws Exception {
        super.setUp();
        mHandlerThread = new HandlerThread(TAG);
        mHandlerThread.start();
        mHandler = new Handler(mHandlerThread.getLooper());
        mCameraListener = new BlockingStateCallback();
        mCollector = new CameraErrorCollector();

        File filesDir = mContext.getPackageManager().isInstantApp()
                ? mContext.getFilesDir()
                : mContext.getExternalFilesDir(null);

        mDebugFileNameBase = filesDir.getPath();

        mAllStaticInfo = new HashMap<String, StaticMetadata>();
        List<String> hiddenPhysicalIds = new ArrayList<>();
        for (String cameraId : mCameraIdsUnderTest) {
            CameraCharacteristics props = mCameraManager.getCameraCharacteristics(cameraId);
            StaticMetadata staticMetadata = new StaticMetadata(props,
                    CheckLevel.ASSERT, /*collector*/null);
            mAllStaticInfo.put(cameraId, staticMetadata);

            for (String physicalId : props.getPhysicalCameraIds()) {
                if (!Arrays.asList(mCameraIdsUnderTest).contains(physicalId) &&
                        !hiddenPhysicalIds.contains(physicalId)) {
                    hiddenPhysicalIds.add(physicalId);
                    props = mCameraManager.getCameraCharacteristics(physicalId);
                    staticMetadata = new StaticMetadata(
                            mCameraManager.getCameraCharacteristics(physicalId),
                            CheckLevel.ASSERT, /*collector*/null);
                    mAllStaticInfo.put(physicalId, staticMetadata);
                }
            }
        }

        mWindowManager = (WindowManager) mContext.getSystemService(Context.WINDOW_SERVICE);
    }

    @After
    public void tearDown() throws Exception {
        mHandlerThread.quitSafely();
        mHandler = null;
        mCameraListener = null;

        try {
            mCollector.verify();
        } catch (Throwable e) {
            // When new Exception(e) is used, exception info will be printed twice.
            throw new Exception(e.getMessage());
        }
        super.tearDown();
    }

    /**
     * Start camera preview by using the given request, preview size and capture
     * listener.
     * <p>
     * If preview is already started, calling this function will stop the
     * current preview stream and start a new preview stream with given
     * parameters. No need to call stopPreview between two startPreview calls.
     * </p>
     *
     * @param request The request builder used to start the preview.
     * @param previewSz The size of the camera device output preview stream.
     * @param listener The callbacks the camera device will notify when preview
     *            capture is available.
     */
    protected void startPreview(CaptureRequest.Builder request, Size previewSz,
            CaptureCallback listener) throws Exception {
        // Update preview size.
        updatePreviewSurface(previewSz);
        if (VERBOSE) {
            Log.v(TAG, ""start preview with size "" + mPreviewSize.toString());
        }

        configurePreviewOutput(request);

        mSession.setRepeatingRequest(request.build(), listener, mHandler);
    }

    /**
     * Configure the preview output stream.
     *
     * @param request The request to be configured with preview surface
     */
    protected void configurePreviewOutput(CaptureRequest.Builder request)
            throws CameraAccessException {
        List<Surface> outputSurfaces = new ArrayList<Surface>(/*capacity*/1);
        outputSurfaces.add(mPreviewSurface);
        mSessionListener = new BlockingSessionCallback();
        mSession = configureCameraSession(mCamera, outputSurfaces, mSessionListener, mHandler);

        request.addTarget(mPreviewSurface);
    }

    /**
     * Create a {@link CaptureRequest#Builder} and add the default preview surface.
     *
     * @return The {@link CaptureRequest#Builder} to be created
     * @throws CameraAccessException When create capture request from camera fails
     */
    protected CaptureRequest.Builder createRequestForPreview() throws CameraAccessException {
        if (mPreviewSurface == null) {
            throw new IllegalStateException(
                    ""Preview surface is not set yet, call updatePreviewSurface or startPreview""
                    + ""first to set the preview surface properly."");
        }
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        requestBuilder.addTarget(mPreviewSurface);
        return requestBuilder;
    }

    /**
     * Stop preview for current camera device by closing the session.
     * Does _not_ wait for the device to go idle
     */
    protected void stopPreview() throws Exception {
        // Stop repeat, wait for captures to complete, and disconnect from surfaces
        if (mSession != null) {
            if (VERBOSE) Log.v(TAG, ""Stopping preview"");
            mSession.close();
        }
    }

    /**
     * Stop preview for current camera device by closing the session and waiting for it to close,
     * resulting in an idle device.
     */
    protected void stopPreviewAndDrain() throws Exception {
        // Stop repeat, wait for captures to complete, and disconnect from surfaces
        if (mSession != null) {
            if (VERBOSE) Log.v(TAG, ""Stopping preview and waiting for idle"");
            mSession.close();
            mSessionListener.getStateWaiter().waitForState(BlockingSessionCallback.SESSION_CLOSED,
                    /*timeoutMs*/WAIT_FOR_RESULT_TIMEOUT_MS);
        }
    }

    /**
     * Setup still (JPEG) capture configuration and start preview.
     * <p>
     * The default max number of image is set to image reader.
     * </p>
     *
     * @param previewRequest The capture request to be used for preview
     * @param stillRequest The capture request to be used for still capture
     * @param previewSz Preview size
     * @param stillSz The still capture size
     * @param resultListener Capture result listener
     * @param imageListener The still capture image listener
     */
    protected void prepareStillCaptureAndStartPreview(CaptureRequest.Builder previewRequest,
            CaptureRequest.Builder stillRequest, Size previewSz, Size stillSz,
            CaptureCallback resultListener,
            ImageReader.OnImageAvailableListener imageListener, boolean isHeic) throws Exception {
        prepareCaptureAndStartPreview(previewRequest, stillRequest, previewSz, stillSz,
                isHeic ? ImageFormat.HEIC : ImageFormat.JPEG, resultListener, MAX_READER_IMAGES,
                imageListener);
    }

    /**
     * Setup still (JPEG) capture configuration and start preview.
     *
     * @param previewRequest The capture request to be used for preview
     * @param stillRequest The capture request to be used for still capture
     * @param previewSz Preview size
     * @param stillSz The still capture size
     * @param resultListener Capture result listener
     * @param maxNumImages The max number of images set to the image reader
     * @param imageListener The still capture image listener
     */
    protected void prepareStillCaptureAndStartPreview(CaptureRequest.Builder previewRequest,
            CaptureRequest.Builder stillRequest, Size previewSz, Size stillSz,
            CaptureCallback resultListener, int maxNumImages,
            ImageReader.OnImageAvailableListener imageListener, boolean isHeic) throws Exception {
        prepareCaptureAndStartPreview(previewRequest, stillRequest, previewSz, stillSz,
                isHeic ? ImageFormat.HEIC : ImageFormat.JPEG, resultListener, maxNumImages, imageListener);
    }

    /**
     * Setup raw capture configuration and start preview.
     *
     * <p>
     * The default max number of image is set to image reader.
     * </p>
     *
     * @param previewRequest The capture request to be used for preview
     * @param rawRequest The capture request to be used for raw capture
     * @param previewSz Preview size
     * @param rawSz The raw capture size
     * @param resultListener Capture result listener
     * @param imageListener The raw capture image listener
     */
    protected void prepareRawCaptureAndStartPreview(CaptureRequest.Builder previewRequest,
            CaptureRequest.Builder rawRequest, Size previewSz, Size rawSz,
            CaptureCallback resultListener,
            ImageReader.OnImageAvailableListener imageListener) throws Exception {
        prepareCaptureAndStartPreview(previewRequest, rawRequest, previewSz, rawSz,
                ImageFormat.RAW_SENSOR, resultListener, MAX_READER_IMAGES, imageListener);
    }

    /**
     * Wait for expected result key value available in a certain number of results.
     *
     * <p>
     * Check the result immediately if numFramesWait is 0.
     * </p>
     *
     * @param listener The capture listener to get capture result
     * @param resultKey The capture result key associated with the result value
     * @param expectedValue The result value need to be waited for
     * @param numResultsWait Number of frame to wait before times out
     * @throws TimeoutRuntimeException If more than numResultsWait results are
     * seen before the result matching myRequest arrives, or each individual wait
     * for result times out after {@value #WAIT_FOR_RESULT_TIMEOUT_MS}ms.
     */
    protected static <T> void waitForResultValue(SimpleCaptureCallback listener,
            CaptureResult.Key<T> resultKey,
            T expectedValue, int numResultsWait) {
        CameraTestUtils.waitForResultValue(listener, resultKey, expectedValue,
                numResultsWait, WAIT_FOR_RESULT_TIMEOUT_MS);
    }

    /**
     * Wait for any expected result key values available in a certain number of results.
     *
     * <p>
     * Check the result immediately if numFramesWait is 0.
     * </p>
     *
     * @param listener The capture listener to get capture result.
     * @param resultKey The capture result key associated with the result value.
     * @param expectedValues The list of result value need to be waited for,
     * return immediately if the list is empty.
     * @param numResultsWait Number of frame to wait before times out.
     * @throws TimeoutRuntimeException If more than numResultsWait results are.
     * seen before the result matching myRequest arrives, or each individual wait
     * for result times out after {@value #WAIT_FOR_RESULT_TIMEOUT_MS}ms.
     */
    protected static <T> void waitForAnyResultValue(SimpleCaptureCallback listener,
            CaptureResult.Key<T> resultKey,
            List<T> expectedValues, int numResultsWait) {
        CameraTestUtils.waitForAnyResultValue(listener, resultKey, expectedValues, numResultsWait,
                WAIT_FOR_RESULT_TIMEOUT_MS);
    }

    /**
     * Submit a burst of the same capture request, then submit additional captures in order to
     * ensure that the camera will be synchronized.
     *
     * <p>
     * The additional capture count is determined by android.sync.maxLatency (or
     * a fixed {@value #NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY}) captures if maxLatency is unknown).
     * </p>
     *
     * <p>Returns the number of captures that were submitted (at least 1), which is useful
     * with {@link #waitForNumResults}.</p>
     *
     * @param request capture request to forward to {@link CameraDevice#capture}
     * @param listener request listener to forward to {@link CameraDevice#capture}
     * @param handler handler to forward to {@link CameraDevice#capture}
     *
     * @return the number of captures that were submitted
     *
     * @throws CameraAccessException if capturing failed
     */
    protected int captureRequestsSynchronizedBurst(
            CaptureRequest request, int count, CaptureCallback listener, Handler handler)
                    throws CameraAccessException {
        return captureRequestsSynchronizedImpl(request, count, listener, handler, true);
    }
    /**
     * Submit a capture once, then submit additional captures in order to ensure that
     * the camera will be synchronized.
     *
     * <p>
     * The additional capture count is determined by android.sync.maxLatency (or
     * a fixed {@value #NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY}) captures if maxLatency is unknown).
     * </p>
     *
     * <p>Returns the number of captures that were submitted (at least 1), which is useful
     * with {@link #waitForNumResults}.</p>
     *
     * @param request capture request to forward to {@link CameraDevice#capture}
     * @param listener request listener to forward to {@link CameraDevice#capture}
     * @param handler handler to forward to {@link CameraDevice#capture}
     *
     * @return the number of captures that were submitted
     *
     * @throws CameraAccessException if capturing failed
     */
    protected int captureRequestsSynchronized(
            CaptureRequest request, CaptureCallback listener, Handler handler)
                    throws CameraAccessException {
        return captureRequestsSynchronizedImpl(request, /*count*/1, listener, handler, false);
    }

    /**
     * Submit a capture {@code count} times, then submit additional captures in order to ensure that
     * the camera will be synchronized.
     *
     * <p>
     * The additional capture count is determined by android.sync.maxLatency (or
     * a fixed {@value #NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY}) captures if maxLatency is unknown).
     * </p>
     *
     * <p>Returns the number of captures that were submitted (at least 1), which is useful
     * with {@link #waitForNumResults}.</p>
     *
     * @param request capture request to forward to {@link CameraDevice#capture}
     * @param count the number of times to submit the request (minimally), must be at least 1
     * @param listener request listener to forward to {@link CameraDevice#capture}
     * @param handler handler to forward to {@link CameraDevice#capture}
     *
     * @return the number of captures that were submitted
     *
     * @throws IllegalArgumentException if {@code count} was not at least 1
     * @throws CameraAccessException if capturing failed
     */
    protected int captureRequestsSynchronized(
            CaptureRequest request, int count, CaptureCallback listener, Handler handler)
                    throws CameraAccessException {
        return captureRequestsSynchronizedImpl(request, count, listener, handler, false);
    }

    /**
     * Wait for numResultWait frames
     *
     * @param resultListener The capture listener to get capture result back.
     * @param numResultsWait Number of frame to wait
     *
     * @return the last result, or {@code null} if there was none
     */
    protected static CaptureResult waitForNumResults(SimpleCaptureCallback resultListener,
            int numResultsWait) {
        return CameraTestUtils.waitForNumResults(resultListener, numResultsWait,
                WAIT_FOR_RESULT_TIMEOUT_MS);
    }

    /**
     * Wait for enough results for settings to be applied
     *
     * @param resultListener The capture listener to get capture result back.
     * @param numResultWaitForUnknownLatency Number of frame to wait if camera device latency is
     *                                       unknown.
     */
    protected void waitForSettingsApplied(SimpleCaptureCallback resultListener,
            int numResultWaitForUnknownLatency) {
        int maxLatency = mStaticInfo.getSyncMaxLatency();
        if (maxLatency == CameraMetadata.SYNC_MAX_LATENCY_UNKNOWN) {
            maxLatency = numResultWaitForUnknownLatency;
        }
        // Wait for settings to take effect
        waitForNumResults(resultListener, maxLatency);
    }

    /**
     * Wait for AE to be stabilized before capture: CONVERGED or FLASH_REQUIRED.
     *
     * <p>Waits for {@code android.sync.maxLatency} number of results first, to make sure
     * that the result is synchronized (or {@code numResultWaitForUnknownLatency} if the latency
     * is unknown.</p>
     *
     * <p>This is a no-op for {@code LEGACY} devices since they don't report
     * the {@code aeState} result.</p>
     *
     * @param resultListener The capture listener to get capture result back.
     * @param numResultWaitForUnknownLatency Number of frame to wait if camera device latency is
     *                                       unknown.
     */
    protected void waitForAeStable(SimpleCaptureCallback resultListener,
            int numResultWaitForUnknownLatency) {
        CameraTestUtils.waitForAeStable(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY,
                mStaticInfo, WAIT_FOR_RESULT_TIMEOUT_MS, NUM_RESULTS_WAIT_TIMEOUT);
    }

    /**
     * Wait for AE to be: LOCKED
     *
     * <p>Waits for {@code android.sync.maxLatency} number of results first, to make sure
     * that the result is synchronized (or {@code numResultWaitForUnknownLatency} if the latency
     * is unknown.</p>
     *
     * <p>This is a no-op for {@code LEGACY} devices since they don't report
     * the {@code aeState} result.</p>
     *
     * @param resultListener The capture listener to get capture result back.
     * @param numResultWaitForUnknownLatency Number of frame to wait if camera device latency is
     *                                       unknown.
     */
    protected void waitForAeLocked(SimpleCaptureCallback resultListener,
            int numResultWaitForUnknownLatency) {

        waitForSettingsApplied(resultListener, numResultWaitForUnknownLatency);

        if (!mStaticInfo.isHardwareLevelAtLeastLimited()) {
            // No-op for legacy devices
            return;
        }

        List<Integer> expectedAeStates = new ArrayList<Integer>();
        expectedAeStates.add(new Integer(CaptureResult.CONTROL_AE_STATE_LOCKED));
        CameraTestUtils.waitForAnyResultValue(resultListener, CaptureResult.CONTROL_AE_STATE,
                expectedAeStates, NUM_RESULTS_WAIT_TIMEOUT, WAIT_FOR_RESULT_TIMEOUT_MS);
    }

    /**
     * Create an {@link ImageReader} object and get the surface.
     *
     * @param size The size of this ImageReader to be created.
     * @param format The format of this ImageReader to be created
     * @param maxNumImages The max number of images that can be acquired simultaneously.
     * @param listener The listener used by this ImageReader to notify callbacks.
     */
    protected void createImageReader(Size size, int format, int maxNumImages,
            ImageReader.OnImageAvailableListener listener) throws Exception {
        closeImageReader();

        ImageReader r = makeImageReader(size, format, maxNumImages, listener,
                mHandler);
        mReader = r;
        mReaderSurface = r.getSurface();
    }

    /**
     * Close the pending images then close current active {@link ImageReader} object.
     */
    protected void closeImageReader() {
        CameraTestUtils.closeImageReader(mReader);
        mReader = null;
        mReaderSurface = null;
    }

    /**
     * Close the pending images then close current active {@link ImageReader} objects.
     */
    protected void closeImageReaders(ImageReader[] readers) {
        CameraTestUtils.closeImageReaders(readers);
    }

    /**
     * Setup still capture configuration and start preview.
     *
     * @param previewRequest The capture request to be used for preview
     * @param stillRequest The capture request to be used for still capture
     * @param previewSz Preview size
     * @param captureSizes Still capture sizes
     * @param formats The single capture image formats
     * @param resultListener Capture result listener
     * @param maxNumImages The max number of images set to the image reader
     * @param imageListeners The single capture capture image listeners
     * @param isHeic HEIC still capture if true, JPEG still capture if false
     */
    protected ImageReader[] prepareStillCaptureAndStartPreview(
            CaptureRequest.Builder previewRequest, CaptureRequest.Builder stillRequest,
            Size previewSz, Size[] captureSizes, int[] formats, CaptureCallback resultListener,
            int maxNumImages, ImageReader.OnImageAvailableListener[] imageListeners,
            boolean isHeic)
            throws Exception {

        if ((captureSizes == null) || (formats == null) || (imageListeners == null) &&
                (captureSizes.length != formats.length) ||
                (formats.length != imageListeners.length)) {
            throw new IllegalArgumentException(""Invalid capture sizes/formats or image listeners!"");
        }

        if (VERBOSE) {
            Log.v(TAG, String.format(""Prepare still capture and preview (%s)"",
                    previewSz.toString()));
        }

        // Update preview size.
        updatePreviewSurface(previewSz);

        ImageReader[] readers = new ImageReader[captureSizes.length];
        List<Surface> outputSurfaces = new ArrayList<Surface>();
        outputSurfaces.add(mPreviewSurface);
        for (int i = 0; i < captureSizes.length; i++) {
            readers[i] = makeImageReader(captureSizes[i], formats[i], maxNumImages,
                    imageListeners[i], mHandler);
            outputSurfaces.add(readers[i].getSurface());
        }

        mSessionListener = new BlockingSessionCallback();
        mSession = configureCameraSession(mCamera, outputSurfaces, mSessionListener, mHandler);

        // Configure the requests.
        previewRequest.addTarget(mPreviewSurface);
        stillRequest.addTarget(mPreviewSurface);
        for (int i = 0; i < readers.length; i++) {
            stillRequest.addTarget(readers[i].getSurface());
        }

        // Start preview.
        mSession.setRepeatingRequest(previewRequest.build(), resultListener, mHandler);

        return readers;
    }

    /**
     * Open a camera device and get the StaticMetadata for a given camera id.
     *
     * @param cameraId The id of the camera device to be opened.
     */
    protected void openDevice(String cameraId) throws Exception {
        mCamera = CameraTestUtils.openCamera(
                mCameraManager, cameraId, mCameraListener, mHandler);
        mCollector.setCameraId(cameraId);
        mStaticInfo = new StaticMetadata(mCameraManager.getCameraCharacteristics(cameraId),
                CheckLevel.ASSERT, /*collector*/null);
        if (mStaticInfo.isColorOutputSupported()) {
            mOrderedPreviewSizes = getSupportedPreviewSizes(cameraId, mCameraManager,
                    getPreviewSizeBound(mWindowManager, PREVIEW_SIZE_BOUND));
            m1080pBoundedOrderedPreviewSizes = getSupportedPreviewSizes(cameraId, mCameraManager,
                    PREVIEW_SIZE_BOUND);
            mOrderedVideoSizes = getSupportedVideoSizes(cameraId, mCameraManager, PREVIEW_SIZE_BOUND);
            mOrderedStillSizes = getSupportedStillSizes(cameraId, mCameraManager, null);
            // Use ImageFormat.YUV_420_888 for now. TODO: need figure out what's format for preview
            // in public API side.
            mMinPreviewFrameDurationMap =
                mStaticInfo.getAvailableMinFrameDurationsForFormatChecked(ImageFormat.YUV_420_888);
        }
    }

    /**
     * Close the current actively used camera device.
     */
    protected void closeDevice() {
        if (mCamera != null) {
            mCamera.close();
            mCameraListener.waitForState(STATE_CLOSED, CAMERA_CLOSE_TIMEOUT_MS);
            mCamera = null;
            mSession = null;
            mSessionListener = null;
            mStaticInfo = null;
            mOrderedPreviewSizes = null;
            mOrderedVideoSizes = null;
            mOrderedStillSizes = null;
        }
    }

    /**
     * Update the preview surface size.
     *
     * @param size The preview size to be updated.
     */
    protected void updatePreviewSurface(Size size) {
        if (size.equals(mPreviewSize) && mPreviewSurface != null) {
            Log.w(TAG, ""Skipping update preview surface size..."");
            return;
        }

        mPreviewSize = size;
        Camera2SurfaceViewCtsActivity ctsActivity = mActivityRule.getActivity();
        final SurfaceHolder holder = ctsActivity.getSurfaceView().getHolder();
        Handler handler = new Handler(Looper.getMainLooper());
        handler.post(new Runnable() {
            @Override
            public void run() {
                holder.setFixedSize(mPreviewSize.getWidth(), mPreviewSize.getHeight());
            }
        });

        boolean res = ctsActivity.waitForSurfaceSizeChanged(
                WAIT_FOR_SURFACE_CHANGE_TIMEOUT_MS, mPreviewSize.getWidth(),
                mPreviewSize.getHeight());
        assertTrue(""wait for surface change to "" + mPreviewSize.toString() + "" timed out"", res);
        mPreviewHolder = holder;
        mPreviewSurface = holder.getSurface();
        assertNotNull(""Preview surface is null"", mPreviewSurface);
        assertTrue(""Preview surface is invalid"", mPreviewSurface.isValid());
    }

    /**
     * Recreate the SurfaceView's Surface
     *
     * Hide and unhide the activity's preview SurfaceView, so that its backing Surface is
     * recreated
     */
    protected void recreatePreviewSurface() {
        Camera2SurfaceViewCtsActivity ctsActivity = mActivityRule.getActivity();
        setPreviewVisibility(View.GONE);
        boolean res = ctsActivity.waitForSurfaceState(
            WAIT_FOR_SURFACE_CHANGE_TIMEOUT_MS, /*valid*/ false);
        assertTrue(""wait for surface destroyed timed out"", res);
        setPreviewVisibility(View.VISIBLE);
        res = ctsActivity.waitForSurfaceState(
            WAIT_FOR_SURFACE_CHANGE_TIMEOUT_MS, /*valid*/ true);
        assertTrue(""wait for surface created timed out"", res);
    }

    /**
     * Show/hide the preview SurfaceView.
     *
     * If set to View.GONE, the surfaceDestroyed callback will fire
     * @param visibility the new new visibility to set, one of View.VISIBLE / INVISIBLE / GONE
     */
    protected void setPreviewVisibility(int visibility) {
        final Camera2SurfaceViewCtsActivity ctsActivity = mActivityRule.getActivity();
        Handler handler = new Handler(Looper.getMainLooper());
        handler.post(new Runnable() {
            @Override
            public void run() {
                ctsActivity.getSurfaceView().setVisibility(visibility);
            }
        });
    }

    /**
     * Setup single capture configuration and start preview.
     *
     * @param previewRequest The capture request to be used for preview
     * @param stillRequest The capture request to be used for still capture
     * @param previewSz Preview size
     * @param captureSz Still capture size
     * @param format The single capture image format
     * @param resultListener Capture result listener
     * @param maxNumImages The max number of images set to the image reader
     * @param imageListener The single capture capture image listener
     */
    protected void prepareCaptureAndStartPreview(CaptureRequest.Builder previewRequest,
            CaptureRequest.Builder stillRequest, Size previewSz, Size captureSz, int format,
            CaptureCallback resultListener, int maxNumImages,
            ImageReader.OnImageAvailableListener imageListener) throws Exception {
        prepareCaptureAndStartPreview(previewRequest, stillRequest, previewSz, captureSz,
            format, resultListener, null, maxNumImages, imageListener);
    }

    /**
     * Setup single capture configuration and start preview.
     *
     * @param previewRequest The capture request to be used for preview
     * @param stillRequest The capture request to be used for still capture
     * @param previewSz Preview size
     * @param captureSz Still capture size
     * @param format The single capture image format
     * @param resultListener Capture result listener
     * @param sessionListener Session listener
     * @param maxNumImages The max number of images set to the image reader
     * @param imageListener The single capture capture image listener
     */
    protected void prepareCaptureAndStartPreview(CaptureRequest.Builder previewRequest,
            CaptureRequest.Builder stillRequest, Size previewSz, Size captureSz, int format,
            CaptureCallback resultListener, CameraCaptureSession.StateCallback sessionListener,
            int maxNumImages, ImageReader.OnImageAvailableListener imageListener) throws Exception {
        if (VERBOSE) {
            Log.v(TAG, String.format(""Prepare single capture (%s) and preview (%s)"",
                    captureSz.toString(), previewSz.toString()));
        }

        // Update preview size.
        updatePreviewSurface(previewSz);

        // Create ImageReader.
        createImageReader(captureSz, format, maxNumImages, imageListener);

        // Configure output streams with preview and jpeg streams.
        List<Surface> outputSurfaces = new ArrayList<Surface>();
        outputSurfaces.add(mPreviewSurface);
        outputSurfaces.add(mReaderSurface);
        if (sessionListener == null) {
            mSessionListener = new BlockingSessionCallback();
        } else {
            mSessionListener = new BlockingSessionCallback(sessionListener);
        }
        mSession = configureCameraSession(mCamera, outputSurfaces, mSessionListener, mHandler);

        // Configure the requests.
        previewRequest.addTarget(mPreviewSurface);
        stillRequest.addTarget(mPreviewSurface);
        stillRequest.addTarget(mReaderSurface);

        // Start preview.
        mSession.setRepeatingRequest(previewRequest.build(), resultListener, mHandler);
    }

    /**
     * Get the max preview size that supports the given fpsRange.
     *
     * @param fpsRange The fps range the returned size must support.
     * @return max size that support the given fps range.
     */
    protected Size getMaxPreviewSizeForFpsRange(Range<Integer> fpsRange) {
        if (fpsRange == null || fpsRange.getLower() <= 0 || fpsRange.getUpper() <= 0) {
            throw new IllegalArgumentException(""Invalid fps range argument"");
        }
        if (mOrderedPreviewSizes == null || mMinPreviewFrameDurationMap == null) {
            throw new IllegalStateException(""mOrderedPreviewSizes and mMinPreviewFrameDurationMap""
                    + "" must be initialized"");
        }

        long[] frameDurationRange =
                new long[]{(long) (1e9 / fpsRange.getUpper()), (long) (1e9 / fpsRange.getLower())};
        for (Size size : mOrderedPreviewSizes) {
            Long minDuration = mMinPreviewFrameDurationMap.get(size);
            if (minDuration == null ||
                    minDuration == 0) {
                if (mStaticInfo.isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
                    throw new IllegalArgumentException(
                            ""No min frame duration available for the size "" + size);
                }
                continue;
            }
            if (minDuration <= (frameDurationRange[0] + MIN_FRAME_DURATION_ERROR_MARGIN)) {
                return size;
            }
        }

        // Search again for sizes not bounded by display size
        for (Size size : m1080pBoundedOrderedPreviewSizes) {
            Long minDuration = mMinPreviewFrameDurationMap.get(size);
            if (minDuration == null ||
                    minDuration == 0) {
                if (mStaticInfo.isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
                    throw new IllegalArgumentException(
                            ""No min frame duration available for the size "" + size);
                }
                continue;
            }
            if (minDuration <= (frameDurationRange[0] + MIN_FRAME_DURATION_ERROR_MARGIN)) {
                return size;
            }
        }
        return null;
    }

    protected boolean isReprocessSupported(String cameraId, int format)
            throws CameraAccessException {
        if (format != ImageFormat.YUV_420_888 && format != ImageFormat.PRIVATE) {
            throw new IllegalArgumentException(
                    ""format "" + format + "" is not supported for reprocessing"");
        }

        StaticMetadata info =
                new StaticMetadata(mCameraManager.getCameraCharacteristics(cameraId),
                                   CheckLevel.ASSERT, /*collector*/ null);
        int cap = CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING;
        if (format == ImageFormat.PRIVATE) {
            cap = CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING;
        }
        return info.isCapabilitySupported(cap);
    }

    protected Range<Integer> getSuitableFpsRangeForDuration(String cameraId, long frameDuration) {
        return CameraTestUtils.getSuitableFpsRangeForDuration(cameraId, frameDuration, mStaticInfo);
    }

    private int captureRequestsSynchronizedImpl(
            CaptureRequest request, int count, CaptureCallback listener, Handler handler,
            boolean isBurst) throws CameraAccessException {
        if (count < 1) {
            throw new IllegalArgumentException(""count must be positive"");
        }

        int maxLatency = mStaticInfo.getSyncMaxLatency();
        if (maxLatency == CameraMetadata.SYNC_MAX_LATENCY_UNKNOWN) {
            maxLatency = NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY;
        }

        assertTrue(""maxLatency is non-negative"", maxLatency >= 0);

        int numCaptures = maxLatency + count;
        ArrayList<CaptureRequest> burstCaptureRequests = new ArrayList<>();
        for (int i = 0; i < numCaptures; ++i) {
            if (isBurst) {
                burstCaptureRequests.add(request);
            } else {
                mSession.capture(request, listener, handler);
            }
        }
        if (isBurst) {
            mSession.captureBurst(burstCaptureRequests, listener, handler);
        }

        return numCaptures;
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.multiprocess.camera.cts.CameraEvictionTest"	"CameraEvictionTest"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/multiprocess/camera/cts/CameraEvictionTest.java"	""	"public void test/*
 *.
 */

package android.hardware.multiprocess.camera.cts;

import android.app.Activity;
import android.app.ActivityManager;
import android.app.UiAutomation;
import android.content.Context;
import android.content.Intent;
import android.hardware.Camera;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.cts.CameraTestUtils.HandlerExecutor;
import android.hardware.cts.CameraCtsActivity;
import android.os.Handler;
import android.test.ActivityInstrumentationTestCase2;
import android.util.Log;

import android.Manifest;

import androidx.test.InstrumentationRegistry;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Objects;
import java.util.concurrent.Executor;
import java.util.concurrent.TimeoutException;

import static org.mockito.Mockito.*;

/**
 * Tests for multi-process camera usage behavior.
 */
public class CameraEvictionTest extends ActivityInstrumentationTestCase2<CameraCtsActivity> {

    public static final String TAG = ""CameraEvictionTest"";

    private static final int OPEN_TIMEOUT = 2000; // Timeout for camera to open (ms).
    private static final int SETUP_TIMEOUT = 5000; // Remote camera setup timeout (ms).
    private static final int EVICTION_TIMEOUT = 1000; // Remote camera eviction timeout (ms).
    private static final int WAIT_TIME = 2000; // Time to wait for process to launch (ms).
    private static final int UI_TIMEOUT = 10000; // Time to wait for UI event before timeout (ms).
    // CACHED_APP_MAX_ADJ - FG oom score
    private static final int CACHED_APP_VS_FG_OOM_DELTA = 999;
    ErrorLoggingService.ErrorServiceConnection mErrorServiceConnection;

    private ActivityManager mActivityManager;
    private Context mContext;
    private Camera mCamera;
    private CameraDevice mCameraDevice;
    private UiAutomation mUiAutomation;
    private final Object mLock = new Object();
    private boolean mCompleted = false;
    private int mProcessPid = -1;

    /** Load jni on initialization */
    static {
        System.loadLibrary(""ctscamera2_jni"");
    }

    private static native long initializeAvailabilityCallbacksNative();
    private static native int getAccessCallbacksCountAndResetNative(long context);
    private static native long releaseAvailabilityCallbacksNative(long context);

    public CameraEvictionTest() {
        super(CameraCtsActivity.class);
    }

    public static class StateCallbackImpl extends CameraDevice.StateCallback {
        CameraDevice mCameraDevice;

        public StateCallbackImpl() {
            super();
        }

        @Override
        public void onOpened(CameraDevice cameraDevice) {
            synchronized(this) {
                mCameraDevice = cameraDevice;
            }
            Log.i(TAG, ""CameraDevice onOpened called for main CTS test process."");
        }

        @Override
        public void onClosed(CameraDevice camera) {
            super.onClosed(camera);
            synchronized(this) {
                mCameraDevice = null;
            }
            Log.i(TAG, ""CameraDevice onClosed called for main CTS test process."");
        }

        @Override
        public void onDisconnected(CameraDevice cameraDevice) {
            synchronized(this) {
                mCameraDevice = null;
            }
            Log.i(TAG, ""CameraDevice onDisconnected called for main CTS test process."");

        }

        @Override
        public void onError(CameraDevice cameraDevice, int i) {
            Log.i(TAG, ""CameraDevice onError called for main CTS test process with error "" +
                    ""code: "" + i);
        }

        public synchronized CameraDevice getCameraDevice() {
            return mCameraDevice;
        }
    }

    @Override
    protected void setUp() throws Exception {
        super.setUp();

        mCompleted = false;
        getActivity();
        mUiAutomation = InstrumentationRegistry.getInstrumentation().getUiAutomation();
        mContext = InstrumentationRegistry.getTargetContext();
        System.setProperty(""dexmaker.dexcache"", mContext.getCacheDir().toString());
        mActivityManager = mContext.getSystemService(ActivityManager.class);
        mErrorServiceConnection = new ErrorLoggingService.ErrorServiceConnection(mContext);
        mErrorServiceConnection.start();
    }

    @Override
    protected void tearDown() throws Exception {
        if (mProcessPid != -1) {
            android.os.Process.killProcess(mProcessPid);
            mProcessPid = -1;
        }
        if (mErrorServiceConnection != null) {
            mErrorServiceConnection.stop();
            mErrorServiceConnection = null;
        }
        if (mCamera != null) {
            mCamera.release();
            mCamera = null;
        }
        if (mCameraDevice != null) {
            mCameraDevice.close();
            mCameraDevice = null;
        }
        mContext = null;
        mActivityManager = null;
        super.tearDown();
    }

    /**
     * Test basic eviction scenarios for the Camera1 API.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.multiprocess.camera.cts.CameraEvictionTest"	"testBasicCamera2ActivityEvictionOomScoreOffset"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/multiprocess/camera/cts/CameraEvictionTest.java"	""	"public void testBasicCamera2ActivityEvictionOomScoreOffset() throws Throwable {
        testBasicCamera2ActivityEvictionInternal(/*lowerPriority*/ true);
    }
    /**
     * Test basic eviction scenarios for the Camera2 API.
     */
    private void testBasicCamera2ActivityEvictionInternal(boolean lowerPriority) throws Throwable {
        UiAutomation uiAutomation = null;
        if (lowerPriority && mUiAutomation != null) {
            mUiAutomation.adoptShellPermissionIdentity();
        }
        CameraManager manager = mContext.getSystemService(CameraManager.class);
        assertNotNull(manager);
        String[] cameraIds = manager.getCameraIdListNoLazy();

        if (cameraIds.length == 0) {
            Log.i(TAG, ""Skipping testBasicCamera2ActivityEviction, device has no cameras."");
            return;
        }

        assertTrue(mContext.getMainLooper() != null);

        // Setup camera manager
        String chosenCamera = cameraIds[0];
        Handler cameraHandler = new Handler(mContext.getMainLooper());
        final CameraManager.AvailabilityCallback mockAvailCb =
                mock(CameraManager.AvailabilityCallback.class);

        manager.registerAvailabilityCallback(mockAvailCb, cameraHandler);

        Thread.sleep(WAIT_TIME);

        verify(mockAvailCb, times(1)).onCameraAvailable(chosenCamera);
        verify(mockAvailCb, never()).onCameraUnavailable(chosenCamera);

        // Setup camera device
        final CameraDevice.StateCallback spyStateCb = spy(new StateCallbackImpl());
        manager.openCamera(chosenCamera, spyStateCb, cameraHandler);

        verify(spyStateCb, timeout(OPEN_TIMEOUT).times(1)).onOpened(any(CameraDevice.class));
        verify(spyStateCb, never()).onClosed(any(CameraDevice.class));
        verify(spyStateCb, never()).onDisconnected(any(CameraDevice.class));
        verify(spyStateCb, never()).onError(any(CameraDevice.class), anyInt());

        // Open camera from remote process
        startRemoteProcess(Camera2Activity.class, ""camera2ActivityProcess"");

        // Verify that the remote camera was opened correctly
        List<ErrorLoggingService.LogEvent> allEvents  = mErrorServiceConnection.getLog(SETUP_TIMEOUT,
                TestConstants.EVENT_CAMERA_CONNECT);
        assertNotNull(""Camera device not setup in remote process!"", allEvents);

        // Filter out relevant events for other camera devices
        ArrayList<ErrorLoggingService.LogEvent> events = new ArrayList<>();
        for (ErrorLoggingService.LogEvent e : allEvents) {
            int eventTag = e.getEvent();
            if (eventTag == TestConstants.EVENT_CAMERA_UNAVAILABLE ||
                    eventTag == TestConstants.EVENT_CAMERA_CONNECT ||
                    eventTag == TestConstants.EVENT_CAMERA_AVAILABLE) {
                if (!Objects.equals(e.getLogText(), chosenCamera)) {
                    continue;
                }
            }
            events.add(e);
        }
        int[] eventList = new int[events.size()];
        int eventIdx = 0;
        for (ErrorLoggingService.LogEvent e : events) {
            eventList[eventIdx++] = e.getEvent();
        }
        String[] actualEvents = TestConstants.convertToStringArray(eventList);
        String[] expectedEvents = new String[] {TestConstants.EVENT_CAMERA_UNAVAILABLE_STR,
                TestConstants.EVENT_CAMERA_CONNECT_STR};
        String[] ignoredEvents = new String[] { TestConstants.EVENT_CAMERA_AVAILABLE_STR,
                TestConstants.EVENT_CAMERA_UNAVAILABLE_STR };
        assertOrderedEvents(actualEvents, expectedEvents, ignoredEvents);

        // Verify that the local camera was evicted properly
        verify(spyStateCb, times(1)).onDisconnected(any(CameraDevice.class));
        verify(spyStateCb, never()).onClosed(any(CameraDevice.class));
        verify(spyStateCb, never()).onError(any(CameraDevice.class), anyInt());
        verify(spyStateCb, times(1)).onOpened(any(CameraDevice.class));

        // Verify that we can no longer open the camera, as it is held by a higher priority process
       try {
            if (!lowerPriority) {
                manager.openCamera(chosenCamera, spyStateCb, cameraHandler);
            } else {
                // Go to top again, try getting hold of camera with priority lowered, we should get
                // an exception
                Executor cameraExecutor = new HandlerExecutor(cameraHandler);
                forceCtsActivityToTop();
                manager.openCamera(chosenCamera, CACHED_APP_VS_FG_OOM_DELTA, cameraExecutor,
                        spyStateCb);
            }
            fail(""Didn't receive exception when trying to open camera held by higher priority "" +
                    ""process."");
        } catch(CameraAccessException e) {
            assertTrue(""Received incorrect camera exception when opening camera: "" + e,
                    e.getReason() == CameraAccessException.CAMERA_IN_USE);
        }

        // Verify that attempting to open the camera didn't cause anything weird to happen in the
        // other process.
        List<ErrorLoggingService.LogEvent> eventList2 = null;
        boolean timeoutExceptionHit = false;
        try {
            eventList2 = mErrorServiceConnection.getLog(EVICTION_TIMEOUT);
        } catch (TimeoutException e) {
            timeoutExceptionHit = true;
        }

        assertNone(""Remote camera service received invalid events: "", eventList2);
        assertTrue(""Remote camera service exited early"", timeoutExceptionHit);
        android.os.Process.killProcess(mProcessPid);
        mProcessPid = -1;
        forceCtsActivityToTop();
        if (lowerPriority && mUiAutomation != null) {
            mUiAutomation.dropShellPermissionIdentity();
        }
    }

    /**
     * Tests that a client without SYSTEM_CAMERA permissions receives a security exception when
     * trying to modify the oom score for camera framework.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.multiprocess.camera.cts.CameraEvictionTest"	"testCamera2AccessCallback"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/multiprocess/camera/cts/CameraEvictionTest.java"	""	"public void testCamera2AccessCallback() throws Throwable {
        int PERMISSION_CALLBACK_TIMEOUT_MS = 2000;
        CameraManager manager = mContext.getSystemService(CameraManager.class);
        assertNotNull(manager);
        String[] cameraIds = manager.getCameraIdListNoLazy();

        if (cameraIds.length == 0) {
            Log.i(TAG, ""Skipping testCamera2AccessCallback, device has no cameras."");
            return;
        }

        assertTrue(mContext.getMainLooper() != null);

        // Setup camera manager
        Handler cameraHandler = new Handler(mContext.getMainLooper());

        final CameraManager.AvailabilityCallback mockAvailCb =
                mock(CameraManager.AvailabilityCallback.class);
        manager.registerAvailabilityCallback(mockAvailCb, cameraHandler);

        // Remove current task from top of stack. This will impact the camera access
        // pririorties.
        getActivity().moveTaskToBack(/*nonRoot*/true);

        verify(mockAvailCb, timeout(
                PERMISSION_CALLBACK_TIMEOUT_MS).atLeastOnce()).onCameraAccessPrioritiesChanged();

        forceCtsActivityToTop();

        verify(mockAvailCb, timeout(
                PERMISSION_CALLBACK_TIMEOUT_MS).atLeastOnce()).onCameraAccessPrioritiesChanged();
    }

    /**
     * Test native camera availability access callback.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.multiprocess.camera.cts.CameraEvictionTest"	"testMediaRecorderCameraActivityEviction"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/multiprocess/camera/cts/CameraEvictionTest.java"	""	"public void testMediaRecorderCameraActivityEviction() throws Throwable {
        testAPI1ActivityEviction(MediaRecorderCameraActivity.class,
                ""mediaRecorderCameraActivityProcess"");
    }

    /**
     * Test basic eviction scenarios for Camera1 API.
     *
     * This test will open camera, create a higher priority process to run the specified activity,
     * open camera again, and verify the right clients are evicted.
     *
     * @param activityKlass An activity to run in a higher priority process.
     * @param processName The process name.
     */
    private void testAPI1ActivityEviction (java.lang.Class<?> activityKlass, String processName)
            throws Throwable {
        // Open a camera1 client in the main CTS process's activity
        final Camera.ErrorCallback mockErrorCb1 = mock(Camera.ErrorCallback.class);
        final boolean[] skip = {false};
        runTestOnUiThread(new Runnable() {
            @Override
            public void run() {
                // Open camera
                mCamera = Camera.open();
                if (mCamera == null) {
                    skip[0] = true;
                } else {
                    mCamera.setErrorCallback(mockErrorCb1);
                }
                notifyFromUI();
            }
        });
        waitForUI();

        if (skip[0]) {
            Log.i(TAG, ""Skipping testCamera1ActivityEviction, device has no cameras."");
            return;
        }

        verifyZeroInteractions(mockErrorCb1);

        startRemoteProcess(activityKlass, processName);

        // Make sure camera was setup correctly in remote activity
        List<ErrorLoggingService.LogEvent> events = null;
        try {
            events = mErrorServiceConnection.getLog(SETUP_TIMEOUT,
                    TestConstants.EVENT_CAMERA_CONNECT);
        } finally {
            if (events != null) assertOnly(TestConstants.EVENT_CAMERA_CONNECT, events);
        }

        Thread.sleep(WAIT_TIME);

        // Ensure UI thread has a chance to process callbacks.
        runTestOnUiThread(new Runnable() {
            @Override
            public void run() {
                Log.i(""CTS"", ""Did something on UI thread."");
                notifyFromUI();
            }
        });
        waitForUI();

        // Make sure we received correct callback in error listener, and nothing else
        verify(mockErrorCb1, only()).onError(eq(Camera.CAMERA_ERROR_EVICTED), isA(Camera.class));
        mCamera = null;

        // Try to open the camera again (even though other TOP process holds the camera).
        final boolean[] pass = {false};
        runTestOnUiThread(new Runnable() {
            @Override
            public void run() {
                // Open camera
                try {
                    mCamera = Camera.open();
                } catch (RuntimeException e) {
                    pass[0] = true;
                }
                notifyFromUI();
            }
        });
        waitForUI();

        assertTrue(""Did not receive exception when opening camera while camera is held by a"" +
                "" higher priority client process."", pass[0]);

        // Verify that attempting to open the camera didn't cause anything weird to happen in the
        // other process.
        List<ErrorLoggingService.LogEvent> eventList2 = null;
        boolean timeoutExceptionHit = false;
        try {
            eventList2 = mErrorServiceConnection.getLog(EVICTION_TIMEOUT);
        } catch (TimeoutException e) {
            timeoutExceptionHit = true;
        }

        assertNone(""Remote camera service received invalid events: "", eventList2);
        assertTrue(""Remote camera service exited early"", timeoutExceptionHit);
        android.os.Process.killProcess(mProcessPid);
        mProcessPid = -1;
        forceCtsActivityToTop();
    }

    /**
     * Ensure the CTS activity becomes foreground again instead of launcher.
     */
    private void forceCtsActivityToTop() throws InterruptedException {
        Thread.sleep(WAIT_TIME);
        Activity a = getActivity();
        Intent activityIntent = new Intent(a, CameraCtsActivity.class);
        activityIntent.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
        a.startActivity(activityIntent);
        Thread.sleep(WAIT_TIME);
    }

    /**
     * Block until UI thread calls {@link #notifyFromUI()}.
     * @throws InterruptedException
     */
    private void waitForUI() throws InterruptedException {
        synchronized(mLock) {
            if (mCompleted) return;
            while (!mCompleted) {
                mLock.wait();
            }
            mCompleted = false;
        }
    }

    /**
     * Wake up any threads waiting in calls to {@link #waitForUI()}.
     */
    private void notifyFromUI() {
        synchronized (mLock) {
            mCompleted = true;
            mLock.notifyAll();
        }
    }

    /**
     * Return the PID for the process with the given name in the given list of process info.
     *
     * @param processName the name of the process who's PID to return.
     * @param list a list of {@link ActivityManager.RunningAppProcessInfo} to check.
     * @return the PID of the given process, or -1 if it was not included in the list.
     */
    private static int getPid(String processName,
                              List<ActivityManager.RunningAppProcessInfo> list) {
        for (ActivityManager.RunningAppProcessInfo rai : list) {
            if (processName.equals(rai.processName))
                return rai.pid;
        }
        return -1;
    }

    /**
     * Start an activity of the given class running in a remote process with the given name.
     *
     * @param klass the class of the {@link android.app.Activity} to start.
     * @param processName the remote activity name.
     * @throws InterruptedException
     */
    public void startRemoteProcess(java.lang.Class<?> klass, String processName)
            throws InterruptedException {
        // Ensure no running activity process with same name
        Activity a = getActivity();
        String cameraActivityName = a.getPackageName() + "":"" + processName;
        List<ActivityManager.RunningAppProcessInfo> list =
                mActivityManager.getRunningAppProcesses();
        assertEquals(-1, getPid(cameraActivityName, list));

        // Start activity in a new top foreground process
        Intent activityIntent = new Intent(a, klass);
        a.startActivity(activityIntent);
        Thread.sleep(WAIT_TIME);

        // Fail if activity isn't running
        list = mActivityManager.getRunningAppProcesses();
        mProcessPid = getPid(cameraActivityName, list);
        assertTrue(-1 != mProcessPid);
    }

    /**
     * Assert that there is only one event of the given type in the event list.
     *
     * @param event event type to check for.
     * @param events {@link List} of events.
     */
    public static void assertOnly(int event, List<ErrorLoggingService.LogEvent> events) {
        assertTrue(""Remote camera activity never received event: "" + event, events != null);
        for (ErrorLoggingService.LogEvent e : events) {
            assertFalse(""Remote camera activity received invalid event ("" + e +
                    "") while waiting for event: "" + event,
                    e.getEvent() < 0 || e.getEvent() != event);
        }
        assertTrue(""Remote camera activity never received event: "" + event, events.size() >= 1);
        assertTrue(""Remote camera activity received too many "" + event + "" events, received: "" +
                events.size(), events.size() == 1);
    }

    /**
     * Assert there were no logEvents in the given list.
     *
     * @param msg message to show on assertion failure.
     * @param events {@link List} of events.
     */
    public static void assertNone(String msg, List<ErrorLoggingService.LogEvent> events) {
        if (events == null) return;
        StringBuilder builder = new StringBuilder(msg + ""\n"");
        for (ErrorLoggingService.LogEvent e : events) {
            builder.append(e).append(""\n"");
        }
        assertTrue(builder.toString(), events.isEmpty());
    }

    /**
     * Assert array is null or empty.
     *
     * @param array array to check.
     */
    public static <T> void assertNotEmpty(T[] array) {
        assertNotNull(array);
        assertFalse(""Array is empty: "" + Arrays.toString(array), array.length == 0);
    }

    /**
     * Given an 'actual' array of objects, check that the objects given in the 'expected'
     * array are also present in the 'actual' array in the same order.  Objects in the 'actual'
     * array that are not in the 'expected' array are skipped and ignored if they are given
     * in the 'ignored' array, otherwise this assertion will fail.
     *
     * @param actual the ordered array of objects to check.
     * @param expected the ordered array of expected objects.
     * @param ignored the array of objects that will be ignored if present in actual,
     *                but not in expected (or are out of order).
     * @param <T>
     */
    public static <T> void assertOrderedEvents(T[] actual, T[] expected, T[] ignored) {
        assertNotNull(actual);
        assertNotNull(expected);
        assertNotNull(ignored);

        int expIndex = 0;
        int index = 0;
        for (T i : actual) {
            // If explicitly expected, move to next
            if (expIndex < expected.length && Objects.equals(i, expected[expIndex])) {
                expIndex++;
                continue;
            }

            // Fail if not ignored
            boolean canIgnore = false;
            for (T j : ignored) {
                if (Objects.equals(i, j)) {
                    canIgnore = true;
                    break;
                }

            }

            // Fail if not ignored.
            assertTrue(""Event at index "" + index + "" in actual array "" +
                    Arrays.toString(actual) + "" was unexpected: expected array was "" +
                    Arrays.toString(expected) + "", ignored array was: "" +
                    Arrays.toString(ignored), canIgnore);
            index++;
        }
        assertTrue(""Only had "" + expIndex + "" of "" + expected.length +
                "" expected objects in array "" + Arrays.toString(actual) + "", expected was "" +
                Arrays.toString(expected), expIndex == expected.length);
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaExtractorTest"	"testProgramStreamExtraction"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaExtractorTest.java"	""	"public void testProgramStreamExtraction() throws Exception {
        AssetFileDescriptor testFd = getAssetFileDescriptorFor(""programstream.mpeg"");

        MediaExtractor extractor = new MediaExtractor();
        extractor.setDataSource(testFd.getFileDescriptor(), testFd.getStartOffset(),
                testFd.getLength());
        testFd.close();
        assertEquals(""There must be 2 tracks"", 2, extractor.getTrackCount());
        extractor.selectTrack(0);
        extractor.selectTrack(1);
        boolean lastAdvanceResult = true;
        boolean lastReadResult = true;
        int [] bytesRead = new int[2];
        MediaCodec [] codecs = { null, null };

        try {
            MediaFormat f = extractor.getTrackFormat(0);
            codecs[0] = MediaCodec.createDecoderByType(f.getString(MediaFormat.KEY_MIME));
            codecs[0].configure(f, null /* surface */, null /* crypto */, 0 /* flags */);
            codecs[0].start();
        } catch (IOException | IllegalArgumentException e) {
            // ignore
        }
        try {
            MediaFormat f = extractor.getTrackFormat(1);
            codecs[1] = MediaCodec.createDecoderByType(f.getString(MediaFormat.KEY_MIME));
            codecs[1].configure(f, null /* surface */, null /* crypto */, 0 /* flags */);
            codecs[1].start();
        } catch (IOException | IllegalArgumentException e) {
            // ignore
        }

        ByteBuffer buf = ByteBuffer.allocate(2*1024*1024);
        MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
        while(true) {
            for (MediaCodec codec : codecs) {
                if (codec != null) {
                    int idx = codec.dequeueOutputBuffer(info, 5);
                    if (idx >= 0) {
                        codec.releaseOutputBuffer(idx, false);
                    }
                }
            }
            int trackIdx = extractor.getSampleTrackIndex();
            MediaCodec codec = codecs[trackIdx];
            ByteBuffer b = buf;
            int bufIdx = -1;
            if (codec != null) {
                bufIdx = codec.dequeueInputBuffer(-1);
                b = codec.getInputBuffer(bufIdx);
            }
            int n = extractor.readSampleData(b, 0);
            if (n > 0) {
                bytesRead[trackIdx] += n;
            }
            if (codec != null) {
                int sampleFlags = extractor.getSampleFlags();
                long sampleTime = extractor.getSampleTime();
                codec.queueInputBuffer(bufIdx, 0, n, sampleTime, sampleFlags);
            }
            if (!extractor.advance()) {
                break;
            }
        }
        assertTrue(""did not read from track 0"", bytesRead[0] > 0);
        assertTrue(""did not read from track 1"", bytesRead[1] > 0);
        extractor.release();
    }

    private void doTestAdvance(final String res) throws Exception {
        AssetFileDescriptor testFd = getAssetFileDescriptorFor(res);

        MediaExtractor extractor = new MediaExtractor();
        extractor.setDataSource(testFd.getFileDescriptor(), testFd.getStartOffset(),
                testFd.getLength());
        testFd.close();
        extractor.selectTrack(0);
        boolean lastAdvanceResult = true;
        boolean lastReadResult = true;
        ByteBuffer buf = ByteBuffer.allocate(2*1024*1024);
        while(lastAdvanceResult || lastReadResult) {
            int n = extractor.readSampleData(buf, 0);
            if (lastAdvanceResult) {
                // previous advance() was successful, so readSampleData() should succeed
                assertTrue(""readSampleData() failed after successful advance()"", n >= 0);
                assertTrue(""getSampleTime() failed after successful advance()"",
                        extractor.getSampleTime() >= 0);
                assertTrue(""getSampleSize() failed after successful advance()"",
                        extractor.getSampleSize() >= 0);
                assertTrue(""getSampleTrackIndex() failed after successful advance()"",
                        extractor.getSampleTrackIndex() >= 0);
            } else {
                // previous advance() failed, so readSampleData() should fail too
                assertTrue(""readSampleData() succeeded after failed advance()"", n < 0);
                assertTrue(""getSampleTime() succeeded after failed advance()"",
                        extractor.getSampleTime() < 0);
                assertTrue(""getSampleSize() succeeded after failed advance()"",
                        extractor.getSampleSize() < 0);
                assertTrue(""getSampleTrackIndex() succeeded after failed advance()"",
                        extractor.getSampleTrackIndex() < 0);
            }
            lastReadResult = (n >= 0);
            lastAdvanceResult = extractor.advance();
        }
        extractor.release();
    }

    public void SKIP_testAdvance() throws Exception {
        // duplicate of CtsMediaV2TestCases:ExtractorTest$SetDataSourceTest and
        // duplicate of CtsMediaV2TestCases:ExtractorTest$FunctionalityTest#testExtract[*]
        // audio-only
        doTestAdvance(""sinesweepm4a.m4a"");
        doTestAdvance(""sinesweepmp3lame.mp3"");
        doTestAdvance(""sinesweepmp3smpb.mp3"");
        doTestAdvance(""sinesweepwav.wav"");
        doTestAdvance(""sinesweepflac.flac"");
        doTestAdvance(""sinesweepogg.ogg"");
        doTestAdvance(""sinesweepoggmkv.mkv"");

        // video-only
        doTestAdvance(""swirl_144x136_mpeg4.mp4"");
        doTestAdvance(""video_640x360_mp4_hevc_450kbps_no_b.mp4"");

        // audio+video
        doTestAdvance(""video_480x360_mp4_h264_500kbps_30fps_aac_stereo_128kbps_44100hz.mp4"");
        doTestAdvance(""video_1280x720_mkv_h265_500kbps_25fps_aac_stereo_128kbps_44100hz.mkv"");
    }

    private void readAllData() {
        // 1MB is enough for any sample.
        final ByteBuffer buf = ByteBuffer.allocate(1024*1024);
        final int trackCount = mExtractor.getTrackCount();

        for (int i = 0; i < trackCount; i++) {
            mExtractor.selectTrack(i);
        }
        do {
            mExtractor.readSampleData(buf, 0);
        } while (mExtractor.advance());
        mExtractor.seekTo(0, MediaExtractor.SEEK_TO_NEXT_SYNC);
        do {
            mExtractor.readSampleData(buf, 0);
        } while (mExtractor.advance());
    }

    public void SKIP_testAC3inMP4() throws Exception {
        // duplicate of CtsMediaV2TestCases:ExtractorTest$FunctionalityTest[audio/ac3]
        setDataSource(""testac3mp4.mp4"");
        readAllData();
    }

    public void SKIP_testEAC3inMP4() throws Exception {
        // duplicate of CtsMediaV2TestCases:ExtractorTest$FunctionalityTest[audio/eac3]
        setDataSource(""testeac3mp4.mp4"");
        readAllData();
    }

    public void SKIP_testAC3inTS() throws Exception {
        // duplicate of CtsMediaV2TestCases:ExtractorTest$FunctionalityTest[audio/ac3]
        setDataSource(""testac3ts.ts"");
        readAllData();
    }

    public void SKIP_testEAC3inTS() throws Exception {
        // duplicate of CtsMediaV2TestCases:ExtractorTest$FunctionalityTest[audio/eac3]
        setDataSource(""testeac3ts.ts"");
        readAllData();
    }

    public void SKIP_testAC4inMP4() throws Exception {
        // duplicate of CtsMediaV2TestCases:ExtractorTest$FunctionalityTest[audio/ac4]
        setDataSource(""multi0.mp4"");
        readAllData();
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.streamquality.StreamingVideoActivity"	"setPassFailButtonClickListeners"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/streamquality/StreamingVideoActivity.java"	""	"public void test/*
 *.
 */

package com.android.cts.verifier.streamquality;

import com.android.cts.verifier.ArrayTestListAdapter;
import com.android.cts.verifier.PassFailButtons;
import com.android.cts.verifier.R;
import com.android.cts.verifier.TestListAdapter;
import com.android.cts.verifier.TestListAdapter.TestListItem;

import android.app.AlertDialog;
import android.app.Dialog;
import android.content.DialogInterface;
import android.content.DialogInterface.OnClickListener;
import android.content.Intent;
import android.database.DataSetObserver;
import android.os.AsyncTask;
import android.os.Bundle;
import android.util.Log;
import android.widget.TextView;

import java.io.BufferedReader;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.IOException;
import java.io.Serializable;
import java.net.HttpURLConnection;
import java.net.URL;

/**
 * Tests for verifying the quality of streaming videos.  Plays streams of different formats over
 * different protocols for a short amount of time, after which users can mark Pass/Fail depending
 * on the smoothness and subjective quality of the video.
 */
public class StreamingVideoActivity extends PassFailButtons.TestListActivity {
    /**
     * Simple storage class for stream information.
     */
    static class Stream implements Serializable {
        /**
         * Human-readable name for the stream.
         */
        public final String name;

        /**
         * Code name to append to the class name to identify this test.
         */
        public final String code;

        /**
         * URI of the stream
         */
        public final String uri;

        public Stream(String name, String code, String uri) {
            this.name = name;
            this.code = code;
            this.uri = uri;
        }

        @Override
        public boolean equals(Object o) {
            if (this == o) {
                return true;
            } else if (o == null || !(o instanceof Stream)) {
                return false;
            } else {
                Stream stream = (Stream) o;
                return name.equals(stream.name)
                        && code.equals(stream.code)
                        && uri.equals(stream.uri);
            }
        }

        @Override
        public int hashCode() {
            return name.hashCode() ^ uri.hashCode() ^ code.hashCode();
        }

        @Override
        public String toString() {
            return name;
        }
    }

    private static final String TAG = StreamingVideoActivity.class.getName();
    private static final int RTSP_URL_ERROR = 1;
    private static final String ITAG_13_SIGNATURE =
            ""53A3A3A46DAB71E3C599DE8FA6A1484593DFACE3"" +
            "".9476B91AD5035D88C3895CC2A4703B6DD442E972"";
    private static final String ITAG_17_SIGNATURE =
            ""10D6D263112C41DA98822D74821DF47340F1A361"" +
            "".803649A2258E26BF40E76A95E646FBAE4009CEE8"";
    private static final String ITAG_18_SIGNATURE =
            ""618FBB112E1B2FBB66DA9F203AE8CC7DF93C7400"" +
            "".20498AA006E999F42BE69D66E3596F2C7CA18114"";
    private static final String RTSP_LOOKUP_URI_TEMPLATE =
            ""http://redirector.gvt1.com/videoplayback?id=271de9756065677e"" +
            ""&source=youtube&protocol=rtsp&sparams=ip,ipbits,expire,id,itag,source"" +
            ""&ip=0.0.0.0&ipbits=0&expire=19000000000&key=ik0&alr=yes"" +
            ""&itag=%d"" +
            ""&signature=%s"";

    private static final Stream[] HTTP_STREAMS = {
        // TODO(b/150835350): Re-enable H263 test once the clip is updated to profile 0
        /*
        new Stream(""H263 Video, AMR Audio"", ""http_h263_amr"",
                ""http://redirector.gvt1.com/""
                + ""videoplayback?id=271de9756065677e""
                + ""&itag=13&ip=0.0.0.0&ipbits=0&expire=19000000000""
                + ""&sparams=ip,ipbits,expire,id,itag""
                + ""&signature=073A731E2BDF1E05206AC7B9B895C922ABCBA01D""
                + "".1DDA3F999541D2136E6755F16FC44CA972767169""
                + ""&source=youtube""
                + ""&key=ik0&user=android-device-test""),
        */
        new Stream(""MPEG4 SP Video, AAC Audio"", ""http_mpeg4_aac"",
                ""http://redirector.gvt1.com/""
                + ""videoplayback?id=271de9756065677e""
                + ""&itag=17&ip=0.0.0.0&ipbits=0&expire=19000000000""
                + ""&sparams=ip,ipbits,expire,id,itag""
                + ""&signature=6B0F8B8A6A7FD9E4CDF123349C2E061ED2020D74""
                + "".3460FC81D6C8894BA2D241597D2E1D059845F5F0""
                + ""&source=youtube""
                + ""&key=ik0&user=android-device-test""),
        new Stream(""H264 Base Video, AAC Audio"", ""http_h264_aac"",
                ""http://redirector.gvt1.com/""
                + ""videoplayback?id=271de9756065677e""
                + ""&itag=18&ip=0.0.0.0&ipbits=0&expire=19000000000""
                + ""&sparams=ip,ipbits,expire,id,itag""
                + ""&signature=75627CD4CEA73D7868CBDE3CE5C4011955164107""
                + "".1DCFB0EF1372B48DDCFBE69645FE137AC02AF561""
                + ""&source=youtube""
                + ""&key=ik0&user=android-device-test""),
    };

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.pass_fail_list);
        setPassFailButtonClickListeners();
        setInfoResources(R.string.streaming_video, R.string.streaming_video_info, -1);

        TextView empty = (TextView) findViewById(android.R.id.empty);
        empty.setText(R.string.sv_no_data);

        getPassButton().setEnabled(false);
        setTestListAdapter(getStreamAdapter());
    }

    @Override
    public Dialog onCreateDialog(int id, Bundle args) {
        switch (id) {
            case RTSP_URL_ERROR:
                return new AlertDialog.Builder(this)
                        .setTitle(getString(R.string.sv_failed_title))
                        .setMessage(getString(R.string.sv_failed_message))
                        .setNegativeButton(""Close"", new OnClickListener() {
                            @Override
                            public void onClick(DialogInterface dialog, int which) {
                                setTestResultAndFinish(false);
                            }
                        }).show();
            default:
                return super.onCreateDialog(id, args);
        }
    }

    private TestListAdapter getStreamAdapter() {
        ArrayTestListAdapter adapter = new ArrayTestListAdapter(this);

        // TODO(b/161675976): Re-enable RTSP tests once they can be served reliably.
        /*
        adapter.add(TestListItem.newCategory(""RTSP""));
        addRtspStreamToTest(
                adapter, ""H263 Video, AMR Audio"", ""rtsp_h263_amr"", 13, ITAG_13_SIGNATURE);
        addRtspStreamToTest(
                adapter, ""MPEG4 SP Video, AAC Audio"", ""rtsp_mpeg4_aac"", 17, ITAG_17_SIGNATURE);
        addRtspStreamToTest(
                adapter, ""H264 Base Video, AAC Audio"", ""rtsp_h264_aac"", 18, ITAG_18_SIGNATURE);
        */

        adapter.add(TestListItem.newCategory(""HTTP Progressive""));
        for (Stream stream : HTTP_STREAMS) {
            addStreamToTests(adapter, stream);
        }

        adapter.registerDataSetObserver(new DataSetObserver() {
            @Override
            public void onChanged() {
                updatePassButton();
            }
        });

        return adapter;
    }

    private void addRtspStreamToTest(
            ArrayTestListAdapter adapter, String name, String code, int itag, String signature) {
        String rtspUrl = lookupRtspUrl(itag, signature);
        if (rtspUrl == null) {
            showDialog(RTSP_URL_ERROR);
        }
        Stream stream = new Stream(name, code, rtspUrl);
        addStreamToTests(adapter, stream);
    }

    private void addStreamToTests(ArrayTestListAdapter streams, Stream stream) {
        Intent i = new Intent(StreamingVideoActivity.this, PlayVideoActivity.class);
        i.putExtra(PlayVideoActivity.EXTRA_STREAM, stream);
        streams.add(TestListItem.newTest(stream.name, PlayVideoActivity.getTestId(stream.code),
                i, null));
    }

    /** @returns the appropriate RTSP url, or null in case of failure */
    private String lookupRtspUrl(int itag, String signature) {
        String rtspLookupUri = String.format(RTSP_LOOKUP_URI_TEMPLATE, itag, signature);
        try {
            return new LookupRtspUrlTask().execute(rtspLookupUri).get();
        } catch (Exception e) {
            Log.e(TAG, ""RTSP URL lookup time out."", e);
            showDialog(RTSP_URL_ERROR);
            return null;
        }
    }

    /** Retrieve the URL for an RTSP stream */
    private class LookupRtspUrlTask extends AsyncTask<String, Void, String> {
        protected String doInBackground(String... rtspLookupUri) {
            HttpURLConnection urlConnection = null;
            try {
                URL url = new URL(rtspLookupUri[0]);
                urlConnection = (HttpURLConnection) url.openConnection();
                if (urlConnection.getResponseCode() != 200) {
                    throw new IOException(""unable to get rtsp uri. Response Code:""
                            + urlConnection.getResponseCode());
                }
                BufferedReader reader = new BufferedReader(
                        new InputStreamReader(urlConnection.getInputStream()));
                return reader.readLine();
            } catch (Exception e) {
                Log.e(TAG, ""RTSP URL lookup failed."", e);
                return null;
            } finally {
                if (urlConnection != null) {
                    urlConnection.disconnect();
                }
            }
        }
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testSetPlaybackParamsPositiveSpeed"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testSetPlaybackParamsPositiveSpeed() throws Exception {
        if (!checkLoadResource(
                ""video_480x360_mp4_h264_1000kbps_30fps_aac_stereo_128kbps_44100hz.mp4"")) {
            return; // skip
        }

        mMediaPlayer.setOnSeekCompleteListener(mp -> mOnSeekCompleteCalled.signal());
        mOnCompletionCalled.reset();
        mMediaPlayer.setOnCompletionListener(mp -> mOnCompletionCalled.signal());
        mMediaPlayer.setDisplay(mActivity.getSurfaceHolder());

        mMediaPlayer.prepare();

        mOnSeekCompleteCalled.reset();
        mMediaPlayer.seekTo(0);
        mOnSeekCompleteCalled.waitForSignal();

        final float playbackRate = 1.0f;

        int playTime = 2000;  // The testing clip is about 10 second long.
        mMediaPlayer.setPlaybackParams(new PlaybackParams().setSpeed(playbackRate));
        assertTrue(""MediaPlayer should be playing"", mMediaPlayer.isPlaying());
        Thread.sleep(playTime);
        assertTrue(""MediaPlayer should still be playing"",
                mMediaPlayer.getCurrentPosition() > 0);

        int duration = mMediaPlayer.getDuration();
        mOnSeekCompleteCalled.reset();
        mMediaPlayer.seekTo(duration - 1000);
        mOnSeekCompleteCalled.waitForSignal();

        mOnCompletionCalled.waitForSignal();
        assertFalse(""MediaPlayer should not be playing"", mMediaPlayer.isPlaying());
        int eosPosition = mMediaPlayer.getCurrentPosition();

        mMediaPlayer.setPlaybackParams(new PlaybackParams().setSpeed(playbackRate));
        assertTrue(""MediaPlayer should be playing after EOS"", mMediaPlayer.isPlaying());
        Thread.sleep(playTime);
        int position = mMediaPlayer.getCurrentPosition();
        assertTrue(""MediaPlayer should still be playing after EOS"",
                position > 0 && position < eosPosition);

        mMediaPlayer.stop();
    }

    // setPlaybackParams() with zero speed should pause playback."	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testSetPlaybackParamsZeroSpeed"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testSetPlaybackParamsZeroSpeed() throws Exception {
        if (!checkLoadResource(
                ""video_480x360_mp4_h264_1000kbps_30fps_aac_stereo_128kbps_44100hz.mp4"")) {
            return; // skip
        }

        mMediaPlayer.setOnSeekCompleteListener(mp -> mOnSeekCompleteCalled.signal());
        mMediaPlayer.setDisplay(mActivity.getSurfaceHolder());

        mMediaPlayer.prepare();

        mMediaPlayer.setPlaybackParams(new PlaybackParams().setSpeed(0.0f));
        assertFalse(""MediaPlayer should not be playing"", mMediaPlayer.isPlaying());

        int playTime = 2000;  // The testing clip is about 10 second long.
        mOnSeekCompleteCalled.reset();
        mMediaPlayer.seekTo(0);
        mOnSeekCompleteCalled.waitForSignal();
        Thread.sleep(playTime);
        assertFalse(""MediaPlayer should not be playing"", mMediaPlayer.isPlaying());
        assertEquals(""MediaPlayer position should be 0"", 0, mMediaPlayer.getCurrentPosition());

        mMediaPlayer.start();
        Thread.sleep(playTime);
        assertTrue(""MediaPlayer should be playing"", mMediaPlayer.isPlaying());
        assertTrue(""MediaPlayer position should be > 0"", mMediaPlayer.getCurrentPosition() > 0);

        mMediaPlayer.setPlaybackParams(new PlaybackParams().setSpeed(0.0f));
        assertFalse(""MediaPlayer should not be playing"", mMediaPlayer.isPlaying());
        Thread.sleep(1000);
        int position = mMediaPlayer.getCurrentPosition();
        Thread.sleep(playTime);
        assertEquals(""MediaPlayer should be paused"", mMediaPlayer.getCurrentPosition(), position);

        mMediaPlayer.stop();
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testPlaybackRate"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testPlaybackRate() throws Exception {
        final int toleranceMs = 1000;
        if (!checkLoadResource(
                ""video_480x360_mp4_h264_1000kbps_30fps_aac_stereo_128kbps_44100hz.mp4"")) {
            return; // skip
        }

        mMediaPlayer.setDisplay(mActivity.getSurfaceHolder());
        mMediaPlayer.prepare();
        SyncParams sync = new SyncParams().allowDefaults();
        mMediaPlayer.setSyncParams(sync);
        sync = mMediaPlayer.getSyncParams();

        float[] rates = { 0.25f, 0.5f, 1.0f, 2.0f };
        for (float playbackRate : rates) {
            mMediaPlayer.seekTo(0);
            Thread.sleep(1000);
            int playTime = 4000;  // The testing clip is about 10 second long.
            mMediaPlayer.setPlaybackParams(new PlaybackParams().setSpeed(playbackRate));
            mMediaPlayer.start();
            Thread.sleep(playTime);
            PlaybackParams pbp = mMediaPlayer.getPlaybackParams();
            assertEquals(
                    playbackRate, pbp.getSpeed(),
                    FLOAT_TOLERANCE + playbackRate * sync.getTolerance());
            assertTrue(""MediaPlayer should still be playing"", mMediaPlayer.isPlaying());

            int playedMediaDurationMs = mMediaPlayer.getCurrentPosition();
            int diff = Math.abs((int)(playedMediaDurationMs / playbackRate) - playTime);
            if (diff > toleranceMs) {
                fail(""Media player had error in playback rate "" + playbackRate
                     + "", play time is "" + playTime + "" vs expected "" + playedMediaDurationMs);
            }
            mMediaPlayer.pause();
            pbp = mMediaPlayer.getPlaybackParams();
            assertEquals(0.f, pbp.getSpeed(), FLOAT_TOLERANCE);
        }
        mMediaPlayer.stop();
    }

    @Presubmit"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testSeekModes"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testSeekModes() throws Exception {
        // This clip has 2 I frames at 66687us and 4299687us.
        if (!checkLoadResource(
                ""bbb_s1_320x240_mp4_h264_mp2_800kbps_30fps_aac_lc_5ch_240kbps_44100hz.mp4"")) {
            return; // skip
        }

        mMediaPlayer.setOnSeekCompleteListener(mp -> mOnSeekCompleteCalled.signal());
        mMediaPlayer.setDisplay(mActivity.getSurfaceHolder());
        mMediaPlayer.prepare();
        mOnSeekCompleteCalled.reset();
        mMediaPlayer.start();

        final int seekPosMs = 3000;
        final int timeToleranceMs = 100;
        final int syncTime1Ms = 67;
        final int syncTime2Ms = 4300;

        // TODO: tighten checking range. For now, ensure mediaplayer doesn't
        // seek to previous sync or next sync.
        int cp = runSeekMode(MediaPlayer.SEEK_CLOSEST, seekPosMs);
        assertTrue(""MediaPlayer did not seek to closest position"",
                cp > seekPosMs && cp < syncTime2Ms);

        // TODO: tighten checking range. For now, ensure mediaplayer doesn't
        // seek to closest position or next sync.
        cp = runSeekMode(MediaPlayer.SEEK_PREVIOUS_SYNC, seekPosMs);
        assertTrue(""MediaPlayer did not seek to preivous sync position"",
                cp < seekPosMs - timeToleranceMs);

        // TODO: tighten checking range. For now, ensure mediaplayer doesn't
        // seek to closest position or previous sync.
        cp = runSeekMode(MediaPlayer.SEEK_NEXT_SYNC, seekPosMs);
        assertTrue(""MediaPlayer did not seek to next sync position"",
                cp > syncTime2Ms - timeToleranceMs);

        // TODO: tighten checking range. For now, ensure mediaplayer doesn't
        // seek to closest position or previous sync.
        cp = runSeekMode(MediaPlayer.SEEK_CLOSEST_SYNC, seekPosMs);
        assertTrue(""MediaPlayer did not seek to closest sync position"",
                cp > syncTime2Ms - timeToleranceMs);

        mMediaPlayer.stop();
    }

    private int runSeekMode(int seekMode, int seekPosMs) throws Exception {
        final int sleepIntervalMs = 100;
        int timeRemainedMs = 10000;  // total time for testing
        final int timeToleranceMs = 100;

        mMediaPlayer.seekTo(seekPosMs, seekMode);
        mOnSeekCompleteCalled.waitForSignal();
        mOnSeekCompleteCalled.reset();
        int cp = -seekPosMs;
        while (timeRemainedMs > 0) {
            cp = mMediaPlayer.getCurrentPosition();
            // Wait till MediaPlayer starts rendering since MediaPlayer caches
            // seek position as current position.
            if (cp < seekPosMs - timeToleranceMs || cp > seekPosMs + timeToleranceMs) {
                break;
            }
            timeRemainedMs -= sleepIntervalMs;
            Thread.sleep(sleepIntervalMs);
        }
        assertTrue(""MediaPlayer did not finish seeking in time for mode "" + seekMode,
                timeRemainedMs > 0);
        return cp;
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testGetTimestamp"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testGetTimestamp() throws Exception {
        final int toleranceUs = 100000;
        final float playbackRate = 1.0f;
        if (!checkLoadResource(
                ""video_480x360_mp4_h264_1000kbps_30fps_aac_stereo_128kbps_44100hz.mp4"")) {
            return; // skip
        }

        mMediaPlayer.setDisplay(mActivity.getSurfaceHolder());
        mMediaPlayer.prepare();
        mMediaPlayer.start();
        mMediaPlayer.setPlaybackParams(new PlaybackParams().setSpeed(playbackRate));
        Thread.sleep(SLEEP_TIME);  // let player get into stable state.
        long nt1 = System.nanoTime();
        MediaTimestamp ts1 = mMediaPlayer.getTimestamp();
        long nt2 = System.nanoTime();
        assertNotNull(""Media player should return a valid time stamp"", ts1);
        assertEquals(""MediaPlayer had error in clockRate "" + ts1.getMediaClockRate(),
                playbackRate, ts1.getMediaClockRate(), 0.001f);
        assertTrue(""The nanoTime of Media timestamp should be taken when getTimestamp is called."",
                nt1 <= ts1.getAnchorSystemNanoTime() && ts1.getAnchorSystemNanoTime() <= nt2);

        mMediaPlayer.pause();
        ts1 = mMediaPlayer.getTimestamp();
        assertNotNull(""Media player should return a valid time stamp"", ts1);
        assertEquals(""Media player should have play rate of 0.0f when paused"", 0.0f,
                ts1.getMediaClockRate());

        mMediaPlayer.seekTo(0);
        mMediaPlayer.start();
        Thread.sleep(SLEEP_TIME);  // let player get into stable state.
        int playTime = 4000;  // The testing clip is about 10 second long.
        ts1 = mMediaPlayer.getTimestamp();
        assertNotNull(""Media player should return a valid time stamp"", ts1);
        Thread.sleep(playTime);
        MediaTimestamp ts2 = mMediaPlayer.getTimestamp();
        assertNotNull(""Media player should return a valid time stamp"", ts2);
        assertEquals(""The clockRate should not be changed."", ts1.getMediaClockRate(),
                ts2.getMediaClockRate());
        assertEquals(""MediaPlayer had error in timestamp."",
                ts1.getAnchorMediaTimeUs() + (long)(playTime * ts1.getMediaClockRate() * 1000),
                ts2.getAnchorMediaTimeUs(), toleranceUs);

        mMediaPlayer.stop();
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testMediaTimeDiscontinuity"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testMediaTimeDiscontinuity() throws Exception {
        if (!checkLoadResource(
                ""bbb_s1_320x240_mp4_h264_mp2_800kbps_30fps_aac_lc_5ch_240kbps_44100hz.mp4"")) {
            return; // skip
        }

        mMediaPlayer.setOnSeekCompleteListener(mp -> mOnSeekCompleteCalled.signal());
        final BlockingDeque<MediaTimestamp> timestamps = new LinkedBlockingDeque<>();
        mMediaPlayer.setOnMediaTimeDiscontinuityListener(
                (mp, timestamp) -> {
                    mOnMediaTimeDiscontinuityCalled.signal();
                    timestamps.add(timestamp);
                });
        mMediaPlayer.setDisplay(mActivity.getSurfaceHolder());
        mMediaPlayer.prepare();

        // Timestamp needs to be reported when playback starts.
        mOnMediaTimeDiscontinuityCalled.reset();
        mMediaPlayer.start();
        do {
            assertTrue(mOnMediaTimeDiscontinuityCalled.waitForSignal(1000));
        } while (timestamps.getLast().getMediaClockRate() != 1.0f);

        // Timestamp needs to be reported when seeking is done.
        mOnSeekCompleteCalled.reset();
        mOnMediaTimeDiscontinuityCalled.reset();
        mMediaPlayer.seekTo(3000);
        mOnSeekCompleteCalled.waitForSignal();
        do {
            assertTrue(mOnMediaTimeDiscontinuityCalled.waitForSignal(1000));
        } while (timestamps.getLast().getMediaClockRate() != 1.0f);

        // Timestamp needs to be updated when playback rate changes.
        mOnMediaTimeDiscontinuityCalled.reset();
        mMediaPlayer.setPlaybackParams(new PlaybackParams().setSpeed(0.5f));
        do {
            assertTrue(mOnMediaTimeDiscontinuityCalled.waitForSignal(1000));
        } while (timestamps.getLast().getMediaClockRate() != 0.5f);

        // Timestamp needs to be updated when player is paused.
        mOnMediaTimeDiscontinuityCalled.reset();
        mMediaPlayer.pause();
        do {
            assertTrue(mOnMediaTimeDiscontinuityCalled.waitForSignal(1000));
        } while (timestamps.getLast().getMediaClockRate() != 0.0f);

        // Check if there is no more notification after clearing listener.
        mMediaPlayer.clearOnMediaTimeDiscontinuityListener();
        mMediaPlayer.start();
        mOnMediaTimeDiscontinuityCalled.reset();
        Thread.sleep(1000);
        assertEquals(0, mOnMediaTimeDiscontinuityCalled.getNumSignal());

        mMediaPlayer.reset();
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_MKV_H265_1280x720_500kbps_25fps_AAC_Stereo_128kbps_44100Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_MKV_H265_1280x720_500kbps_25fps_AAC_Stereo_128kbps_44100Hz()
            throws Exception {
        playLoadedVideoTest(""video_1280x720_mkv_h265_500kbps_25fps_aac_stereo_128kbps_44100hz.mkv"",
                1280, 720);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_MP4_H264_480x360_500kbps_25fps_AAC_Stereo_128kbps_44110Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_MP4_H264_480x360_500kbps_25fps_AAC_Stereo_128kbps_44110Hz()
            throws Exception {
        playLoadedVideoTest(""video_480x360_mp4_h264_500kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                480, 360);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_MP4_H264_480x360_500kbps_30fps_AAC_Stereo_128kbps_44110Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_MP4_H264_480x360_500kbps_30fps_AAC_Stereo_128kbps_44110Hz()
            throws Exception {
        playLoadedVideoTest(""video_480x360_mp4_h264_500kbps_30fps_aac_stereo_128kbps_44100hz.mp4"",
                480, 360);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_MP4_H264_480x360_1000kbps_25fps_AAC_Stereo_128kbps_44110Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_MP4_H264_480x360_1000kbps_25fps_AAC_Stereo_128kbps_44110Hz()
            throws Exception {
        playLoadedVideoTest(""video_480x360_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                480, 360);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_MP4_H264_480x360_1000kbps_30fps_AAC_Stereo_128kbps_44110Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_MP4_H264_480x360_1000kbps_30fps_AAC_Stereo_128kbps_44110Hz()
            throws Exception {
        playLoadedVideoTest(""video_480x360_mp4_h264_1000kbps_30fps_aac_stereo_128kbps_44100hz.mp4"",
                480, 360);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_MP4_H264_480x360_1350kbps_25fps_AAC_Stereo_128kbps_44110Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_MP4_H264_480x360_1350kbps_25fps_AAC_Stereo_128kbps_44110Hz()
            throws Exception {
        playLoadedVideoTest(""video_480x360_mp4_h264_1350kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                480, 360);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_MP4_H264_480x360_1350kbps_30fps_AAC_Stereo_128kbps_44110Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_MP4_H264_480x360_1350kbps_30fps_AAC_Stereo_128kbps_44110Hz()
            throws Exception {
        playLoadedVideoTest(""video_480x360_mp4_h264_1350kbps_30fps_aac_stereo_128kbps_44100hz.mp4"",
                480, 360);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_MP4_H264_480x360_1350kbps_30fps_AAC_Stereo_128kbps_44110Hz_frag"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_MP4_H264_480x360_1350kbps_30fps_AAC_Stereo_128kbps_44110Hz_frag()
            throws Exception {
        playLoadedVideoTest(
                ""video_480x360_mp4_h264_1350kbps_30fps_aac_stereo_128kbps_44100hz_fragmented.mp4"",
                480, 360);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_MP4_H264_480x360_1350kbps_30fps_AAC_Stereo_192kbps_44110Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_MP4_H264_480x360_1350kbps_30fps_AAC_Stereo_192kbps_44110Hz()
            throws Exception {
        playLoadedVideoTest(""video_480x360_mp4_h264_1350kbps_30fps_aac_stereo_192kbps_44100hz.mp4"",
                480, 360);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_56kbps_12fps_AAC_Mono_24kbps_11025Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_56kbps_12fps_AAC_Mono_24kbps_11025Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_56kbps_12fps_aac_mono_24kbps_11025hz.3gp"", 176,
                144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_56kbps_12fps_AAC_Mono_24kbps_22050Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_56kbps_12fps_AAC_Mono_24kbps_22050Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_56kbps_12fps_aac_mono_24kbps_22050hz.3gp"", 176,
                144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_56kbps_12fps_AAC_Stereo_24kbps_11025Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_56kbps_12fps_AAC_Stereo_24kbps_11025Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_56kbps_12fps_aac_stereo_24kbps_11025hz.3gp"",
                176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_56kbps_12fps_AAC_Stereo_24kbps_22050Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_56kbps_12fps_AAC_Stereo_24kbps_22050Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_56kbps_12fps_aac_stereo_24kbps_22050hz.3gp"",
                176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_56kbps_12fps_AAC_Stereo_128kbps_11025Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_56kbps_12fps_AAC_Stereo_128kbps_11025Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_56kbps_12fps_aac_stereo_128kbps_11025hz.3gp"",
                176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_56kbps_12fps_AAC_Stereo_128kbps_22050Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_56kbps_12fps_AAC_Stereo_128kbps_22050Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_56kbps_12fps_aac_stereo_128kbps_22050hz.3gp"",
                176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_56kbps_25fps_AAC_Mono_24kbps_11025Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_56kbps_25fps_AAC_Mono_24kbps_11025Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_56kbps_25fps_aac_mono_24kbps_11025hz.3gp"", 176,
                144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_56kbps_25fps_AAC_Mono_24kbps_22050Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_56kbps_25fps_AAC_Mono_24kbps_22050Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_56kbps_25fps_aac_mono_24kbps_22050hz.3gp"", 176,
                144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_56kbps_25fps_AAC_Stereo_24kbps_11025Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_56kbps_25fps_AAC_Stereo_24kbps_11025Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_56kbps_25fps_aac_stereo_24kbps_11025hz.3gp"",
                176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_56kbps_25fps_AAC_Stereo_24kbps_22050Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_56kbps_25fps_AAC_Stereo_24kbps_22050Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_56kbps_25fps_aac_stereo_24kbps_22050hz.3gp"",
                176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_56kbps_25fps_AAC_Stereo_128kbps_11025Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_56kbps_25fps_AAC_Stereo_128kbps_11025Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_56kbps_25fps_aac_stereo_128kbps_11025hz.3gp"",
                176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_56kbps_25fps_AAC_Stereo_128kbps_22050Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_56kbps_25fps_AAC_Stereo_128kbps_22050Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_56kbps_25fps_aac_stereo_128kbps_22050hz.3gp"",
                176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_300kbps_12fps_AAC_Mono_24kbps_11025Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_300kbps_12fps_AAC_Mono_24kbps_11025Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_300kbps_12fps_aac_mono_24kbps_11025hz.3gp"", 176,
                144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_300kbps_12fps_AAC_Mono_24kbps_22050Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_300kbps_12fps_AAC_Mono_24kbps_22050Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_300kbps_12fps_aac_mono_24kbps_22050hz.3gp"", 176,
                144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_300kbps_12fps_AAC_Stereo_24kbps_11025Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_300kbps_12fps_AAC_Stereo_24kbps_11025Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_300kbps_12fps_aac_stereo_24kbps_11025hz.3gp"",
                176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_300kbps_12fps_AAC_Stereo_24kbps_22050Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_300kbps_12fps_AAC_Stereo_24kbps_22050Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_300kbps_12fps_aac_stereo_24kbps_22050hz.3gp"",
                176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_300kbps_12fps_AAC_Stereo_128kbps_11025Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_300kbps_12fps_AAC_Stereo_128kbps_11025Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_300kbps_12fps_aac_stereo_128kbps_11025hz.3gp"",
                176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_300kbps_12fps_AAC_Stereo_128kbps_22050Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_300kbps_12fps_AAC_Stereo_128kbps_22050Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_300kbps_12fps_aac_stereo_128kbps_22050hz.3gp"",
                176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_300kbps_25fps_AAC_Mono_24kbps_11025Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_300kbps_25fps_AAC_Mono_24kbps_11025Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_300kbps_25fps_aac_mono_24kbps_11025hz.3gp"", 176,
                144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_300kbps_25fps_AAC_Mono_24kbps_22050Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_300kbps_25fps_AAC_Mono_24kbps_22050Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_300kbps_25fps_aac_mono_24kbps_22050hz.3gp"", 176,
                144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_300kbps_25fps_AAC_Stereo_24kbps_11025Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_300kbps_25fps_AAC_Stereo_24kbps_11025Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_300kbps_25fps_aac_stereo_24kbps_11025hz.3gp"",
                176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_300kbps_25fps_AAC_Stereo_24kbps_22050Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_300kbps_25fps_AAC_Stereo_24kbps_22050Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_300kbps_25fps_aac_stereo_24kbps_22050hz.3gp"",
                176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_300kbps_25fps_AAC_Stereo_128kbps_11025Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_300kbps_25fps_AAC_Stereo_128kbps_11025Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_300kbps_25fps_aac_stereo_128kbps_11025hz.3gp"",
                176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testLocalVideo_3gp_H263_176x144_300kbps_25fps_AAC_Stereo_128kbps_22050Hz"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testLocalVideo_3gp_H263_176x144_300kbps_25fps_AAC_Stereo_128kbps_22050Hz()
            throws Exception {
        playLoadedVideoTest(""video_176x144_3gp_h263_300kbps_25fps_aac_stereo_128kbps_22050hz.3gp"",
                176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testResumeAtEnd"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testResumeAtEnd() throws Throwable {
        int testsRun =
            testResumeAtEnd(""loudsoftmp3.mp3"") +
            testResumeAtEnd(""loudsoftwav.wav"") +
            testResumeAtEnd(""loudsoftogg.ogg"") +
            testResumeAtEnd(""loudsoftitunes.m4a"") +
            testResumeAtEnd(""loudsoftfaac.m4a"") +
            testResumeAtEnd(""loudsoftaac.aac"");
        if (testsRun == 0) {
            MediaUtils.skipTest(""no decoder found"");
        }
    }

    // returns 1 if test was run, 0 otherwise
    private int testResumeAtEnd(final String res) throws Throwable {
        if (!loadResource(res)) {
            Log.i(LOG_TAG, ""testResumeAtEnd: No decoder found for "" + res + "" --- skipping."");
            return 0; // skip
        }
        mMediaPlayer.prepare();
        mOnCompletionCalled.reset();
        mMediaPlayer.setOnCompletionListener(mp -> {
            mOnCompletionCalled.signal();
            mMediaPlayer.start();
        });
        // skip the first part of the file so we reach EOF sooner
        mMediaPlayer.seekTo(5000);
        mMediaPlayer.start();
        // sleep long enough that we restart playback at least once, but no more
        Thread.sleep(10000);
        assertTrue(""MediaPlayer should still be playing"", mMediaPlayer.isPlaying());
        mMediaPlayer.reset();
        assertEquals(""wrong number of repetitions"", 1, mOnCompletionCalled.getNumSignal());
        return 1;
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testPositionAtEnd"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testPositionAtEnd() throws Throwable {
        int testsRun =
            testPositionAtEnd(""test1m1shighstereo.mp3"") +
            testPositionAtEnd(""loudsoftmp3.mp3"") +
            testPositionAtEnd(""loudsoftwav.wav"") +
            testPositionAtEnd(""loudsoftogg.ogg"") +
            testPositionAtEnd(""loudsoftitunes.m4a"") +
            testPositionAtEnd(""loudsoftfaac.m4a"") +
            testPositionAtEnd(""loudsoftaac.aac"");
        if (testsRun == 0) {
            MediaUtils.skipTest(LOG_TAG, ""no decoder found"");
        }
    }

    private int testPositionAtEnd(final String res) throws Throwable {
        if (!loadResource(res)) {
            Log.i(LOG_TAG, ""testPositionAtEnd: No decoder found for "" + res + "" --- skipping."");
            return 0; // skip
        }
        mMediaPlayer.setAudioStreamType(AudioManager.STREAM_MUSIC);
        mMediaPlayer.prepare();
        int duration = mMediaPlayer.getDuration();
        assertTrue(""resource too short"", duration > 6000);
        mOnCompletionCalled.reset();
        mMediaPlayer.setOnCompletionListener(mp -> mOnCompletionCalled.signal());
        mMediaPlayer.seekTo(duration - 5000);
        mMediaPlayer.start();
        while (mMediaPlayer.isPlaying()) {
            Log.i(""@@@@"", ""position: "" + mMediaPlayer.getCurrentPosition());
            Thread.sleep(500);
        }
        Log.i(""@@@@"", ""final position: "" + mMediaPlayer.getCurrentPosition());
        assertTrue(mMediaPlayer.getCurrentPosition() > duration - 1000);
        mMediaPlayer.reset();
        return 1;
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testPlaybackFromAMediaDataSource"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testPlaybackFromAMediaDataSource() throws Exception {
        final String res = ""video_480x360_mp4_h264_1350kbps_30fps_aac_stereo_192kbps_44100hz.mp4"";
        final int duration = 10000;

        Preconditions.assertTestFileExists(mInpPrefix + res);
        if (!MediaUtils.hasCodecsForResource(mInpPrefix + res)) {
            return;
        }

        TestMediaDataSource dataSource =
                TestMediaDataSource.fromAssetFd(getAssetFileDescriptorFor(res));
        // Test returning -1 from getSize() to indicate unknown size.
        dataSource.returnFromGetSize(-1);
        mMediaPlayer.setDataSource(dataSource);
        playLoadedVideo(null, null, -1);
        assertTrue(mMediaPlayer.isPlaying());

        // Test pause and restart.
        mMediaPlayer.pause();
        Thread.sleep(SLEEP_TIME);
        assertFalse(mMediaPlayer.isPlaying());
        mMediaPlayer.start();
        assertTrue(mMediaPlayer.isPlaying());

        // Test reset.
        mMediaPlayer.stop();
        mMediaPlayer.reset();
        mMediaPlayer.setDataSource(dataSource);
        mMediaPlayer.prepare();
        mMediaPlayer.start();
        assertTrue(mMediaPlayer.isPlaying());

        // Test seek. Note: the seek position is cached and returned as the
        // current position so there's no point in comparing them.
        mMediaPlayer.seekTo(duration - SLEEP_TIME);
        while (mMediaPlayer.isPlaying()) {
            Thread.sleep(SLEEP_TIME);
        }
    }

    @Presubmit"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testPlaybackFailsIfMediaDataSourceThrows"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testPlaybackFailsIfMediaDataSourceThrows() throws Exception {
        final String res = ""video_480x360_mp4_h264_1350kbps_30fps_aac_stereo_192kbps_44100hz.mp4"";
        Preconditions.assertTestFileExists(mInpPrefix + res);
        if (!MediaUtils.hasCodecsForResource(mInpPrefix + res)) {
            return;
        }

        setOnErrorListener();
        TestMediaDataSource dataSource =
                TestMediaDataSource.fromAssetFd(getAssetFileDescriptorFor(res));
        mMediaPlayer.setDataSource(dataSource);
        mMediaPlayer.prepare();

        dataSource.throwFromReadAt();
        mMediaPlayer.start();
        assertTrue(mOnErrorCalled.waitForSignal());
    }

    @Presubmit"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaPlayerTest"	"testPlaybackFailsIfMediaDataSourceReturnsAnError"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaPlayerTest.java"	""	"public void testPlaybackFailsIfMediaDataSourceReturnsAnError() throws Exception {
        final String res = ""video_480x360_mp4_h264_1350kbps_30fps_aac_stereo_192kbps_44100hz.mp4"";
        Preconditions.assertTestFileExists(mInpPrefix + res);
        if (!MediaUtils.hasCodecsForResource(mInpPrefix + res)) {
            return;
        }

        setOnErrorListener();
        TestMediaDataSource dataSource =
                TestMediaDataSource.fromAssetFd(getAssetFileDescriptorFor(res));
        mMediaPlayer.setDataSource(dataSource);
        mMediaPlayer.prepare();

        dataSource.returnFromReadAt(-2);
        mMediaPlayer.start();
        assertTrue(mOnErrorCalled.waitForSignal());
    }

    @Presubmit"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.app.stubs.shared.CloseSystemDialogsTestService"	"isRotationFrozen"	""	"/home/gpoor/cts-12-source/cts/tests/app/shared/src/android/app/stubs/shared/CloseSystemDialogsTestService.java"	""	"public void test/*
 *.
 */

package android.app.stubs.shared;

import static android.app.PendingIntent.FLAG_IMMUTABLE;

import static java.util.concurrent.TimeUnit.MILLISECONDS;

import android.app.IActivityManager;
import android.app.Notification;
import android.app.NotificationChannel;
import android.app.NotificationManager;
import android.app.PendingIntent;
import android.app.Service;
import android.content.BroadcastReceiver;
import android.content.Context;
import android.content.Intent;
import android.content.IntentFilter;
import android.os.IBinder;
import android.os.ParcelableException;
import android.os.RemoteException;
import android.os.ResultReceiver;
import android.os.ServiceManager;
import android.view.IWindowManager;

import com.android.compatibility.common.util.PollingCheck;

import java.util.concurrent.ExecutionException;
import java.util.concurrent.TimeoutException;

/**
 * This is a bound service used in conjunction with CloseSystemDialogsTest.
 */
public class CloseSystemDialogsTestService extends Service {
    private static final String TAG = ""CloseSystemDialogsTestService"";
    private static final String NOTIFICATION_ACTION = TAG;
    private static final String NOTIFICATION_CHANNEL_ID = ""cts/"" + TAG;
    private static final Intent INTENT_ACSD = new Intent(Intent.ACTION_CLOSE_SYSTEM_DIALOGS);

    private final ICloseSystemDialogsTestsService mBinder = new Binder();
    private NotificationManager mNotificationManager;
    private IWindowManager mWindowManager;
    private IActivityManager mActivityManager;
    private BroadcastReceiver mNotificationReceiver;

    @Override
    public void onCreate() {
        super.onCreate();
        mNotificationManager = getSystemService(NotificationManager.class);
        mWindowManager = IWindowManager.Stub.asInterface(
                ServiceManager.getService(Context.WINDOW_SERVICE));
        mActivityManager = IActivityManager.Stub.asInterface(
                ServiceManager.getService(Context.ACTIVITY_SERVICE));
    }

    @Override
    public IBinder onBind(Intent intent) {
        return mBinder.asBinder();
    }

    @Override
    public void onDestroy() {
        super.onDestroy();
        if (mNotificationReceiver != null) {
            unregisterReceiver(mNotificationReceiver);
        }
    }

    private class Binder extends ICloseSystemDialogsTestsService.Stub {
        private final Context mContext = CloseSystemDialogsTestService.this;

        /** Checks that it can call @hide methods. */
        @Override
        public boolean waitUntilReady(long timeoutMs) {
            try {
                PollingCheck.check(""Can't call @hide methods"", timeoutMs, () -> {
                    try {
                        // Any method suffices since IWindowManager is @hide
                        mWindowManager.isRotationFrozen();
                        return true;
                    } catch (NoSuchMethodError e) {
                        return false;
                    }
                });
                return true;
            } catch (AssertionError e) {
                return false;
            } catch (Exception e) {
                throw new RuntimeException(e);
            }
        }

        @Override
        public void sendCloseSystemDialogsBroadcast() {
            mContext.sendBroadcast(INTENT_ACSD);
        }

        @Override
        public void postNotification(int notificationId, ResultReceiver receiver,
                boolean usePendingIntent) {
            mNotificationReceiver = new BroadcastReceiver() {
                @Override
                public void onReceive(Context context, Intent intent) {
                    try {
                        if (usePendingIntent) {
                            PendingIntent.getBroadcast(mContext, /* requestCode */ 0, INTENT_ACSD,
                                    FLAG_IMMUTABLE).send();
                        } else {
                            mContext.sendBroadcast(INTENT_ACSD);
                        }
                        receiver.send(RESULT_OK, null);
                    } catch (SecurityException e) {
                        receiver.send(RESULT_SECURITY_EXCEPTION, null);
                    } catch (PendingIntent.CanceledException e) {
                        receiver.send(RESULT_ERROR, null);
                    }
                }
            };
            mContext.registerReceiver(mNotificationReceiver, new IntentFilter(NOTIFICATION_ACTION));
            Intent intent = new Intent(NOTIFICATION_ACTION);
            intent.setPackage(mContext.getPackageName());
            CloseSystemDialogsTestService.this.notify(
                    notificationId,
                    PendingIntent.getBroadcast(mContext, 0, intent, FLAG_IMMUTABLE));
        }

        @Override
        public void closeSystemDialogsViaWindowManager(String reason) throws RemoteException {
            mWindowManager.closeSystemDialogs(reason);
        }

        @Override
        public void closeSystemDialogsViaActivityManager(String reason) throws RemoteException {
            mActivityManager.closeSystemDialogs(reason);
        }

        @Override
        public boolean waitForAccessibilityServiceWindow(long timeoutMs) throws RemoteException {
            final AppAccessibilityService service;
            try {
                service = AppAccessibilityService.getConnected().get(timeoutMs, MILLISECONDS);
            } catch (TimeoutException e) {
                return false;
            } catch (ExecutionException | InterruptedException e) {
                throw new ParcelableException(e);
            }
            return service.waitWindowAdded(timeoutMs);
        }
    }

    private void notify(int notificationId, PendingIntent intent) {
        Notification notification =
                new Notification.Builder(this, NOTIFICATION_CHANNEL_ID)
                        .setSmallIcon(android.R.drawable.ic_info)
                        .setContentIntent(intent)
                        .build();
        NotificationChannel notificationChannel = new NotificationChannel(NOTIFICATION_CHANNEL_ID,
                NOTIFICATION_CHANNEL_ID, NotificationManager.IMPORTANCE_DEFAULT);
        mNotificationManager.createNotificationChannel(notificationChannel);
        mNotificationManager.notify(notificationId, notification);
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.usespermissiondiffcertapp.UriGrantsActivityTest"	"finishCurInstanceSync"	""	"/home/gpoor/cts-12-source/cts/hostsidetests/appsecurity/test-apps/UsePermissionDiffCert/src/com/android/cts/usespermissiondiffcertapp/UriGrantsActivityTest.java"	""	"/*
 *.
 */

package com.android.cts.usespermissiondiffcertapp;

import static com.android.cts.usespermissiondiffcertapp.AccessPermissionWithDiffSigTest.GRANTABLE;
import static com.android.cts.usespermissiondiffcertapp.AccessPermissionWithDiffSigTest.GRANTABLE_MODES;
import static com.android.cts.usespermissiondiffcertapp.AccessPermissionWithDiffSigTest.NOT_GRANTABLE;
import static com.android.cts.usespermissiondiffcertapp.AccessPermissionWithDiffSigTest.NOT_GRANTABLE_MODES;
import static com.android.cts.usespermissiondiffcertapp.Asserts.assertAccess;
import static com.android.cts.usespermissiondiffcertapp.UriGrantsTest.TAG;
import static com.android.cts.usespermissiondiffcertapp.Utils.grantClipUriPermissionViaActivities;
import static com.android.cts.usespermissiondiffcertapp.Utils.grantClipUriPermissionViaActivity;

import static junit.framework.Assert.fail;

import android.content.ClipData;
import android.content.Context;
import android.content.Intent;
import android.net.Uri;
import android.os.SystemClock;
import android.util.Log;

import androidx.test.InstrumentationRegistry;
import androidx.test.runner.AndroidJUnit4;

import org.junit.After;
import org.junit.Test;
import org.junit.runner.RunWith;

import java.util.function.Function;

@RunWith(AndroidJUnit4.class)
public class UriGrantsActivityTest {
    private static Context getContext() {
        return InstrumentationRegistry.getTargetContext();
    }

    @After
    public void tearDown() throws Exception {
        // Always dispose, usually to clean up from failed tests
        ReceiveUriActivity.finishCurInstanceSync();
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.usespermissiondiffcertapp.UriGrantsActivityTest"	"testGrantableToActivity"	""	"/home/gpoor/cts-12-source/cts/hostsidetests/appsecurity/test-apps/UsePermissionDiffCert/src/com/android/cts/usespermissiondiffcertapp/UriGrantsActivityTest.java"	""	"public void testGrantableToActivity() {
        for (Uri uri : GRANTABLE) {
            for (int mode : GRANTABLE_MODES) {
                Log.d(TAG, ""Testing "" + uri + "" "" + mode);
                assertGrantableToActivity(uri, mode, UriGrantsTest::makeSingleClipData);
                assertGrantableToActivity(uri, mode, UriGrantsTest::makeMultiClipData);
            }
        }
    }

    private void assertGrantableToActivity(Uri uri, int mode, Function<Uri, ClipData> clipper) {
        final Uri subUri = Uri.withAppendedPath(uri, ""foo"");
        final Uri subSubUri = Uri.withAppendedPath(subUri, ""bar"");
        final Uri sub2Uri = Uri.withAppendedPath(uri, ""yes"");
        final Uri sub2SubUri = Uri.withAppendedPath(sub2Uri, ""no"");

        final ClipData subClip = clipper.apply(subUri);
        final ClipData sub2Clip = clipper.apply(sub2Uri);

        assertAccess(uri, 0);
        assertAccess(subClip, 0);
        assertAccess(subUri, 0);
        assertAccess(subSubUri, 0);
        assertAccess(sub2Clip, 0);
        assertAccess(sub2Uri, 0);
        assertAccess(sub2SubUri, 0);

        // --------------------------------

        ReceiveUriActivity.clearStarted();
        grantClipUriPermissionViaActivities(subClip, mode);
        ReceiveUriActivity.waitForStart();

        assertAccess(uri, 0);
        assertAccess(subClip, mode);
        assertAccess(subUri, mode);
        assertAccess(subSubUri, 0);
        assertAccess(sub2Clip, 0);
        assertAccess(sub2Uri, 0);
        assertAccess(sub2SubUri, 0);

        // --------------------------------

        ReceiveUriActivity.clearNewIntent();
        grantClipUriPermissionViaActivity(sub2Clip, mode);
        ReceiveUriActivity.waitForNewIntent();

        assertAccess(uri, 0);
        assertAccess(subClip, mode);
        assertAccess(subUri, mode);
        assertAccess(subSubUri, 0);
        assertAccess(sub2Clip, mode);
        assertAccess(sub2Uri, mode);
        assertAccess(sub2SubUri, 0);

        // And make sure we can't generate a permission to a running activity.
        assertNotGrantableToActivity(Uri.withAppendedPath(uri, ""hah""), mode, clipper);

        // --------------------------------

        // Dispose of activity.
        ReceiveUriActivity.finishCurInstanceSync();
        SystemClock.sleep(200);

        assertAccess(uri, 0);
        assertAccess(subClip, 0);
        assertAccess(subUri, 0);
        assertAccess(subSubUri, 0);
        assertAccess(sub2Clip, 0);
        assertAccess(sub2Uri, 0);
        assertAccess(sub2SubUri, 0);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaActivityTest"	"ArrayList"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaActivityTest.java"	""	"/*
 *
 */

package android.media.cts;

import static junit.framework.Assert.assertEquals;

import static org.junit.Assert.fail;
import static org.junit.Assert.assertTrue;
import static org.testng.Assert.assertFalse;

import android.app.Activity;
import android.app.Instrumentation;
import android.content.Context;
import android.content.Intent;
import android.content.res.Resources;
import android.media.AudioAttributes;
import android.media.AudioManager;
import android.media.session.MediaSession;
import android.os.Handler;
import android.os.Looper;
import android.os.SystemClock;
import android.util.Log;
import android.view.KeyEvent;

import androidx.test.InstrumentationRegistry;
import androidx.test.filters.LargeTest;
import androidx.test.rule.ActivityTestRule;
import androidx.test.runner.AndroidJUnit4;

import org.junit.After;
import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;
import org.junit.runner.RunWith;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.Callable;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;

/**
 * Test {@link MediaSessionTestActivity} which has called {@link Activity#setMediaController}.
 */
@NonMediaMainlineTest
@LargeTest
@RunWith(AndroidJUnit4.class)
public class MediaActivityTest {
    private static final String TAG = ""MediaActivityTest"";
    private static final int WAIT_TIME_MS = 5000;
    private static final int TIME_SLICE = 50;
    private static final List<Integer> ALL_VOLUME_STREAMS = new ArrayList();
    static {
        ALL_VOLUME_STREAMS.add(AudioManager.STREAM_ACCESSIBILITY);
        ALL_VOLUME_STREAMS.add(AudioManager.STREAM_ALARM);
        ALL_VOLUME_STREAMS.add(AudioManager.STREAM_DTMF);
        ALL_VOLUME_STREAMS.add(AudioManager.STREAM_MUSIC);
        ALL_VOLUME_STREAMS.add(AudioManager.STREAM_NOTIFICATION);
        ALL_VOLUME_STREAMS.add(AudioManager.STREAM_RING);
        ALL_VOLUME_STREAMS.add(AudioManager.STREAM_SYSTEM);
        ALL_VOLUME_STREAMS.add(AudioManager.STREAM_VOICE_CALL);
    }

    private Instrumentation mInstrumentation;
    private Context mContext;
    private boolean mUseFixedVolume;
    private AudioManager mAudioManager;
    private Map<Integer, Integer> mStreamVolumeMap = new HashMap<>();
    private MediaSession mSession;

    @Rule
    public ActivityTestRule<MediaSessionTestActivity> mActivityRule =
            new ActivityTestRule<>(MediaSessionTestActivity.class, false, false);

    @Before
    public void setUp() throws Exception {
        mInstrumentation = InstrumentationRegistry.getInstrumentation();
        mContext = mInstrumentation.getContext();
        mUseFixedVolume = mContext.getResources().getBoolean(
                Resources.getSystem().getIdentifier(""config_useFixedVolume"", ""bool"", ""android""));
        mAudioManager = (AudioManager) mContext.getSystemService(Context.AUDIO_SERVICE);

        mStreamVolumeMap.clear();
        for (Integer stream : ALL_VOLUME_STREAMS) {
            mStreamVolumeMap.put(stream, mAudioManager.getStreamVolume(stream));
        }

        mSession = new MediaSession(mContext, TAG);

        // Set volume stream other than STREAM_MUSIC.
        // STREAM_MUSIC is the new default stream for changing volume, so it doesn't precisely test
        // whether the session is prioritized for volume control or not.
        mSession.setPlaybackToLocal(new AudioAttributes.Builder()
                .setLegacyStreamType(AudioManager.STREAM_RING).build());

        Intent intent = new Intent(Intent.ACTION_MAIN);
        intent.setFlags(Intent.FLAG_ACTIVITY_NEW_TASK);
        intent.putExtra(MediaSessionTestActivity.KEY_SESSION_TOKEN, mSession.getSessionToken());

        mActivityRule.launchActivity(intent);

        assertTrue(
            ""Failed to bring MediaSessionTestActivity due to the screen lock setting.""
                    + "" Ensure screen lock isn't set before running CTS test."",
            pollingCheck(() -> {
                Activity activity = mActivityRule.getActivity();
                if (activity == null) {
                    return false;
                }
                return activity.getMediaController() != null;
            }));
    }

    @After
    public void cleanUp() {
        if (mSession != null) {
            mSession.release();
            mSession = null;
        }

        try {
            mActivityRule.finishActivity();
        } catch (IllegalStateException e) {
        }

        for (int stream : mStreamVolumeMap.keySet()) {
            int volume = mStreamVolumeMap.get(stream);
            try {
                mAudioManager.setStreamVolume(stream, volume, /* flag= */ 0);
            } catch (SecurityException e) {
                Log.w(TAG, ""Failed to restore volume. The test probably had changed DnD mode""
                        + "", stream="" + stream + "", originalVolume=""
                        + volume + "", currentVolume="" + mAudioManager.getStreamVolume(stream));
            }
        }
    }

    /**
     * Tests whether volume key changes volume with the session's stream.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.telephony.embms.cts.ServiceInfoTest"	"testDataAccess"	"CtsTelephonyTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/telephony/current/src/android/telephony/embms/cts/ServiceInfoTest.java"	""	"public void testDataAccess() {
        assertEquals(LOCALES.size(), STREAMING_SERVICE_INFO.getLocales().size());
        for (int i = 0; i < LOCALES.size(); i++) {
            assertTrue(STREAMING_SERVICE_INFO.getLocales().contains(LOCALES.get(i)));
            assertTrue(LOCALES.contains(STREAMING_SERVICE_INFO.getLocales().get(i)));
        }
        assertEquals(LOCALE_DICT.size(), STREAMING_SERVICE_INFO.getNamedContentLocales().size());
        for (Locale l : STREAMING_SERVICE_INFO.getNamedContentLocales()) {
            assertTrue(LOCALE_DICT.containsKey(l));
            assertEquals(LOCALE_DICT.get(l), STREAMING_SERVICE_INFO.getNameForLocale(l).toString());
        }

        assertEquals(BEGIN_DATE, STREAMING_SERVICE_INFO.getSessionStartTime());
        assertEquals(END_DATE, STREAMING_SERVICE_INFO.getSessionEndTime());
        assertEquals(NAME, STREAMING_SERVICE_INFO.getServiceClassName());
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaCodecBlockModelTest"	"testEncodeShortVideo"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaCodecBlockModelTest.java"	""	"public void testEncodeShortVideo() throws InterruptedException {
        if (!MediaUtils.check(mIsAtLeastR, ""test needs Android 11"")) return;
        MediaCodecBlockModelHelper.runThread(() -> runEncodeShortVideo());
    }

    private MediaCodecBlockModelHelper.Result runDecodeShortAudio(
            String inputResource,
            long lastBufferTimestampUs,
            boolean obtainBlockForEachBuffer) {
        MediaExtractor mediaExtractor = null;
        MediaCodec mediaCodec = null;
        try {
            mediaExtractor = getMediaExtractorForMimeType(inputResource, ""audio/"");
            MediaFormat mediaFormat =
                    mediaExtractor.getTrackFormat(mediaExtractor.getSampleTrackIndex());
            // TODO: b/147748978
            String[] codecs = MediaUtils.getDecoderNames(true /* isGoog */, mediaFormat);
            if (codecs.length == 0) {
                Log.i(TAG, ""No decoder found for format= "" + mediaFormat);
                return MediaCodecBlockModelHelper.Result.SKIP;
            }
            mediaCodec = MediaCodec.createByCodecName(codecs[0]);

            List<Long> timestampList = Collections.synchronizedList(new ArrayList<>());
            MediaCodecBlockModelHelper.Result result =
                MediaCodecBlockModelHelper.runComponentWithLinearInput(
                    mediaCodec,
                    null,  // crypto
                    mediaFormat,
                    null,  // surface
                    false,  // encoder
                    new MediaCodecBlockModelHelper.ExtractorInputSlotListener
                            .Builder()
                            .setExtractor(mediaExtractor)
                            .setLastBufferTimestampUs(lastBufferTimestampUs)
                            .setObtainBlockForEachBuffer(obtainBlockForEachBuffer)
                            .setTimestampQueue(timestampList)
                            .build(),
                    new MediaCodecBlockModelHelper.DummyOutputSlotListener(
                            false /* graphic */, timestampList));
            if (result == MediaCodecBlockModelHelper.Result.SUCCESS) {
                assertTrue(""Timestamp should match between input / output: "" + timestampList,
                        timestampList.isEmpty());
            }
            return result;
        } catch (IOException e) {
            throw new RuntimeException(""error reading input resource"", e);
        } catch (Exception e) {
            throw new RuntimeException(e);
        } finally {
            if (mediaCodec != null) {
                mediaCodec.stop();
                mediaCodec.release();
            }
            if (mediaExtractor != null) {
                mediaExtractor.release();
            }
        }
    }

    private MediaCodecBlockModelHelper.Result runEncodeShortAudio() {
        MediaExtractor mediaExtractor = null;
        MediaCodec mediaCodec = null;
        try {
            mediaExtractor = getMediaExtractorForMimeType(
                    ""okgoogle123_good.wav"", MediaFormat.MIMETYPE_AUDIO_RAW);
            MediaFormat mediaFormat = new MediaFormat(
                    mediaExtractor.getTrackFormat(mediaExtractor.getSampleTrackIndex()));
            mediaFormat.setString(MediaFormat.KEY_MIME, MediaFormat.MIMETYPE_AUDIO_AAC);
            mediaFormat.setInteger(MediaFormat.KEY_BIT_RATE, 128000);
            // TODO: b/147748978
            String[] codecs = MediaUtils.getEncoderNames(true /* isGoog */, mediaFormat);
            if (codecs.length == 0) {
                Log.i(TAG, ""No encoder found for format= "" + mediaFormat);
                return MediaCodecBlockModelHelper.Result.SKIP;
            }
            mediaCodec = MediaCodec.createByCodecName(codecs[0]);

            List<Long> timestampList = Collections.synchronizedList(new ArrayList<>());
            MediaCodecBlockModelHelper.Result result =
                MediaCodecBlockModelHelper.runComponentWithLinearInput(
                    mediaCodec,
                    null,  // crypto
                    mediaFormat,
                    null,  // surface
                    true,  // encoder
                    new MediaCodecBlockModelHelper.ExtractorInputSlotListener
                            .Builder()
                            .setExtractor(mediaExtractor)
                            .setLastBufferTimestampUs(LAST_BUFFER_TIMESTAMP_US)
                            .setTimestampQueue(timestampList)
                            .build(),
                    new MediaCodecBlockModelHelper.DummyOutputSlotListener(
                            false /* graphic */, timestampList));
            if (result == MediaCodecBlockModelHelper.Result.SUCCESS) {
                assertTrue(""Timestamp should match between input / output: "" + timestampList,
                        timestampList.isEmpty());
            }
            return result;
        } catch (IOException e) {
            throw new RuntimeException(""error reading input resource"", e);
        } catch (Exception e) {
            throw new RuntimeException(e);
        } finally {
            if (mediaCodec != null) {
                mediaCodec.stop();
                mediaCodec.release();
            }
            if (mediaExtractor != null) {
                mediaExtractor.release();
            }
        }
    }

    private MediaCodecBlockModelHelper.Result runEncodeShortVideo() {
        final int kWidth = 176;
        final int kHeight = 144;
        final int kFrameRate = 15;
        MediaCodec mediaCodec = null;
        ArrayList<HardwareBuffer> hardwareBuffers = new ArrayList<>();
        try {
            MediaFormat mediaFormat = MediaFormat.createVideoFormat(
                    MediaFormat.MIMETYPE_VIDEO_AVC, kWidth, kHeight);
            mediaFormat.setInteger(MediaFormat.KEY_FRAME_RATE, kFrameRate);
            mediaFormat.setInteger(MediaFormat.KEY_BIT_RATE, 1000000);
            mediaFormat.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, 1);
            mediaFormat.setInteger(
                    MediaFormat.KEY_COLOR_FORMAT,
                    MediaCodecInfo.CodecCapabilities.COLOR_FormatYUV420Flexible);
            // TODO: b/147748978
            String[] codecs = MediaUtils.getEncoderNames(true /* isGoog */, mediaFormat);
            if (codecs.length == 0) {
                Log.i(TAG, ""No encoder found for format= "" + mediaFormat);
                return MediaCodecBlockModelHelper.Result.SKIP;
            }
            mediaCodec = MediaCodec.createByCodecName(codecs[0]);

            long usage = HardwareBuffer.USAGE_CPU_READ_OFTEN;
            usage |= HardwareBuffer.USAGE_CPU_WRITE_OFTEN;
            if (mediaCodec.getCodecInfo().isHardwareAccelerated()) {
                usage |= HardwareBuffer.USAGE_VIDEO_ENCODE;
            }
            if (!HardwareBuffer.isSupported(
                        kWidth, kHeight, HardwareBuffer.YCBCR_420_888, 1 /* layer */, usage)) {
                Log.i(TAG, ""HardwareBuffer doesn't support "" + kWidth + ""x"" + kHeight
                        + ""; YCBCR_420_888; usage("" + Long.toHexString(usage) + "")"");
                return MediaCodecBlockModelHelper.Result.SKIP;
            }

            List<Long> timestampList = Collections.synchronizedList(new ArrayList<>());

            final LinkedBlockingQueue<MediaCodecBlockModelHelper.SlotEvent> queue =
                new LinkedBlockingQueue<>();
            mediaCodec.setCallback(new MediaCodec.Callback() {
                @Override
                public void onInputBufferAvailable(MediaCodec codec, int index) {
                    queue.offer(new MediaCodecBlockModelHelper.SlotEvent(true, index));
                }

                @Override
                public void onOutputBufferAvailable(
                        MediaCodec codec, int index, MediaCodec.BufferInfo info) {
                    queue.offer(new MediaCodecBlockModelHelper.SlotEvent(false, index));
                }

                @Override
                public void onOutputFormatChanged(MediaCodec codec, MediaFormat format) {
                }

                @Override
                public void onError(MediaCodec codec, CodecException e) {
                }
            });

            int flags = MediaCodec.CONFIGURE_FLAG_USE_BLOCK_MODEL;
            flags |= MediaCodec.CONFIGURE_FLAG_ENCODE;

            mediaCodec.configure(mediaFormat, null, null, flags);
            mediaCodec.start();
            boolean eos = false;
            boolean signaledEos = false;
            int frameIndex = 0;
            while (!eos && !Thread.interrupted()) {
                MediaCodecBlockModelHelper.SlotEvent event;
                try {
                    event = queue.take();
                } catch (InterruptedException e) {
                    return MediaCodecBlockModelHelper.Result.FAIL;
                }

                if (event.input) {
                    if (signaledEos) {
                        continue;
                    }
                    while (hardwareBuffers.size() <= event.index) {
                        hardwareBuffers.add(null);
                    }
                    HardwareBuffer buffer = hardwareBuffers.get(event.index);
                    if (buffer == null) {
                        buffer = HardwareBuffer.create(
                                kWidth, kHeight, HardwareBuffer.YCBCR_420_888, 1, usage);
                        hardwareBuffers.set(event.index, buffer);
                    }
                    try (Image image = MediaCodec.mapHardwareBuffer(buffer)) {
                        assertNotNull(""CPU readable/writable image must be mappable"", image);
                        assertEquals(kWidth, image.getWidth());
                        assertEquals(kHeight, image.getHeight());
                        // For Y plane
                        int rowSampling = 1;
                        int colSampling = 1;
                        for (Image.Plane plane : image.getPlanes()) {
                            ByteBuffer planeBuffer = plane.getBuffer();
                            for (int row = 0; row < kHeight / rowSampling; ++row) {
                                int rowOffset = row * plane.getRowStride();
                                for (int col = 0; col < kWidth / rowSampling; ++col) {
                                    planeBuffer.put(
                                            rowOffset + col * plane.getPixelStride(),
                                            (byte)(frameIndex * 4));
                                }
                            }
                            // For Cb and Cr planes
                            rowSampling = 2;
                            colSampling = 2;
                        }
                    }

                    long timestampUs = 1000000l * frameIndex / kFrameRate;
                    ++frameIndex;
                    if (frameIndex >= 32) {
                        signaledEos = true;
                    }
                    timestampList.add(timestampUs);
                    mediaCodec.getQueueRequest(event.index)
                            .setHardwareBuffer(buffer)
                            .setPresentationTimeUs(timestampUs)
                            .setFlags(signaledEos ? MediaCodec.BUFFER_FLAG_END_OF_STREAM : 0)
                            .queue();
                } else {
                    MediaCodec.OutputFrame frame = mediaCodec.getOutputFrame(event.index);
                    eos = (frame.getFlags() & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0;

                    if (!eos) {
                        assertNotNull(frame.getLinearBlock());
                        frame.getLinearBlock().recycle();
                    }

                    timestampList.remove(frame.getPresentationTimeUs());

                    mediaCodec.releaseOutputBuffer(event.index, false);
                }
            }

            if (!timestampList.isEmpty()) {
                assertTrue(""Timestamp should match between input / output: "" + timestampList,
                        timestampList.isEmpty());
            }
            return eos ? MediaCodecBlockModelHelper.Result.SUCCESS
                : MediaCodecBlockModelHelper.Result.FAIL;
        } catch (IOException e) {
            throw new RuntimeException(""error reading input resource"", e);
        } catch (Exception e) {
            throw new RuntimeException(e);
        } finally {
            if (mediaCodec != null) {
                mediaCodec.stop();
                mediaCodec.release();
            }
            for (HardwareBuffer buffer : hardwareBuffers) {
                if (buffer != null) {
                    buffer.close();
                }
            }
        }
    }

    private MediaCodecBlockModelHelper.Result runDecodeShortVideo(
            String inputResource,
            long lastBufferTimestampUs,
            boolean obtainBlockForEachBuffer) {
        return MediaCodecBlockModelHelper.runDecodeShortVideo(
                getMediaExtractorForMimeType(inputResource, ""video/""),
                lastBufferTimestampUs, obtainBlockForEachBuffer, null, null, null);
    }

    private static MediaExtractor getMediaExtractorForMimeType(final String resource,
            String mimeTypePrefix) {
        MediaExtractor mediaExtractor = new MediaExtractor();
        try (AssetFileDescriptor afd = getAssetFileDescriptorFor(resource)) {
            mediaExtractor.setDataSource(
                    afd.getFileDescriptor(), afd.getStartOffset(), afd.getLength());
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
        int trackIndex;
        for (trackIndex = 0; trackIndex < mediaExtractor.getTrackCount(); trackIndex++) {
            MediaFormat trackMediaFormat = mediaExtractor.getTrackFormat(trackIndex);
            if (trackMediaFormat.getString(MediaFormat.KEY_MIME).startsWith(mimeTypePrefix)) {
                mediaExtractor.selectTrack(trackIndex);
                break;
            }
        }
        if (trackIndex == mediaExtractor.getTrackCount()) {
            throw new IllegalStateException(""couldn't get a video track"");
        }

        return mediaExtractor;
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediastress.cts.NativeMediaActivity"	"exists"	"CtsMediaStressTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/mediastress/src/android/mediastress/cts/NativeMediaActivity.java"	""	"public void test/*
 *.
 */

/* Original code copied from NDK Native-media sample code */
package android.mediastress.cts;

import android.app.Activity;
import android.content.res.Configuration;
import android.graphics.SurfaceTexture;
import android.os.Bundle;
import android.util.Log;
import android.view.Surface;
import android.view.WindowManager;

import java.io.File;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.TimeUnit;

import junit.framework.Assert;

public class NativeMediaActivity extends Activity implements OnSurfaceChangedListener {
    public static final String EXTRA_VIDEO_HEIGHT = ""videoHeight"";
    // should be long enough. time-out can be treated as error
    public static final long NATIVE_MEDIA_LIFECYCLE_TIMEOUT_MS = 10000;
    static final String TAG = ""NativeMedia"";
    static final String[] MEDIA = {
        ""bbb_short/480x360/mp4_libx264_libfaac/"" +
        ""bbb_short.ffmpeg.480x360.mp4.libx264_1000kbps_30fps.libfaac_stereo_192kbps_44100Hz.ts"",
        ""bbb_short/720x480/mp4_libx264_libfaac/"" +
        ""bbb_short.ffmpeg.720x480.mp4.libx264_1000kbps_30fps.libfaac_stereo_192kbps_44100Hz.ts"",
        ""bbb_short/1280x720/mp4_libx264_libfaac/"" +
        ""bbb_short.ffmpeg.1280x720.mp4.libx264_1000kbps_30fps.libfaac_stereo_192kbps_44100Hz.ts"",
        ""bbb_short/1920x1080/mp4_libx264_libfaac/"" +
        ""bbb_short.ffmpeg.1920x1080.mp4.libx264_5000kbps_30fps.libfaac_stereo_192kbps_48000Hz.ts""
    };

    private SurfaceTextureGLSurfaceView mGLView;
    private volatile boolean mNativeCreated = false;
    private int mVideoHeight = 360;
    // native media status queued whenever there is a change in life.
    private final BlockingQueue<Boolean> mNativeWaitQ = new LinkedBlockingQueue<Boolean>();

    @Override
    public void onCreate(Bundle icicle) {
        super.onCreate(icicle);
        getWindow().addFlags(WindowManager.LayoutParams.FLAG_KEEP_SCREEN_ON |
                WindowManager.LayoutParams.FLAG_TURN_SCREEN_ON);
        mVideoHeight = getIntent().getIntExtra(EXTRA_VIDEO_HEIGHT, mVideoHeight);
        mGLView = new SurfaceTextureGLSurfaceView(this, this);
        setContentView(mGLView);
    }

    @Override
    public void onConfigurationChanged(Configuration newConfig) {
        Log.w(TAG, ""configuration changed "" + newConfig.orientation);
        super.onConfigurationChanged(newConfig);
    }

    /**
     * should be called by GLThread after GlSurface is created.
     */
    @Override
    public void onSurfaceCreated() {
        runOnUiThread(new Runnable() {
            @Override
            public void run() {
                Log.i(TAG, ""onSurfaceCreated create engine"");
                // initialize native media system
                Assert.assertTrue(createEngine());
                Assert.assertTrue(setSurfaceForNative());
                String fileName = getMediaString();
                File f = new File(fileName);
                Log.i(TAG, ""start playing "" + fileName + "", exists: "" + f.exists());
                Assert.assertTrue(""file '"" + fileName + ""' does not exist"", f.exists());
                Assert.assertTrue(createMediaPlayer(""file://"" + fileName));
                mNativeCreated = true;
                mNativeWaitQ.add(mNativeCreated);
            }
        });
    }

    /**
     * should be called inside main thread
     */
    @Override
    public void onSurfaceDestroyed() {
        shutdownIfActive();
    }

    /**
     * check if native media is alive. If it does not become alive
     * for more than certain time, assertion fail will happen.
     * @return the status of native media, true if it is alive, null if timed-out
     * @throws InterruptedException
     */
    public Boolean waitForNativeMediaLifeCycle() throws InterruptedException {
        return mNativeWaitQ.poll(NATIVE_MEDIA_LIFECYCLE_TIMEOUT_MS, TimeUnit.MILLISECONDS);
    }

    @Override
    protected void onPause() {
        //GLSurfaceView destroys surface on pause. so shutdown should be done.
        shutdownIfActive();
        mGLView.onPause();
        super.onPause();
    }

    @Override
    protected void onResume() {
        super.onResume();
        mGLView.onResume();
        if (mNativeCreated) {
            Assert.assertTrue(playOrPauseMediaPlayer(true));
        }
    }

    @Override
    protected void onDestroy() {
        if (mNativeCreated) {
            shutdown();
        }
        super.onDestroy();
    }

    private void shutdownIfActive() {
        if (mNativeCreated) {
            Log.i(TAG, ""shutdownIfActive shutdown"");
            // surface no longer available, so just shutdown
            shutdown();
            mNativeCreated = false;
            mNativeWaitQ.add(mNativeCreated);
        }
    }

    private boolean setSurfaceForNative() {
        SurfaceTexture st = mGLView.getSurfaceTexture();
        Assert.assertNotNull(st);
        Surface s = new Surface(st);
        boolean res = setSurface(s);
        s.release();
        return res;
    }

    private String getMediaString() {
        int mediaIndex = 0; // default: 480x360
        switch(mVideoHeight) {
        case 1080:
            mediaIndex = 3;
            break;
        case 720:
            mediaIndex = 2;
            break;
        case 480:
            mediaIndex = 1;
            break;
        }
        return WorkDir.getMediaDirString() + MEDIA[mediaIndex];
    }

    /**
     * creates OpenMaxAl Engine
    */
    public static native boolean createEngine();
    /**
     * set surface to render. should be called before creating Media player
     * @param surface
     */
    public static native boolean setSurface(Surface surface);

    public static native boolean createMediaPlayer(String fileUri);
    public static native boolean playOrPauseMediaPlayer(boolean play);
    public static native void shutdown();

    /** Load jni on initialization */
    static {
         System.loadLibrary(""ctsmediastress_jni"");
    }

}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.PerformanceTest"	"testReprocessingCaptureStall"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/PerformanceTest.java"	""	"public void testReprocessingCaptureStall() throws Exception {
        for (String id : mTestRule.getCameraIdsUnderTest()) {
            for (int format : REPROCESS_FORMATS) {
                if (!isReprocessSupported(id, format)) {
                    continue;
                }

                try {
                    mTestRule.openDevice(id);
                    String streamName = ""test_reprocessing_capture_stall"";
                    mReportLog = new DeviceReportLog(REPORT_LOG_NAME, streamName);
                    mReportLog.addValue(""camera_id"", id, ResultType.NEUTRAL, ResultUnit.NONE);
                    mReportLog.addValue(""format"", format, ResultType.NEUTRAL, ResultUnit.NONE);
                    reprocessingCaptureStallTestByCamera(format);
                } finally {
                    closeReaderWriters();
                    mTestRule.closeDevice(id);
                    closePreviewSurface();
                    mReportLog.submit(mInstrumentation);
                }
            }
        }
    }

    private void reprocessingCaptureStallTestByCamera(int reprocessInputFormat) throws Exception {
        prepareReprocessCapture(reprocessInputFormat);

        // Let it stream for a while before reprocessing
        startZslStreaming();
        waitForFrames(NUM_RESULTS_WAIT);

        final int NUM_REPROCESS_TESTED = MAX_REPROCESS_IMAGES / 2;
        // Prepare several reprocessing request
        Image[] inputImages = new Image[NUM_REPROCESS_TESTED];
        CaptureRequest.Builder[] reprocessReqs = new CaptureRequest.Builder[MAX_REPROCESS_IMAGES];
        for (int i = 0; i < NUM_REPROCESS_TESTED; i++) {
            inputImages[i] =
                    mCameraZslImageListener.getImage(CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS);
            TotalCaptureResult zslResult =
                    mZslResultListener.getCaptureResult(
                            WAIT_FOR_RESULT_TIMEOUT_MS, inputImages[i].getTimestamp());
            reprocessReqs[i] = mTestRule.getCamera().createReprocessCaptureRequest(zslResult);
            reprocessReqs[i].addTarget(mJpegReader.getSurface());
            reprocessReqs[i].set(CaptureRequest.NOISE_REDUCTION_MODE,
                    CaptureRequest.NOISE_REDUCTION_MODE_HIGH_QUALITY);
            reprocessReqs[i].set(CaptureRequest.EDGE_MODE,
                    CaptureRequest.EDGE_MODE_HIGH_QUALITY);
            mWriter.queueInputImage(inputImages[i]);
        }

        double[] maxCaptureGapsMs = new double[NUM_REPROCESS_TESTED];
        double[] averageFrameDurationMs = new double[NUM_REPROCESS_TESTED];
        Arrays.fill(averageFrameDurationMs, 0.0);
        final int MAX_REPROCESS_RETURN_FRAME_COUNT = 20;
        SimpleCaptureCallback reprocessResultListener = new SimpleCaptureCallback();
        for (int i = 0; i < NUM_REPROCESS_TESTED; i++) {
            mZslResultListener.drain();
            CaptureRequest reprocessRequest = reprocessReqs[i].build();
            mTestRule.getCameraSession().capture(
                    reprocessRequest, reprocessResultListener, mTestRule.getHandler());
            // Wait for reprocess output jpeg and result come back.
            reprocessResultListener.getCaptureResultForRequest(reprocessRequest,
                    CameraTestUtils.CAPTURE_RESULT_TIMEOUT_MS);
            mJpegListener.getImage(CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS).close();
            long numFramesMaybeStalled = mZslResultListener.getTotalNumFrames();
            assertTrue(""Reprocess capture result should be returned in ""
                            + MAX_REPROCESS_RETURN_FRAME_COUNT + "" frames"",
                    numFramesMaybeStalled <= MAX_REPROCESS_RETURN_FRAME_COUNT);

            // Need look longer time, as the stutter could happen after the reprocessing
            // output frame is received.
            long[] timestampGap = new long[MAX_REPROCESS_RETURN_FRAME_COUNT + 1];
            Arrays.fill(timestampGap, 0);
            CaptureResult[] results = new CaptureResult[timestampGap.length];
            long[] frameDurationsNs = new long[timestampGap.length];
            for (int j = 0; j < results.length; j++) {
                results[j] = mZslResultListener.getCaptureResult(
                        CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS);
                if (j > 0) {
                    timestampGap[j] = results[j].get(CaptureResult.SENSOR_TIMESTAMP) -
                            results[j - 1].get(CaptureResult.SENSOR_TIMESTAMP);
                    assertTrue(""Time stamp should be monotonically increasing"",
                            timestampGap[j] > 0);
                }
                frameDurationsNs[j] = results[j].get(CaptureResult.SENSOR_FRAME_DURATION);
            }

            if (VERBOSE) {
                Log.i(TAG, ""timestampGap: "" + Arrays.toString(timestampGap));
                Log.i(TAG, ""frameDurationsNs: "" + Arrays.toString(frameDurationsNs));
            }

            // Get the number of candidate results, calculate the average frame duration
            // and max timestamp gap.
            Arrays.sort(timestampGap);
            double maxTimestampGapMs = timestampGap[timestampGap.length - 1] / 1000000.0;
            for (int m = 0; m < frameDurationsNs.length; m++) {
                averageFrameDurationMs[i] += (frameDurationsNs[m] / 1000000.0);
            }
            averageFrameDurationMs[i] /= frameDurationsNs.length;

            maxCaptureGapsMs[i] = maxTimestampGapMs;
        }

        blockingStopRepeating();

        String reprocessType = ""YUV reprocessing"";
        if (reprocessInputFormat == ImageFormat.PRIVATE) {
            reprocessType = ""opaque reprocessing"";
        }
        mReportLog.addValue(""reprocess_type"", reprocessType, ResultType.NEUTRAL, ResultUnit.NONE);
        mReportLog.addValues(""max_capture_timestamp_gaps"", maxCaptureGapsMs,
                ResultType.LOWER_BETTER, ResultUnit.MS);
        mReportLog.addValues(""capture_average_frame_duration"", averageFrameDurationMs,
                ResultType.LOWER_BETTER, ResultUnit.MS);
        mReportLog.setSummary(""camera_reprocessing_average_max_capture_timestamp_gaps"",
                Stat.getAverage(maxCaptureGapsMs), ResultType.LOWER_BETTER, ResultUnit.MS);

        // The max timestamp gap should be less than (captureStall + 1) x average frame
        // duration * (1 + error margin).
        int maxCaptureStallFrames = mTestRule.getStaticInfo().getMaxCaptureStallOrDefault();
        for (int i = 0; i < maxCaptureGapsMs.length; i++) {
            double stallDurationBound = averageFrameDurationMs[i] *
                    (maxCaptureStallFrames + 1) * (1 + REPROCESS_STALL_MARGIN);
            assertTrue(""max capture stall duration should be no larger than "" + stallDurationBound,
                    maxCaptureGapsMs[i] <= stallDurationBound);
        }
    }

    private void reprocessingPerformanceTestByCamera(int reprocessInputFormat, boolean asyncMode,
            boolean requireHighQuality)
            throws Exception {
        // Prepare the reprocessing capture
        prepareReprocessCapture(reprocessInputFormat);

        // Start ZSL streaming
        startZslStreaming();
        waitForFrames(NUM_RESULTS_WAIT);

        CaptureRequest.Builder[] reprocessReqs = new CaptureRequest.Builder[MAX_REPROCESS_IMAGES];
        Image[] inputImages = new Image[MAX_REPROCESS_IMAGES];
        double[] getImageLatenciesMs = new double[MAX_REPROCESS_IMAGES];
        long startTimeMs;
        for (int i = 0; i < MAX_REPROCESS_IMAGES; i++) {
            inputImages[i] =
                    mCameraZslImageListener.getImage(CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS);
            TotalCaptureResult zslResult =
                    mZslResultListener.getCaptureResult(
                            WAIT_FOR_RESULT_TIMEOUT_MS, inputImages[i].getTimestamp());
            reprocessReqs[i] = mTestRule.getCamera().createReprocessCaptureRequest(zslResult);
            if (requireHighQuality) {
                // Reprocessing should support high quality for NR and edge modes.
                reprocessReqs[i].set(CaptureRequest.NOISE_REDUCTION_MODE,
                        CaptureRequest.NOISE_REDUCTION_MODE_HIGH_QUALITY);
                reprocessReqs[i].set(CaptureRequest.EDGE_MODE,
                        CaptureRequest.EDGE_MODE_HIGH_QUALITY);
            }
            reprocessReqs[i].addTarget(mJpegReader.getSurface());
        }

        if (asyncMode) {
            // async capture: issue all the reprocess requests as quick as possible, then
            // check the throughput of the output jpegs.
            for (int i = 0; i < MAX_REPROCESS_IMAGES; i++) {
                // Could be slow for YUV reprocessing, do it in advance.
                mWriter.queueInputImage(inputImages[i]);
            }

            // Submit the requests
            for (int i = 0; i < MAX_REPROCESS_IMAGES; i++) {
                mTestRule.getCameraSession().capture(reprocessReqs[i].build(), null, null);
            }

            // Get images
            startTimeMs = SystemClock.elapsedRealtime();
            Image jpegImages[] = new Image[MAX_REPROCESS_IMAGES];
            for (int i = 0; i < MAX_REPROCESS_IMAGES; i++) {
                jpegImages[i] = mJpegListener.getImage(CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS);
                getImageLatenciesMs[i] = SystemClock.elapsedRealtime() - startTimeMs;
                startTimeMs = SystemClock.elapsedRealtime();
            }
            for (Image i : jpegImages) {
                i.close();
            }
        } else {
            // sync capture: issue reprocess request one by one, only submit next one when
            // the previous capture image is returned. This is to test the back to back capture
            // performance.
            Image jpegImages[] = new Image[MAX_REPROCESS_IMAGES];
            for (int i = 0; i < MAX_REPROCESS_IMAGES; i++) {
                startTimeMs = SystemClock.elapsedRealtime();
                mWriter.queueInputImage(inputImages[i]);
                mTestRule.getCameraSession().capture(reprocessReqs[i].build(), null, null);
                jpegImages[i] = mJpegListener.getImage(CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS);
                getImageLatenciesMs[i] = SystemClock.elapsedRealtime() - startTimeMs;
            }
            for (Image i : jpegImages) {
                i.close();
            }
        }

        blockingStopRepeating();

        String reprocessType = ""YUV reprocessing"";
        if (reprocessInputFormat == ImageFormat.PRIVATE) {
            reprocessType = ""opaque reprocessing"";
        }

        // Report the performance data
        String captureMsg;
        if (asyncMode) {
            captureMsg = ""capture latency"";
            if (requireHighQuality) {
                captureMsg += "" for High Quality noise reduction and edge modes"";
            }
            mReportLog.addValue(""reprocess_type"", reprocessType, ResultType.NEUTRAL,
                    ResultUnit.NONE);
            mReportLog.addValue(""capture_message"", captureMsg, ResultType.NEUTRAL,
                    ResultUnit.NONE);
            mReportLog.addValues(""latency"", getImageLatenciesMs, ResultType.LOWER_BETTER,
                    ResultUnit.MS);
            mReportLog.setSummary(""camera_reprocessing_average_latency"",
                    Stat.getAverage(getImageLatenciesMs), ResultType.LOWER_BETTER, ResultUnit.MS);
        } else {
            captureMsg = ""shot to shot latency"";
            if (requireHighQuality) {
                captureMsg += "" for High Quality noise reduction and edge modes"";
            }
            mReportLog.addValue(""reprocess_type"", reprocessType, ResultType.NEUTRAL,
                    ResultUnit.NONE);
            mReportLog.addValue(""capture_message"", captureMsg, ResultType.NEUTRAL,
                    ResultUnit.NONE);
            mReportLog.addValues(""latency"", getImageLatenciesMs, ResultType.LOWER_BETTER,
                    ResultUnit.MS);
            mReportLog.setSummary(""camera_reprocessing_shot_to_shot_average_latency"",
                    Stat.getAverage(getImageLatenciesMs), ResultType.LOWER_BETTER, ResultUnit.MS);
        }
    }

    /**
     * Start preview and ZSL streaming
     */
    private void startZslStreaming() throws Exception {
        CaptureRequest.Builder zslBuilder =
                mTestRule.getCamera().createCaptureRequest(CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG);
        zslBuilder.addTarget(mPreviewSurface);
        zslBuilder.addTarget(mCameraZslReader.getSurface());
        mTestRule.getCameraSession().setRepeatingRequest(
                zslBuilder.build(), mZslResultListener, mTestRule.getHandler());
    }

    /**
     * Wait for a certain number of frames, the images and results will be drained from the
     * listeners to make sure that next reprocessing can get matched results and images.
     *
     * @param numFrameWait The number of frames to wait before return, 0 means that
     *      this call returns immediately after streaming on.
     */
    private void waitForFrames(int numFrameWait) throws Exception {
        if (numFrameWait < 0) {
            throw new IllegalArgumentException(""numFrameWait "" + numFrameWait +
                    "" should be non-negative"");
        }

        for (int i = 0; i < numFrameWait; i++) {
            mCameraZslImageListener.getImage(CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS).close();
        }
    }

    private void closeReaderWriters() {
        mCameraZslImageListener.drain();
        CameraTestUtils.closeImageReader(mCameraZslReader);
        mCameraZslReader = null;
        mJpegListener.drain();
        CameraTestUtils.closeImageReader(mJpegReader);
        mJpegReader = null;
        CameraTestUtils.closeImageWriter(mWriter);
        mWriter = null;
    }

    private void prepareReprocessCapture(int inputFormat)
            throws CameraAccessException {
        // 1. Find the right preview and capture sizes.
        Size maxPreviewSize = mTestRule.getOrderedPreviewSizes().get(0);
        Size[] supportedInputSizes =
                mTestRule.getStaticInfo().getAvailableSizesForFormatChecked(inputFormat,
                        StaticMetadata.StreamDirection.Input);
        Size maxInputSize = CameraTestUtils.getMaxSize(supportedInputSizes);
        Size maxJpegSize = mTestRule.getOrderedStillSizes().get(0);
        updatePreviewSurface(maxPreviewSize);
        mZslResultListener = new SimpleCaptureCallback();

        // 2. Create camera output ImageReaders.
        // YUV/Opaque output, camera should support output with input size/format
        mCameraZslImageListener = new SimpleImageReaderListener(
                /*asyncMode*/true, MAX_ZSL_IMAGES - MAX_REPROCESS_IMAGES);
        mCameraZslReader = CameraTestUtils.makeImageReader(
                maxInputSize, inputFormat, MAX_ZSL_IMAGES,
                mCameraZslImageListener, mTestRule.getHandler());
        // Jpeg reprocess output
        mJpegListener = new SimpleImageReaderListener();
        mJpegReader = CameraTestUtils.makeImageReader(
                maxJpegSize, ImageFormat.JPEG, MAX_JPEG_IMAGES,
                mJpegListener, mTestRule.getHandler());

        // create camera reprocess session
        List<Surface> outSurfaces = new ArrayList<Surface>();
        outSurfaces.add(mPreviewSurface);
        outSurfaces.add(mCameraZslReader.getSurface());
        outSurfaces.add(mJpegReader.getSurface());
        InputConfiguration inputConfig = new InputConfiguration(maxInputSize.getWidth(),
                maxInputSize.getHeight(), inputFormat);
        mTestRule.setCameraSessionListener(new BlockingSessionCallback());
        mTestRule.setCameraSession(CameraTestUtils.configureReprocessableCameraSession(
                mTestRule.getCamera(), inputConfig, outSurfaces,
                mTestRule.getCameraSessionListener(), mTestRule.getHandler()));

        // 3. Create ImageWriter for input
        mWriter = CameraTestUtils.makeImageWriter(
                mTestRule.getCameraSession().getInputSurface(), MAX_INPUT_IMAGES,
                /*listener*/null, /*handler*/null);
    }

    /**
     * Stop repeating requests for current camera and waiting for it to go back to idle, resulting
     * in an idle device.
     */
    private void blockingStopRepeating() throws Exception {
        stopRepeating();
        mTestRule.getCameraSessionListener().getStateWaiter().waitForState(
                BlockingSessionCallback.SESSION_READY, CameraTestUtils.CAMERA_IDLE_TIMEOUT_MS);
    }

    private void blockingStartPreview(String id, CaptureCallback listener,
            CaptureRequest previewRequest, SimpleImageListener imageListener)
            throws Exception {
        mTestRule.getCameraSession().setRepeatingRequest(
                previewRequest, listener, mTestRule.getHandler());
        imageListener.waitForImageAvailable(CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS);
    }

    /**
     * Setup still capture configuration and start preview.
     *
     * @param id The camera id under test
     * @param previewBuilder The capture request builder to be used for preview
     * @param stillBuilder The capture request builder to be used for still capture
     * @param previewSz Preview size
     * @param captureSizes Still capture sizes
     * @param formats The single capture image formats
     * @param resultListener Capture result listener
     * @param maxNumImages The max number of images set to the image reader
     * @param imageListeners The single capture capture image listeners
     * @param isHeic Capture HEIC image if true, JPEG image if false
     */
    private ImageReader[] prepareStillCaptureAndStartPreview(String id,
            CaptureRequest.Builder previewBuilder, CaptureRequest.Builder stillBuilder,
            Size previewSz, Size[] captureSizes, int[] formats, CaptureCallback resultListener,
            int maxNumImages, ImageReader.OnImageAvailableListener[] imageListeners,
            boolean isHeic)
            throws Exception {

        if ((captureSizes == null) || (formats == null) || (imageListeners == null) &&
                (captureSizes.length != formats.length) ||
                (formats.length != imageListeners.length)) {
            throw new IllegalArgumentException(""Invalid capture sizes/formats or image listeners!"");
        }

        if (VERBOSE) {
            Log.v(TAG, String.format(""Prepare still capture and preview (%s)"",
                    previewSz.toString()));
        }

        // Update preview size.
        updatePreviewSurface(previewSz);

        ImageReader[] readers = new ImageReader[captureSizes.length];
        List<Surface> outputSurfaces = new ArrayList<Surface>();
        outputSurfaces.add(mPreviewSurface);
        for (int i = 0; i < captureSizes.length; i++) {
            readers[i] = CameraTestUtils.makeImageReader(captureSizes[i], formats[i], maxNumImages,
                    imageListeners[i], mTestRule.getHandler());
            outputSurfaces.add(readers[i].getSurface());
        }

        // Configure the requests.
        previewBuilder.addTarget(mPreviewSurface);
        stillBuilder.addTarget(mPreviewSurface);
        for (int i = 0; i < readers.length; i++) {
            stillBuilder.addTarget(readers[i].getSurface());
        }

        // Update target fps based on the min frame duration of preview.
        CameraCharacteristics ch = mTestRule.getStaticInfo().getCharacteristics();
        StreamConfigurationMap config = ch.get(
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        long minFrameDuration = Math.max(FRAME_DURATION_NS_30FPS, config.getOutputMinFrameDuration(
                SurfaceTexture.class, previewSz));
        Range<Integer> targetRange =
                CameraTestUtils.getSuitableFpsRangeForDuration(id,
                minFrameDuration, mTestRule.getStaticInfo());
        previewBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, targetRange);
        stillBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, targetRange);

        CaptureRequest previewRequest = previewBuilder.build();
        mTestRule.setCameraSessionListener(new BlockingSessionCallback());
        boolean useSessionKeys = isFpsRangeASessionKey(ch);
        configureAndSetCameraSession(outputSurfaces, useSessionKeys, previewRequest);

        // Start preview.
        mTestRule.getCameraSession().setRepeatingRequest(
                previewRequest, resultListener, mTestRule.getHandler());

        return readers;
    }

    /**
     * Helper function to check if TARGET_FPS_RANGE is a session parameter
     */
    private boolean isFpsRangeASessionKey(CameraCharacteristics ch) {
        List<CaptureRequest.Key<?>> sessionKeys = ch.getAvailableSessionKeys();
        return sessionKeys != null &&
                sessionKeys.contains(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE);
    }

    /**
     * Helper function to configure camera session using parameters provided.
     */
    private void configureAndSetCameraSession(List<Surface> surfaces,
            boolean useInitialRequest, CaptureRequest initialRequest)
            throws CameraAccessException {
        CameraCaptureSession cameraSession;
        if (useInitialRequest) {
            cameraSession = CameraTestUtils.configureCameraSessionWithParameters(
                mTestRule.getCamera(), surfaces,
                mTestRule.getCameraSessionListener(), mTestRule.getHandler(),
                initialRequest);
        } else {
            cameraSession = CameraTestUtils.configureCameraSession(
                mTestRule.getCamera(), surfaces,
                mTestRule.getCameraSessionListener(), mTestRule.getHandler());
        }
        mTestRule.setCameraSession(cameraSession);
    }

    /**
     * Setup single capture configuration and start preview.
     *
     * @param previewBuilder The capture request builder to be used for preview
     * @param stillBuilder The capture request builder to be used for still capture
     * @param previewSz Preview size
     * @param captureSz Still capture size
     * @param format The single capture image format
     * @param resultListener Capture result listener
     * @param sessionListener Session listener
     * @param maxNumImages The max number of images set to the image reader
     * @param imageListener The single capture capture image listener
     * @param useSessionKeys Create capture session using session keys from previewRequest
     */
    private void prepareCaptureAndStartPreview(CaptureRequest.Builder previewBuilder,
            CaptureRequest.Builder stillBuilder, Size previewSz, Size captureSz, int format,
            CaptureCallback resultListener, CameraCaptureSession.StateCallback sessionListener,
            int maxNumImages, ImageReader.OnImageAvailableListener imageListener,
            boolean  useSessionKeys) throws Exception {
        if ((captureSz == null) || (imageListener == null)) {
            throw new IllegalArgumentException(""Invalid capture size or image listener!"");
        }

        if (VERBOSE) {
            Log.v(TAG, String.format(""Prepare single capture (%s) and preview (%s)"",
                    captureSz.toString(), previewSz.toString()));
        }

        // Update preview size.
        updatePreviewSurface(previewSz);

        // Create ImageReader.
        mTestRule.createDefaultImageReader(captureSz, format, maxNumImages, imageListener);

        // Configure output streams with preview and jpeg streams.
        List<Surface> outputSurfaces = new ArrayList<Surface>();
        outputSurfaces.add(mPreviewSurface);
        outputSurfaces.add(mTestRule.getReaderSurface());
        if (sessionListener == null) {
            mTestRule.setCameraSessionListener(new BlockingSessionCallback());
        } else {
            mTestRule.setCameraSessionListener(new BlockingSessionCallback(sessionListener));
        }

        // Configure the requests.
        previewBuilder.addTarget(mPreviewSurface);
        stillBuilder.addTarget(mPreviewSurface);
        stillBuilder.addTarget(mTestRule.getReaderSurface());
        CaptureRequest previewRequest = previewBuilder.build();

        configureAndSetCameraSession(outputSurfaces, useSessionKeys, previewRequest);

        // Start preview.
        mTestRule.getCameraSession().setRepeatingRequest(
                previewRequest, resultListener, mTestRule.getHandler());
    }

    /**
     * Update the preview surface size.
     *
     * @param size The preview size to be updated.
     */
    private void updatePreviewSurface(Size size) {
        if ((mPreviewSurfaceTexture != null ) || (mPreviewSurface != null)) {
            closePreviewSurface();
        }

        mPreviewSurfaceTexture = new SurfaceTexture(/*random int*/ 1);
        mPreviewSurfaceTexture.setDefaultBufferSize(size.getWidth(), size.getHeight());
        mPreviewSurface = new Surface(mPreviewSurfaceTexture);
    }

    /**
     * Release preview surface and corresponding surface texture.
     */
    private void closePreviewSurface() {
        if (mPreviewSurface != null) {
            mPreviewSurface.release();
            mPreviewSurface = null;
        }

        if (mPreviewSurfaceTexture != null) {
            mPreviewSurfaceTexture.release();
            mPreviewSurfaceTexture = null;
        }
    }

    private boolean isReprocessSupported(String cameraId, int format)
            throws CameraAccessException {
        if (format != ImageFormat.YUV_420_888 && format != ImageFormat.PRIVATE) {
            throw new IllegalArgumentException(
                    ""format "" + format + "" is not supported for reprocessing"");
        }

        StaticMetadata info = new StaticMetadata(
                mTestRule.getCameraManager().getCameraCharacteristics(cameraId), CheckLevel.ASSERT,
                /*collector*/ null);
        int cap = CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING;
        if (format == ImageFormat.PRIVATE) {
            cap = CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING;
        }
        return info.isCapabilitySupported(cap);
    }

    /**
     * Stop the repeating requests of current camera.
     * Does _not_ wait for the device to go idle
     */
    private void stopRepeating() throws Exception {
        // Stop repeat, wait for captures to complete, and disconnect from surfaces
        if (mTestRule.getCameraSession() != null) {
            if (VERBOSE) Log.v(TAG, ""Stopping preview"");
            mTestRule.getCameraSession().stopRepeating();
        }
    }

    /**
     * Configure reader and preview outputs and wait until done.
     *
     * @return The preview capture request
     */
    private CaptureRequest configureReaderAndPreviewOutputs(
            String id, boolean isColorOutputSupported)
            throws Exception {
        if (mPreviewSurface == null || mTestRule.getReaderSurface() == null) {
            throw new IllegalStateException(""preview and reader surface must be initilized first"");
        }

        // Create previewBuilder
        CaptureRequest.Builder previewBuilder =
                mTestRule.getCamera().createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        if (isColorOutputSupported) {
            previewBuilder.addTarget(mPreviewSurface);
        }
        previewBuilder.addTarget(mTestRule.getReaderSurface());


        // Figure out constant target FPS range no larger than 30fps
        CameraCharacteristics ch = mTestRule.getStaticInfo().getCharacteristics();
        StreamConfigurationMap config =
                ch.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        long minFrameDuration = Math.max(FRAME_DURATION_NS_30FPS,
                config.getOutputMinFrameDuration(mImageReaderFormat, mPreviewSize));

        List<Surface> outputSurfaces = new ArrayList<>();
        outputSurfaces.add(mTestRule.getReaderSurface());
        if (isColorOutputSupported) {
            outputSurfaces.add(mPreviewSurface);
            minFrameDuration = Math.max(minFrameDuration,
                    config.getOutputMinFrameDuration(SurfaceTexture.class, mPreviewSize));
        }
        Range<Integer> targetRange =
                CameraTestUtils.getSuitableFpsRangeForDuration(id,
                        minFrameDuration, mTestRule.getStaticInfo());
        previewBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, targetRange);

        // Create capture session
        boolean useSessionKeys = isFpsRangeASessionKey(ch);
        CaptureRequest previewRequest = previewBuilder.build();
        mTestRule.setCameraSessionListener(new BlockingSessionCallback());
        configureAndSetCameraSession(outputSurfaces, useSessionKeys, previewRequest);

        return previewRequest;
    }

    /**
     * Initialize the ImageReader instance and preview surface.
     * @param cameraId The camera to be opened.
     * @param format The format used to create ImageReader instance.
     */
    private void initializeImageReader(String cameraId, int format) throws Exception {
        mTestRule.setOrderedPreviewSizes(CameraTestUtils.getSortedSizesForFormat(
                cameraId, mTestRule.getCameraManager(), format,
                CameraTestUtils.getPreviewSizeBound(mTestRule.getWindowManager(),
                        CameraTestUtils.PREVIEW_SIZE_BOUND)));
        mPreviewSize = mTestRule.getOrderedPreviewSizes().get(0);
        mImageReaderFormat = format;
        mTestRule.createDefaultImageReader(
                mPreviewSize, format, NUM_MAX_IMAGES, /*listener*/null);
        updatePreviewSurface(mPreviewSize);
    }

    private void simpleOpenCamera(String cameraId) throws Exception {
        mTestRule.setCamera(CameraTestUtils.openCamera(
                mTestRule.getCameraManager(), cameraId,
                mTestRule.getCameraListener(), mTestRule.getHandler()));
        mTestRule.getCollector().setCameraId(cameraId);
        mTestRule.setStaticInfo(new StaticMetadata(
                mTestRule.getCameraManager().getCameraCharacteristics(cameraId),
                CheckLevel.ASSERT, /*collector*/null));
    }

    /**
     * Simple image listener that can be used to time the availability of first image.
     *
     */
    private static class SimpleImageListener implements ImageReader.OnImageAvailableListener {
        private ConditionVariable imageAvailable = new ConditionVariable();
        private boolean imageReceived = false;
        private long mTimeReceivedImage = 0;

        @Override
        public void onImageAvailable(ImageReader reader) {
            Image image = null;
            if (!imageReceived) {
                if (VERBOSE) {
                    Log.v(TAG, ""First image arrives"");
                }
                imageReceived = true;
                mTimeReceivedImage = SystemClock.elapsedRealtime();
                imageAvailable.open();
            }
            image = reader.acquireNextImage();
            if (image != null) {
                image.close();
            }
        }

        /**
         * Wait for image available, return immediately if the image was already
         * received, otherwise wait until an image arrives.
         */
        public void waitForImageAvailable(long timeout) {
            if (imageReceived) {
                imageReceived = false;
                return;
            }

            if (imageAvailable.block(timeout)) {
                imageAvailable.close();
                imageReceived = true;
            } else {
                throw new TimeoutRuntimeException(""Unable to get the first image after ""
                        + CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS + ""ms"");
            }
        }

        public long getTimeReceivedImage() {
            return mTimeReceivedImage;
        }
    }

    private static class SimpleTimingResultListener
            extends CameraCaptureSession.CaptureCallback {
        private final LinkedBlockingQueue<Pair<CaptureResult, Long> > mPartialResultQueue =
                new LinkedBlockingQueue<Pair<CaptureResult, Long> >();
        private final LinkedBlockingQueue<Pair<CaptureResult, Long> > mResultQueue =
                new LinkedBlockingQueue<Pair<CaptureResult, Long> > ();

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                TotalCaptureResult result) {
            try {
                Long time = SystemClock.elapsedRealtime();
                mResultQueue.put(new Pair<CaptureResult, Long>(result, time));
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureCompleted"");
            }
        }

        @Override
        public void onCaptureProgressed(CameraCaptureSession session, CaptureRequest request,
                CaptureResult partialResult) {
            try {
                // check if AE and AF state exists
                Long time = -1L;
                if (partialResult.get(CaptureResult.CONTROL_AE_STATE) != null &&
                        partialResult.get(CaptureResult.CONTROL_AF_STATE) != null) {
                    time = SystemClock.elapsedRealtime();
                }
                mPartialResultQueue.put(new Pair<CaptureResult, Long>(partialResult, time));
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureProgressed"");
            }
        }

        public Pair<CaptureResult, Long> getPartialResultNTime(long timeout) {
            try {
                Pair<CaptureResult, Long> result =
                        mPartialResultQueue.poll(timeout, TimeUnit.MILLISECONDS);
                return result;
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }
        }

        public Pair<CaptureResult, Long> getCaptureResultNTime(long timeout) {
            try {
                Pair<CaptureResult, Long> result =
                        mResultQueue.poll(timeout, TimeUnit.MILLISECONDS);
                assertNotNull(""Wait for a capture result timed out in "" + timeout + ""ms"", result);
                return result;
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }
        }

        public Pair<CaptureResult, Long> getPartialResultNTimeForRequest(CaptureRequest myRequest,
                int numResultsWait) {
            if (numResultsWait < 0) {
                throw new IllegalArgumentException(""numResultsWait must be no less than 0"");
            }

            Pair<CaptureResult, Long> result;
            int i = 0;
            do {
                result = getPartialResultNTime(CameraTestUtils.CAPTURE_RESULT_TIMEOUT_MS);
                // The result may be null if no partials are produced on this particular path, so
                // stop trying
                if (result == null) break;
                if (result.first.getRequest().equals(myRequest)) {
                    return result;
                }
            } while (i++ < numResultsWait);

            // No partials produced - this may not be an error, since a given device may not
            // produce any partials on this testing path
            return null;
        }

        public Pair<CaptureResult, Long> getCaptureResultNTimeForRequest(CaptureRequest myRequest,
                int numResultsWait) {
            if (numResultsWait < 0) {
                throw new IllegalArgumentException(""numResultsWait must be no less than 0"");
            }

            Pair<CaptureResult, Long> result;
            int i = 0;
            do {
                result = getCaptureResultNTime(CameraTestUtils.CAPTURE_RESULT_TIMEOUT_MS);
                if (result.first.getRequest().equals(myRequest)) {
                    return result;
                }
            } while (i++ < numResultsWait);

            throw new TimeoutRuntimeException(""Unable to get the expected capture result after ""
                    + ""waiting for "" + numResultsWait + "" results"");
        }

    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.jvmti.cts.JvmtiRedefineClassesTest"	"Finish"	""	"/home/gpoor/cts-12-source/cts/hostsidetests/jvmti/redefining/app/src/android/jvmti/cts/JvmtiRedefineClassesTest.java"	""	"/*
 * Copyright (C) 2017 The Android Open Source Project
 *
 * Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except
 * in compliance with the License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License
 * is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
 * or implied. See the License for the specific language governing permissions and limitations under
 * the License.
 */
package android.jvmti.cts;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertArrayEquals;
import static org.junit.Assert.assertTrue;

import dalvik.system.InMemoryDexClassLoader;

import java.lang.ref.WeakReference;
import java.nio.ByteBuffer;
import java.util.Arrays;
import java.util.ArrayList;
import java.util.Base64;
import java.util.Collections;
import java.util.LinkedList;
import java.util.List;

import org.junit.Assert;
import org.junit.After;
import org.junit.Before;
import org.junit.BeforeClass;
import org.junit.Test;

import art.Main;

/**
 * Check redefineClasses-related functionality.
 */
public class JvmtiRedefineClassesTest extends JvmtiTestBase {

    @Before
    public void setUp() throws Exception {
        // make sure everything is cleared.
        setTransformationEvent(false);
        setPopTransformations(true);
        clearTransformations();
        // Make sure Transform.class is in the initial state.
        checkRedefinition(INITIAL_TRANSFORM);
    }

    static class RedefineError {
        public int expectedError;
        public Class<?> target;
        public byte[] dexData;

        public RedefineError(int err, Class<?> target, String base64string) {
            this(err, target, Base64.getDecoder().decode(base64string));
        }

        public RedefineError(int err, Class<?> klass, byte[] dex) {
            this.expectedError = err;
            this.target = klass;
            this.dexData = dex;
        }
    }

    // Just an interface.
    interface Consumer<T> {
        public void accept(T data);
    }

    static class StringCollector implements Consumer<String> {
        public ArrayList<String> reports = new ArrayList<>();

        public void accept(String data) {
            reports.add(data);
        }
    }

    /**
     * Try to redefine a class and assert that the redefinition matches whats expected.
     */
    private static void checkRedefinition(RedefineError err) {
        assertEquals(err.expectedError, redefineClass(err.target, err.dexData));
    }

    // This is a class that we will transform for tests.
    // NB This has the actual name Landroid/jvmti/cts/JvmtiRedefineClassesTest$Transform;
    static class Transform {
        // NB This field has type Landroid/jvmti/cts/JvmtiRedefineClassesTest$Consumer;
        private Consumer<String> reporter;

        public Transform(Consumer<String> reporter) {
            this.reporter = reporter;
        }

        private void Start() {
            reporter.accept(""hello - private"");
        }

        private void Finish() {
            reporter.accept(""goodbye - private"");
        }

        public void sayHi(Runnable r) {
            reporter.accept(""Pre Start private method call"");
            Start();
            reporter.accept(""Post Start private method call"");
            r.run();
            reporter.accept(""Pre Finish private method call"");
            Finish();
            reporter.accept(""Post Finish private method call"");
        }
    }

    /**
     * Base64 encoded dex file for the initial version of Transform class.
     */
    private static final RedefineError INITIAL_TRANSFORM = new RedefineError(
            JvmtiErrors.NONE, Transform.class,
            ""ZGV4CjAzNQA+L+iHAAAAAAAAAAAAAAAAAAAAAAAAAACgBgAAcAAAAHhWNBIAAAAAAAAAANwFAAAi"" +
            ""AAAAcAAAAAkAAAD4AAAABAAAABwBAAABAAAATAEAAAcAAABUAQAAAQAAAIwBAAD0BAAArAEAAKwB"" +
            ""AACvAQAAsgEAALoBAAC+AQAAxAEAAMwBAADrAQAAIQIAAFgCAACQAgAAvgIAAOICAAACAwAAIQMA"" +
            ""ADUDAABLAwAAXwMAAIADAACgAwAAwAMAAN8DAADmAwAA8QMAAPQDAAD4AwAAAAQAAA0EAAAgBAAA"" +
            ""MQQAADcEAABBBAAARgQAAE0EAAAIAAAACQAAAAoAAAALAAAADAAAAA0AAAAOAAAADwAAABcAAAAX"" +
            ""AAAACAAAAAAAAAAYAAAACAAAAFQEAAAYAAAACAAAAFwEAAAYAAAACAAAAGQEAAABAAAAHgAAAAAA"" +
            ""AgAZAAAAAQABAAIAAAABAAAABQAAAAEAAAAVAAAAAQADACAAAAAGAAAAAgAAAAcAAAAfAAAAAQAA"" +
            ""AAAAAAAGAAAAAAAAAAYAAAC8BQAAWAUAAAAAAAABKAABPAAGPGluaXQ+AAI+OwAEPjspVgAGRmlu"" +
            ""aXNoAB1Kdm10aVJlZGVmaW5lQ2xhc3Nlc1Rlc3QuamF2YQA0TGFuZHJvaWQvanZtdGkvY3RzL0p2"" +
            ""bXRpUmVkZWZpbmVDbGFzc2VzVGVzdCRDb25zdW1lcgA1TGFuZHJvaWQvanZtdGkvY3RzL0p2bXRp"" +
            ""UmVkZWZpbmVDbGFzc2VzVGVzdCRDb25zdW1lcjsANkxhbmRyb2lkL2p2bXRpL2N0cy9Kdm10aVJl"" +
            ""ZGVmaW5lQ2xhc3Nlc1Rlc3QkVHJhbnNmb3JtOwAsTGFuZHJvaWQvanZtdGkvY3RzL0p2bXRpUmVk"" +
            ""ZWZpbmVDbGFzc2VzVGVzdDsAIkxkYWx2aWsvYW5ub3RhdGlvbi9FbmNsb3NpbmdDbGFzczsAHkxk"" +
            ""YWx2aWsvYW5ub3RhdGlvbi9Jbm5lckNsYXNzOwAdTGRhbHZpay9hbm5vdGF0aW9uL1NpZ25hdHVy"" +
            ""ZTsAEkxqYXZhL2xhbmcvT2JqZWN0OwAUTGphdmEvbGFuZy9SdW5uYWJsZTsAEkxqYXZhL2xhbmcv"" +
            ""U3RyaW5nOwAfUG9zdCBGaW5pc2ggcHJpdmF0ZSBtZXRob2QgY2FsbAAeUG9zdCBTdGFydCBwcml2"" +
            ""YXRlIG1ldGhvZCBjYWxsAB5QcmUgRmluaXNoIHByaXZhdGUgbWV0aG9kIGNhbGwAHVByZSBTdGFy"" +
            ""dCBwcml2YXRlIG1ldGhvZCBjYWxsAAVTdGFydAAJVHJhbnNmb3JtAAFWAAJWTAAGYWNjZXB0AAth"" +
            ""Y2Nlc3NGbGFncwARZ29vZGJ5ZSAtIHByaXZhdGUAD2hlbGxvIC0gcHJpdmF0ZQAEbmFtZQAIcmVw"" +
            ""b3J0ZXIAA3J1bgAFc2F5SGkABXZhbHVlAAEAAAAAAAAAAQAAAAYAAAABAAAABwAAAAwBAAcOPC0A"" +
            ""FQAHDocAEQAHDocAGQEABw6HPIc8hzyHAAAAAAIAAgABAAAAbAQAAAYAAABwEAUAAABbAQAADgAD"" +
            ""AAEAAgAAAHQEAAAJAAAAVCAAABsBGwAAAHIgAAAQAA4AAAADAAEAAgAAAHoEAAAJAAAAVCAAABsB"" +
            ""HAAAAHIgAAAQAA4AAAAEAAIAAgAAAIAEAAAqAAAAVCAAABsBFAAAAHIgAAAQAHAQAwACAFQgAAAb"" +
            ""ARIAAAByIAAAEAByEAYAAwBUIAAAGwETAAAAciAAABAAcBACAAIAVCAAABsBEQAAAHIgAAAQAA4A"" +
            ""AAEDAQACAYGABJAJAQKsCQEC0AkEAfQJAgMBIRgCAgQCGgQIHRcWAgUBIRwEFwcXARcQFwMCBQEh"" +
            ""HAUXABcHFwEXEBcEAAAAAgAAAHAFAAB2BQAAAQAAAH8FAAABAAAAjQUAAKAFAAABAAAAAQAAAAAA"" +
            ""AAAAAAAArAUAAAEAAAC0BQAAEAAAAAAAAAABAAAAAAAAAAEAAAAiAAAAcAAAAAIAAAAJAAAA+AAA"" +
            ""AAMAAAAEAAAAHAEAAAQAAAABAAAATAEAAAUAAAAHAAAAVAEAAAYAAAABAAAAjAEAAAIgAAAiAAAA"" +
            ""rAEAAAEQAAADAAAAVAQAAAMgAAAEAAAAbAQAAAEgAAAEAAAAkAQAAAAgAAABAAAAWAUAAAQgAAAE"" +
            ""AAAAcAUAAAMQAAADAAAAoAUAAAYgAAABAAAAvAUAAAAQAAABAAAA3AUAAA=="");

    /**
     * Base64 encoded dex file containing the following inner class.
     * <code>
     * // NB This has the actual name Landroid/jvmti/cts/JvmtiRedefineClassesTest$Transform;
     * static class Transform {
     *     // NB This field has type Landroid/jvmti/cts/JvmtiRedefineClassesTest$Consumer;
     *     private Consumer<String> reporter;
     *     public Transform(Consumer<String> reporter) {
     *         this.reporter = reporter;
     *     }
     *     private void Start() {
     *         reporter.accept(""TRANSFORMED - Hello - private"");
     *     }
     *     private void Finish() {
     *         reporter.accept(""TRANSFORMED - Goodbye - private"");
     *     }
     *     public void sayHi(Runnable r) {
     *         reporter.accept(""TRANSFORMED - pre Start private method call"");
     *         Start();
     *         reporter.accept(""TRANSFORMED - post Start private method call"");
     *         r.run();
     *         reporter.accept(""TRANSFORMED - pre Finish private method call"");
     *         Finish();
     *         reporter.accept(""TRANSFORMED - post Finish private method call"");
     *     }
     * }
     * </code>
     */
    private static final RedefineError GOOD_TRANSFORM = new RedefineError(
            JvmtiErrors.NONE, Transform.class,
            ""ZGV4CjAzNQBmR3TRAAAAAAAAAAAAAAAAAAAAAAAAAAD0BgAAcAAAAHhWNBIAAAAAAAAAADAGAAAi"" +
            ""AAAAcAAAAAkAAAD4AAAABAAAABwBAAABAAAATAEAAAcAAABUAQAAAQAAAIwBAABIBQAArAEAAKwB"" +
            ""AACvAQAAsgEAALoBAAC+AQAAxAEAAMwBAADrAQAAIQIAAFgCAACQAgAAvgIAAOICAAACAwAAIQMA"" +
            ""ADUDAABLAwAAXwMAAGYDAACHAwAApgMAANUDAAADBAAAMQQAAF4EAABpBAAAbAQAAHAEAAB4BAAA"" +
            ""hQQAAIsEAACVBAAAmgQAAKEEAAAIAAAACQAAAAoAAAALAAAADAAAAA0AAAAOAAAADwAAABkAAAAZ"" +
            ""AAAACAAAAAAAAAAaAAAACAAAAKgEAAAaAAAACAAAALAEAAAaAAAACAAAALgEAAABAAAAHgAAAAAA"" +
            ""AgAbAAAAAQABAAIAAAABAAAABQAAAAEAAAARAAAAAQADACAAAAAGAAAAAgAAAAcAAAAfAAAAAQAA"" +
            ""AAAAAAAGAAAAAAAAAAYAAAAQBgAArAUAAAAAAAABKAABPAAGPGluaXQ+AAI+OwAEPjspVgAGRmlu"" +
            ""aXNoAB1Kdm10aVJlZGVmaW5lQ2xhc3Nlc1Rlc3QuamF2YQA0TGFuZHJvaWQvanZtdGkvY3RzL0p2"" +
            ""bXRpUmVkZWZpbmVDbGFzc2VzVGVzdCRDb25zdW1lcgA1TGFuZHJvaWQvanZtdGkvY3RzL0p2bXRp"" +
            ""UmVkZWZpbmVDbGFzc2VzVGVzdCRDb25zdW1lcjsANkxhbmRyb2lkL2p2bXRpL2N0cy9Kdm10aVJl"" +
            ""ZGVmaW5lQ2xhc3Nlc1Rlc3QkVHJhbnNmb3JtOwAsTGFuZHJvaWQvanZtdGkvY3RzL0p2bXRpUmVk"" +
            ""ZWZpbmVDbGFzc2VzVGVzdDsAIkxkYWx2aWsvYW5ub3RhdGlvbi9FbmNsb3NpbmdDbGFzczsAHkxk"" +
            ""YWx2aWsvYW5ub3RhdGlvbi9Jbm5lckNsYXNzOwAdTGRhbHZpay9hbm5vdGF0aW9uL1NpZ25hdHVy"" +
            ""ZTsAEkxqYXZhL2xhbmcvT2JqZWN0OwAUTGphdmEvbGFuZy9SdW5uYWJsZTsAEkxqYXZhL2xhbmcv"" +
            ""U3RyaW5nOwAFU3RhcnQAH1RSQU5TRk9STUVEIC0gR29vZGJ5ZSAtIHByaXZhdGUAHVRSQU5TRk9S"" +
            ""TUVEIC0gSGVsbG8gLSBwcml2YXRlAC1UUkFOU0ZPUk1FRCAtIHBvc3QgRmluaXNoIHByaXZhdGUg"" +
            ""bWV0aG9kIGNhbGwALFRSQU5TRk9STUVEIC0gcG9zdCBTdGFydCBwcml2YXRlIG1ldGhvZCBjYWxs"" +
            ""ACxUUkFOU0ZPUk1FRCAtIHByZSBGaW5pc2ggcHJpdmF0ZSBtZXRob2QgY2FsbAArVFJBTlNGT1JN"" +
            ""RUQgLSBwcmUgU3RhcnQgcHJpdmF0ZSBtZXRob2QgY2FsbAAJVHJhbnNmb3JtAAFWAAJWTAAGYWNj"" +
            ""ZXB0AAthY2Nlc3NGbGFncwAEbmFtZQAIcmVwb3J0ZXIAA3J1bgAFc2F5SGkABXZhbHVlAAEAAAAA"" +
            ""AAAAAQAAAAYAAAABAAAABwAAAAsBAAcOPC0AFAAHDocAEAAHDocAGAEABw6HPIc8hzyHAAAAAAIA"" +
            ""AgABAAAAwAQAAAYAAABwEAUAAABbAQAADgADAAEAAgAAAMgEAAAJAAAAVCAAABsBEgAAAHIgAAAQ"" +
            ""AA4AAAADAAEAAgAAAM4EAAAJAAAAVCAAABsBEwAAAHIgAAAQAA4AAAAEAAIAAgAAANQEAAAqAAAA"" +
            ""VCAAABsBFwAAAHIgAAAQAHAQAwACAFQgAAAbARUAAAByIAAAEAByEAYAAwBUIAAAGwEWAAAAciAA"" +
            ""ABAAcBACAAIAVCAAABsBFAAAAHIgAAAQAA4AAAEDAQACAYGABOQJAQKACgECpAoEAcgKAgMBIRgC"" +
            ""AgQCHAQIHRcYAgUBIRwEFwcXARcQFwMCBQEhHAUXABcHFwEXEBcEAAAAAgAAAMQFAADKBQAAAQAA"" +
            ""ANMFAAABAAAA4QUAAPQFAAABAAAAAQAAAAAAAAAAAAAAAAYAAAEAAAAIBgAAEAAAAAAAAAABAAAA"" +
            ""AAAAAAEAAAAiAAAAcAAAAAIAAAAJAAAA+AAAAAMAAAAEAAAAHAEAAAQAAAABAAAATAEAAAUAAAAH"" +
            ""AAAAVAEAAAYAAAABAAAAjAEAAAIgAAAiAAAArAEAAAEQAAADAAAAqAQAAAMgAAAEAAAAwAQAAAEg"" +
            ""AAAEAAAA5AQAAAAgAAABAAAArAUAAAQgAAAEAAAAxAUAAAMQAAADAAAA9AUAAAYgAAABAAAAEAYA"" +
            ""AAAQAAABAAAAMAYAAA=="");

    /**
     * Tests that we can redefine Transform class from INITIAL_TRANSFORM to GOOD_TRANSFORM.
     * <p>
     * It uses doRedefine to do the transformation.
     */
    private void checkRedefinedTransform(Runnable doRedefine) {
        // The consumer that we use to observe the changes to the Transform class.
        final StringCollector c = new StringCollector();
        Transform t = new Transform(c);
        // Run once without changes.
        t.sayHi(new Runnable() {
            public void run() {
                c.accept(""Initial test run. No changes."");
            }
        });
        // Run once with obsolete methods.
        t.sayHi(new Runnable() {
            public void run() {
                c.accept(""Redefining calling function."");
                doRedefine.run();
            }
        });
        // Run once with new definition.
        t.sayHi(new Runnable() {
            public void run() {
                c.accept(""Final test run."");
            }
        });

        String[] output = c.reports.toArray(new String[0]);
        assertArrayEquals(
                new String[]{
                        // The first call to sayHi
                        ""Pre Start private method call"",
                        ""hello - private"",
                        ""Post Start private method call"",
                        ""Initial test run. No changes."",
                        ""Pre Finish private method call"",
                        ""goodbye - private"",
                        ""Post Finish private method call"",

                        // The second call to sayHi.
                        ""Pre Start private method call"",
                        ""hello - private"",
                        ""Post Start private method call"",
                        ""Redefining calling function."",
                        ""Pre Finish private method call"",
                        ""TRANSFORMED - Goodbye - private"",
                        ""Post Finish private method call"",

                        // The final call to sayHi.
                        ""TRANSFORMED - pre Start private method call"",
                        ""TRANSFORMED - Hello - private"",
                        ""TRANSFORMED - post Start private method call"",
                        ""Final test run."",
                        ""TRANSFORMED - pre Finish private method call"",
                        ""TRANSFORMED - Goodbye - private"",
                        ""TRANSFORMED - post Finish private method call"",
                }, output);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.jvmti.cts.JvmtiRedefineClassesTest"	"testSucessfulRetransform"	""	"/home/gpoor/cts-12-source/cts/hostsidetests/jvmti/redefining/app/src/android/jvmti/cts/JvmtiRedefineClassesTest.java"	""	"public void testSucessfulRetransform() throws Exception {
        pushTransformationResult(GOOD_TRANSFORM.target, GOOD_TRANSFORM.dexData);
        checkRedefinedTransform(new Runnable() {
            public void run() {
                setTransformationEvent(true);
                assertEquals(JvmtiErrors.NONE, retransformClass(Transform.class));
            }
        });
    }

    // This is a class that we will transform for tests.
    // NB This has the actual name Landroid/jvmti/cts/JvmtiRedefineClassesTest$Transform2;
    static class Transform2 {
        public void sayHi() {
            Assert.fail(""Should not be called!"");
        }
    }

    /**
     * Test cases for failing redefines.
     */
    private static final RedefineError[] FAILING_DEX_FILES = {
            /**
             * Base64 for this class.
             *
             *  .class Landroid/jvmti/cts/JvmtiRedefineClassesTest$Transform2;
             *  .super Ljava/lang/Object;
             *  .source ""JvmtiRedefineClassesTest.java""
             *
             *  # annotations
             *  .annotation system Ldalvik/annotation/EnclosingClass;
             *      value = Landroid/jvmti/cts/JvmtiRedefineClassesTest;
             *  .end annotation
             *
             *  .annotation system Ldalvik/annotation/InnerClass;
             *      accessFlags = 0x8
             *      name = ""Transform2""
             *  .end annotation
             *
             *  # direct methods
             *  .method constructor <init>()V
             *      .registers 1
             *      .prologue
             *      .line 33
             *      invoke-direct {p0}, Ljava/lang/Object;-><init>()V
             *      return-void
             *  .end method
             *
             *  # virtual methods
             *  .method public sayHi()V
             *      .registers 1
             *      .prologue
             *      .line 35
             *      return-object v0
             *  .end method
            */
            new RedefineError(JvmtiErrors.FAILS_VERIFICATION, Transform2.class,
                    ""ZGV4CjAzNQBOhefYdQRcgqmkwhWsSyzb5I3udX0SnJ44AwAAcAAAAHhWNBIAAAAAAAAAAIwCAAAN"" +
                    ""AAAAcAAAAAYAAACkAAAAAQAAALwAAAAAAAAAAAAAAAMAAADIAAAAAQAAAOAAAAA4AgAAAAEAAAAB"" +
                    ""AAAIAQAAJwEAAGABAACOAQAAsgEAANIBAADmAQAA8gEAAPUBAAACAgAACAIAAA8CAAACAAAAAwAA"" +
                    ""AAQAAAAFAAAABgAAAAgAAAAIAAAABQAAAAAAAAAAAAAAAAAAAAAAAAALAAAABAAAAAAAAAAAAAAA"" +
                    ""AAAAAAQAAAAAAAAAAQAAADgCAAB+AgAAAAAAAAY8aW5pdD4AHUp2bXRpUmVkZWZpbmVDbGFzc2Vz"" +
                    ""VGVzdC5qYXZhADdMYW5kcm9pZC9qdm10aS9jdHMvSnZtdGlSZWRlZmluZUNsYXNzZXNUZXN0JFRy"" +
                    ""YW5zZm9ybTI7ACxMYW5kcm9pZC9qdm10aS9jdHMvSnZtdGlSZWRlZmluZUNsYXNzZXNUZXN0OwAi"" +
                    ""TGRhbHZpay9hbm5vdGF0aW9uL0VuY2xvc2luZ0NsYXNzOwAeTGRhbHZpay9hbm5vdGF0aW9uL0lu"" +
                    ""bmVyQ2xhc3M7ABJMamF2YS9sYW5nL09iamVjdDsAClRyYW5zZm9ybTIAAVYAC2FjY2Vzc0ZsYWdz"" +
                    ""AARuYW1lAAVzYXlIaQAFdmFsdWUAAAACAwIJBAgKFwcCAgEMGAEAAAAAAAIAAAAhAgAAGAIAACwC"" +
                    ""AAAAAAAAAAAAAAAAAAAhAAcOACMABw4AAAABAAEAAQAAAEgCAAAEAAAAcBACAAAADgACAAEAAAAA"" +
                    ""AE0CAAABAAAAEQAAAAEBAICABNQEAQHsBA4AAAAAAAAAAQAAAAAAAAABAAAADQAAAHAAAAACAAAA"" +
                    ""BgAAAKQAAAADAAAAAQAAALwAAAAFAAAAAwAAAMgAAAAGAAAAAQAAAOAAAAACIAAADQAAAAABAAAE"" +
                    ""IAAAAgAAABgCAAADEAAAAgAAACgCAAAGIAAAAQAAADgCAAADIAAAAgAAAEgCAAABIAAAAgAAAFQC"" +
                    ""AAAAIAAAAQAAAH4CAAAAEAAAAQAAAIwCAAA=""),
            /**
             * Base64 for this class.
             *
             *  static class Transform {
             *    private Consumer<String> reporter;
             *    public Transform(Consumer<String> reporter) {
             *      this.reporter = reporter;
             *    }
             *    private void Start() { }
             *    private void Finish() { }
             *    protected void sayHi(Runnable r) { }
             *  }
            */
            new RedefineError(
                    JvmtiErrors.UNSUPPORTED_REDEFINITION_METHOD_MODIFIERS_CHANGED, Transform.class,
                    ""ZGV4CjAzNQAf2DrkAAAAAAAAAAAAAAAAAAAAAAAAAAAwBQAAcAAAAHhWNBIAAAAAAAAAAGwEAAAa"" +
                    ""AAAAcAAAAAkAAADYAAAAAwAAAPwAAAABAAAAIAEAAAUAAAAoAQAAAQAAAFABAADAAwAAcAEAAHAB"" +
                    ""AABzAQAAdgEAAH4BAACCAQAAiAEAAJABAACvAQAA5QEAABwCAABUAgAAggIAAKYCAADGAgAA5QIA"" +
                    ""APkCAAAPAwAAIwMAACoDAAA1AwAAOAMAADwDAABJAwAATwMAAFkDAABgAwAACAAAAAkAAAAKAAAA"" +
                    ""CwAAAAwAAAANAAAADgAAAA8AAAATAAAAEwAAAAgAAAAAAAAAFAAAAAgAAABoAwAAFAAAAAgAAABw"" +
                    ""AwAAAQAAABcAAAABAAEAAgAAAAEAAAAFAAAAAQAAABEAAAABAAIAGAAAAAYAAAACAAAAAQAAAAAA"" +
                    ""AAAGAAAAAAAAAAYAAABMBAAA6AMAAAAAAAABKAABPAAGPGluaXQ+AAI+OwAEPjspVgAGRmluaXNo"" +
                    ""AB1Kdm10aVJlZGVmaW5lQ2xhc3Nlc1Rlc3QuamF2YQA0TGFuZHJvaWQvanZtdGkvY3RzL0p2bXRp"" +
                    ""UmVkZWZpbmVDbGFzc2VzVGVzdCRDb25zdW1lcgA1TGFuZHJvaWQvanZtdGkvY3RzL0p2bXRpUmVk"" +
                    ""ZWZpbmVDbGFzc2VzVGVzdCRDb25zdW1lcjsANkxhbmRyb2lkL2p2bXRpL2N0cy9Kdm10aVJlZGVm"" +
                    ""aW5lQ2xhc3Nlc1Rlc3QkVHJhbnNmb3JtOwAsTGFuZHJvaWQvanZtdGkvY3RzL0p2bXRpUmVkZWZp"" +
                    ""bmVDbGFzc2VzVGVzdDsAIkxkYWx2aWsvYW5ub3RhdGlvbi9FbmNsb3NpbmdDbGFzczsAHkxkYWx2"" +
                    ""aWsvYW5ub3RhdGlvbi9Jbm5lckNsYXNzOwAdTGRhbHZpay9hbm5vdGF0aW9uL1NpZ25hdHVyZTsA"" +
                    ""EkxqYXZhL2xhbmcvT2JqZWN0OwAUTGphdmEvbGFuZy9SdW5uYWJsZTsAEkxqYXZhL2xhbmcvU3Ry"" +
                    ""aW5nOwAFU3RhcnQACVRyYW5zZm9ybQABVgACVkwAC2FjY2Vzc0ZsYWdzAARuYW1lAAhyZXBvcnRl"" +
                    ""cgAFc2F5SGkABXZhbHVlAAABAAAAAAAAAAEAAAAHAAAACwEABw48LQARAAcOAA8ABw4AEwEABw4A"" +
                    ""AgACAAEAAAB4AwAABgAAAHAQBAAAAFsBAAAOAAEAAQAAAAAAgAMAAAEAAAAOAAAAAQABAAAAAACF"" +
                    ""AwAAAQAAAA4AAAACAAIAAAAAAIoDAAABAAAADgAAAAABAwEAAgCBgASQBwECrAcBAsAHAwTUBwID"" +
                    ""ARkYAgIEAhUECBYXEgIFARkcBBcHFwEXEBcDAgUBGRwFFwAXBxcBFxAXBAAAAAIAAAAABAAABgQA"" +
                    ""AAEAAAAPBAAAAQAAAB0EAAAwBAAAAQAAAAEAAAAAAAAAAAAAADwEAAAAAAAARAQAABAAAAAAAAAA"" +
                    ""AQAAAAAAAAABAAAAGgAAAHAAAAACAAAACQAAANgAAAADAAAAAwAAAPwAAAAEAAAAAQAAACABAAAF"" +
                    ""AAAABQAAACgBAAAGAAAAAQAAAFABAAACIAAAGgAAAHABAAABEAAAAgAAAGgDAAADIAAABAAAAHgD"" +
                    ""AAABIAAABAAAAJADAAAAIAAAAQAAAOgDAAAEIAAABAAAAAAEAAADEAAAAwAAADAEAAAGIAAAAQAA"" +
                    ""AEwEAAAAEAAAAQAAAGwEAAA=""),
            /**
             * Base64 for this class.
             *
             *  static final class Transform {
             *    private Consumer<String> reporter;
             *    public Transform(Consumer<String> reporter) {
             *      this.reporter = reporter;
             *    }
             *    private void Start() { }
             *    private void Finish() { }
             *    public void sayHi(Runnable r) { }
             *  }
            */
            new RedefineError(
                    JvmtiErrors.UNSUPPORTED_REDEFINITION_CLASS_MODIFIERS_CHANGED, Transform.class,
                    ""ZGV4CjAzNQA82MAwAAAAAAAAAAAAAAAAAAAAAAAAAAAwBQAAcAAAAHhWNBIAAAAAAAAAAGwEAAAa"" +
                    ""AAAAcAAAAAkAAADYAAAAAwAAAPwAAAABAAAAIAEAAAUAAAAoAQAAAQAAAFABAADAAwAAcAEAAHAB"" +
                    ""AABzAQAAdgEAAH4BAACCAQAAiAEAAJABAACvAQAA5QEAABwCAABUAgAAggIAAKYCAADGAgAA5QIA"" +
                    ""APkCAAAPAwAAIwMAACoDAAA1AwAAOAMAADwDAABJAwAATwMAAFkDAABgAwAACAAAAAkAAAAKAAAA"" +
                    ""CwAAAAwAAAANAAAADgAAAA8AAAATAAAAEwAAAAgAAAAAAAAAFAAAAAgAAABoAwAAFAAAAAgAAABw"" +
                    ""AwAAAQAAABcAAAABAAEAAgAAAAEAAAAFAAAAAQAAABEAAAABAAIAGAAAAAYAAAACAAAAAQAAABAA"" +
                    ""AAAGAAAAAAAAAAYAAABMBAAA6AMAAAAAAAABKAABPAAGPGluaXQ+AAI+OwAEPjspVgAGRmluaXNo"" +
                    ""AB1Kdm10aVJlZGVmaW5lQ2xhc3Nlc1Rlc3QuamF2YQA0TGFuZHJvaWQvanZtdGkvY3RzL0p2bXRp"" +
                    ""UmVkZWZpbmVDbGFzc2VzVGVzdCRDb25zdW1lcgA1TGFuZHJvaWQvanZtdGkvY3RzL0p2bXRpUmVk"" +
                    ""ZWZpbmVDbGFzc2VzVGVzdCRDb25zdW1lcjsANkxhbmRyb2lkL2p2bXRpL2N0cy9Kdm10aVJlZGVm"" +
                    ""aW5lQ2xhc3Nlc1Rlc3QkVHJhbnNmb3JtOwAsTGFuZHJvaWQvanZtdGkvY3RzL0p2bXRpUmVkZWZp"" +
                    ""bmVDbGFzc2VzVGVzdDsAIkxkYWx2aWsvYW5ub3RhdGlvbi9FbmNsb3NpbmdDbGFzczsAHkxkYWx2"" +
                    ""aWsvYW5ub3RhdGlvbi9Jbm5lckNsYXNzOwAdTGRhbHZpay9hbm5vdGF0aW9uL1NpZ25hdHVyZTsA"" +
                    ""EkxqYXZhL2xhbmcvT2JqZWN0OwAUTGphdmEvbGFuZy9SdW5uYWJsZTsAEkxqYXZhL2xhbmcvU3Ry"" +
                    ""aW5nOwAFU3RhcnQACVRyYW5zZm9ybQABVgACVkwAC2FjY2Vzc0ZsYWdzAARuYW1lAAhyZXBvcnRl"" +
                    ""cgAFc2F5SGkABXZhbHVlAAABAAAAAAAAAAEAAAAHAAAACwEABw48LQARAAcOAA8ABw4AEwEABw4A"" +
                    ""AgACAAEAAAB4AwAABgAAAHAQBAAAAFsBAAAOAAEAAQAAAAAAgAMAAAEAAAAOAAAAAQABAAAAAACF"" +
                    ""AwAAAQAAAA4AAAACAAIAAAAAAIoDAAABAAAADgAAAAABAwEAAgCBgASQBwECrAcBAsAHAwHUBwID"" +
                    ""ARkYAgIEAhUEGBYXEgIFARkcBBcHFwEXEBcDAgUBGRwFFwAXBxcBFxAXBAAAAAIAAAAABAAABgQA"" +
                    ""AAEAAAAPBAAAAQAAAB0EAAAwBAAAAQAAAAEAAAAAAAAAAAAAADwEAAAAAAAARAQAABAAAAAAAAAA"" +
                    ""AQAAAAAAAAABAAAAGgAAAHAAAAACAAAACQAAANgAAAADAAAAAwAAAPwAAAAEAAAAAQAAACABAAAF"" +
                    ""AAAABQAAACgBAAAGAAAAAQAAAFABAAACIAAAGgAAAHABAAABEAAAAgAAAGgDAAADIAAABAAAAHgD"" +
                    ""AAABIAAABAAAAJADAAAAIAAAAQAAAOgDAAAEIAAABAAAAAAEAAADEAAAAwAAADAEAAAGIAAAAQAA"" +
                    ""AEwEAAAAEAAAAQAAAGwEAAA=""),
            /**
             * Base64 for this class.
             *
             *  static class Transform {
             *    private Consumer<String> reporter;
             *    public Transform(Consumer<String> reporter) {
             *      this.reporter = reporter;
             *    }
             *    private void Finish() { }
             *    public void sayHi(Runnable r) { }
             *  }
            */
            new RedefineError(
                    JvmtiErrors.UNSUPPORTED_REDEFINITION_METHOD_DELETED, Transform.class,
                    ""ZGV4CjAzNQBG028hAAAAAAAAAAAAAAAAAAAAAAAAAAAABQAAcAAAAHhWNBIAAAAAAAAAADwEAAAZ"" +
                    ""AAAAcAAAAAkAAADUAAAAAwAAAPgAAAABAAAAHAEAAAQAAAAkAQAAAQAAAEQBAACcAwAAZAEAAGQB"" +
                    ""AABnAQAAagEAAHIBAAB2AQAAfAEAAJsBAADRAQAACAIAAEACAABuAgAAkgIAALICAADRAgAA5QIA"" +
                    ""APsCAAAPAwAAFgMAACEDAAAkAwAAKAMAADUDAAA7AwAARQMAAEwDAAAHAAAACAAAAAkAAAAKAAAA"" +
                    ""CwAAAAwAAAANAAAADgAAABIAAAASAAAACAAAAAAAAAATAAAACAAAAFQDAAATAAAACAAAAFwDAAAB"" +
                    ""AAAAFgAAAAEAAQACAAAAAQAAABAAAAABAAIAFwAAAAYAAAACAAAAAQAAAAAAAAAGAAAAAAAAAAUA"" +
                    ""AAAcBAAAvAMAAAAAAAABKAABPAAGPGluaXQ+AAI+OwAEPjspVgAdSnZtdGlSZWRlZmluZUNsYXNz"" +
                    ""ZXNUZXN0LmphdmEANExhbmRyb2lkL2p2bXRpL2N0cy9Kdm10aVJlZGVmaW5lQ2xhc3Nlc1Rlc3Qk"" +
                    ""Q29uc3VtZXIANUxhbmRyb2lkL2p2bXRpL2N0cy9Kdm10aVJlZGVmaW5lQ2xhc3Nlc1Rlc3QkQ29u"" +
                    ""c3VtZXI7ADZMYW5kcm9pZC9qdm10aS9jdHMvSnZtdGlSZWRlZmluZUNsYXNzZXNUZXN0JFRyYW5z"" +
                    ""Zm9ybTsALExhbmRyb2lkL2p2bXRpL2N0cy9Kdm10aVJlZGVmaW5lQ2xhc3Nlc1Rlc3Q7ACJMZGFs"" +
                    ""dmlrL2Fubm90YXRpb24vRW5jbG9zaW5nQ2xhc3M7AB5MZGFsdmlrL2Fubm90YXRpb24vSW5uZXJD"" +
                    ""bGFzczsAHUxkYWx2aWsvYW5ub3RhdGlvbi9TaWduYXR1cmU7ABJMamF2YS9sYW5nL09iamVjdDsA"" +
                    ""FExqYXZhL2xhbmcvUnVubmFibGU7ABJMamF2YS9sYW5nL1N0cmluZzsABVN0YXJ0AAlUcmFuc2Zv"" +
                    ""cm0AAVYAAlZMAAthY2Nlc3NGbGFncwAEbmFtZQAIcmVwb3J0ZXIABXNheUhpAAV2YWx1ZQAAAQAA"" +
                    ""AAAAAAABAAAABwAAAA0BAAcOAA4ABw4AEAEABw4AAAAAAgACAAEAAABkAwAABgAAAHAQAwAAAFsB"" +
                    ""AAAOAAEAAQAAAAAAagMAAAEAAAAOAAAAAgACAAAAAABvAwAAAQAAAA4AAAAAAQIBAAIAgYAE+AYB"" +
                    ""ApQHAgGoBwIDARgYAgIEAhQECBUXEQIFARgcBBcGFwEXDxcDAgUBGBwFFwAXBhcBFw8XBAAAAAIA"" +
                    ""AADQAwAA1gMAAAEAAADfAwAAAQAAAO0DAAAABAAAAQAAAAEAAAAAAAAAAAAAAAwEAAAAAAAAFAQA"" +
                    ""ABAAAAAAAAAAAQAAAAAAAAABAAAAGQAAAHAAAAACAAAACQAAANQAAAADAAAAAwAAAPgAAAAEAAAA"" +
                    ""AQAAABwBAAAFAAAABAAAACQBAAAGAAAAAQAAAEQBAAACIAAAGQAAAGQBAAABEAAAAgAAAFQDAAAD"" +
                    ""IAAAAwAAAGQDAAABIAAAAwAAAHgDAAAAIAAAAQAAALwDAAAEIAAABAAAANADAAADEAAAAwAAAAAE"" +
                    ""AAAGIAAAAQAAABwEAAAAEAAAAQAAADwEAAA=""),
            /**
             * Base6is class.
             *
             *  static class Transform {
             *    private Consumer<String> reporter;
             *    public Transform(Consumer<String> reporter) {
             *      this.reporter = reporter;
             *    }
             *    private void Start() { }
             *    private void Start2() { }
             *    private void Finish() { }
             *    public void sayHi(Runnable r) { }
             *  }
            */
            new RedefineError(
                    JvmtiErrors.UNSUPPORTED_REDEFINITION_METHOD_ADDED, Transform.class,
                    ""ZGV4CjAzNQC43HElAAAAAAAAAAAAAAAAAAAAAAAAAABgBQAAcAAAAHhWNBIAAAAAAAAAAJwEAAAb"" +
                    ""AAAAcAAAAAkAAADcAAAAAwAAAAABAAABAAAAJAEAAAYAAAAsAQAAAQAAAFwBAADkAwAAfAEAAHwB"" +
                    ""AAB/AQAAggEAAIoBAACOAQAAlAEAAJwBAAC7AQAA8QEAACgCAABgAgAAjgIAALICAADSAgAA8QIA"" +
                    ""AAUDAAAbAwAALwMAADYDAAA+AwAASQMAAEwDAABQAwAAXQMAAGMDAABtAwAAdAMAAAgAAAAJAAAA"" +
                    ""CgAAAAsAAAAMAAAADQAAAA4AAAAPAAAAFAAAABQAAAAIAAAAAAAAABUAAAAIAAAAfAMAABUAAAAI"" +
                    ""AAAAhAMAAAEAAAAYAAAAAQABAAIAAAABAAAABQAAAAEAAAARAAAAAQAAABIAAAABAAIAGQAAAAYA"" +
                    ""AAACAAAAAQAAAAAAAAAGAAAAAAAAAAYAAAB8BAAAFAQAAAAAAAABKAABPAAGPGluaXQ+AAI+OwAE"" +
                    ""PjspVgAGRmluaXNoAB1Kdm10aVJlZGVmaW5lQ2xhc3Nlc1Rlc3QuamF2YQA0TGFuZHJvaWQvanZt"" +
                    ""dGkvY3RzL0p2bXRpUmVkZWZpbmVDbGFzc2VzVGVzdCRDb25zdW1lcgA1TGFuZHJvaWQvanZtdGkv"" +
                    ""Y3RzL0p2bXRpUmVkZWZpbmVDbGFzc2VzVGVzdCRDb25zdW1lcjsANkxhbmRyb2lkL2p2bXRpL2N0"" +
                    ""cy9Kdm10aVJlZGVmaW5lQ2xhc3Nlc1Rlc3QkVHJhbnNmb3JtOwAsTGFuZHJvaWQvanZtdGkvY3Rz"" +
                    ""L0p2bXRpUmVkZWZpbmVDbGFzc2VzVGVzdDsAIkxkYWx2aWsvYW5ub3RhdGlvbi9FbmNsb3NpbmdD"" +
                    ""bGFzczsAHkxkYWx2aWsvYW5ub3RhdGlvbi9Jbm5lckNsYXNzOwAdTGRhbHZpay9hbm5vdGF0aW9u"" +
                    ""L1NpZ25hdHVyZTsAEkxqYXZhL2xhbmcvT2JqZWN0OwAUTGphdmEvbGFuZy9SdW5uYWJsZTsAEkxq"" +
                    ""YXZhL2xhbmcvU3RyaW5nOwAFU3RhcnQABlN0YXJ0MgAJVHJhbnNmb3JtAAFWAAJWTAALYWNjZXNz"" +
                    ""RmxhZ3MABG5hbWUACHJlcG9ydGVyAAVzYXlIaQAFdmFsdWUAAAEAAAAAAAAAAQAAAAcAAAANAQAH"" +
                    ""DgAQAAcOAA4ABw4ADwAHDgARAQAHDgAAAgACAAEAAACMAwAABgAAAHAQBQAAAFsBAAAOAAEAAQAA"" +
                    ""AAAAkgMAAAEAAAAOAAAAAQABAAAAAACXAwAAAQAAAA4AAAABAAEAAAAAAJwDAAABAAAADgAAAAIA"" +
                    ""AgAAAAAAoQMAAAEAAAAOAAAAAAEEAQACAIGABKgHAQLEBwEC2AcBAuwHBAGACAIDARoYAgIEAhYE"" +
                    ""CBcXEwIFARocBBcHFwEXEBcDAgUBGhwFFwAXBxcBFxAXBAAAAAIAAAAwBAAANgQAAAEAAAA/BAAA"" +
                    ""AQAAAE0EAABgBAAAAQAAAAEAAAAAAAAAAAAAAGwEAAAAAAAAdAQAABAAAAAAAAAAAQAAAAAAAAAB"" +
                    ""AAAAGwAAAHAAAAACAAAACQAAANwAAAADAAAAAwAAAAABAAAEAAAAAQAAACQBAAAFAAAABgAAACwB"" +
                    ""AAAGAAAAAQAAAFwBAAACIAAAGwAAAHwBAAABEAAAAgAAAHwDAAADIAAABQAAAIwDAAABIAAABQAA"" +
                    ""AKgDAAAAIAAAAQAAABQEAAAEIAAABAAAADAEAAADEAAAAwAAAGAEAAAGIAAAAQAAAHwEAAAAEAAA"" +
                    ""AQAAAJwEAAA=""),
            /**
             * Base64 for this class.
             *
             *  static class Transform {
             *    public Transform(Consumer<String> reporter) { }
             *    private void Start() { }
             *    private void Finish() { }
             *    public void sayHi(Runnable r) { }
             *  }
            */
            new RedefineError(
                    JvmtiErrors.UNSUPPORTED_REDEFINITION_SCHEMA_CHANGED, Transform.class,
                    ""ZGV4CjAzNQCn0K9/AAAAAAAAAAAAAAAAAAAAAAAAAADkBAAAcAAAAHhWNBIAAAAAAAAAACwEAAAY"" +
                    ""AAAAcAAAAAkAAADQAAAAAwAAAPQAAAAAAAAAAAAAAAUAAAAYAQAAAQAAAEABAACEAwAAYAEAAGAB"" +
                    ""AABjAQAAZgEAAG4BAAB0AQAAfAEAAJsBAADRAQAACAIAAEACAABuAgAAkgIAALICAADRAgAA5QIA"" +
                    ""APsCAAAPAwAAFgMAACEDAAAkAwAAKAMAADUDAAA7AwAAQgMAAAcAAAAIAAAACQAAAAoAAAALAAAA"" +
                    ""DAAAAA0AAAAOAAAAEgAAABIAAAAIAAAAAAAAABMAAAAIAAAATAMAABMAAAAIAAAAVAMAAAEAAQAC"" +
                    ""AAAAAQAAAAQAAAABAAAAEAAAAAEAAgAWAAAABgAAAAIAAAABAAAAAAAAAAYAAAAAAAAABQAAABQE"" +
                    ""AADIAwAAAAAAAAEoAAE8AAY8aW5pdD4ABD47KVYABkZpbmlzaAAdSnZtdGlSZWRlZmluZUNsYXNz"" +
                    ""ZXNUZXN0LmphdmEANExhbmRyb2lkL2p2bXRpL2N0cy9Kdm10aVJlZGVmaW5lQ2xhc3Nlc1Rlc3Qk"" +
                    ""Q29uc3VtZXIANUxhbmRyb2lkL2p2bXRpL2N0cy9Kdm10aVJlZGVmaW5lQ2xhc3Nlc1Rlc3QkQ29u"" +
                    ""c3VtZXI7ADZMYW5kcm9pZC9qdm10aS9jdHMvSnZtdGlSZWRlZmluZUNsYXNzZXNUZXN0JFRyYW5z"" +
                    ""Zm9ybTsALExhbmRyb2lkL2p2bXRpL2N0cy9Kdm10aVJlZGVmaW5lQ2xhc3Nlc1Rlc3Q7ACJMZGFs"" +
                    ""dmlrL2Fubm90YXRpb24vRW5jbG9zaW5nQ2xhc3M7AB5MZGFsdmlrL2Fubm90YXRpb24vSW5uZXJD"" +
                    ""bGFzczsAHUxkYWx2aWsvYW5ub3RhdGlvbi9TaWduYXR1cmU7ABJMamF2YS9sYW5nL09iamVjdDsA"" +
                    ""FExqYXZhL2xhbmcvUnVubmFibGU7ABJMamF2YS9sYW5nL1N0cmluZzsABVN0YXJ0AAlUcmFuc2Zv"" +
                    ""cm0AAVYAAlZMAAthY2Nlc3NGbGFncwAEbmFtZQAFc2F5SGkABXZhbHVlAAAAAAEAAAAAAAAAAQAA"" +
                    ""AAcAAAAMAQAHDgAOAAcOAA0ABw4ADwEABw4AAAACAAIAAQAAAFwDAAAEAAAAcBAEAAAADgABAAEA"" +
                    ""AAAAAGIDAAABAAAADgAAAAEAAQAAAAAAZwMAAAEAAAAOAAAAAgACAAAAAABsAwAAAQAAAA4AAAAA"" +
                    ""AAMBAIGABPQGAQKMBwECoAcDAbQHAAACAwEXGAICBAIUBAgVFxECBQEXHAUXABcGFwEXDxcDAAIA"" +
                    ""AADgAwAA5gMAAAEAAADvAwAAAAQAAAAAAAABAAAAAAAAAAAAAAAMBAAADwAAAAAAAAABAAAAAAAA"" +
                    ""AAEAAAAYAAAAcAAAAAIAAAAJAAAA0AAAAAMAAAADAAAA9AAAAAUAAAAFAAAAGAEAAAYAAAABAAAA"" +
                    ""QAEAAAIgAAAYAAAAYAEAAAEQAAACAAAATAMAAAMgAAAEAAAAXAMAAAEgAAAEAAAAdAMAAAAgAAAB"" +
                    ""AAAAyAMAAAQgAAADAAAA4AMAAAMQAAACAAAAAAQAAAYgAAABAAAAFAQAAAAQAAABAAAALAQAAA==""),
            /**
             * Base64 for this class.
             *
             *  static class Transform3 {
             *      // NB This field has type Landroid/jvmti/cts/JvmtiRedefineClassesTest$Consumer;
             *      private Consumer<String> reporter;
             *      public Transform3(Consumer<String> reporter) { this.reporter = reporter; }
             *      private void Start() { }
             *      private void Finish() { }
             *      public void sayHi(Runnable r) { }
             *  }
            */
            new RedefineError(
                    JvmtiErrors.NAMES_DONT_MATCH, Transform.class,
                    ""ZGV4CjAzNQCc2B3nAAAAAAAAAAAAAAAAAAAAAAAAAAA0BQAAcAAAAHhWNBIAAAAAAAAAAHAEAAAa"" +
                    ""AAAAcAAAAAkAAADYAAAAAwAAAPwAAAABAAAAIAEAAAUAAAAoAQAAAQAAAFABAADEAwAAcAEAAHAB"" +
                    ""AABzAQAAdgEAAH4BAACCAQAAiAEAAJABAACvAQAA5QEAABwCAABVAgAAgwIAAKcCAADHAgAA5gIA"" +
                    ""APoCAAAQAwAAJAMAACsDAAA3AwAAOgMAAD4DAABLAwAAUQMAAFsDAABiAwAACAAAAAkAAAAKAAAA"" +
                    ""CwAAAAwAAAANAAAADgAAAA8AAAATAAAAEwAAAAgAAAAAAAAAFAAAAAgAAABsAwAAFAAAAAgAAAB0"" +
                    ""AwAAAQAAABcAAAABAAEAAgAAAAEAAAAFAAAAAQAAABEAAAABAAIAGAAAAAYAAAACAAAAAQAAAAAA"" +
                    ""AAAGAAAAAAAAAAYAAABQBAAA7AMAAAAAAAABKAABPAAGPGluaXQ+AAI+OwAEPjspVgAGRmluaXNo"" +
                    ""AB1Kdm10aVJlZGVmaW5lQ2xhc3Nlc1Rlc3QuamF2YQA0TGFuZHJvaWQvanZtdGkvY3RzL0p2bXRp"" +
                    ""UmVkZWZpbmVDbGFzc2VzVGVzdCRDb25zdW1lcgA1TGFuZHJvaWQvanZtdGkvY3RzL0p2bXRpUmVk"" +
                    ""ZWZpbmVDbGFzc2VzVGVzdCRDb25zdW1lcjsAN0xhbmRyb2lkL2p2bXRpL2N0cy9Kdm10aVJlZGVm"" +
                    ""aW5lQ2xhc3Nlc1Rlc3QkVHJhbnNmb3JtMzsALExhbmRyb2lkL2p2bXRpL2N0cy9Kdm10aVJlZGVm"" +
                    ""aW5lQ2xhc3Nlc1Rlc3Q7ACJMZGFsdmlrL2Fubm90YXRpb24vRW5jbG9zaW5nQ2xhc3M7AB5MZGFs"" +
                    ""dmlrL2Fubm90YXRpb24vSW5uZXJDbGFzczsAHUxkYWx2aWsvYW5ub3RhdGlvbi9TaWduYXR1cmU7"" +
                    ""ABJMamF2YS9sYW5nL09iamVjdDsAFExqYXZhL2xhbmcvUnVubmFibGU7ABJMamF2YS9sYW5nL1N0"" +
                    ""cmluZzsABVN0YXJ0AApUcmFuc2Zvcm0zAAFWAAJWTAALYWNjZXNzRmxhZ3MABG5hbWUACHJlcG9y"" +
                    ""dGVyAAVzYXlIaQAFdmFsdWUAAAAAAQAAAAAAAAABAAAABwAAAAwBAAcOAA4ABw4ADQAHDgAPAQAH"" +
                    ""DgAAAAIAAgABAAAAfAMAAAYAAABwEAQAAABbAQAADgABAAEAAAAAAIIDAAABAAAADgAAAAEAAQAA"" +
                    ""AAAAhwMAAAEAAAAOAAAAAgACAAAAAACMAwAAAQAAAA4AAAAAAQMBAAIAgYAElAcBArAHAQLEBwMB"" +
                    ""2AcCAwEZGAICBAIVBAgWFxICBQEZHAQXBxcBFxAXAwIFARkcBRcAFwcXARcQFwQAAAACAAAABAQA"" +
                    ""AAoEAAABAAAAEwQAAAEAAAAhBAAANAQAAAEAAAABAAAAAAAAAAAAAABABAAAAAAAAEgEAAAQAAAA"" +
                    ""AAAAAAEAAAAAAAAAAQAAABoAAABwAAAAAgAAAAkAAADYAAAAAwAAAAMAAAD8AAAABAAAAAEAAAAg"" +
                    ""AQAABQAAAAUAAAAoAQAABgAAAAEAAABQAQAAAiAAABoAAABwAQAAARAAAAIAAABsAwAAAyAAAAQA"" +
                    ""AAB8AwAAASAAAAQAAACUAwAAACAAAAEAAADsAwAABCAAAAQAAAAEBAAAAxAAAAMAAAA0BAAABiAA"" +
                    ""AAEAAABQBAAAABAAAAEAAABwBAAA""),
            /**
             * Base64 for this class.
             *
             *  static class Transform extends Observable {
             *      // NB This field has type Landroid/jvmti/cts/JvmtiRedefineClassesTest$Consumer;
             *      private Consumer<String> reporter;
             *      public Transform(Consumer<String> reporter) { super(); this.reporter = reporter; }
             *      private void Start() { }
             *      private void Finish() { }
             *      public void sayHi(Runnable r) { }
             *  }
            */
            new RedefineError(
                    JvmtiErrors.UNSUPPORTED_REDEFINITION_HIERARCHY_CHANGED, Transform.class,
                    ""ZGV4CjAzNQAV2qEZAAAAAAAAAAAAAAAAAAAAAAAAAAA0BQAAcAAAAHhWNBIAAAAAAAAAAHAEAAAa"" +
                    ""AAAAcAAAAAkAAADYAAAAAwAAAPwAAAABAAAAIAEAAAUAAAAoAQAAAQAAAFABAADEAwAAcAEAAHAB"" +
                    ""AABzAQAAdgEAAH4BAACCAQAAiAEAAJABAACvAQAA5QEAABwCAABUAgAAggIAAKYCAADGAgAA5QIA"" +
                    ""APsCAAAPAwAAJwMAAC4DAAA5AwAAPAMAAEADAABNAwAAUwMAAF0DAABkAwAACAAAAAkAAAAKAAAA"" +
                    ""CwAAAAwAAAANAAAADgAAABAAAAATAAAAEwAAAAgAAAAAAAAAFAAAAAgAAABsAwAAFAAAAAgAAAB0"" +
                    ""AwAAAQAAABcAAAABAAEAAgAAAAEAAAAFAAAAAQAAABEAAAABAAIAGAAAAAcAAAACAAAAAQAAAAAA"" +
                    ""AAAHAAAAAAAAAAYAAABQBAAA7AMAAAAAAAABKAABPAAGPGluaXQ+AAI+OwAEPjspVgAGRmluaXNo"" +
                    ""AB1Kdm10aVJlZGVmaW5lQ2xhc3Nlc1Rlc3QuamF2YQA0TGFuZHJvaWQvanZtdGkvY3RzL0p2bXRp"" +
                    ""UmVkZWZpbmVDbGFzc2VzVGVzdCRDb25zdW1lcgA1TGFuZHJvaWQvanZtdGkvY3RzL0p2bXRpUmVk"" +
                    ""ZWZpbmVDbGFzc2VzVGVzdCRDb25zdW1lcjsANkxhbmRyb2lkL2p2bXRpL2N0cy9Kdm10aVJlZGVm"" +
                    ""aW5lQ2xhc3Nlc1Rlc3QkVHJhbnNmb3JtOwAsTGFuZHJvaWQvanZtdGkvY3RzL0p2bXRpUmVkZWZp"" +
                    ""bmVDbGFzc2VzVGVzdDsAIkxkYWx2aWsvYW5ub3RhdGlvbi9FbmNsb3NpbmdDbGFzczsAHkxkYWx2"" +
                    ""aWsvYW5ub3RhdGlvbi9Jbm5lckNsYXNzOwAdTGRhbHZpay9hbm5vdGF0aW9uL1NpZ25hdHVyZTsA"" +
                    ""FExqYXZhL2xhbmcvUnVubmFibGU7ABJMamF2YS9sYW5nL1N0cmluZzsAFkxqYXZhL3V0aWwvT2Jz"" +
                    ""ZXJ2YWJsZTsABVN0YXJ0AAlUcmFuc2Zvcm0AAVYAAlZMAAthY2Nlc3NGbGFncwAEbmFtZQAIcmVw"" +
                    ""b3J0ZXIABXNheUhpAAV2YWx1ZQAAAQAAAAAAAAABAAAABgAAAA0BAAcOAA8ABw4ADgAHDgAQAQAH"" +
                    ""DgAAAAIAAgABAAAAfAMAAAYAAABwEAQAAABbAQAADgABAAEAAAAAAIIDAAABAAAADgAAAAEAAQAA"" +
                    ""AAAAhwMAAAEAAAAOAAAAAgACAAAAAACMAwAAAQAAAA4AAAAAAQMBAAIAgYAElAcBArAHAQLEBwMB"" +
                    ""2AcCAwEZGAICBAIVBAgWFxICBQEZHAQXBxcBFw8XAwIFARkcBRcAFwcXARcPFwQAAAACAAAABAQA"" +
                    ""AAoEAAABAAAAEwQAAAEAAAAhBAAANAQAAAEAAAABAAAAAAAAAAAAAABABAAAAAAAAEgEAAAQAAAA"" +
                    ""AAAAAAEAAAAAAAAAAQAAABoAAABwAAAAAgAAAAkAAADYAAAAAwAAAAMAAAD8AAAABAAAAAEAAAAg"" +
                    ""AQAABQAAAAUAAAAoAQAABgAAAAEAAABQAQAAAiAAABoAAABwAQAAARAAAAIAAABsAwAAAyAAAAQA"" +
                    ""AAB8AwAAASAAAAQAAACUAwAAACAAAAEAAADsAwAABCAAAAQAAAAEBAAAAxAAAAMAAAA0BAAABiAA"" +
                    ""AAEAAABQBAAAABAAAAEAAABwBAAA""),
            /**
             * Array classes are never modifiable.
             *
             * The base64 data is just an empty dex file. It has no classes associated with it.
            */
            new RedefineError(JvmtiErrors.UNMODIFIABLE_CLASS, Transform[].class,
                    ""ZGV4CjAzNQCRAy8PAAAAAAAAAAAAAAAAAAAAAAAAAACMAAAAcAAAAHhWNBIAAAAAAAAAAHAAAAAA"" +
                    ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAcAAAAcAAAAAIA"" +
                    ""AAAAAAAAAQAAAAAAAAAAEAAAAQAAAHAAAAA=""),
    };"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.jvmti.cts.JvmtiRedefineClassesTest"	"testRetransformFailures"	""	"/home/gpoor/cts-12-source/cts/hostsidetests/jvmti/redefining/app/src/android/jvmti/cts/JvmtiRedefineClassesTest.java"	""	"public void testRetransformFailures() throws Exception {
        setTransformationEvent(true);
        setPopTransformations(true);
        for (RedefineError e : FAILING_DEX_FILES) {
            checkRetransformation(e);
        }
    }

    private static final String ONLOAD_TEST_CLASS_NAME =
            ""android.jvmti.cts.memory_dex.TransformTarget"";
    /**
     * Base64 encoded version of the following class.
     * package android.jvmti.cts.memory_dex;
     * <p>
     * public class TransformTarget {
     * public void alpha() { }
     * public void beta() { }
     * }
     */
    private static final byte[] ONLOAD_INITIAL_CLASS = Base64.getDecoder().decode(
            ""ZGV4CjAzNQAQT37mO0bz7SniP0I8RLnGvVsfM5ybXmdYAgAAcAAAAHhWNBIAAAAAAAAAANABAAAI"" +
            ""AAAAcAAAAAMAAACQAAAAAQAAAJwAAAAAAAAAAAAAAAQAAACoAAAAAQAAAMgAAABwAQAA6AAAACgB"" +
            ""AAAwAQAAYAEAAHQBAACKAQAAjQEAAJQBAACaAQAAAQAAAAIAAAAEAAAABAAAAAIAAAAAAAAAAAAA"" +
            ""AAAAAAAAAAAABQAAAAAAAAAGAAAAAQAAAAAAAAAAAAAAAQAAAAEAAAAAAAAAAwAAAAAAAAC9AQAA"" +
            ""AAAAAAEAAQABAAAArgEAAAQAAABwEAMAAAAOAAEAAQAAAAAAswEAAAEAAAAOAAAAAQABAAAAAAC4"" +
            ""AQAAAQAAAA4AAAAGPGluaXQ+AC5MYW5kcm9pZC9qdm10aS9jdHMvbWVtb3J5X2RleC9UcmFuc2Zv"" +
            ""cm1UYXJnZXQ7ABJMamF2YS9sYW5nL09iamVjdDsAFFRyYW5zZm9ybVRhcmdldC5qYXZhAAFWAAVh"" +
            ""bHBoYQAEYmV0YQASZW1pdHRlcjogamFjay00LjI4AAIABw4AAwAHDgAEAAcOAAAAAQIAgYAE6AEB"" +
            ""AYACAQGUAgALAAAAAAAAAAEAAAAAAAAAAQAAAAgAAABwAAAAAgAAAAMAAACQAAAAAwAAAAEAAACc"" +
            ""AAAABQAAAAQAAACoAAAABgAAAAEAAADIAAAAASAAAAMAAADoAAAAAiAAAAgAAAAoAQAAAyAAAAMA"" +
            ""AACuAQAAACAAAAEAAAC9AQAAABAAAAEAAADQAQAA"");
    /**
     * Base64 encoded version of the following class.
     * Note that this would be an illegal transformation if the class had been loaded as the first
     * one.
     * <p>
     * package android.jvmti.cts.memory_dex;
     * <p>
     * public class TransformTarget {
     * public void alpha(int abc) {
     * }
     * public int beta() {
     * return 12;
     * }
     * public void gamma() {
     * }
     * }
     */
    private static final byte[] ONLOAD_FINAL_CLASS = Base64.getDecoder().decode(
            ""ZGV4CjAzNQA5VqbusSyl8/G1EXrbm9uRuiHvkP4XixrMAgAAcAAAAHhWNBIAAAAAAAAAADgCAAAL"" +
            ""AAAAcAAAAAQAAACcAAAAAwAAAKwAAAAAAAAAAAAAAAUAAADQAAAAAQAAAPgAAAC0AQAAGAEAAHYB"" +
            ""AAB+AQAAgQEAALEBAADFAQAA2wEAAN4BAADiAQAA6QEAAO8BAAADAgAAAQAAAAIAAAADAAAABQAA"" +
            ""AAEAAAAAAAAAAAAAAAUAAAADAAAAAAAAAAYAAAADAAAAcAEAAAEAAQAAAAAAAQACAAcAAAABAAAA"" +
            ""CAAAAAEAAQAKAAAAAgABAAAAAAABAAAAAQAAAAIAAAAAAAAABAAAAAAAAAAfAgAAAAAAAAEAAQAB"" +
            ""AAAACgIAAAQAAABwEAQAAAAOAAIAAgAAAAAADwIAAAEAAAAOAAAAAgABAAAAAAAVAgAAAwAAABMA"" +
            ""DAAPAAAAAQABAAAAAAAaAgAAAQAAAA4AAAABAAAAAAAGPGluaXQ+AAFJAC5MYW5kcm9pZC9qdm10"" +
            ""aS9jdHMvbWVtb3J5X2RleC9UcmFuc2Zvcm1UYXJnZXQ7ABJMamF2YS9sYW5nL09iamVjdDsAFFRy"" +
            ""YW5zZm9ybVRhcmdldC5qYXZhAAFWAAJWSQAFYWxwaGEABGJldGEAEmVtaXR0ZXI6IGphY2stNC4y"" +
            ""OAAFZ2FtbWEAAgAHDgAEAQAHDgAGAAcOAAkABw4AAAABAwCBgASYAgEBsAIBAcQCAQHcAgAAAAwA"" +
            ""AAAAAAAAAQAAAAAAAAABAAAACwAAAHAAAAACAAAABAAAAJwAAAADAAAAAwAAAKwAAAAFAAAABQAA"" +
            ""ANAAAAAGAAAAAQAAAPgAAAABIAAABAAAABgBAAABEAAAAQAAAHABAAACIAAACwAAAHYBAAADIAAA"" +
            ""BAAAAAoCAAAAIAAAAQAAAB8CAAAAEAAAAQAAADgCAAA="");

    private static class ExpectedMethod {
        public final Class<?> returnType;
        public final String name;
        public final Class<?>[] params;

        public ExpectedMethod(Class<?> returnType, String name, Class<?>... params) {
            this.returnType = returnType;
            this.name = name;
            this.params = params;
        }

        public void ensureHasMethod(Class<?> klass) throws Exception {
            try {
                assertEquals(returnType, klass.getDeclaredMethod(name, params).getReturnType());
            } catch (NoSuchMethodException e) {
                Assert.fail(""Could not find method: "" + klass + "": "" + name +
                        "" (params: "" + Arrays.toString(params) + ""). Reason was "" + e);
            }
        }
    }

    private void checkClassHasMethods(Class<?> target, ExpectedMethod[] methods) throws Exception {
        for (ExpectedMethod m : methods) {
            m.ensureHasMethod(target);
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.telephony.cts.TelephonyManagerTest"	"testTelephonyManager"	"CtsTelephonyTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/telephony/current/src/android/telephony/cts/TelephonyManagerTest.java"	""	"public void testTelephonyManager() {
        if (!InstrumentationRegistry.getContext().getPackageManager()
                .hasSystemFeature(PackageManager.FEATURE_TELEPHONY)) {
            Log.d(TAG, ""Skipping test that requires PackageManager.FEATURE_TELEPHONY"");
            return;
        }
        assertTrue(mTelephonyManager.getNetworkType() >= TelephonyManager.NETWORK_TYPE_UNKNOWN);
        assertTrue(mTelephonyManager.getPhoneType() >= TelephonyManager.PHONE_TYPE_NONE);
        assertTrue(mTelephonyManager.getSimState() >= TelephonyManager.SIM_STATE_UNKNOWN);
        assertTrue(mTelephonyManager.getDataActivity() >= TelephonyManager.DATA_ACTIVITY_NONE);
        assertTrue(mTelephonyManager.getDataState() >= TelephonyManager.DATA_DISCONNECTED);
        assertTrue(mTelephonyManager.getCallState() >= TelephonyManager.CALL_STATE_IDLE);

        for (int i = 0; i < mTelephonyManager.getPhoneCount(); ++i) {
            assertTrue(mTelephonyManager.getSimState(i) >= TelephonyManager.SIM_STATE_UNKNOWN);
        }

        // Make sure devices without MMS service won't fail on this
        if (InstrumentationRegistry.getContext().getPackageManager()
                .hasSystemFeature(PackageManager.FEATURE_TELEPHONY)
                && (mTelephonyManager.getPhoneType() != TelephonyManager.PHONE_TYPE_NONE)) {
            assertFalse(mTelephonyManager.getMmsUserAgent().isEmpty());
            assertFalse(mTelephonyManager.getMmsUAProfUrl().isEmpty());
        }

        // The following methods may return any value depending on the state of the device. Simply
        // call them to make sure they do not throw any exceptions.
        mTelephonyManager.getVoiceMailNumber();
        mTelephonyManager.getSimOperatorName();
        mTelephonyManager.getNetworkCountryIso();
        mTelephonyManager.getCellLocation();
        mTelephonyManager.getSimCarrierId();
        mTelephonyManager.getSimCarrierIdName();
        mTelephonyManager.getSimSpecificCarrierId();
        mTelephonyManager.getSimSpecificCarrierIdName();
        mTelephonyManager.getCarrierIdFromSimMccMnc();
        mTelephonyManager.isDataRoamingEnabled();
        ShellIdentityUtils.invokeMethodWithShellPermissions(mTelephonyManager,
                (tm) -> tm.getSimSerialNumber());
        mTelephonyManager.getSimOperator();
        mTelephonyManager.getSignalStrength();
        mTelephonyManager.getNetworkOperatorName();
        ShellIdentityUtils.invokeMethodWithShellPermissions(mTelephonyManager,
                (tm) -> tm.getSubscriberId());
        mTelephonyManager.getLine1Number();
        mTelephonyManager.getNetworkOperator();

        try {
            InstrumentationRegistry.getInstrumentation().getUiAutomation()
                    .adoptShellPermissionIdentity(
                            ""android.permission.READ_PRIVILEGED_PHONE_STATE"");
            mTelephonyManager.getPhoneAccountHandle();
        } catch (SecurityException e) {
            fail(""TelephonyManager#getPhoneAccountHandle requires READ_PRIVILEGED_PHONE_STATE"");
        } finally {
            InstrumentationRegistry.getInstrumentation().getUiAutomation()
                    .dropShellPermissionIdentity();
        }
        mTelephonyManager.getSimCountryIso();
        mTelephonyManager.getVoiceMailAlphaTag();
        mTelephonyManager.isNetworkRoaming();
        ShellIdentityUtils.invokeMethodWithShellPermissions(mTelephonyManager,
                (tm) -> tm.getDeviceId());
        ShellIdentityUtils.invokeMethodWithShellPermissions(mTelephonyManager,
                (tm) -> tm.getDeviceId(mTelephonyManager.getSlotIndex()));
        mTelephonyManager.getDeviceSoftwareVersion();
        ShellIdentityUtils.invokeMethodWithShellPermissions(mTelephonyManager,
                (tm) -> tm.getDeviceSoftwareVersion(mTelephonyManager.getSlotIndex()));
        ShellIdentityUtils.invokeMethodWithShellPermissions(mTelephonyManager,
                (tm) -> tm.getImei());
        ShellIdentityUtils.invokeMethodWithShellPermissions(mTelephonyManager,
                (tm) -> tm.getImei(mTelephonyManager.getSlotIndex()));
        ShellIdentityUtils.invokeMethodWithShellPermissions(mTelephonyManager,
                (tm) -> tm.isManualNetworkSelectionAllowed());
        ShellIdentityUtils.invokeMethodWithShellPermissions(mTelephonyManager,
                (tm) -> tm.getManualNetworkSelectionPlmn());

        mTelephonyManager.getPhoneCount();
        mTelephonyManager.getDataEnabled();
        mTelephonyManager.getNetworkSpecifier();
        ShellIdentityUtils.invokeMethodWithShellPermissions(mTelephonyManager, (tm) -> tm.getNai());
        TelecomManager telecomManager = getContext().getSystemService(TelecomManager.class);
        PhoneAccountHandle defaultAccount = telecomManager
                .getDefaultOutgoingPhoneAccount(PhoneAccount.SCHEME_TEL);
        mTelephonyManager.getVoicemailRingtoneUri(defaultAccount);
        mTelephonyManager.isVoicemailVibrationEnabled(defaultAccount);
        mTelephonyManager.getSubscriptionId(defaultAccount);
        mTelephonyManager.getCarrierConfig();
        mTelephonyManager.isVoiceCapable();
        mTelephonyManager.isSmsCapable();
        mTelephonyManager.isLteCdmaEvdoGsmWcdmaEnabled();
        ShellIdentityUtils.invokeMethodWithShellPermissions(mTelephonyManager,
                (tm) -> tm.isDataConnectionAllowed());
        ShellIdentityUtils.invokeMethodWithShellPermissions(mTelephonyManager,
                (tm) -> tm.isAnyRadioPoweredOn());
        ShellIdentityUtils.invokeMethodWithShellPermissionsNoReturn(mTelephonyManager,
                (tm) -> tm.resetIms(tm.getSlotIndex()));

        // Verify TelephonyManager.getCarrierPrivilegeStatus
        List<Integer> validCarrierPrivilegeStatus = new ArrayList<>();
        validCarrierPrivilegeStatus.add(TelephonyManager.CARRIER_PRIVILEGE_STATUS_HAS_ACCESS);
        validCarrierPrivilegeStatus.add(TelephonyManager.CARRIER_PRIVILEGE_STATUS_NO_ACCESS);
        validCarrierPrivilegeStatus.add(
                TelephonyManager.CARRIER_PRIVILEGE_STATUS_RULES_NOT_LOADED);
        validCarrierPrivilegeStatus.add(
                TelephonyManager.CARRIER_PRIVILEGE_STATUS_ERROR_LOADING_RULES);
        int carrierPrivilegeStatusResult = ShellIdentityUtils.invokeMethodWithShellPermissions(
                mTelephonyManager, (tm) -> tm.getCarrierPrivilegeStatus(Process.myUid()));
        assertTrue(validCarrierPrivilegeStatus.contains(carrierPrivilegeStatusResult));

        // Verify TelephonyManager.getCarrierPrivilegedPackagesForAllActiveSubscriptions
        List<String> resultForGetCarrierPrivilegedApis =
                ShellIdentityUtils.invokeMethodWithShellPermissions(mTelephonyManager,
                        (tm) -> tm.getCarrierPrivilegedPackagesForAllActiveSubscriptions());
        assertNotNull(resultForGetCarrierPrivilegedApis);
        for (String result : resultForGetCarrierPrivilegedApis) {
            assertFalse(TextUtils.isEmpty(result));
        }

        mTelephonyManager.getDefaultRespondViaMessageApplication();
        ShellIdentityUtils.invokeMethodWithShellPermissions(mTelephonyManager,
                TelephonyManager::getAndUpdateDefaultRespondViaMessageApplication);

        // Verify getImei/getSubscriberId/getIccAuthentication:
        // With app ops permision USE_ICC_AUTH_WITH_DEVICE_IDENTIFIER, should not throw
        // SecurityException.
        try {
            setAppOpsPermissionAllowed(true, OPSTR_USE_ICC_AUTH_WITH_DEVICE_IDENTIFIER);

            mTelephonyManager.getImei();
            mTelephonyManager.getSubscriberId();
            mTelephonyManager.getIccAuthentication(
                    TelephonyManager.APPTYPE_USIM, TelephonyManager.AUTHTYPE_EAP_AKA, """");
        } finally {
            setAppOpsPermissionAllowed(false, OPSTR_USE_ICC_AUTH_WITH_DEVICE_IDENTIFIER);
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.StreamingMediaPlayerTest"	"testRTSP_MPEG4SP_AAC_Video1"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/StreamingMediaPlayerTest.java"	""	"public void testRTSP_MPEG4SP_AAC_Video1() throws Exception {
        playVideoTest(""rtsp://v2.cache7.c.youtube.com/video.3gp?cid=0x271de9756065677e""
                + ""&fmt=17&user=android-device-test"", 176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.StreamingMediaPlayerTest"	"testRTSP_MPEG4SP_AAC_Video2"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/StreamingMediaPlayerTest.java"	""	"public void testRTSP_MPEG4SP_AAC_Video2() throws Exception {
        playVideoTest(""rtsp://v2.cache7.c.youtube.com/video.3gp?cid=0xc80658495af60617""
                + ""&fmt=17&user=android-device-test"", 176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.StreamingMediaPlayerTest"	"testRTSP_H264Base_AAC_Video1"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/StreamingMediaPlayerTest.java"	""	"public void testRTSP_H264Base_AAC_Video1() throws Exception {
        playVideoTest(""rtsp://v2.cache7.c.youtube.com/video.3gp?cid=0x271de9756065677e""
                + ""&fmt=18&user=android-device-test"", 480, 270);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.StreamingMediaPlayerTest"	"testRTSP_H264Base_AAC_Video2"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/StreamingMediaPlayerTest.java"	""	"public void testRTSP_H264Base_AAC_Video2() throws Exception {
        playVideoTest(""rtsp://v2.cache7.c.youtube.com/video.3gp?cid=0xc80658495af60617""
                + ""&fmt=18&user=android-device-test"", 480, 270);
    }
*/
    // Streaming HTTP video from YouTube"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.StreamingMediaPlayerTest"	"testHTTP_MPEG4SP_AAC_Video1"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/StreamingMediaPlayerTest.java"	""	"public void testHTTP_MPEG4SP_AAC_Video1() throws Exception {
        if (!MediaUtils.checkDecoder(MediaFormat.MIMETYPE_VIDEO_MPEG4)) {
            return; // skip
        }

        String urlString = dynamicConfig.getValue(HTTP_MPEG4_SP_AAC_VIDEO_1_KEY);
        playVideoTest(urlString, 176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.StreamingMediaPlayerTest"	"testHTTP_MPEG4SP_AAC_Video2"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/StreamingMediaPlayerTest.java"	""	"public void testHTTP_MPEG4SP_AAC_Video2() throws Exception {
        if (!MediaUtils.checkDecoder(MediaFormat.MIMETYPE_VIDEO_MPEG4)) {
            return; // skip
        }

        String urlString = dynamicConfig.getValue(HTTP_MPEG4_SP_AAC_VIDEO_2_KEY);
        playVideoTest(urlString, 176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.StreamingMediaPlayerTest"	"testHTTP_H264Base_AAC_Video1"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/StreamingMediaPlayerTest.java"	""	"public void testHTTP_H264Base_AAC_Video1() throws Exception {
        if (!MediaUtils.checkDecoder(MediaFormat.MIMETYPE_VIDEO_AVC)) {
            return; // skip
        }

        String urlString = dynamicConfig.getValue(HTTP_H264_BASE_AAC_VIDEO_1_KEY);
        playVideoTest(urlString, 640, 360);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.StreamingMediaPlayerTest"	"testHTTP_H264Base_AAC_Video2"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/StreamingMediaPlayerTest.java"	""	"public void testHTTP_H264Base_AAC_Video2() throws Exception {
        if (!MediaUtils.checkDecoder(MediaFormat.MIMETYPE_VIDEO_AVC)) {
            return; // skip
        }

        String urlString = dynamicConfig.getValue(HTTP_H264_BASE_AAC_VIDEO_2_KEY);
        playVideoTest(urlString, 640, 360);
    }

    // Streaming HLS video downloaded from YouTube"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.security.cts.TestMedi"	"testPocCVE_2020_0451"	"CtsSecurityBulletinHostTestCases"	"/home/gpoor/cts-12-source/cts/hostsidetests/securitybulletin/src/android/security/cts/TestMedia.java"	""	"@AsbSecurityTest(cveBugId = 158762825)
    public void testPocCVE_2020_0451() throws Exception {
        assumeFalse(moduleIsPlayManaged(""com.google.android.media.swcodec""));
        String inputFiles[] = {""cve_2020_0451.aac""};
        String binaryName = ""CVE-2020-0451"";
        String signals[] = {CrashUtils.SIGSEGV, CrashUtils.SIGBUS, CrashUtils.SIGABRT};
        AdbUtils.pocConfig testConfig = new AdbUtils.pocConfig(binaryName, getDevice());
        testConfig.config = new CrashUtils.Config().setProcessPatterns(binaryName);
        testConfig.config.setSignals(signals);
        testConfig.arguments = AdbUtils.TMP_PATH + inputFiles[0];
        testConfig.inputFiles = Arrays.asList(inputFiles);
        testConfig.inputFilesDestination = AdbUtils.TMP_PATH;
        AdbUtils.runPocAssertNoCrashesNotVulnerable(testConfig);
    }

    /**
     * b/112891564
     * Vulnerability Behaviour: SIGSEGV in self (Android P),
     *                          SIGABRT in self (Android Q onward)
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testBug11696552"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testBug11696552() throws Exception {
        MediaCodec mMediaCodec = MediaCodec.createDecoderByType(MediaFormat.MIMETYPE_AUDIO_AAC);
        MediaFormat mFormat = MediaFormat.createAudioFormat(
                MediaFormat.MIMETYPE_AUDIO_AAC, 48000 /* frequency */, 2 /* channels */);
        mFormat.setByteBuffer(""csd-0"", ByteBuffer.wrap( new byte [] {0x13, 0x10} ));
        mFormat.setInteger(MediaFormat.KEY_IS_ADTS, 1);
        mMediaCodec.configure(mFormat, null, null, 0);
        mMediaCodec.start();
        int index = mMediaCodec.dequeueInputBuffer(250000);
        mMediaCodec.queueInputBuffer(index, 0, 0, 0, MediaCodec.BUFFER_FLAG_END_OF_STREAM);
        MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
        mMediaCodec.dequeueOutputBuffer(info, 250000);
    }

    // The allowed errors in the following tests are the actual maximum measured
    // errors with the standard decoders, plus 10%.
    // This should allow for some variation in decoders, while still detecting
    // phase and delay errors, channel swap, etc."	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testDecodeAacTs"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testDecodeAacTs() throws Exception {
        testTimeStampOrdering(""sinesweeptsaac.m4a"");
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testTrackSelection"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testTrackSelection() throws Exception {
        testTrackSelection(""video_480x360_mp4_h264_1350kbps_30fps_aac_stereo_128kbps_44100hz.mp4"");
        testTrackSelection(
                ""video_480x360_mp4_h264_1350kbps_30fps_aac_stereo_128kbps_44100hz_fragmented.mp4"");
        testTrackSelection(
                ""video_480x360_mp4_h264_1350kbps_30fps_aac_stereo_128kbps_44100hz_dash.mp4"");
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testDecodeFragmented"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testDecodeFragmented() throws Exception {
        testDecodeFragmented(""video_480x360_mp4_h264_1350kbps_30fps_aac_stereo_128kbps_44100hz.mp4"",
                ""video_480x360_mp4_h264_1350kbps_30fps_aac_stereo_128kbps_44100hz_fragmented.mp4"");
        testDecodeFragmented(""video_480x360_mp4_h264_1350kbps_30fps_aac_stereo_128kbps_44100hz.mp4"",
                ""video_480x360_mp4_h264_1350kbps_30fps_aac_stereo_128kbps_44100hz_dash.mp4"");
    }

    private void testDecodeFragmented(final String reference, final String teststream)
            throws Exception {
        Preconditions.assertTestFileExists(mInpPrefix + reference);
        Preconditions.assertTestFileExists(mInpPrefix + teststream);
        try {
            MediaExtractor ex1 = new MediaExtractor();
            ex1.setDataSource(mInpPrefix + reference);
            MediaExtractor ex2 = new MediaExtractor();
            ex2.setDataSource(mInpPrefix + teststream);

            assertEquals(""different track count"", ex1.getTrackCount(), ex2.getTrackCount());

            ByteBuffer buf1 = ByteBuffer.allocate(1024*1024);
            ByteBuffer buf2 = ByteBuffer.allocate(1024*1024);

            for (int i = 0; i < ex1.getTrackCount(); i++) {
                // note: this assumes the tracks are reported in the order in which they appear
                // in the file.
                ex1.seekTo(0, MediaExtractor.SEEK_TO_NEXT_SYNC);
                ex1.selectTrack(i);
                ex2.seekTo(0, MediaExtractor.SEEK_TO_NEXT_SYNC);
                ex2.selectTrack(i);

                while(true) {
                    int n1 = ex1.readSampleData(buf1, 0);
                    int n2 = ex2.readSampleData(buf2, 0);
                    assertEquals(""different buffer size on track "" + i, n1, n2);

                    if (n1 < 0) {
                        break;
                    }
                    // see bug 13008204
                    buf1.limit(n1);
                    buf2.limit(n2);
                    buf1.rewind();
                    buf2.rewind();

                    assertEquals(""limit does not match return value on track "" + i,
                            n1, buf1.limit());
                    assertEquals(""limit does not match return value on track "" + i,
                            n2, buf2.limit());

                    assertEquals(""buffer data did not match on track "" + i, buf1, buf2);

                    ex1.advance();
                    ex2.advance();
                }
                ex1.unselectTrack(i);
                ex2.unselectTrack(i);
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    /**
     * Verify correct decoding of MPEG-4 AAC-LC mono and stereo streams
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testDecodeAacLcM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testDecodeAacLcM4a() throws Exception {
        // mono
        decodeNtest(""sinesweep1_1ch_8khz_aot2_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep1_1ch_11khz_aot2_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep1_1ch_12khz_aot2_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep1_1ch_16khz_aot2_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep1_1ch_22khz_aot2_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep1_1ch_24khz_aot2_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep1_1ch_32khz_aot2_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep1_1ch_44khz_aot2_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep1_1ch_48khz_aot2_mp4.m4a"", 40.f);
        // stereo
        decodeNtest(""sinesweep_2ch_8khz_aot2_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep_2ch_11khz_aot2_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep_2ch_12khz_aot2_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep_2ch_16khz_aot2_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep_2ch_22khz_aot2_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep_2ch_24khz_aot2_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep_2ch_32khz_aot2_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep_2ch_44khz_aot2_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep_2ch_48khz_aot2_mp4.m4a"", 40.f);
    }

    /**
     * Verify correct decoding of MPEG-4 AAC-LC 5.0 and 5.1 channel streams
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testDecodeAacLcMcM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testDecodeAacLcMcM4a() throws Exception {
        for (String codecName : codecsFor(""noise_6ch_48khz_aot2_mp4.m4a"")) {
            AudioParameter decParams = new AudioParameter();
            short[] decSamples = decodeToMemory(codecName, decParams,
                    ""noise_6ch_48khz_aot2_mp4.m4a"", RESET_MODE_NONE,
                    CONFIG_MODE_NONE, -1, null);
            checkEnergy(decSamples, decParams, 6);
            decParams.reset();

            decSamples = decodeToMemory(codecName, decParams, ""noise_5ch_44khz_aot2_mp4.m4a"",
                    RESET_MODE_NONE, CONFIG_MODE_NONE, -1, null);
            checkEnergy(decSamples, decParams, 5);
            decParams.reset();
        }
    }

    /**
     * Verify correct decoding of MPEG-4 HE-AAC mono and stereo streams
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testDecodeHeAacM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testDecodeHeAacM4a() throws Exception {
        Object [][] samples = {
                //  {resource, numChannels},
                {""noise_1ch_24khz_aot5_dr_sbr_sig1_mp4.m4a"", 1},
                {""noise_1ch_24khz_aot5_ds_sbr_sig1_mp4.m4a"", 1},
                {""noise_1ch_32khz_aot5_dr_sbr_sig2_mp4.m4a"", 1},
                {""noise_1ch_44khz_aot5_dr_sbr_sig0_mp4.m4a"", 1},
                {""noise_1ch_44khz_aot5_ds_sbr_sig2_mp4.m4a"", 1},
                {""noise_2ch_24khz_aot5_dr_sbr_sig2_mp4.m4a"", 2},
                {""noise_2ch_32khz_aot5_ds_sbr_sig2_mp4.m4a"", 2},
                {""noise_2ch_48khz_aot5_dr_sbr_sig1_mp4.m4a"", 2},
                {""noise_2ch_48khz_aot5_ds_sbr_sig1_mp4.m4a"", 2},
        };

        for (Object [] sample: samples) {
            for (String codecName : codecsFor((String)sample[0])) {
                AudioParameter decParams = new AudioParameter();
                short[] decSamples = decodeToMemory(codecName, decParams,
                        (String)sample[0] /* resource */, RESET_MODE_NONE, CONFIG_MODE_NONE,
                        -1, null);
                checkEnergy(decSamples, decParams, (Integer)sample[1] /* number of channels */);
                decParams.reset();
            }
        }
    }

    /**
     * Verify correct decoding of MPEG-4 HE-AAC 5.0 and 5.1 channel streams
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testDecodeHeAacMcM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testDecodeHeAacMcM4a() throws Exception {
        Object [][] samples = {
                //  {resource, numChannels},
                {""noise_5ch_48khz_aot5_dr_sbr_sig1_mp4.m4a"", 5},
                {""noise_6ch_44khz_aot5_dr_sbr_sig2_mp4.m4a"", 6},
        };
        for (Object [] sample: samples) {
            for (String codecName : codecsFor((String)sample[0] /* resource */)) {
                AudioParameter decParams = new AudioParameter();
                short[] decSamples = decodeToMemory(codecName, decParams,
                        (String)sample[0] /* resource */, RESET_MODE_NONE, CONFIG_MODE_NONE,
                        -1, null);
                checkEnergy(decSamples, decParams, (Integer)sample[1] /* number of channels */);
                decParams.reset();
            }
        }
    }

    /**
     * Verify correct decoding of MPEG-4 HE-AAC v2 stereo streams
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testDecodeHeAacV2M4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testDecodeHeAacV2M4a() throws Exception {
        String [] samples = {
                ""noise_2ch_24khz_aot29_dr_sbr_sig0_mp4.m4a"",
                ""noise_2ch_44khz_aot29_dr_sbr_sig1_mp4.m4a"",
                ""noise_2ch_48khz_aot29_dr_sbr_sig2_mp4.m4a""
        };
        for (String sample: samples) {
            for (String codecName : codecsFor(sample)) {
                AudioParameter decParams = new AudioParameter();
                short[] decSamples = decodeToMemory(codecName, decParams, sample,
                        RESET_MODE_NONE, CONFIG_MODE_NONE, -1, null);
                checkEnergy(decSamples, decParams, 2);
            }
        }
    }

    /**
     * Verify correct decoding of MPEG-4 AAC-ELD mono and stereo streams
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testDecodeAacEldM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testDecodeAacEldM4a() throws Exception {
        // mono
        decodeNtest(""sinesweep1_1ch_16khz_aot39_fl480_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep1_1ch_22khz_aot39_fl512_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep1_1ch_24khz_aot39_fl480_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep1_1ch_32khz_aot39_fl512_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep1_1ch_44khz_aot39_fl480_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep1_1ch_48khz_aot39_fl512_mp4.m4a"", 40.f);

        // stereo
        decodeNtest(""sinesweep_2ch_16khz_aot39_fl512_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep_2ch_22khz_aot39_fl480_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep_2ch_24khz_aot39_fl512_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep_2ch_32khz_aot39_fl480_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep_2ch_44khz_aot39_fl512_mp4.m4a"", 40.f);
        decodeNtest(""sinesweep_2ch_48khz_aot39_fl480_mp4.m4a"", 40.f);

        AudioParameter decParams = new AudioParameter();

        Object [][] samples = {
                //  {resource, numChannels},
                {""noise_1ch_16khz_aot39_ds_sbr_fl512_mp4.m4a"", 1},
                {""noise_1ch_24khz_aot39_ds_sbr_fl512_mp4.m4a"", 1},
                {""noise_1ch_32khz_aot39_dr_sbr_fl480_mp4.m4a"", 1},
                {""noise_1ch_44khz_aot39_ds_sbr_fl512_mp4.m4a"", 1},
                {""noise_1ch_44khz_aot39_ds_sbr_fl512_mp4.m4a"", 1},
                {""noise_1ch_48khz_aot39_dr_sbr_fl480_mp4.m4a"", 1},
                {""noise_2ch_22khz_aot39_ds_sbr_fl512_mp4.m4a"", 2},
                {""noise_2ch_32khz_aot39_ds_sbr_fl512_mp4.m4a"", 2},
                {""noise_2ch_44khz_aot39_dr_sbr_fl480_mp4.m4a"", 2},
                {""noise_2ch_48khz_aot39_ds_sbr_fl512_mp4.m4a"", 2},
        };
        for (Object [] sample: samples) {
            for (String codecName : codecsFor((String)sample[0])) {
                short[] decSamples = decodeToMemory(codecName, decParams,
                        (String)sample[0] /* resource */, RESET_MODE_NONE, CONFIG_MODE_NONE,
                        -1, null);
                checkEnergy(decSamples, decParams, (Integer)sample[1] /* number of channels */);
                decParams.reset();
            }
        }
    }

    /**
     * Perform a segmented energy analysis on given audio signal samples and run several tests on
     * the energy values.
     *
     * The main purpose is to verify whether an AAC decoder implementation applies Spectral Band
     * Replication (SBR) and Parametric Stereo (PS) correctly. Both tools are inherent parts to the
     * MPEG-4 HE-AAC and HE-AAC v2 audio codecs.
     *
     * In addition, this test can verify the correct decoding of multi-channel (e.g. 5.1 channel)
     * streams or the creation of a mixdown signal.
     *
     * Note: This test procedure is not an MPEG Conformance Test and can not serve as a replacement.
     *
     * @param decSamples the decoded audio samples to be tested
     * @param decParams the audio parameters of the given audio samples (decSamples)
     * @param encNch the encoded number of audio channels (number of channels of the original
     *               input)
     * @param nrgRatioThresh threshold to classify the energy ratios ]0.0, 1.0[
     * @throws RuntimeException
     */
    protected void checkEnergy(short[] decSamples, AudioParameter decParams, int encNch,
                             float nrgRatioThresh) throws RuntimeException
    {
        final int nSegPerBlk = 4;                          // the number of segments per block
        final int nCh = decParams.getNumChannels();        // the number of input channels
        final int nBlkSmp = decParams.getSamplingRate();   // length of one (LB/HB) block [samples]
        final int nSegSmp = nBlkSmp / nSegPerBlk;          // length of one segment [samples]
        final int smplPerChan = decSamples.length / nCh;   // actual # samples per channel (total)

        final int nSegSmpTot = nSegSmp * nCh;              // actual # samples per segment (all ch)
        final int nSegChOffst = 2 * nSegPerBlk;            // signal offset between chans [segments]
        final int procNch = Math.min(nCh, encNch);         // the number of channels to be analyzed
        if (encNch > 4) {
            assertTrue(String.format(""multichannel content (%dch) was downmixed (%dch)"",
                    encNch, nCh), procNch > 4);
        }
        assertTrue(String.format(""got less channels(%d) than encoded (%d)"", nCh, encNch),
                nCh >= encNch);

        final int encEffNch = (encNch > 5) ? encNch-1 : encNch;  // all original configs with more
                                                           // ... than five channel have an LFE */
        final int expSmplPerChan = Math.max(encEffNch, 2) * nSegChOffst * nSegSmp;
        final boolean isDmx = nCh < encNch;                // flag telling that input is dmx signal
        int effProcNch = procNch;                          // the num analyzed channels with signal

        assertTrue(""got less input samples than expected"", smplPerChan >= expSmplPerChan);

        // get the signal offset by counting zero samples at the very beginning (over all channels)
        final int zeroSigThresh = 1;                     // sample value threshold for signal search
        int signalStart = smplPerChan;                   // receives the number of samples that
                                                         // ... are in front of the actual signal
        int noiseStart = signalStart;                    // receives the number of null samples
                                                         // ... (per chan) at the very beginning
        for (int smpl = 0; smpl < decSamples.length; smpl++) {
            int value = Math.abs(decSamples[smpl]);
            if (value > 0 && noiseStart == signalStart) {
                noiseStart = smpl / nCh;                   // store start of prepended noise
            }                                              // ... (can be same as signalStart)
            if (value > zeroSigThresh) {
                signalStart = smpl / nCh;                  // store signal start offset [samples]
                break;
            }
        }
        signalStart = (signalStart > noiseStart+1) ? signalStart : noiseStart;
        assertTrue (""no signal found in any channel!"", signalStart < smplPerChan);
        final int totSeg = (smplPerChan-signalStart) / nSegSmp; // max num seg that fit into signal
        final int totSmp = nSegSmp * totSeg;               // max num relevant samples (per channel)
        assertTrue(""no segments left to test after signal search"", totSeg > 0);

        // get the energies and the channel offsets by searching for the first segment above the
        //  energy threshold
        final double zeroMaxNrgRatio = 0.001f;             // ratio of zeroNrgThresh to the max nrg
        double zeroNrgThresh = nSegSmp * nSegSmp;          // threshold to classify segment energies
        double totMaxNrg = 0.0f;                           // will store the max seg nrg over all ch
        double[][] nrg = new double[procNch][totSeg];      // array receiving the segment energies
        int[] offset = new int[procNch];                   // array for channel offsets
        boolean[] sigSeg = new boolean[totSeg];            // array receiving the segment ...
                                                           // ... energy status over all channels
        for (int ch = 0; ch < procNch; ch++) {
            offset[ch] = -1;
            for (int seg = 0; seg < totSeg; seg++) {
                final int smpStart = (signalStart * nCh) + (seg * nSegSmpTot) + ch;
                final int smpStop = smpStart + nSegSmpTot;
                for (int smpl = smpStart; smpl < smpStop; smpl += nCh) {
                    nrg[ch][seg] += decSamples[smpl] * decSamples[smpl];  // accumulate segment nrg
                }
                if (nrg[ch][seg] > zeroNrgThresh && offset[ch] < 0) { // store 1st segment (index)
                    offset[ch] = seg / nSegChOffst;        // ... per ch which has energy above the
                }                                          // ... threshold to get the ch offsets
                if (nrg[ch][seg] > totMaxNrg) {
                    totMaxNrg = nrg[ch][seg];              // store the max segment nrg over all ch
                }
                sigSeg[seg] |= nrg[ch][seg] > zeroNrgThresh;  // store whether the channel has
                                                           // ... energy in this segment
            }
            if (offset[ch] < 0) {                          // if one channel has no signal it is
                effProcNch -= 1;                           // ... most probably the LFE
                offset[ch] = effProcNch;                   // the LFE is no effective channel
            }
            if (ch == 0) {                                 // recalculate the zero signal threshold
                zeroNrgThresh = zeroMaxNrgRatio * totMaxNrg; // ... based on the 1st channels max
            }                                              // ... energy for all subsequent checks
        }
        // check the channel mapping
        assertTrue(""more than one LFE detected"", effProcNch >= procNch - 1);
        assertTrue(String.format(""less samples decoded than expected: %d < %d"",
                decSamples.length-(signalStart * nCh), totSmp * effProcNch),
                decSamples.length-(signalStart * nCh) >= totSmp * effProcNch);
        if (procNch >= 5) {                                // for multi-channel signals the only
            final int[] frontChMap1 = {2, 0, 1};           // valid front channel orders are L, R, C
            final int[] frontChMap2 = {0, 1, 2};           // or C, L, R (L=left, R=right, C=center)
            if ( !(Arrays.equals(Arrays.copyOfRange(offset, 0, 3), frontChMap1)
                    || Arrays.equals(Arrays.copyOfRange(offset, 0, 3), frontChMap2)) ) {
                fail(""wrong front channel mapping"");
            }
        }
        // check whether every channel occurs exactly once
        int[] chMap = new int[nCh];                        // mapping array to sort channels
        for (int ch = 0; ch < effProcNch; ch++) {
            int occurred = 0;
            for (int idx = 0; idx < procNch; idx++) {
                if (offset[idx] == ch) {
                    occurred += 1;
                    chMap[ch] = idx;                       // create mapping table to address chans
                }                                          // ... from front to back
            }                                              // the LFE must be last
            assertTrue(String.format(""channel %d occurs %d times in the mapping"", ch, occurred),
                    occurred == 1);
        }

        // go over all segment energies in all channels and check them
        double refMinNrg = zeroNrgThresh;                  // reference min energy for the 1st ch;
                                                           // others will be compared against 1st
        for (int ch = 0; ch < procNch; ch++) {
            int idx = chMap[ch];                           // resolve channel mapping
            final int ofst = offset[idx] * nSegChOffst;    // signal offset [segments]
            if (ch < effProcNch && ofst < totSeg) {
                int nrgSegEnd;                             // the last segment that has energy
                int nrgSeg;                                // the number of segments with energy
                if ((encNch <= 2) && (ch == 0)) {          // the first channel of a mono or ...
                    nrgSeg = totSeg;                       // stereo signal has full signal ...
                } else {                                   // all others have one LB + one HB block
                    nrgSeg = Math.min(totSeg, (2 * nSegPerBlk) + ofst) - ofst;
                }
                nrgSegEnd = ofst + nrgSeg;
                // find min and max energy of all segments that should have signal
                double minNrg = nrg[idx][ofst];            // channels minimum segment energy
                double maxNrg = nrg[idx][ofst];            // channels maximum segment energy
                for (int seg = ofst+1; seg < nrgSegEnd; seg++) {          // values of 1st segment
                    if (nrg[idx][seg] < minNrg) minNrg = nrg[idx][seg];   // ... already assigned
                    if (nrg[idx][seg] > maxNrg) maxNrg = nrg[idx][seg];
                }
                assertTrue(String.format(""max energy of channel %d is zero"", ch),
                        maxNrg > 0.0f);
                assertTrue(String.format(""channel %d has not enough energy"", ch),
                        minNrg >= refMinNrg);              // check the channels minimum energy
                if (ch == 0) {                             // use 85% of 1st channels min energy as
                    refMinNrg = minNrg * 0.85f;            // ... reference the other chs must meet
                } else if (isDmx && (ch == 1)) {           // in case of mixdown signal the energy
                    refMinNrg *= 0.50f;                    // ... can be lower depending on the
                }                                          // ... downmix equation
                // calculate and check the energy ratio
                final double nrgRatio = minNrg / maxNrg;
                assertTrue(String.format(""energy ratio of channel %d below threshold"", ch),
                        nrgRatio >= nrgRatioThresh);
                if (!isDmx) {
                    if (nrgSegEnd < totSeg) {
                        // consider that some noise can extend into the subsequent segment
                        // allow this to be at max 20% of the channels minimum energy
                        assertTrue(String.format(""min energy after noise above threshold (%.2f)"",
                                nrg[idx][nrgSegEnd]),
                                nrg[idx][nrgSegEnd] < minNrg * 0.20f);
                        nrgSegEnd += 1;
                    }
                } else {                                   // ignore all subsequent segments
                    nrgSegEnd = totSeg;                    // ... in case of a mixdown signal
                }
                // zero-out the verified energies to simplify the subsequent check
                for (int seg = ofst; seg < nrgSegEnd; seg++) nrg[idx][seg] = 0.0f;
            }
            // check zero signal parts
            for (int seg = 0; seg < totSeg; seg++) {
                assertTrue(String.format(""segment %d in channel %d has signal where should "" +
                        ""be none (%.2f)"", seg, ch, nrg[idx][seg]), nrg[idx][seg] < zeroNrgThresh);
            }
        }
        // test whether each segment has energy in at least one channel
        for (int seg = 0; seg < totSeg; seg++) {
            assertTrue(String.format(""no channel has energy in segment %d"", seg), sigSeg[seg]);
        }
    }

    private void checkEnergy(short[] decSamples, AudioParameter decParams, int encNch)
            throws RuntimeException {
        checkEnergy(decSamples, decParams, encNch, 0.50f);  // default energy ratio threshold: 0.50
    }

    /**
     * Calculate the RMS of the difference signal between a given signal and the reference samples
     * located in mMasterBuffer.
     * @param signal the decoded samples to test
     * @return RMS of error signal
     * @throws RuntimeException
     */
    private double getRmsError(short[] signal) throws RuntimeException {
        long totalErrorSquared = 0;
        int stride = mMasterBuffer.length / signal.length;
        assertEquals(""wrong data size"", mMasterBuffer.length, signal.length * stride);

        for (int i = 0; i < signal.length; i++) {
            short sample = signal[i];
            short mastersample = mMasterBuffer[i * stride];
            int d = sample - mastersample;
            totalErrorSquared += d * d;
        }
        long avgErrorSquared = (totalErrorSquared / signal.length);
        return Math.sqrt(avgErrorSquared);
    }

    /**
     * Decode a given input stream and compare the output against the reference signal. The RMS of
     * the error signal must be below the given threshold (maxerror).
     * Important note about the test signals: this method expects test signals to have been
     *   ""stretched"" relative to the reference signal. The reference, sinesweepraw, is 3s long at
     *   44100Hz. For instance for comparing this reference to a test signal at 8000Hz, the test
     *   signal needs to be 44100/8000 = 5.5125 times longer, containing frequencies 5.5125
     *   times lower than the reference.
     * @param testinput the file to decode
     * @param maxerror  the maximum allowed root mean squared error
     * @throws Exception
     */
    private void decodeNtest(final String testinput, float maxerror) throws Exception {
        String localTag = TAG + ""#decodeNtest"";

        for (String codecName: codecsFor(testinput)) {
            AudioParameter decParams = new AudioParameter();
            short[] decoded = decodeToMemory(codecName, decParams, testinput,
                    RESET_MODE_NONE, CONFIG_MODE_NONE, -1, null);
            double rmse = getRmsError(decoded);

            assertTrue(codecName + "": decoding error too big: "" + rmse, rmse <= maxerror);
            Log.v(localTag, String.format(""rms = %f (max = %f)"", rmse, maxerror));
        }
    }

    private void monoTest(final String res, int expectedLength) throws Exception {
        for (String codecName: codecsFor(res)) {
            short [] mono = decodeToMemory(codecName, res,
                    RESET_MODE_NONE, CONFIG_MODE_NONE, -1, null);
            if (mono.length == expectedLength) {
                // expected
            } else if (mono.length == expectedLength * 2) {
                // the decoder output 2 channels instead of 1, check that the left and right channel
                // are identical
                for (int i = 0; i < mono.length; i += 2) {
                    assertEquals(codecName + "": mismatched samples at "" + i, mono[i], mono[i+1]);
                }
            } else {
                fail(codecName + "": wrong number of samples: "" + mono.length);
            }

            short [] mono2 = decodeToMemory(codecName, res,
                    RESET_MODE_RECONFIGURE, CONFIG_MODE_NONE, -1, null);

            assertEquals(codecName + "": count different after reconfigure: "",
                    mono.length, mono2.length);
            for (int i = 0; i < mono.length; i++) {
                assertEquals(codecName + "": samples at "" + i + "" don't match"", mono[i], mono2[i]);
            }

            short [] mono3 = decodeToMemory(codecName, res,
                    RESET_MODE_FLUSH, CONFIG_MODE_NONE, -1, null);

            assertEquals(codecName + "": count different after flush: "", mono.length, mono3.length);
            for (int i = 0; i < mono.length; i++) {
                assertEquals(codecName + "": samples at "" + i + "" don't match"", mono[i], mono3[i]);
            }
        }
    }

    protected static List<String> codecsFor(String resource) throws IOException {
        MediaExtractor ex = new MediaExtractor();
        AssetFileDescriptor fd = getAssetFileDescriptorFor(resource);
        try {
            ex.setDataSource(fd.getFileDescriptor(), fd.getStartOffset(), fd.getLength());
        } finally {
            fd.close();
        }
        MediaCodecInfo[] codecInfos = new MediaCodecList(
                MediaCodecList.REGULAR_CODECS).getCodecInfos();
        ArrayList<String> matchingCodecs = new ArrayList<String>();
        MediaFormat format = ex.getTrackFormat(0);
        String mime = format.getString(MediaFormat.KEY_MIME);
        for (MediaCodecInfo info: codecInfos) {
            if (info.isEncoder()) {
                continue;
            }
            try {
                MediaCodecInfo.CodecCapabilities caps = info.getCapabilitiesForType(mime);
                if (caps != null) {
                    matchingCodecs.add(info.getName());
                }
            } catch (IllegalArgumentException e) {
                // type is not supported
            }
        }
        assertTrue(""no matching codecs found"", matchingCodecs.size() != 0);
        return matchingCodecs;
    }

    /**
     * @param testinput the file to decode
     * @param maxerror the maximum allowed root mean squared error
     * @throws IOException
     */
    private void decode(final String testinput, float maxerror) throws IOException {

        for (String codecName: codecsFor(testinput)) {
            short[] decoded = decodeToMemory(codecName, testinput,
                    RESET_MODE_NONE, CONFIG_MODE_NONE, -1, null);

            assertEquals(codecName + "": wrong data size"", mMasterBuffer.length, decoded.length);

            double rmse = getRmsError(decoded);

            assertTrue(codecName + "": decoding error too big: "" + rmse, rmse <= maxerror);

            int[] resetModes = new int[] { RESET_MODE_NONE, RESET_MODE_RECONFIGURE,
                    RESET_MODE_FLUSH, RESET_MODE_EOS_FLUSH };
            int[] configModes = new int[] { CONFIG_MODE_NONE, CONFIG_MODE_QUEUE };

            for (int conf : configModes) {
                for (int reset : resetModes) {
                    if (conf == CONFIG_MODE_NONE && reset == RESET_MODE_NONE) {
                        // default case done outside of loop
                        continue;
                    }
                    if (conf == CONFIG_MODE_QUEUE && !hasAudioCsd(testinput)) {
                        continue;
                    }

                    String params = String.format(""(using reset: %d, config: %s)"", reset, conf);
                    short[] decoded2 = decodeToMemory(codecName, testinput, reset, conf, -1, null);
                    assertEquals(codecName + "": count different with reconfigure"" + params,
                            decoded.length, decoded2.length);
                    for (int i = 0; i < decoded.length; i++) {
                        assertEquals(codecName + "": samples don't match"" + params,
                                decoded[i], decoded2[i]);
                    }
                }
            }
        }
    }

    private boolean hasAudioCsd(final String testinput) throws IOException {
        AssetFileDescriptor fd = null;
        try {
            MediaExtractor extractor = new MediaExtractor();
            extractor.setDataSource(mInpPrefix + testinput);
            MediaFormat format = extractor.getTrackFormat(0);

            return format.containsKey(CSD_KEYS[0]);

        } finally {
            if (fd != null) {
                fd.close();
            }
        }
    }

    protected static int getOutputFormatInteger(MediaCodec codec, String key) {
        if (codec == null) {
            fail(""Null MediaCodec before attempting to retrieve output format key "" + key);
        }
        MediaFormat format = null;
        try {
            format = codec.getOutputFormat();
        } catch (Exception e) {
            fail(""Exception "" + e + "" when attempting to obtain output format"");
        }
        if (format == null) {
            fail(""Null output format returned from MediaCodec"");
        }
        try {
            return format.getInteger(key);
        } catch (NullPointerException e) {
            fail(""Key "" + key + "" not present in output format"");
        } catch (ClassCastException e) {
            fail(""Key "" + key + "" not stored as integer in output format"");
        } catch (Exception e) {
            fail(""Exception "" + e + "" when attempting to retrieve output format key "" + key);
        }
        // never used
        return Integer.MIN_VALUE;
    }

    // Class handling all audio parameters relevant for testing
    protected static class AudioParameter {

        public AudioParameter() {
            this.reset();
        }

        public void reset() {
            this.numChannels = 0;
            this.samplingRate = 0;
        }

        public int getNumChannels() {
            return this.numChannels;
        }

        public int getSamplingRate() {
            return this.samplingRate;
        }

        public void setNumChannels(int numChannels) {
            this.numChannels = numChannels;
        }

        public void setSamplingRate(int samplingRate) {
            this.samplingRate = samplingRate;
        }

        private int numChannels;
        private int samplingRate;
    }

    private short[] decodeToMemory(String codecName, final String testinput, int resetMode,
            int configMode, int eossample, List<Long> timestamps) throws IOException {

        AudioParameter audioParams = new AudioParameter();
        return decodeToMemory(codecName, audioParams, testinput,
                resetMode, configMode, eossample, timestamps);
    }

    private short[] decodeToMemory(String codecName, AudioParameter audioParams,
            final String testinput, int resetMode, int configMode, int eossample,
            List<Long> timestamps) throws IOException {
        String localTag = TAG + ""#decodeToMemory"";
        Log.v(localTag, String.format(""reset = %d; config: %s"", resetMode, configMode));
        short [] decoded = new short[0];
        int decodedIdx = 0;

        MediaExtractor extractor;
        MediaCodec codec;
        ByteBuffer[] codecInputBuffers;
        ByteBuffer[] codecOutputBuffers;

        extractor = new MediaExtractor();
        extractor.setDataSource(mInpPrefix + testinput);

        assertEquals(""wrong number of tracks"", 1, extractor.getTrackCount());
        MediaFormat format = extractor.getTrackFormat(0);
        String mime = format.getString(MediaFormat.KEY_MIME);
        assertTrue(""not an audio file"", mime.startsWith(""audio/""));

        MediaFormat configFormat = format;
        codec = MediaCodec.createByCodecName(codecName);
        if (configMode == CONFIG_MODE_QUEUE && format.containsKey(CSD_KEYS[0])) {
            configFormat = MediaFormat.createAudioFormat(mime,
                    format.getInteger(MediaFormat.KEY_SAMPLE_RATE),
                    format.getInteger(MediaFormat.KEY_CHANNEL_COUNT));

            configFormat.setLong(MediaFormat.KEY_DURATION,
                    format.getLong(MediaFormat.KEY_DURATION));
            String[] keys = new String[] { ""max-input-size"", ""encoder-delay"", ""encoder-padding"" };
            for (String k : keys) {
                if (format.containsKey(k)) {
                    configFormat.setInteger(k, format.getInteger(k));
                }
            }
        }
        Log.v(localTag, ""configuring with "" + configFormat);
        codec.configure(configFormat, null /* surface */, null /* crypto */, 0 /* flags */);

        codec.start();
        codecInputBuffers = codec.getInputBuffers();
        codecOutputBuffers = codec.getOutputBuffers();

        if (resetMode == RESET_MODE_RECONFIGURE) {
            codec.stop();
            codec.configure(configFormat, null /* surface */, null /* crypto */, 0 /* flags */);
            codec.start();
            codecInputBuffers = codec.getInputBuffers();
            codecOutputBuffers = codec.getOutputBuffers();
        } else if (resetMode == RESET_MODE_FLUSH) {
            codec.flush();
        }

        extractor.selectTrack(0);

        if (configMode == CONFIG_MODE_QUEUE) {
            queueConfig(codec, format);
        }

        // start decoding
        final long kTimeOutUs = 5000;
        MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
        boolean sawInputEOS = false;
        boolean sawOutputEOS = false;
        int noOutputCounter = 0;
        int samplecounter = 0;
        while (!sawOutputEOS && noOutputCounter < 50) {
            noOutputCounter++;
            if (!sawInputEOS) {
                int inputBufIndex = codec.dequeueInputBuffer(kTimeOutUs);

                if (inputBufIndex >= 0) {
                    ByteBuffer dstBuf = codecInputBuffers[inputBufIndex];

                    int sampleSize =
                        extractor.readSampleData(dstBuf, 0 /* offset */);

                    long presentationTimeUs = 0;

                    if (sampleSize < 0 && eossample > 0) {
                        fail(""test is broken: never reached eos sample"");
                    }
                    if (sampleSize < 0) {
                        Log.d(TAG, ""saw input EOS."");
                        sawInputEOS = true;
                        sampleSize = 0;
                    } else {
                        if (samplecounter == eossample) {
                            sawInputEOS = true;
                        }
                        samplecounter++;
                        presentationTimeUs = extractor.getSampleTime();
                    }
                    codec.queueInputBuffer(
                            inputBufIndex,
                            0 /* offset */,
                            sampleSize,
                            presentationTimeUs,
                            sawInputEOS ? MediaCodec.BUFFER_FLAG_END_OF_STREAM : 0);

                    if (!sawInputEOS) {
                        extractor.advance();
                    }
                }
            }

            int res = codec.dequeueOutputBuffer(info, kTimeOutUs);

            if (res >= 0) {
                //Log.d(TAG, ""got frame, size "" + info.size + ""/"" + info.presentationTimeUs);

                if (info.size > 0) {
                    noOutputCounter = 0;
                    if (timestamps != null) {
                        timestamps.add(info.presentationTimeUs);
                    }
                }
                if (info.size > 0 &&
                        resetMode != RESET_MODE_NONE && resetMode != RESET_MODE_EOS_FLUSH) {
                    // once we've gotten some data out of the decoder, reset and start again
                    if (resetMode == RESET_MODE_RECONFIGURE) {
                        codec.stop();
                        codec.configure(configFormat, null /* surface */, null /* crypto */,
                                0 /* flags */);
                        codec.start();
                        codecInputBuffers = codec.getInputBuffers();
                        codecOutputBuffers = codec.getOutputBuffers();
                        if (configMode == CONFIG_MODE_QUEUE) {
                            queueConfig(codec, format);
                        }
                    } else /* resetMode == RESET_MODE_FLUSH */ {
                        codec.flush();
                    }
                    resetMode = RESET_MODE_NONE;
                    extractor.seekTo(0, MediaExtractor.SEEK_TO_NEXT_SYNC);
                    sawInputEOS = false;
                    samplecounter = 0;
                    if (timestamps != null) {
                        timestamps.clear();
                    }
                    continue;
                }

                int outputBufIndex = res;
                ByteBuffer buf = codecOutputBuffers[outputBufIndex];

                if (decodedIdx + (info.size / 2) >= decoded.length) {
                    decoded = Arrays.copyOf(decoded, decodedIdx + (info.size / 2));
                }

                buf.position(info.offset);
                for (int i = 0; i < info.size; i += 2) {
                    decoded[decodedIdx++] = buf.getShort();
                }

                codec.releaseOutputBuffer(outputBufIndex, false /* render */);

                if ((info.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                    Log.d(TAG, ""saw output EOS."");
                    if (resetMode == RESET_MODE_EOS_FLUSH) {
                        resetMode = RESET_MODE_NONE;
                        codec.flush();
                        extractor.seekTo(0, MediaExtractor.SEEK_TO_NEXT_SYNC);
                        sawInputEOS = false;
                        samplecounter = 0;
                        decoded = new short[0];
                        decodedIdx = 0;
                        if (timestamps != null) {
                            timestamps.clear();
                        }
                    } else {
                        sawOutputEOS = true;
                    }
                }
            } else if (res == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                codecOutputBuffers = codec.getOutputBuffers();

                Log.d(TAG, ""output buffers have changed."");
            } else if (res == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                MediaFormat oformat = codec.getOutputFormat();
                audioParams.setNumChannels(oformat.getInteger(MediaFormat.KEY_CHANNEL_COUNT));
                audioParams.setSamplingRate(oformat.getInteger(MediaFormat.KEY_SAMPLE_RATE));
                Log.d(TAG, ""output format has changed to "" + oformat);
            } else {
                Log.d(TAG, ""dequeueOutputBuffer returned "" + res);
            }
        }
        if (noOutputCounter >= 50) {
            fail(""decoder stopped outputing data"");
        }

        codec.stop();
        codec.release();
        return decoded;
    }

    private static void queueConfig(MediaCodec codec, MediaFormat format) {
        for (String csdKey : CSD_KEYS) {
            if (!format.containsKey(csdKey)) {
                continue;
            }
            ByteBuffer[] codecInputBuffers = codec.getInputBuffers();
            int inputBufIndex = codec.dequeueInputBuffer(-1);
            if (inputBufIndex < 0) {
                fail(""failed to queue configuration buffer "" + csdKey);
            } else {
                ByteBuffer csd = (ByteBuffer) format.getByteBuffer(csdKey).rewind();
                Log.v(TAG + ""#queueConfig"", String.format(""queueing %s:%s"", csdKey, csd));
                codecInputBuffers[inputBufIndex].put(csd);
                codec.queueInputBuffer(
                        inputBufIndex,
                        0 /* offset */,
                        csd.limit(),
                        0 /* presentation time (us) */,
                        MediaCodec.BUFFER_FLAG_CODEC_CONFIG);
            }
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecBasicH264"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecBasicH264() throws Exception {
        testDecode(""video_480x360_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"", 240);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecBasicHEVC"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecBasicHEVC() throws Exception {
        testDecode(
                ""bbb_s1_720x480_mp4_hevc_mp3_1600kbps_30fps_aac_he_6ch_240kbps_48000hz.mp4"", 300);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecBasicH263"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecBasicH263() throws Exception {
        testDecode(""video_176x144_3gp_h263_300kbps_12fps_aac_stereo_128kbps_22050hz.3gp"", 122);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecBasicMpeg2"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecBasicMpeg2() throws Exception {
        testDecode(""video_480x360_mp4_mpeg2_1500kbps_30fps_aac_stereo_128kbps_48000hz.mp4"", 300);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecBasicMpeg4"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecBasicMpeg4() throws Exception {
        testDecode(""video_480x360_mp4_mpeg4_860kbps_25fps_aac_stereo_128kbps_44100hz.mp4"", 249);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testH264Decode320x240"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testH264Decode320x240() throws Exception {
        testDecode(""bbb_s1_320x240_mp4_h264_mp2_800kbps_30fps_aac_lc_5ch_240kbps_44100hz.mp4"", 300);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testH264Decode720x480"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testH264Decode720x480() throws Exception {
        testDecode(""bbb_s1_720x480_mp4_h264_mp3_2mbps_30fps_aac_lc_5ch_320kbps_48000hz.mp4"", 300);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testH264Decode30fps1280x720"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testH264Decode30fps1280x720() throws Exception {
        testDecode(""bbb_s4_1280x720_mp4_h264_mp31_8mbps_30fps_aac_he_mono_40kbps_44100hz.mp4"", 300);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testH264Decode60fps1280x720Tv"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testH264Decode60fps1280x720Tv() throws Exception {
        if (checkTv()) {
            assertTrue(MediaUtils.canDecodeVideo(
                    MediaFormat.MIMETYPE_VIDEO_AVC, 1280, 720, 60,
                    AVCProfileHigh, AVCLevel32, 8000000));
            testDecode(
                    ""bbb_s3_1280x720_mp4_h264_hp32_8mbps_60fps_aac_he_v2_stereo_48kbps_48000hz.mp4"",
                    600);
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testH264Decode60fps1280x720"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testH264Decode60fps1280x720() throws Exception {
        testDecode(""bbb_s3_1280x720_mp4_h264_mp32_8mbps_60fps_aac_he_v2_6ch_144kbps_44100hz.mp4"",
                600);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testH264Decode30fps1920x1080Tv"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testH264Decode30fps1920x1080Tv() throws Exception {
        if (checkTv()) {
            assertTrue(MediaUtils.canDecodeVideo(
                    MediaFormat.MIMETYPE_VIDEO_AVC, 1920, 1080, 30,
                    AVCProfileHigh, AVCLevel4, 20000000));
            testDecode(
                    ""bbb_s4_1920x1080_wide_mp4_h264_hp4_20mbps_30fps_aac_lc_6ch_384kbps_44100hz.mp4"",
                    150);
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testH264Decode30fps1920x1080"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testH264Decode30fps1920x1080() throws Exception {
        testDecode(""bbb_s4_1920x1080_wide_mp4_h264_mp4_20mbps_30fps_aac_he_5ch_200kbps_44100hz.mp4"",
                150);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testH264Decode60fps1920x1080Tv"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testH264Decode60fps1920x1080Tv() throws Exception {
        if (checkTv()) {
            assertTrue(MediaUtils.canDecodeVideo(
                    MediaFormat.MIMETYPE_VIDEO_AVC, 1920, 1080, 60,
                    AVCProfileHigh, AVCLevel42, 20000000));
            testDecode(""bbb_s2_1920x1080_mp4_h264_hp42_20mbps_60fps_aac_lc_6ch_384kbps_48000hz.mp4"",
                    300);
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testH264Decode60fps1920x1080"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testH264Decode60fps1920x1080() throws Exception {
        testDecode(""bbb_s2_1920x1080_mp4_h264_mp42_20mbps_60fps_aac_he_v2_5ch_160kbps_48000hz.mp4"",
                300);
        testDecode(""bbb_s2_1920x1080_mkv_h264_mp42_20mbps_60fps_aac_he_v2_5ch_160kbps_48000hz.mkv"",
                300);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testH265Decode25fps1280x720"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testH265Decode25fps1280x720() throws Exception {
        testDecode(""video_1280x720_mkv_h265_500kbps_25fps_aac_stereo_128kbps_44100hz.mkv"", 240);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testHEVCDecode352x288"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testHEVCDecode352x288() throws Exception {
        testDecode(""bbb_s1_352x288_mp4_hevc_mp2_600kbps_30fps_aac_he_stereo_96kbps_48000hz.mp4"",
                300);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testHEVCDecode720x480"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testHEVCDecode720x480() throws Exception {
        testDecode(""bbb_s1_720x480_mp4_hevc_mp3_1600kbps_30fps_aac_he_6ch_240kbps_48000hz.mp4"",
                300);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testHEVCDecode30fps1280x720"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testHEVCDecode30fps1280x720() throws Exception {
        testDecode(""bbb_s4_1280x720_mp4_hevc_mp31_4mbps_30fps_aac_he_stereo_80kbps_32000hz.mp4"",
                300);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testHEVCDecode60fps1920x1080"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testHEVCDecode60fps1920x1080() throws Exception {
        testDecode(""bbb_s2_1920x1080_mp4_hevc_mp41_10mbps_60fps_aac_lc_6ch_384kbps_22050hz.mp4"",
                300);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testHEVCDecode30fps3840x2160"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testHEVCDecode30fps3840x2160() throws Exception {
        testDecode(""bbb_s4_3840x2160_mp4_hevc_mp5_20mbps_30fps_aac_lc_6ch_384kbps_24000hz.mp4"",
                150);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testHEVCDecode60fps3840x2160"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testHEVCDecode60fps3840x2160() throws Exception {
        testDecode(""bbb_s2_3840x2160_mp4_hevc_mp51_20mbps_60fps_aac_lc_6ch_384kbps_32000hz.mp4"",
                300);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testMpeg2Decode352x288"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testMpeg2Decode352x288() throws Exception {
        testDecode(""video_352x288_mp4_mpeg2_1000kbps_30fps_aac_stereo_128kbps_48000hz.mp4"", 300);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testMpeg2Decode720x480"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testMpeg2Decode720x480() throws Exception {
        testDecode(""video_720x480_mp4_mpeg2_2000kbps_30fps_aac_stereo_128kbps_48000hz.mp4"", 300);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testMpeg2Decode30fps1280x720"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testMpeg2Decode30fps1280x720() throws Exception {
        testDecode(""video_1280x720_mp4_mpeg2_6000kbps_30fps_aac_stereo_128kbps_48000hz.mp4"", 150);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testMpeg2Decode30fps1920x1080"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testMpeg2Decode30fps1920x1080() throws Exception {
        testDecode(""video_1920x1080_mp4_mpeg2_12000kbps_30fps_aac_stereo_128kbps_48000hz.mp4"", 150);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testMpeg2Decode30fps3840x2160"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testMpeg2Decode30fps3840x2160() throws Exception {
        testDecode(""video_3840x2160_mp4_mpeg2_20000kbps_30fps_aac_stereo_128kbps_48000hz.mp4"", 150);
    }

    private void testCodecEarlyEOS(final String res, int eosFrame) throws Exception {
        if (!MediaUtils.checkCodecForResource(mInpPrefix + res, 0 /* track */)) {
            return; // skip
        }
        Surface s = getActivity().getSurfaceHolder().getSurface();
        int frames1 = countFrames(res, RESET_MODE_NONE, eosFrame, s);
        assertEquals(""wrong number of frames decoded"", eosFrame, frames1);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecEarlyEOSH263"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecEarlyEOSH263() throws Exception {
        testCodecEarlyEOS(""video_176x144_3gp_h263_300kbps_12fps_aac_stereo_128kbps_22050hz.3gp"",
                64 /* eosframe */);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecEarlyEOSH264"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecEarlyEOSH264() throws Exception {
        testCodecEarlyEOS(""video_480x360_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                120 /* eosframe */);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecEarlyEOSHEVC"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecEarlyEOSHEVC() throws Exception {
        testCodecEarlyEOS(""video_480x360_mp4_hevc_650kbps_30fps_aac_stereo_128kbps_48000hz.mp4"",
                120 /* eosframe */);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecEarlyEOSMpeg2"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecEarlyEOSMpeg2() throws Exception {
        testCodecEarlyEOS(""vdeo_480x360_mp4_mpeg2_1500kbps_30fps_aac_stereo_128kbps_48000hz.mp4"",
                120 /* eosframe */);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecEarlyEOSMpeg4"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecEarlyEOSMpeg4() throws Exception {
        testCodecEarlyEOS(""video_480x360_mp4_mpeg4_860kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                120 /* eosframe */);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecResetsH264WithoutSurface"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecResetsH264WithoutSurface() throws Exception {
        testCodecResets(""video_480x360_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                null);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecResetsH264WithSurface"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecResetsH264WithSurface() throws Exception {
        Surface s = getActivity().getSurfaceHolder().getSurface();
        testCodecResets(""video_480x360_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"", s);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecResetsHEVCWithoutSurface"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecResetsHEVCWithoutSurface() throws Exception {
        testCodecResets(""bbb_s1_720x480_mp4_hevc_mp3_1600kbps_30fps_aac_he_6ch_240kbps_48000hz.mp4"",
                null);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecResetsHEVCWithSurface"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecResetsHEVCWithSurface() throws Exception {
        Surface s = getActivity().getSurfaceHolder().getSurface();
        testCodecResets(""bbb_s1_720x480_mp4_hevc_mp3_1600kbps_30fps_aac_he_6ch_240kbps_48000hz.mp4"",
                s);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecResetsMpeg2WithoutSurface"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecResetsMpeg2WithoutSurface() throws Exception {
        testCodecResets(""video_1280x720_mp4_mpeg2_6000kbps_30fps_aac_stereo_128kbps_48000hz.mp4"",
                null);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecResetsMpeg2WithSurface"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecResetsMpeg2WithSurface() throws Exception {
        Surface s = getActivity().getSurfaceHolder().getSurface();
        testCodecResets(""video_176x144_mp4_mpeg2_105kbps_25fps_aac_stereo_128kbps_44100hz.mp4"", s);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecResetsH263WithoutSurface"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecResetsH263WithoutSurface() throws Exception {
        testCodecResets(""video_176x144_3gp_h263_300kbps_12fps_aac_stereo_128kbps_22050hz.3gp"",null);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecResetsH263WithSurface"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecResetsH263WithSurface() throws Exception {
        Surface s = getActivity().getSurfaceHolder().getSurface();
        testCodecResets(""video_176x144_3gp_h263_300kbps_12fps_aac_stereo_128kbps_22050hz.3gp"", s);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecResetsMpeg4WithoutSurface"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecResetsMpeg4WithoutSurface() throws Exception {
        testCodecResets(""video_480x360_mp4_mpeg4_860kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                null);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testCodecResetsMpeg4WithSurface"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testCodecResetsMpeg4WithSurface() throws Exception {
        Surface s = getActivity().getSurfaceHolder().getSurface();
        testCodecResets(""video_480x360_mp4_mpeg4_860kbps_25fps_aac_stereo_128kbps_44100hz.mp4"", s);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testEOSBehaviorH264"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testEOSBehaviorH264() throws Exception {
        // this video has an I frame at 44
        testEOSBehavior(""video_480x360_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                new int[]{1, 44, 45, 55});
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testEOSBehaviorHEVC"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testEOSBehaviorHEVC() throws Exception {
        testEOSBehavior(""video_480x360_mp4_hevc_650kbps_30fps_aac_stereo_128kbps_48000hz.mp4"",
                new int[]{1, 17, 23, 49});
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testEOSBehaviorMpeg2"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testEOSBehaviorMpeg2() throws Exception {
        testEOSBehavior(""video_480x360_mp4_mpeg2_1500kbps_30fps_aac_stereo_128kbps_48000hz.mp4"",
                17);
        testEOSBehavior(""video_480x360_mp4_mpeg2_1500kbps_30fps_aac_stereo_128kbps_48000hz.mp4"",
                23);
        testEOSBehavior(""video_480x360_mp4_mpeg2_1500kbps_30fps_aac_stereo_128kbps_48000hz.mp4"",
                49);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testEOSBehaviorH263"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testEOSBehaviorH263() throws Exception {
        // this video has an I frame every 12 frames.
        testEOSBehavior(""video_176x144_3gp_h263_300kbps_12fps_aac_stereo_128kbps_22050hz.3gp"",
                new int[]{1, 24, 25, 48, 50});
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testEOSBehaviorMpeg4"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testEOSBehaviorMpeg4() throws Exception {
        // this video has an I frame every 12 frames
        testEOSBehavior(""video_480x360_mp4_mpeg4_860kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                new int[]{1, 24, 25, 48, 50, 2});
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testFlush"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testFlush() throws Exception {
        testFlush(""loudsoftwav.wav"");
        testFlush(""loudsoftogg.ogg"");
        testFlush(""loudsoftoggmkv.mkv"");
        testFlush(""loudsoftoggmp4.mp4"");
        testFlush(""loudsoftmp3.mp3"");
        testFlush(""loudsoftaac.aac"");
        testFlush(""loudsoftfaac.m4a"");
        testFlush(""loudsoftitunes.m4a"");
    }

    private void testFlush(final String resource) throws Exception {
        MediaExtractor extractor;
        MediaCodec codec;
        ByteBuffer[] codecInputBuffers;
        ByteBuffer[] codecOutputBuffers;

        extractor = new MediaExtractor();
        extractor.setDataSource(mInpPrefix + resource);

        assertEquals(""wrong number of tracks"", 1, extractor.getTrackCount());
        MediaFormat format = extractor.getTrackFormat(0);
        String mime = format.getString(MediaFormat.KEY_MIME);
        assertTrue(""not an audio file"", mime.startsWith(""audio/""));

        codec = MediaCodec.createDecoderByType(mime);
        assertNotNull(""couldn't find codec "" + mime, codec);

        codec.configure(format, null /* surface */, null /* crypto */, 0 /* flags */);
        codec.start();
        codecInputBuffers = codec.getInputBuffers();
        codecOutputBuffers = codec.getOutputBuffers();

        extractor.selectTrack(0);

        // decode a bit of the first part of the file, and verify the amplitude
        short maxvalue1 = getAmplitude(extractor, codec);

        // flush the codec and seek the extractor a different position, then decode a bit more
        // and check the amplitude
        extractor.seekTo(8000000, 0);
        codec.flush();
        short maxvalue2 = getAmplitude(extractor, codec);

        assertTrue(""first section amplitude too low"", maxvalue1 > 20000);
        assertTrue(""second section amplitude too high"", maxvalue2 < 5000);
        codec.stop();
        codec.release();

    }

    private short getAmplitude(MediaExtractor extractor, MediaCodec codec) {
        short maxvalue = 0;
        int numBytesDecoded = 0;
        final long kTimeOutUs = 5000;
        ByteBuffer[] codecInputBuffers = codec.getInputBuffers();
        ByteBuffer[] codecOutputBuffers = codec.getOutputBuffers();
        MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();

        while(numBytesDecoded < 44100 * 2) {
            int inputBufIndex = codec.dequeueInputBuffer(kTimeOutUs);

            if (inputBufIndex >= 0) {
                ByteBuffer dstBuf = codecInputBuffers[inputBufIndex];

                int sampleSize = extractor.readSampleData(dstBuf, 0 /* offset */);
                long presentationTimeUs = extractor.getSampleTime();

                codec.queueInputBuffer(
                        inputBufIndex,
                        0 /* offset */,
                        sampleSize,
                        presentationTimeUs,
                        0 /* flags */);

                extractor.advance();
            }
            int res = codec.dequeueOutputBuffer(info, kTimeOutUs);

            if (res >= 0) {

                int outputBufIndex = res;
                ByteBuffer buf = codecOutputBuffers[outputBufIndex];

                buf.position(info.offset);
                for (int i = 0; i < info.size; i += 2) {
                    short sample = buf.getShort();
                    if (maxvalue < sample) {
                        maxvalue = sample;
                    }
                    int idx = (numBytesDecoded + i) / 2;
                }

                numBytesDecoded += info.size;

                codec.releaseOutputBuffer(outputBufIndex, false /* render */);
            } else if (res == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                codecOutputBuffers = codec.getOutputBuffers();
            } else if (res == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                MediaFormat oformat = codec.getOutputFormat();
            }
        }
        return maxvalue;
    }

    /* return true if a particular video feature is supported for the given mimetype */
    private boolean isVideoFeatureSupported(String mimeType, String feature) {
        MediaFormat format = MediaFormat.createVideoFormat( mimeType, 1920, 1080);
        format.setFeatureEnabled(feature, true);
        MediaCodecList mcl = new MediaCodecList(MediaCodecList.ALL_CODECS);
        String codecName = mcl.findDecoderForFormat(format);
        return (codecName == null) ? false : true;
    }

    /**
     * Test tunneled video playback mode if supported
     *
     * TODO(b/182915887): Test all the codecs advertised by the DUT for the provided test content
     */
    private void tunneledVideoPlayback(String mimeType, String videoName) throws Exception {
        if (!isVideoFeatureSupported(mimeType,
                CodecCapabilities.FEATURE_TunneledPlayback)) {
            MediaUtils.skipTest(
                    TAG,
                    ""No tunneled video playback codec found for MIME "" + mimeType);
            return;
        }

        AudioManager am = (AudioManager)mContext.getSystemService(Context.AUDIO_SERVICE);
        mMediaCodecPlayer = new MediaCodecTunneledPlayer(
                mContext, getActivity().getSurfaceHolder(), true, am.generateAudioSessionId());

        Uri mediaUri = Uri.fromFile(new File(mInpPrefix, videoName));
        mMediaCodecPlayer.setAudioDataSource(mediaUri, null);
        mMediaCodecPlayer.setVideoDataSource(mediaUri, null);
        assertTrue(""MediaCodecPlayer.start() failed!"", mMediaCodecPlayer.start());
        assertTrue(""MediaCodecPlayer.prepare() failed!"", mMediaCodecPlayer.prepare());

        // starts video playback
        mMediaCodecPlayer.startThread();

        final long durationMs = mMediaCodecPlayer.getDuration();
        final long timeOutMs = System.currentTimeMillis() + durationMs + 5 * 1000; // add 5 sec
        while (!mMediaCodecPlayer.isEnded()) {
            // Log.d(TAG, ""currentPosition: "" + mMediaCodecPlayer.getCurrentPosition()
            //         + ""  duration: "" + mMediaCodecPlayer.getDuration());
            assertTrue(""Tunneled video playback timeout exceeded"",
                    timeOutMs > System.currentTimeMillis());
            Thread.sleep(SLEEP_TIME_MS);
            if (mMediaCodecPlayer.getCurrentPosition() >= mMediaCodecPlayer.getDuration()) {
                Log.d(TAG, ""testTunneledVideoPlayback -- current pos = "" +
                        mMediaCodecPlayer.getCurrentPosition() +
                        "">= duration = "" + mMediaCodecPlayer.getDuration());
                break;
            }
        }
        // mMediaCodecPlayer.reset() handled in TearDown();
    }

    /**
     * Test tunneled video playback mode with HEVC if supported
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testTunneledVideoPlaybackHevc"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testTunneledVideoPlaybackHevc() throws Exception {
        tunneledVideoPlayback(MediaFormat.MIMETYPE_VIDEO_HEVC,
                    ""video_1280x720_mkv_h265_500kbps_25fps_aac_stereo_128kbps_44100hz.mkv"");
    }

    /**
     * Test tunneled video playback mode with AVC if supported
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testTunneledVideoPlaybackAvc"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testTunneledVideoPlaybackAvc() throws Exception {
        tunneledVideoPlayback(MediaFormat.MIMETYPE_VIDEO_AVC,
                ""video_480x360_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"");
    }

    /**
     * Test tunneled video playback mode with VP9 if supported
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testTunneledVideoFlushHevc"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testTunneledVideoFlushHevc() throws Exception {
        testTunneledVideoFlush(MediaFormat.MIMETYPE_VIDEO_HEVC,
                ""video_1280x720_mkv_h265_500kbps_25fps_aac_stereo_128kbps_44100hz.mkv"");
    }

    /**
     * Test tunneled video playback flush with AVC if supported
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testTunneledVideoFlushAvc"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testTunneledVideoFlushAvc() throws Exception {
        testTunneledVideoFlush(MediaFormat.MIMETYPE_VIDEO_AVC,
                ""video_480x360_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"");
    }

    /**
     * Test tunneled video playback flush with VP9 if supported
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testTunneledVideoPeekDefaultHevc"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testTunneledVideoPeekDefaultHevc() throws Exception {
        testTunneledVideoPeekDefault(MediaFormat.MIMETYPE_VIDEO_HEVC,
                ""video_1280x720_mkv_h265_500kbps_25fps_aac_stereo_128kbps_44100hz.mkv"");
    }

    /**
     * Test default tunneled video peek with AVC if supported
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testTunneledVideoPeekDefaultAvc"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testTunneledVideoPeekDefaultAvc() throws Exception {
        testTunneledVideoPeekDefault(MediaFormat.MIMETYPE_VIDEO_AVC,
                ""video_480x360_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"");
    }

    /**
     * Test default tunneled video peek with VP9 if supported
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testTunneledVideoPeekOffHevc"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testTunneledVideoPeekOffHevc() throws Exception {
        testTunneledVideoPeekOff(MediaFormat.MIMETYPE_VIDEO_HEVC,
                ""video_1280x720_mkv_h265_500kbps_25fps_aac_stereo_128kbps_44100hz.mkv"");
    }

    /**
     * Test tunneled video peek can be turned off then on with AVC if supported
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testTunneledVideoPeekOffAvc"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testTunneledVideoPeekOffAvc() throws Exception {
        testTunneledVideoPeekOff(MediaFormat.MIMETYPE_VIDEO_AVC,
                ""video_480x360_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"");
    }

    /**
     * Test tunneled video peek can be turned off then on with VP9 if supported
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testTunneledAccurateVideoFlushHevc"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testTunneledAccurateVideoFlushHevc() throws Exception {
        testTunneledAccurateVideoFlush(MediaFormat.MIMETYPE_VIDEO_HEVC,
                ""video_1280x720_mkv_h265_500kbps_25fps_aac_stereo_128kbps_44100hz.mkv"");
    }

    /**
     * Test accurate video rendering after a video MediaCodec flush with AVC if supported
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testTunneledAccurateVideoFlushAvc"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testTunneledAccurateVideoFlushAvc() throws Exception {
        testTunneledAccurateVideoFlush(MediaFormat.MIMETYPE_VIDEO_AVC,
                ""video_480x360_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"");
    }

    /**
     * Test accurate video rendering after a video MediaCodec flush with VP9 if supported
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testTunneledAudioTimestampProgressHevc"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testTunneledAudioTimestampProgressHevc() throws Exception {
        testTunneledAudioTimestampProgress(MediaFormat.MIMETYPE_VIDEO_HEVC,
                ""video_1280x720_mkv_h265_500kbps_25fps_aac_stereo_128kbps_44100hz.mkv"");
    }

    /**
     * Test tunneled audioTimestamp progress with AVC if supported
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testTunneledAudioTimestampProgressAvc"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testTunneledAudioTimestampProgressAvc() throws Exception {
        testTunneledAudioTimestampProgress(MediaFormat.MIMETYPE_VIDEO_AVC,
                ""video_480x360_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"");
    }

    /**
     * Test tunneled audioTimestamp progress with VP9 if supported
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testLowLatencyAVCAt1280x720"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testLowLatencyAVCAt1280x720() throws Exception {
        testLowLatencyVideo(
                ""video_1280x720_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"", 300,
                false /* useNdk */);
        testLowLatencyVideo(
                ""video_1280x720_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"", 300,
                true /* useNdk */);
    }

    @NonMediaMainlineTest"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTest"	"testLowLatencyHEVCAt480x360"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTest.java"	""	"public void testLowLatencyHEVCAt480x360() throws Exception {
        testLowLatencyVideo(
                ""video_480x360_mp4_hevc_650kbps_30fps_aac_stereo_128kbps_48000hz.mp4"", 300,
                false /* useNdk */);
        testLowLatencyVideo(
                ""video_480x360_mp4_hevc_650kbps_30fps_aac_stereo_128kbps_48000hz.mp4"", 300,
                true /* useNdk */);
    }

    private void testLowLatencyVideo(String testVideo, int frameCount, boolean useNdk)
            throws Exception {
        AssetFileDescriptor fd = getAssetFileDescriptorFor(testVideo);
        MediaExtractor extractor = new MediaExtractor();
        extractor.setDataSource(fd.getFileDescriptor(), fd.getStartOffset(), fd.getLength());
        fd.close();

        MediaFormat format = null;
        int trackIndex = -1;
        for (int i = 0; i < extractor.getTrackCount(); i++) {
            format = extractor.getTrackFormat(i);
            if (format.getString(MediaFormat.KEY_MIME).startsWith(""video/"")) {
                trackIndex = i;
                break;
            }
        }

        assertTrue(""No video track was found"", trackIndex >= 0);

        extractor.selectTrack(trackIndex);
        format.setFeatureEnabled(MediaCodecInfo.CodecCapabilities.FEATURE_LowLatency,
                true /* enable */);

        MediaCodecList mcl = new MediaCodecList(MediaCodecList.ALL_CODECS);
        String decoderName = mcl.findDecoderForFormat(format);
        if (decoderName == null) {
            MediaUtils.skipTest(""no low latency decoder for "" + format);
            return;
        }
        String entry = (useNdk ? ""NDK"" : ""SDK"");
        Log.v(TAG, ""found "" + entry + "" decoder "" + decoderName + "" for format: "" + format);

        Surface surface = getActivity().getSurfaceHolder().getSurface();
        MediaCodecWrapper decoder = null;
        if (useNdk) {
            decoder = new NdkMediaCodec(decoderName);
        } else {
            decoder = new SdkMediaCodec(MediaCodec.createByCodecName(decoderName));
        }
        format.removeFeature(MediaCodecInfo.CodecCapabilities.FEATURE_LowLatency);
        format.setInteger(MediaFormat.KEY_LOW_LATENCY, 1);
        decoder.configure(format, 0 /* flags */, surface);
        decoder.start();

        if (!useNdk) {
            decoder.getInputBuffers();
        }
        ByteBuffer[] codecOutputBuffers = decoder.getOutputBuffers();
        String decoderOutputFormatString = null;

        // start decoding
        final long kTimeOutUs = 1000000;  // 1 second
        MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
        int bufferCounter = 0;
        long[] latencyMs = new long[frameCount];
        boolean waitingForOutput = false;
        long startTimeMs = System.currentTimeMillis();
        while (bufferCounter < frameCount) {
            if (!waitingForOutput) {
                int inputBufferId = decoder.dequeueInputBuffer(kTimeOutUs);
                if (inputBufferId < 0) {
                    Log.v(TAG, ""no input buffer"");
                    break;
                }

                ByteBuffer dstBuf = decoder.getInputBuffer(inputBufferId);

                int sampleSize = extractor.readSampleData(dstBuf, 0 /* offset */);
                long presentationTimeUs = 0;
                if (sampleSize < 0) {
                    Log.v(TAG, ""had input EOS, early termination at frame "" + bufferCounter);
                    break;
                } else {
                    presentationTimeUs = extractor.getSampleTime();
                }

                startTimeMs = System.currentTimeMillis();
                decoder.queueInputBuffer(
                        inputBufferId,
                        0 /* offset */,
                        sampleSize,
                        presentationTimeUs,
                        0 /* flags */);

                extractor.advance();
                waitingForOutput = true;
            }

            int outputBufferId = decoder.dequeueOutputBuffer(info, kTimeOutUs);

            if (outputBufferId >= 0) {
                waitingForOutput = false;
                //Log.d(TAG, ""got output, size "" + info.size + "", time "" + info.presentationTimeUs);
                latencyMs[bufferCounter++] = System.currentTimeMillis() - startTimeMs;
                // TODO: render the frame and find the rendering time to calculate the total delay
                decoder.releaseOutputBuffer(outputBufferId, false /* render */);
            } else if (outputBufferId == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                codecOutputBuffers = decoder.getOutputBuffers();
                Log.d(TAG, ""output buffers have changed."");
            } else if (outputBufferId == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                decoderOutputFormatString = decoder.getOutputFormatString();
                Log.d(TAG, ""output format has changed to "" + decoderOutputFormatString);
            } else {
                fail(""No output buffer returned without frame delay, status "" + outputBufferId);
            }
        }

        assertTrue(""No INFO_OUTPUT_FORMAT_CHANGED from decoder"", decoderOutputFormatString != null);

        long latencyMean = 0;
        long latencyMax = 0;
        int maxIndex = 0;
        for (int i = 0; i < bufferCounter; ++i) {
            latencyMean += latencyMs[i];
            if (latencyMs[i] > latencyMax) {
                latencyMax = latencyMs[i];
                maxIndex = i;
            }
        }
        if (bufferCounter > 0) {
            latencyMean /= bufferCounter;
        }
        Log.d(TAG, entry + "" latency average "" + latencyMean + "" ms, max "" + latencyMax +
                "" ms at frame "" + maxIndex);

        DeviceReportLog log = new DeviceReportLog(REPORT_LOG_NAME, ""video_decoder_latency"");
        String mime = format.getString(MediaFormat.KEY_MIME);
        int width = format.getInteger(MediaFormat.KEY_WIDTH);
        int height = format.getInteger(MediaFormat.KEY_HEIGHT);
        log.addValue(""codec_name"", decoderName, ResultType.NEUTRAL, ResultUnit.NONE);
        log.addValue(""mime_type"", mime, ResultType.NEUTRAL, ResultUnit.NONE);
        log.addValue(""width"", width, ResultType.NEUTRAL, ResultUnit.NONE);
        log.addValue(""height"", height, ResultType.NEUTRAL, ResultUnit.NONE);
        log.addValue(""video_res"", testVideo, ResultType.NEUTRAL, ResultUnit.NONE);
        log.addValue(""decode_to"", surface == null ? ""buffer"" : ""surface"",
                ResultType.NEUTRAL, ResultUnit.NONE);

        log.addValue(""average_latency"", latencyMean, ResultType.LOWER_BETTER, ResultUnit.MS);
        log.addValue(""max_latency"", latencyMax, ResultType.LOWER_BETTER, ResultUnit.MS);

        log.submit(getInstrumentation());

        decoder.stop();
        decoder.release();
        extractor.release();
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.MuxerUnitTest"	"testIfCorruptMediaFormatIsRejected"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/MuxerUnitTest.java"	""	"@Ignore(""TODO(b/146923138)"")
        public void testIfCorruptMediaFormatIsRejected() throws IOException {
            MediaMuxer muxer = new MediaMuxer(mOutLoc, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);

             /* TODO: Audio/Video formats, have certain keys required to be set. It is noticed
                that even when these keys are not set, no exceptions were raised. Do we need to
                add fixtures for those cases. */
            try {
                MediaFormat format = new MediaFormat();
                format.setString(MediaFormat.KEY_MIME, MediaFormat.MIMETYPE_AUDIO_AAC);
                format.setInteger(MediaFormat.KEY_SAMPLE_RATE, -1);
                muxer.addTrack(format);
                muxer.start();
                fail(""muxer accepts media format with required key-value pairs missing"");
            } catch (Exception e) {
                // expected
            } finally {
                muxer.release();
            }
        }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediadrm.cts.MediaDrmClearkeyTest"	"MediaDrmClearkeyTest"	"CtsMediaDrmTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/mediadrm/src/android/mediadrm/cts/MediaDrmClearkeyTest.java"	""	"public void test/*
 *.
 */
package android.mediadrm.cts;

import android.media.MediaCodecInfo.CodecCapabilities;
import android.media.MediaDrm;
import android.media.MediaDrm.KeyStatus;
import android.media.MediaDrm.MediaDrmStateException;
import android.media.MediaDrmException;
import android.media.MediaFormat;
import android.media.NotProvisionedException;
import android.media.ResourceBusyException;
import android.media.UnsupportedSchemeException;
import android.media.cts.AudioManagerStub;
import android.media.cts.AudioManagerStubHelper;
import android.media.cts.CodecState;
import android.media.cts.ConnectionStatus;
import android.media.cts.IConnectionStatus;
import android.media.cts.InputSurface;
import android.media.cts.InputSurfaceInterface;
import android.media.cts.MediaCodecClearKeyPlayer;
import android.media.cts.MediaCodecPlayerTestBase;
import android.media.cts.MediaCodecWrapper;
import android.media.cts.MediaTimeProvider;
import android.media.cts.MediaStubActivity;
import android.media.cts.NdkInputSurface;
import android.media.cts.NdkMediaCodec;
import android.media.cts.TestUtils.Monitor;
import android.net.Uri;
import android.os.Build;
import android.os.Looper;
import android.platform.test.annotations.Presubmit;
import android.util.Base64;
import android.util.Log;

import android.view.Surface;

import com.android.compatibility.common.util.ApiLevelUtil;

import org.json.JSONArray;
import org.json.JSONException;
import org.json.JSONObject;

import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Set;
import java.util.UUID;
import java.util.Vector;

import androidx.annotation.NonNull;
import androidx.test.filters.SdkSuppress;


/**
 * Tests of MediaPlayer streaming capabilities.
 */
public class MediaDrmClearkeyTest extends MediaCodecPlayerTestBase<MediaStubActivity> {

    private static final String TAG = MediaDrmClearkeyTest.class.getSimpleName();

    // Add additional keys here if the content has more keys.
    private static final byte[] CLEAR_KEY_CENC = {
            (byte)0x3f, (byte)0x0a, (byte)0x33, (byte)0xf3, (byte)0x40, (byte)0x98, (byte)0xb9, (byte)0xe2,
            (byte)0x2b, (byte)0xc0, (byte)0x78, (byte)0xe0, (byte)0xa1, (byte)0xb5, (byte)0xe8, (byte)0x54 };

    private static final byte[] CLEAR_KEY_WEBM = ""_CLEAR_KEY_WEBM_"".getBytes();

    private static final int NUMBER_OF_SECURE_STOPS = 10;
    private static final int VIDEO_WIDTH_CENC = 1280;
    private static final int VIDEO_HEIGHT_CENC = 720;
    private static final int VIDEO_WIDTH_WEBM = 352;
    private static final int VIDEO_HEIGHT_WEBM = 288;
    private static final int VIDEO_WIDTH_MPEG2TS = 320;
    private static final int VIDEO_HEIGHT_MPEG2TS = 240;
    private static final String MIME_VIDEO_AVC = MediaFormat.MIMETYPE_VIDEO_AVC;
    private static final String MIME_VIDEO_VP8 = MediaFormat.MIMETYPE_VIDEO_VP8;

    // Property Keys
    private static final String ALGORITHMS_PROPERTY_KEY = MediaDrm.PROPERTY_ALGORITHMS;
    private static final String DESCRIPTION_PROPERTY_KEY = MediaDrm.PROPERTY_DESCRIPTION;
    private static final String DEVICEID_PROPERTY_KEY = ""deviceId"";
    private static final String INVALID_PROPERTY_KEY = ""invalid property key"";
    private static final String LISTENER_TEST_SUPPORT_PROPERTY_KEY = ""listenerTestSupport"";
    private static final String VENDOR_PROPERTY_KEY = MediaDrm.PROPERTY_VENDOR;
    private static final String VERSION_PROPERTY_KEY = MediaDrm.PROPERTY_VERSION;

    // Error message
    private static final String ERR_MSG_CRYPTO_SCHEME_NOT_SUPPORTED = ""Crypto scheme is not supported"";

    private static final String CENC_AUDIO_PATH = ""/clear/h264/llama/llama_aac_audio.mp4"";
    private static final String CENC_VIDEO_PATH = ""/clearkey/llama_h264_main_720p_8000.mp4"";
    private static final Uri WEBM_URL = Uri.parse(
            ""android.resource://android.mediadrm.cts/"" + R.raw.video_320x240_webm_vp8_800kbps_30fps_vorbis_stereo_128kbps_44100hz_crypt);
    private static final Uri MPEG2TS_SCRAMBLED_URL = Uri.parse(
            ""android.resource://android.mediadrm.cts/"" + R.raw.segment000001_scrambled);
    private static final Uri MPEG2TS_CLEAR_URL = Uri.parse(
            ""android.resource://android.mediadrm.cts/"" + R.raw.segment000001);

    private static final UUID COMMON_PSSH_SCHEME_UUID =
            new UUID(0x1077efecc0b24d02L, 0xace33c1e52e2fb4bL);
    private static final UUID CLEARKEY_SCHEME_UUID =
            new UUID(0xe2719d58a985b3c9L, 0x781ab030af78d30eL);

    private byte[] mDrmInitData;
    private byte[] mKeySetId;
    private byte[] mSessionId;
    private Monitor mSessionMonitor = new Monitor();
    private Looper mLooper;
    private MediaDrm mDrm = null;
    private final Object mLock = new Object();
    private boolean mEventListenerCalled;
    private boolean mExpirationUpdateReceived;
    private boolean mLostStateReceived;

    private static boolean sIsAtLeastS = ApiLevelUtil.isAtLeast(Build.VERSION_CODES.S);

    public MediaDrmClearkeyTest() {
        super(MediaStubActivity.class);
    }

    @Override
    protected void setUp() throws Exception {
        super.setUp();
        if (false == deviceHasMediaDrm()) {
            tearDown();
        }
    }

    @Override
    protected void tearDown() throws Exception {
        super.tearDown();
    }

    private boolean deviceHasMediaDrm() {
        // ClearKey is introduced after KitKat.
        if (ApiLevelUtil.isAtMost(android.os.Build.VERSION_CODES.KITKAT)) {
            return false;
        }
        return true;
    }

    /**
     * Extracts key ids from the pssh blob returned by getKeyRequest() and
     * places it in keyIds.
     * keyRequestBlob format (section 5.1.3.1):
     * https://dvcs.w3.org/hg/html-media/raw-file/default/encrypted-media/encrypted-media.html#clear-key
     *
     * @return size of keyIds vector that contains the key ids, 0 for error
     */
    private static int getKeyIds(byte[] keyRequestBlob, Vector<String> keyIds) {
        if (0 == keyRequestBlob.length || keyIds == null)
            return 0;

        String jsonLicenseRequest = new String(keyRequestBlob);
        keyIds.clear();

        try {
            JSONObject license = new JSONObject(jsonLicenseRequest);
            final JSONArray ids = license.getJSONArray(""kids"");
            for (int i = 0; i < ids.length(); ++i) {
                keyIds.add(ids.getString(i));
            }
        } catch (JSONException e) {
            Log.e(TAG, ""Invalid JSON license = "" + jsonLicenseRequest);
            return 0;
        }
        return keyIds.size();
    }

    /**
     * Creates the JSON Web Key string.
     *
     * @return JSON Web Key string.
     */
    private static String createJsonWebKeySet(
            Vector<String> keyIds, Vector<String> keys, int keyType) {
        String jwkSet = ""{\""keys\"":["";
        for (int i = 0; i < keyIds.size(); ++i) {
            String id = new String(keyIds.get(i).getBytes(Charset.forName(""UTF-8"")));
            String key = new String(keys.get(i).getBytes(Charset.forName(""UTF-8"")));

            jwkSet += ""{\""kty\"":\""oct\"",\""kid\"":\"""" + id +
                    ""\"",\""k\"":\"""" + key + ""\""}"";
        }
        jwkSet += ""], \""type\"":"";
        if (keyType == MediaDrm.KEY_TYPE_OFFLINE || keyType == MediaDrm.KEY_TYPE_RELEASE) {
            jwkSet += ""\""persistent-license\"" }"";
        } else {
            jwkSet += ""\""temporary\"" }"";
        }
        return jwkSet;
    }

    /**
     * Retrieves clear key ids from getKeyRequest(), create JSON Web Key
     * set and send it to the CDM via provideKeyResponse().
     *
     * @return key set ID
     */
    public static byte[] retrieveKeys(MediaDrm drm, String initDataType,
            byte[] sessionId, byte[] drmInitData, int keyType, byte[][] clearKeyIds) {
        MediaDrm.KeyRequest drmRequest = null;
        try {
            drmRequest = drm.getKeyRequest(sessionId, drmInitData, initDataType,
                    keyType, null);
        } catch (Exception e) {
            e.printStackTrace();
            Log.i(TAG, ""Failed to get key request: "" + e.toString());
        }
        if (drmRequest == null) {
            Log.e(TAG, ""Failed getKeyRequest"");
            return null;
        }

        Vector<String> keyIds = new Vector<String>();
        if (0 == getKeyIds(drmRequest.getData(), keyIds)) {
            Log.e(TAG, ""No key ids found in initData"");
            return null;
        }

        if (clearKeyIds.length != keyIds.size()) {
            Log.e(TAG, ""Mismatch number of key ids and keys: ids="" +
                    keyIds.size() + "", keys="" + clearKeyIds.length);
            return null;
        }

        // Base64 encodes clearkeys. Keys are known to the application.
        Vector<String> keys = new Vector<String>();
        for (int i = 0; i < clearKeyIds.length; ++i) {
            String clearKey = Base64.encodeToString(clearKeyIds[i],
                    Base64.NO_PADDING | Base64.NO_WRAP);
            keys.add(clearKey);
        }

        String jwkSet = createJsonWebKeySet(keyIds, keys, keyType);
        byte[] jsonResponse = jwkSet.getBytes(Charset.forName(""UTF-8""));

        try {
            try {
                return drm.provideKeyResponse(sessionId, jsonResponse);
            } catch (IllegalStateException e) {
                Log.e(TAG, ""Failed to provide key response: "" + e.toString());
            }
        } catch (Exception e) {
            e.printStackTrace();
            Log.e(TAG, ""Failed to provide key response: "" + e.toString());
        }
        return null;
    }

    /**
     * Retrieves clear key ids from getKeyRequest(), create JSON Web Key
     * set and send it to the CDM via provideKeyResponse().
     */
    private void getKeys(MediaDrm drm, String initDataType,
            byte[] sessionId, byte[] drmInitData, int keyType, byte[][] clearKeyIds) {
        mKeySetId = retrieveKeys(drm, initDataType, sessionId, drmInitData, keyType, clearKeyIds);
    }

    private @NonNull MediaDrm startDrm(final byte[][] clearKeyIds, final String initDataType,
                                       final UUID drmSchemeUuid, int keyType) {
        if (!MediaDrm.isCryptoSchemeSupported(drmSchemeUuid)) {
            throw new Error(ERR_MSG_CRYPTO_SCHEME_NOT_SUPPORTED);
        }

        new Thread() {
            @Override
            public void run() {
                if (mDrm != null) {
                    Log.e(TAG, ""Failed to startDrm: already started"");
                    return;
                }
                // Set up a looper to handle events
                Looper.prepare();

                // Save the looper so that we can terminate this thread
                // after we are done with it.
                mLooper = Looper.myLooper();

                try {
                    mDrm = new MediaDrm(drmSchemeUuid);
                } catch (MediaDrmException e) {
                    Log.e(TAG, ""Failed to create MediaDrm: "" + e.getMessage());
                    return;
                }

                synchronized(mLock) {
                    mDrm.setOnEventListener(new MediaDrm.OnEventListener() {
                            @Override
                            public void onEvent(MediaDrm md, byte[] sid, int event,
                                    int extra, byte[] data) {
                                if (md != mDrm) {
                                    Log.e(TAG, ""onEvent callback: drm object mismatch"");
                                    return;
                                } else if (!Arrays.equals(mSessionId, sid)) {
                                    Log.e(TAG, ""onEvent callback: sessionId mismatch: |"" +
                                            Arrays.toString(mSessionId) + ""| vs |"" + Arrays.toString(sid) + ""|"");
                                    return;
                                }

                                mEventListenerCalled = true;
                                if (event == MediaDrm.EVENT_PROVISION_REQUIRED) {
                                    Log.i(TAG, ""MediaDrm event: Provision required"");
                                } else if (event == MediaDrm.EVENT_KEY_REQUIRED) {
                                    Log.i(TAG, ""MediaDrm event: Key required"");
                                    getKeys(mDrm, initDataType, mSessionId, mDrmInitData,
                                            keyType, clearKeyIds);
                                } else if (event == MediaDrm.EVENT_KEY_EXPIRED) {
                                    Log.i(TAG, ""MediaDrm event: Key expired"");
                                    getKeys(mDrm, initDataType, mSessionId, mDrmInitData,
                                            keyType, clearKeyIds);
                                } else if (event == MediaDrm.EVENT_VENDOR_DEFINED) {
                                    Log.i(TAG, ""MediaDrm event: Vendor defined"");
                                } else if (event == MediaDrm.EVENT_SESSION_RECLAIMED) {
                                    Log.i(TAG, ""MediaDrm event: Session reclaimed"");
                                } else {
                                    Log.e(TAG, ""MediaDrm event not supported: "" + event);
                                }
                            }
                        });
                    mDrm.setOnExpirationUpdateListener(new MediaDrm.OnExpirationUpdateListener() {
                            @Override
                            public void onExpirationUpdate(MediaDrm md, byte[] sid, long expirationTime) {
                                if (md != mDrm) {
                                    Log.e(TAG, ""onExpirationUpdate callback: drm object mismatch"");
                                } else if (!Arrays.equals(mSessionId, sid)) {
                                    Log.e(TAG, ""onExpirationUpdate callback: sessionId mismatch: |"" +
                                            Arrays.toString(mSessionId) + ""| vs |"" + Arrays.toString(sid) + ""|"");
                                } else {
                                    mExpirationUpdateReceived = true;
                                }
                            }
                        }, null);
                    mDrm.setOnSessionLostStateListener(new MediaDrm.OnSessionLostStateListener() {
                            @Override
                            public void onSessionLostState(MediaDrm md, byte[] sid) {
                                if (md != mDrm) {
                                    Log.e(TAG, ""onSessionLostState callback: drm object mismatch"");
                                } else if (!Arrays.equals(mSessionId, sid)) {
                                    Log.e(TAG, ""onSessionLostState callback: sessionId mismatch: |"" +
                                            Arrays.toString(mSessionId) + ""| vs |"" + Arrays.toString(sid) + ""|"");
                                } else {
                                    mLostStateReceived = true;
                                }
                            }
                        }, null);
                    mDrm.setOnKeyStatusChangeListener(new MediaDrm.OnKeyStatusChangeListener() {
                            @Override
                            public void onKeyStatusChange(MediaDrm md, byte[] sessionId,
                                    List<KeyStatus> keyInformation, boolean hasNewUsableKey) {
                                Log.d(TAG, ""onKeyStatusChange"");
                                assertTrue(md == mDrm);
                                assertTrue(Arrays.equals(sessionId, mSessionId));
                                mSessionMonitor.signal();
                                assertTrue(hasNewUsableKey);

                                assertEquals(3, keyInformation.size());
                                KeyStatus keyStatus = keyInformation.get(0);
                                assertTrue(Arrays.equals(keyStatus.getKeyId(), new byte[] {0xa, 0xb, 0xc}));
                                assertTrue(keyStatus.getStatusCode() == MediaDrm.KeyStatus.STATUS_USABLE);
                                keyStatus = keyInformation.get(1);
                                assertTrue(Arrays.equals(keyStatus.getKeyId(), new byte[] {0xd, 0xe, 0xf}));
                                assertTrue(keyStatus.getStatusCode() == MediaDrm.KeyStatus.STATUS_EXPIRED);
                                keyStatus = keyInformation.get(2);
                                assertTrue(Arrays.equals(keyStatus.getKeyId(), new byte[] {0x0, 0x1, 0x2}));
                                assertTrue(keyStatus.getStatusCode() == MediaDrm.KeyStatus.STATUS_USABLE_IN_FUTURE);
                            }
                        }, null);

                    mLock.notify();
                }
                Looper.loop();  // Blocks forever until Looper.quit() is called.
            }
        }.start();

        // wait for mDrm to be created
        synchronized(mLock) {
            try {
                mLock.wait(1000);
            } catch (Exception e) {
            }
        }
        return mDrm;
    }

    private void stopDrm(MediaDrm drm) {
        if (drm != mDrm) {
            Log.e(TAG, ""invalid drm specified in stopDrm"");
        }
        mLooper.quit();
        mDrm.close();
        mDrm = null;
    }

    private @NonNull byte[] openSession(MediaDrm drm) {
        byte[] mSessionId = null;
        boolean mRetryOpen;
        do {
            try {
                mRetryOpen = false;
                mSessionId = drm.openSession();
            } catch (Exception e) {
                mRetryOpen = true;
            }
        } while (mRetryOpen);
        return mSessionId;
    }

    private void closeSession(MediaDrm drm, byte[] sessionId) {
        drm.closeSession(sessionId);
    }

    /**
     * Tests clear key system playback.
     */
    private void testClearKeyPlayback(
            UUID drmSchemeUuid,
            String videoMime, String[] videoFeatures,
            String initDataType, byte[][] clearKeyIds,
            Uri audioUrl, boolean audioEncrypted,
            Uri videoUrl, boolean videoEncrypted,
            int videoWidth, int videoHeight, boolean scrambled, int keyType) throws Exception {

        if (isWatchDevice()) {
            return;
        }

        MediaDrm drm = null;
        mSessionId = null;
        final boolean hasDrm = !scrambled && drmSchemeUuid != null;
        if (hasDrm) {
            drm = startDrm(clearKeyIds, initDataType, drmSchemeUuid, keyType);
            mSessionId = openSession(drm);
        }

        if (!preparePlayback(videoMime, videoFeatures, audioUrl, audioEncrypted, videoUrl,
                videoEncrypted, videoWidth, videoHeight, scrambled, mSessionId, getSurfaces())) {
            // Allow device to skip test to keep existing behavior.
            // We should throw an exception for new tests.
            return;
        }

        if (hasDrm) {
            mDrmInitData = mMediaCodecPlayer.getDrmInitData();
            getKeys(mDrm, initDataType, mSessionId, mDrmInitData, keyType, clearKeyIds);
        }

        if (hasDrm && keyType == MediaDrm.KEY_TYPE_OFFLINE) {
            closeSession(drm, mSessionId);
            mSessionMonitor.waitForSignal();
            mSessionId = openSession(drm);
            if (mKeySetId.length > 0) {
                drm.restoreKeys(mSessionId, mKeySetId);
            } else {
                closeSession(drm, mSessionId);
                stopDrm(drm);
                throw new Error(""Invalid keySetId size for offline license"");
            }
        }

        // starts video playback
        playUntilEnd();
        if (hasDrm) {
            closeSession(drm, mSessionId);
            stopDrm(drm);
        }
    }

    /**
     * Tests KEY_TYPE_RELEASE for offline license.
     */
    @Presubmit"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.CodecTestBase"	"isEmpty"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/CodecTestBase.java"	""	"public void test/*
 *.
 */

package android.mediav2.cts;

import android.content.Context;
import android.content.pm.PackageManager;
import android.graphics.ImageFormat;
import android.graphics.Rect;
import android.hardware.display.DisplayManager;
import android.media.Image;
import android.media.MediaCodec;
import android.media.MediaCodecInfo;
import android.media.MediaCodecList;
import android.media.MediaExtractor;
import android.media.MediaFormat;
import android.os.Build;
import android.os.PersistableBundle;
import android.util.Log;
import android.util.Pair;
import android.view.Display;
import android.view.Surface;

import androidx.annotation.NonNull;
import androidx.test.platform.app.InstrumentationRegistry;

import org.junit.Assert;
import org.junit.Before;

import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.locks.Condition;
import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReentrantLock;
import java.util.zip.CRC32;

import com.android.compatibility.common.util.ApiLevelUtil;

import static android.media.MediaCodecInfo.CodecCapabilities.COLOR_FormatSurface;
import static android.media.MediaCodecInfo.CodecCapabilities.COLOR_FormatYUV420Flexible;
import static android.media.MediaCodecInfo.CodecProfileLevel.*;
import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;
import static org.junit.Assert.fail;

class CodecAsyncHandler extends MediaCodec.Callback {
    private static final String LOG_TAG = CodecAsyncHandler.class.getSimpleName();
    private final Lock mLock = new ReentrantLock();
    private final Condition mCondition = mLock.newCondition();
    private final LinkedList<Pair<Integer, MediaCodec.BufferInfo>> mCbInputQueue;
    private final LinkedList<Pair<Integer, MediaCodec.BufferInfo>> mCbOutputQueue;
    private MediaFormat mOutFormat;
    private boolean mSignalledOutFormatChanged;
    private volatile boolean mSignalledError;

    CodecAsyncHandler() {
        mCbInputQueue = new LinkedList<>();
        mCbOutputQueue = new LinkedList<>();
        mSignalledError = false;
        mSignalledOutFormatChanged = false;
    }

    void clearQueues() {
        mLock.lock();
        mCbInputQueue.clear();
        mCbOutputQueue.clear();
        mLock.unlock();
    }

    void resetContext() {
        clearQueues();
        mOutFormat = null;
        mSignalledOutFormatChanged = false;
        mSignalledError = false;
    }

    @Override
    public void onInputBufferAvailable(@NonNull MediaCodec codec, int bufferIndex) {
        assertTrue(bufferIndex >= 0);
        mLock.lock();
        mCbInputQueue.add(new Pair<>(bufferIndex, (MediaCodec.BufferInfo) null));
        mCondition.signalAll();
        mLock.unlock();
    }

    @Override
    public void onOutputBufferAvailable(@NonNull MediaCodec codec, int bufferIndex,
            @NonNull MediaCodec.BufferInfo info) {
        assertTrue(bufferIndex >= 0);
        mLock.lock();
        mCbOutputQueue.add(new Pair<>(bufferIndex, info));
        mCondition.signalAll();
        mLock.unlock();
    }

    @Override
    public void onError(@NonNull MediaCodec codec, MediaCodec.CodecException e) {
        mLock.lock();
        mSignalledError = true;
        mCondition.signalAll();
        mLock.unlock();
        Log.e(LOG_TAG, ""received media codec error : "" + e.getMessage());
    }

    @Override
    public void onOutputFormatChanged(@NonNull MediaCodec codec, @NonNull MediaFormat format) {
        mOutFormat = format;
        mSignalledOutFormatChanged = true;
        Log.i(LOG_TAG, ""Output format changed: "" + format.toString());
    }

    void setCallBack(MediaCodec codec, boolean isCodecInAsyncMode) {
        if (isCodecInAsyncMode) {
            codec.setCallback(this);
        } else {
            codec.setCallback(null);
        }
    }

    Pair<Integer, MediaCodec.BufferInfo> getInput() throws InterruptedException {
        Pair<Integer, MediaCodec.BufferInfo> element = null;
        mLock.lock();
        while (!mSignalledError) {
            if (mCbInputQueue.isEmpty()) {
                mCondition.await();
            } else {
                element = mCbInputQueue.remove(0);
                break;
            }
        }
        mLock.unlock();
        return element;
    }

    Pair<Integer, MediaCodec.BufferInfo> getOutput() throws InterruptedException {
        Pair<Integer, MediaCodec.BufferInfo> element = null;
        mLock.lock();
        while (!mSignalledError) {
            if (mCbOutputQueue.isEmpty()) {
                mCondition.await();
            } else {
                element = mCbOutputQueue.remove(0);
                break;
            }
        }
        mLock.unlock();
        return element;
    }

    Pair<Integer, MediaCodec.BufferInfo> getWork() throws InterruptedException {
        Pair<Integer, MediaCodec.BufferInfo> element = null;
        mLock.lock();
        while (!mSignalledError) {
            if (mCbInputQueue.isEmpty() && mCbOutputQueue.isEmpty()) {
                mCondition.await();
            } else {
                if (!mCbOutputQueue.isEmpty()) {
                    element = mCbOutputQueue.remove(0);
                    break;
                }
                if (!mCbInputQueue.isEmpty()) {
                    element = mCbInputQueue.remove(0);
                    break;
                }
            }
        }
        mLock.unlock();
        return element;
    }

    boolean isInputQueueEmpty() {
        mLock.lock();
        boolean isEmpty = mCbInputQueue.isEmpty();
        mLock.unlock();
        return isEmpty;
    }

    boolean hasSeenError() {
        return mSignalledError;
    }

    boolean hasOutputFormatChanged() {
        return mSignalledOutFormatChanged;
    }

    MediaFormat getOutputFormat() {
        return mOutFormat;
    }
}

class OutputManager {
    private static final String LOG_TAG = OutputManager.class.getSimpleName();
    private byte[] memory;
    private int memIndex;
    private CRC32 mCrc32UsingImage;
    private CRC32 mCrc32UsingBuffer;
    private ArrayList<Long> inpPtsList;
    private ArrayList<Long> outPtsList;

    OutputManager() {
        memory = new byte[1024];
        memIndex = 0;
        mCrc32UsingImage = new CRC32();
        mCrc32UsingBuffer = new CRC32();
        inpPtsList = new ArrayList<>();
        outPtsList = new ArrayList<>();
    }

    void saveInPTS(long pts) {
        // Add only Unique timeStamp, discarding any duplicate frame / non-display frame
        if (!inpPtsList.contains(pts)) {
            inpPtsList.add(pts);
        }
    }

    void saveOutPTS(long pts) {
        outPtsList.add(pts);
    }

    boolean isPtsStrictlyIncreasing(long lastPts) {
        boolean res = true;
        for (int i = 0; i < outPtsList.size(); i++) {
            if (lastPts < outPtsList.get(i)) {
                lastPts = outPtsList.get(i);
            } else {
                Log.e(LOG_TAG, ""Timestamp ordering check failed: last timestamp: "" + lastPts +
                        "" current timestamp:"" + outPtsList.get(i));
                res = false;
                break;
            }
        }
        return res;
    }

    boolean isOutPtsListIdenticalToInpPtsList(boolean requireSorting) {
        boolean res;
        Collections.sort(inpPtsList);
        if (requireSorting) {
            Collections.sort(outPtsList);
        }
        if (outPtsList.size() != inpPtsList.size()) {
            Log.e(LOG_TAG, ""input and output presentation timestamp list sizes are not identical"" +
                    ""exp/rec"" + inpPtsList.size() + '/' + outPtsList.size());
            return false;
        } else {
            int count = 0;
            for (int i = 0; i < outPtsList.size(); i++) {
                if (!outPtsList.get(i).equals(inpPtsList.get(i))) {
                    count ++;
                    Log.e(LOG_TAG, ""input output pts mismatch, exp/rec "" + outPtsList.get(i) + '/' +
                            inpPtsList.get(i));
                    if (count == 20) {
                        Log.e(LOG_TAG, ""stopping after 20 mismatches, ..."");
                        break;
                    }
                }
            }
            res = (count == 0);
        }
        return res;
    }

    int getOutStreamSize() {
        return memIndex;
    }

    void checksum(ByteBuffer buf, int size) {
        checksum(buf, size, 0, 0, 0);
    }

    void checksum(ByteBuffer buf, int size, int width, int height, int stride) {
        int cap = buf.capacity();
        assertTrue(""checksum() params are invalid: size = "" + size + "" cap = "" + cap,
                size > 0 && size <= cap);
        if (buf.hasArray()) {
            if (width > 0 && height > 0 && stride > 0) {
                int offset = buf.position() + buf.arrayOffset();
                byte[] bb = new byte[width * height];
                for (int i = 0; i < height; ++i) {
                    System.arraycopy(buf.array(), offset, bb, i * width, width);
                    offset += stride;
                }
                mCrc32UsingBuffer.update(bb, 0, width * height);
            } else {
                mCrc32UsingBuffer.update(buf.array(), buf.position() + buf.arrayOffset(), size);
            }
        } else if (width > 0 && height > 0 && stride > 0) {
            // Checksum only the Y plane
            int pos = buf.position();
            int offset = pos;
            byte[] bb = new byte[width * height];
            for (int i = 0; i < height; ++i) {
                buf.position(offset);
                buf.get(bb, i * width, width);
                offset += stride;
            }
            mCrc32UsingBuffer.update(bb, 0, width * height);
            buf.position(pos);
        } else {
            int pos = buf.position();
            final int rdsize = Math.min(4096, size);
            byte[] bb = new byte[rdsize];
            int chk;
            for (int i = 0; i < size; i += chk) {
                chk = Math.min(rdsize, size - i);
                buf.get(bb, 0, chk);
                mCrc32UsingBuffer.update(bb, 0, chk);
            }
            buf.position(pos);
        }
    }

    void checksum(Image image) {
        int format = image.getFormat();
        assertEquals(""unexpected image format"", ImageFormat.YUV_420_888, format);

        Rect cropRect = image.getCropRect();
        int imageWidth = cropRect.width();
        int imageHeight = cropRect.height();
        assertTrue(""unexpected image dimensions"", imageWidth > 0 && imageHeight > 0);

        int imageLeft = cropRect.left;
        int imageTop = cropRect.top;
        Image.Plane[] planes = image.getPlanes();
        for (int i = 0; i < planes.length; ++i) {
            ByteBuffer buf = planes[i].getBuffer();
            int width, height, rowStride, pixelStride, x, y, left, top;
            rowStride = planes[i].getRowStride();
            pixelStride = planes[i].getPixelStride();
            if (i == 0) {
                width = imageWidth;
                height = imageHeight;
                left = imageLeft;
                top = imageTop;
            } else {
                width = imageWidth / 2;
                height = imageHeight / 2;
                left = imageLeft / 2;
                top = imageTop / 2;
            }
            int cropOffset = left + top * rowStride;
            // local contiguous pixel buffer
            byte[] bb = new byte[width * height];
            if (buf.hasArray()) {
                byte[] b = buf.array();
                int offs = buf.arrayOffset() + cropOffset;
                if (pixelStride == 1) {
                    for (y = 0; y < height; ++y) {
                        System.arraycopy(b, offs + y * rowStride, bb, y * width, width);
                    }
                } else {
                    // do it pixel-by-pixel
                    for (y = 0; y < height; ++y) {
                        int lineOffset = offs + y * rowStride;
                        for (x = 0; x < width; ++x) {
                            bb[y * width + x] = b[lineOffset + x * pixelStride];
                        }
                    }
                }
            } else { // almost always ends up here due to direct buffers
                int base = buf.position();
                int pos = base + cropOffset;
                if (pixelStride == 1) {
                    for (y = 0; y < height; ++y) {
                        buf.position(pos + y * rowStride);
                        buf.get(bb, y * width, width);
                    }
                } else {
                    // local line buffer
                    byte[] lb = new byte[rowStride];
                    // do it pixel-by-pixel
                    for (y = 0; y < height; ++y) {
                        buf.position(pos + y * rowStride);
                        // we're only guaranteed to have pixelStride * (width - 1) + 1 bytes
                        buf.get(lb, 0, pixelStride * (width - 1) + 1);
                        for (x = 0; x < width; ++x) {
                            bb[y * width + x] = lb[x * pixelStride];
                        }
                    }
                }
                buf.position(base);
            }
            mCrc32UsingImage.update(bb, 0, width * height);
        }
    }

    void saveToMemory(ByteBuffer buf, MediaCodec.BufferInfo info) {
        if (memIndex + info.size >= memory.length) {
            memory = Arrays.copyOf(memory, memIndex + info.size);
        }
        buf.position(info.offset);
        buf.get(memory, memIndex, info.size);
        memIndex += info.size;
    }

    void position(int index) {
        if (index < 0 || index >= memory.length) index = 0;
        memIndex = index;
    }

    ByteBuffer getBuffer() {
        return ByteBuffer.wrap(memory);
    }

    void reset() {
        position(0);
        mCrc32UsingImage.reset();
        mCrc32UsingBuffer.reset();
        inpPtsList.clear();
        outPtsList.clear();
    }

    float getRmsError(short[] refData) {
        long totalErrorSquared = 0;
        assertTrue(0 == (memIndex & 1));
        short[] shortData = new short[memIndex / 2];
        ByteBuffer.wrap(memory, 0, memIndex).order(ByteOrder.LITTLE_ENDIAN).asShortBuffer()
                .get(shortData);
        if (refData.length != shortData.length) return Float.MAX_VALUE;
        for (int i = 0; i < shortData.length; i++) {
            int d = shortData[i] - refData[i];
            totalErrorSquared += d * d;
        }
        long avgErrorSquared = (totalErrorSquared / shortData.length);
        return (float) Math.sqrt(avgErrorSquared);
    }

    long getCheckSumImage() {
        return mCrc32UsingImage.getValue();
    }

    long getCheckSumBuffer() {
        return mCrc32UsingBuffer.getValue();
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;
        OutputManager that = (OutputManager) o;
        // TODO: Timestamps for deinterlaced content are under review. (E.g. can decoders
        // produce multiple progressive frames?) For now, do not verify timestamps.
        boolean isEqual = this.equalsInterlaced(o);
        if (!outPtsList.equals(that.outPtsList)) {
            isEqual = false;
            Log.e(LOG_TAG, ""ref and test presentation timestamp mismatch"");
        }
        return isEqual;
    }

    public boolean equalsInterlaced(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;
        OutputManager that = (OutputManager) o;
        boolean isEqual = true;
        if (mCrc32UsingImage.getValue() != that.mCrc32UsingImage.getValue()) {
            isEqual = false;
            Log.e(LOG_TAG, ""ref and test crc32 checksums calculated using image mismatch "" +
                          mCrc32UsingImage.getValue() + '/' + that.mCrc32UsingImage.getValue());
        }
        if (mCrc32UsingBuffer.getValue() != that.mCrc32UsingBuffer.getValue()) {
            isEqual = false;
            Log.e(LOG_TAG, ""ref and test crc32 checksums calculated using buffer mismatch "" +
                          mCrc32UsingBuffer.getValue() + '/' + that.mCrc32UsingBuffer.getValue());
            if (memIndex == that.memIndex) {
                int count = 0;
                for (int i = 0; i < memIndex; i++) {
                    if (memory[i] != that.memory[i]) {
                        count++;
                        if (count < 20) {
                            Log.d(LOG_TAG, ""sample at "" + i + "" exp/got:: "" + memory[i] + '/' +
                                    that.memory[i]);
                        }
                    }
                }
                if (count != 0) {
                    Log.e(LOG_TAG, ""ref and test o/p samples mismatch "" + count);
                }
            } else {
                Log.e(LOG_TAG, ""ref and test o/p sizes mismatch "" + memIndex + '/' + that.memIndex);
            }
        }
        return isEqual;
    }
}

abstract class CodecTestBase {
    public static final boolean IS_AT_LEAST_R = ApiLevelUtil.isAtLeast(Build.VERSION_CODES.R);
    private static final String LOG_TAG = CodecTestBase.class.getSimpleName();

    static final String CODEC_PREFIX_KEY = ""codec-prefix"";
    static final String MIME_SEL_KEY = ""mime-sel"";
    static final Map<String, String> codecSelKeyMimeMap = new HashMap<>();
    static final Map<String, String> mDefaultEncoders = new HashMap<>();
    static final Map<String, String> mDefaultDecoders = new HashMap<>();
    static final boolean ENABLE_LOGS = false;
    static final int PER_TEST_TIMEOUT_LARGE_TEST_MS = 300000;
    static final int PER_TEST_TIMEOUT_SMALL_TEST_MS = 60000;
    static final int UNSPECIFIED = 0;
    static final int CODEC_ALL = 0; // All codecs must support
    static final int CODEC_ANY = 1; // At least one codec must support
    static final int CODEC_DEFAULT = 2; // Default codec must support
    static final int CODEC_OPTIONAL = 3; // Codec support is optional
    // Maintain Timeouts in sync with their counterpart in NativeMediaCommon.h
    static final long Q_DEQ_TIMEOUT_US = 5000; // block at most 5ms while looking for io buffers
    static final int RETRY_LIMIT = 100; // max poll counter before test aborts and returns error
    static final String INVALID_CODEC = ""unknown.codec_"";
    static final String mInpPrefix = WorkDir.getMediaDirString();
    static final Context mContext = InstrumentationRegistry.getInstrumentation().getTargetContext();
    static final PackageManager pm = mContext.getPackageManager();
    static String mimeSelKeys;
    static String codecPrefix;

    CodecAsyncHandler mAsyncHandle;
    boolean mIsCodecInAsyncMode;
    boolean mSawInputEOS;
    boolean mSawOutputEOS;
    boolean mSignalEOSWithLastFrame;
    int mInputCount;
    int mOutputCount;
    long mPrevOutputPts;
    boolean mSignalledOutFormatChanged;
    MediaFormat mOutFormat;
    boolean mIsAudio;

    boolean mSaveToMem;
    OutputManager mOutputBuff;

    String mCodecName;
    MediaCodec mCodec;
    Surface mSurface;

    static {
        System.loadLibrary(""ctsmediav2codec_jni"");

        codecSelKeyMimeMap.put(""vp8"", MediaFormat.MIMETYPE_VIDEO_VP8);
        codecSelKeyMimeMap.put(""vp9"", MediaFormat.MIMETYPE_VIDEO_VP9);
        codecSelKeyMimeMap.put(""av1"", MediaFormat.MIMETYPE_VIDEO_AV1);
        codecSelKeyMimeMap.put(""avc"", MediaFormat.MIMETYPE_VIDEO_AVC);
        codecSelKeyMimeMap.put(""hevc"", MediaFormat.MIMETYPE_VIDEO_HEVC);
        codecSelKeyMimeMap.put(""mpeg4"", MediaFormat.MIMETYPE_VIDEO_MPEG4);
        codecSelKeyMimeMap.put(""h263"", MediaFormat.MIMETYPE_VIDEO_H263);
        codecSelKeyMimeMap.put(""mpeg2"", MediaFormat.MIMETYPE_VIDEO_MPEG2);
        codecSelKeyMimeMap.put(""vraw"", MediaFormat.MIMETYPE_VIDEO_RAW);
        codecSelKeyMimeMap.put(""amrnb"", MediaFormat.MIMETYPE_AUDIO_AMR_NB);
        codecSelKeyMimeMap.put(""amrwb"", MediaFormat.MIMETYPE_AUDIO_AMR_WB);
        codecSelKeyMimeMap.put(""mp3"", MediaFormat.MIMETYPE_AUDIO_MPEG);
        codecSelKeyMimeMap.put(""aac"", MediaFormat.MIMETYPE_AUDIO_AAC);
        codecSelKeyMimeMap.put(""vorbis"", MediaFormat.MIMETYPE_AUDIO_VORBIS);
        codecSelKeyMimeMap.put(""opus"", MediaFormat.MIMETYPE_AUDIO_OPUS);
        codecSelKeyMimeMap.put(""g711alaw"", MediaFormat.MIMETYPE_AUDIO_G711_ALAW);
        codecSelKeyMimeMap.put(""g711mlaw"", MediaFormat.MIMETYPE_AUDIO_G711_MLAW);
        codecSelKeyMimeMap.put(""araw"", MediaFormat.MIMETYPE_AUDIO_RAW);
        codecSelKeyMimeMap.put(""flac"", MediaFormat.MIMETYPE_AUDIO_FLAC);
        codecSelKeyMimeMap.put(""gsm"", MediaFormat.MIMETYPE_AUDIO_MSGSM);

        android.os.Bundle args = InstrumentationRegistry.getArguments();
        mimeSelKeys = args.getString(MIME_SEL_KEY);
        codecPrefix = args.getString(CODEC_PREFIX_KEY);
    }

    static boolean isTv() {
        return pm.hasSystemFeature(PackageManager.FEATURE_LEANBACK);
    }

    static boolean hasMicrophone() {
        return pm.hasSystemFeature(PackageManager.FEATURE_MICROPHONE);
    }

    static boolean hasCamera() {
        return pm.hasSystemFeature(PackageManager.FEATURE_CAMERA_ANY);
    }

    static boolean isWatch() {
        return pm.hasSystemFeature(PackageManager.FEATURE_WATCH);
    }

    static boolean isAutomotive() {
        return pm.hasSystemFeature(PackageManager.FEATURE_AUTOMOTIVE);
    }

    static boolean isPc() {
        return pm.hasSystemFeature(PackageManager.FEATURE_PC);
    }

    static boolean hasAudioOutput() {
        return pm.hasSystemFeature(PackageManager.FEATURE_AUDIO_OUTPUT);
    }

    static boolean isHandheld() {
        // handheld nature is not exposed to package manager, for now
        // we check for touchscreen and NOT watch and NOT tv and NOT pc
        return pm.hasSystemFeature(PackageManager.FEATURE_TOUCHSCREEN) && !isWatch() && !isTv() &&
                !isAutomotive() && !isPc();
    }

    static boolean hasDecoder(String mime) {
        return CodecTestBase.selectCodecs(mime, null, null, false).size() != 0;
    }

    static boolean hasEncoder(String mime) {
        return CodecTestBase.selectCodecs(mime, null, null, true).size() != 0;
    }

    static boolean isFeatureSupported(String name, String mime, String feature) throws IOException {
        MediaCodec codec = MediaCodec.createByCodecName(name);
        MediaCodecInfo.CodecCapabilities codecCapabilities =
                codec.getCodecInfo().getCapabilitiesForType(mime);
        boolean isSupported = codecCapabilities.isFeatureSupported(feature);
        codec.release();
        return isSupported;
    }

    static boolean doesAnyFormatHaveHDRProfile(String mime, ArrayList<MediaFormat> formats) {
        boolean isHDR = false;
        for (MediaFormat format : formats) {
            assertEquals(mime, format.getString(MediaFormat.KEY_MIME));
            if (mime.equals(MediaFormat.MIMETYPE_VIDEO_AVC)) {
                int profile = format.getInteger(MediaFormat.KEY_PROFILE);
                if (profile == AVCProfileHigh10 || profile == AVCProfileHigh422 ||
                        profile == AVCProfileHigh444) {
                    isHDR = true;
                    break;
                }
            } else if (mime.equals(MediaFormat.MIMETYPE_VIDEO_VP9)) {
                int profile = format.getInteger(MediaFormat.KEY_PROFILE, VP9Profile0);
                if (profile == VP9Profile2HDR || profile == VP9Profile3HDR ||
                        profile == VP9Profile2HDR10Plus || profile == VP9Profile3HDR10Plus) {
                    isHDR = true;
                    break;
                }
            } else if (mime.equals(MediaFormat.MIMETYPE_VIDEO_HEVC)) {
                int profile = format.getInteger(MediaFormat.KEY_PROFILE, HEVCProfileMain);
                if (profile == HEVCProfileMain10HDR10 || profile == HEVCProfileMain10HDR10Plus) {
                    isHDR = true;
                    break;
                }
            } else if (mime.equals(MediaFormat.MIMETYPE_VIDEO_AV1)) {
                int profile = format.getInteger(MediaFormat.KEY_PROFILE, AV1ProfileMain8);
                if (profile == AV1ProfileMain10HDR10 || profile == AV1ProfileMain10HDR10Plus) {
                    isHDR = true;
                    break;
                }
            }
        }
        return isHDR;
    }

    static boolean canDisplaySupportHDRContent() {
        DisplayManager displayManager = mContext.getSystemService(DisplayManager.class);
        return displayManager.getDisplay(Display.DEFAULT_DISPLAY).getHdrCapabilities()
                .getSupportedHdrTypes().length != 0;
    }

    static boolean areFormatsSupported(String name, String mime, ArrayList<MediaFormat> formats)
            throws IOException {
        MediaCodec codec = MediaCodec.createByCodecName(name);
        MediaCodecInfo.CodecCapabilities codecCapabilities =
                codec.getCodecInfo().getCapabilitiesForType(mime);
        boolean isSupported = true;
        if (formats != null) {
            for (int i = 0; i < formats.size() && isSupported; i++) {
                isSupported = codecCapabilities.isFormatSupported(formats.get(i));
            }
        }
        codec.release();
        return isSupported;
    }

    static boolean isDefaultCodec(String codecName, String mime, boolean isEncoder)
            throws IOException {
        Map<String,String> mDefaultCodecs = isEncoder ? mDefaultEncoders:  mDefaultDecoders;
        if (mDefaultCodecs.containsKey(mime)) {
            return mDefaultCodecs.get(mime).equalsIgnoreCase(codecName);
        }
        MediaCodec codec = isEncoder ? MediaCodec.createEncoderByType(mime)
                : MediaCodec.createDecoderByType(mime);
        boolean isDefault = codec.getName().equalsIgnoreCase(codecName);
        mDefaultCodecs.put(mime, codec.getName());
        codec.release();
        return isDefault;
    }

    static ArrayList<String> compileRequiredMimeList(boolean isEncoder, boolean needAudio,
            boolean needVideo) {
        Set<String> list = new HashSet<>();
        if (!isEncoder) {
            if (hasAudioOutput() && needAudio) {
                // sec 5.1.2
                list.add(MediaFormat.MIMETYPE_AUDIO_AAC);
                list.add(MediaFormat.MIMETYPE_AUDIO_FLAC);
                list.add(MediaFormat.MIMETYPE_AUDIO_MPEG);
                list.add(MediaFormat.MIMETYPE_AUDIO_VORBIS);
                list.add(MediaFormat.MIMETYPE_AUDIO_RAW);
                list.add(MediaFormat.MIMETYPE_AUDIO_OPUS);
            }
            if (isHandheld() || isTv() || isAutomotive()) {
                // sec 2.2.2, 2.3.2, 2.5.2
                if (needAudio) {
                    list.add(MediaFormat.MIMETYPE_AUDIO_AAC);
                }
                if (needVideo) {
                    list.add(MediaFormat.MIMETYPE_VIDEO_AVC);
                    list.add(MediaFormat.MIMETYPE_VIDEO_MPEG4);
                    list.add(MediaFormat.MIMETYPE_VIDEO_H263);
                    list.add(MediaFormat.MIMETYPE_VIDEO_VP8);
                    list.add(MediaFormat.MIMETYPE_VIDEO_VP9);
                }
            }
            if (isHandheld()) {
                // sec 2.2.2
                if (needAudio) {
                    list.add(MediaFormat.MIMETYPE_AUDIO_AMR_NB);
                    list.add(MediaFormat.MIMETYPE_AUDIO_AMR_WB);
                }
                if (needVideo) {
                    list.add(MediaFormat.MIMETYPE_VIDEO_HEVC);
                }
            }
            if (isTv() && needVideo) {
                // sec 2.3.2
                list.add(MediaFormat.MIMETYPE_VIDEO_HEVC);
                list.add(MediaFormat.MIMETYPE_VIDEO_MPEG2);
            }
        } else {
            if (hasMicrophone() && needAudio) {
                // sec 5.1.1
                // TODO(b/154423550)
                // list.add(MediaFormat.MIMETYPE_AUDIO_RAW);
                list.add(MediaFormat.MIMETYPE_AUDIO_FLAC);
                list.add(MediaFormat.MIMETYPE_AUDIO_OPUS);
            }
            if (isHandheld() || isTv() || isAutomotive()) {
                // sec 2.2.2, 2.3.2, 2.5.2
                if (needAudio) {
                    list.add(MediaFormat.MIMETYPE_AUDIO_AAC);
                }
                if (needVideo) {
                    list.add(MediaFormat.MIMETYPE_VIDEO_AVC);
                    list.add(MediaFormat.MIMETYPE_VIDEO_VP8);
                }
            }
            if (isHandheld() && needAudio) {
                // sec 2.2.2
                list.add(MediaFormat.MIMETYPE_AUDIO_AMR_NB);
                list.add(MediaFormat.MIMETYPE_AUDIO_AMR_WB);
            }
        }
        return new ArrayList<>(list);
    }

    static ArrayList<String> compileCompleteTestMimeList(boolean isEncoder, boolean needAudio,
            boolean needVideo) {
        ArrayList<String> mimes = new ArrayList<>();
        if (mimeSelKeys == null) {
            ArrayList<String> cddRequiredMimeList =
                    compileRequiredMimeList(isEncoder, needAudio, needVideo);
            MediaCodecList codecList = new MediaCodecList(MediaCodecList.REGULAR_CODECS);
            MediaCodecInfo[] codecInfos = codecList.getCodecInfos();
            for (MediaCodecInfo codecInfo : codecInfos) {
                if (codecInfo.isEncoder() != isEncoder) continue;
                if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.Q && codecInfo.isAlias()) continue;
                String[] types = codecInfo.getSupportedTypes();
                for (String type : types) {
                    if (!needAudio && type.startsWith(""audio/"")) continue;
                    if (!needVideo && type.startsWith(""video/"")) continue;
                    if (!mimes.contains(type)) {
                        mimes.add(type);
                    }
                }
            }
            // TODO(b/154423708): add checks for video o/p port and display length >= 2.5""
            /* sec 5.2: device implementations include an embedded screen display with the
            diagonal length of at least 2.5inches or include a video output port or declare the
            support of a camera */
            if (isEncoder && hasCamera() && needVideo &&
                    !mimes.contains(MediaFormat.MIMETYPE_VIDEO_AVC) &&
                    !mimes.contains(MediaFormat.MIMETYPE_VIDEO_VP8)) {
                // Add required cdd mimes here so that respective codec tests fail.
                mimes.add(MediaFormat.MIMETYPE_VIDEO_AVC);
                mimes.add(MediaFormat.MIMETYPE_VIDEO_VP8);
                Log.e(LOG_TAG,""device must support at least one of VP8 or AVC video encoders"");
            }
            for (String mime : cddRequiredMimeList) {
                if (!mimes.contains(mime)) {
                    // Add required cdd mimes here so that respective codec tests fail.
                    mimes.add(mime);
                    Log.e(LOG_TAG, ""no codec found for mime "" + mime + "" as required by cdd"");
                }
            }
        } else {
            for (Map.Entry<String, String> entry : codecSelKeyMimeMap.entrySet()) {
                String key = entry.getKey();
                String value = entry.getValue();
                if (mimeSelKeys.contains(key) && !mimes.contains(value)) mimes.add(value);
            }
        }
        return mimes;
    }

    static List<Object[]> prepareParamList(List<Object[]> exhaustiveArgsList, boolean isEncoder,
            boolean needAudio, boolean needVideo, boolean mustTestAllCodecs) {
        ArrayList<String> mimes = compileCompleteTestMimeList(isEncoder, needAudio, needVideo);
        ArrayList<String> cddRequiredMimeList =
                compileRequiredMimeList(isEncoder, needAudio, needVideo);
        final List<Object[]> argsList = new ArrayList<>();
        int argLength = exhaustiveArgsList.get(0).length;
        for (String mime : mimes) {
            ArrayList<String> totalListOfCodecs = selectCodecs(mime, null, null, isEncoder);
            ArrayList<String> listOfCodecs = new ArrayList<>();
            if (codecPrefix != null) {
                for (String codec : totalListOfCodecs) {
                    if (codec.startsWith(codecPrefix)) {
                        listOfCodecs.add(codec);
                    }
                }
            } else {
                listOfCodecs = totalListOfCodecs;
            }
            if (mustTestAllCodecs && listOfCodecs.size() == 0 && codecPrefix == null) {
                listOfCodecs.add(INVALID_CODEC + mime);
            }
            boolean miss = true;
            for (Object[] arg : exhaustiveArgsList) {
                if (mime.equals(arg[0])) {
                    for (String codec : listOfCodecs) {
                        Object[] arg_ = new Object[argLength + 1];
                        arg_[0] = codec;
                        System.arraycopy(arg, 0, arg_, 1, argLength);
                        argsList.add(arg_);
                    }
                    miss = false;
                }
            }
            if (miss && mustTestAllCodecs) {
                if (!cddRequiredMimeList.contains(mime)) {
                    Log.w(LOG_TAG, ""no test vectors available for optional mime type "" + mime);
                    continue;
                }
                for (String codec : listOfCodecs) {
                    Object[] arg_ = new Object[argLength + 1];
                    arg_[0] = codec;
                    arg_[1] = mime;
                    System.arraycopy(exhaustiveArgsList.get(0), 1, arg_, 2, argLength - 1);
                    argsList.add(arg_);
                }
            }
        }
        return argsList;
    }

    abstract void enqueueInput(int bufferIndex) throws IOException;

    abstract void dequeueOutput(int bufferIndex, MediaCodec.BufferInfo info);

    void configureCodec(MediaFormat format, boolean isAsync, boolean signalEOSWithLastFrame,
            boolean isEncoder) {
        resetContext(isAsync, signalEOSWithLastFrame);
        mAsyncHandle.setCallBack(mCodec, isAsync);
        // signalEOS flag has nothing to do with configure. We are using this flag to try all
        // available configure apis
        if (signalEOSWithLastFrame) {
            mCodec.configure(format, mSurface, null,
                    isEncoder ? MediaCodec.CONFIGURE_FLAG_ENCODE : 0);
        } else {
            mCodec.configure(format, mSurface, isEncoder ? MediaCodec.CONFIGURE_FLAG_ENCODE : 0,
                    null);
        }
        if (ENABLE_LOGS) {
            Log.v(LOG_TAG, ""codec configured"");
        }
    }

    void flushCodec() {
        mCodec.flush();
        // TODO(b/147576107): is it ok to clearQueues right away or wait for some signal
        mAsyncHandle.clearQueues();
        mSawInputEOS = false;
        mSawOutputEOS = false;
        mInputCount = 0;
        mOutputCount = 0;
        mPrevOutputPts = Long.MIN_VALUE;
        if (ENABLE_LOGS) {
            Log.v(LOG_TAG, ""codec flushed"");
        }
    }

    void reConfigureCodec(MediaFormat format, boolean isAsync, boolean signalEOSWithLastFrame,
            boolean isEncoder) {
        /* TODO(b/147348711) */
        if (false) mCodec.stop();
        else mCodec.reset();
        configureCodec(format, isAsync, signalEOSWithLastFrame, isEncoder);
    }

    void resetContext(boolean isAsync, boolean signalEOSWithLastFrame) {
        mAsyncHandle.resetContext();
        mIsCodecInAsyncMode = isAsync;
        mSawInputEOS = false;
        mSawOutputEOS = false;
        mSignalEOSWithLastFrame = signalEOSWithLastFrame;
        mInputCount = 0;
        mOutputCount = 0;
        mPrevOutputPts = Long.MIN_VALUE;
        mSignalledOutFormatChanged = false;
    }

    void enqueueEOS(int bufferIndex) {
        if (!mSawInputEOS) {
            mCodec.queueInputBuffer(bufferIndex, 0, 0, 0, MediaCodec.BUFFER_FLAG_END_OF_STREAM);
            mSawInputEOS = true;
            if (ENABLE_LOGS) {
                Log.v(LOG_TAG, ""Queued End of Stream"");
            }
        }
    }

    void doWork(int frameLimit) throws InterruptedException, IOException {
        int frameCount = 0;
        if (mIsCodecInAsyncMode) {
            // dequeue output after inputEOS is expected to be done in waitForAllOutputs()
            while (!mAsyncHandle.hasSeenError() && !mSawInputEOS && frameCount < frameLimit) {
                Pair<Integer, MediaCodec.BufferInfo> element = mAsyncHandle.getWork();
                if (element != null) {
                    int bufferID = element.first;
                    MediaCodec.BufferInfo info = element.second;
                    if (info != null) {
                        // <id, info> corresponds to output callback. Handle it accordingly
                        dequeueOutput(bufferID, info);
                    } else {
                        // <id, null> corresponds to input callback. Handle it accordingly
                        enqueueInput(bufferID);
                        frameCount++;
                    }
                }
            }
        } else {
            MediaCodec.BufferInfo outInfo = new MediaCodec.BufferInfo();
            // dequeue output after inputEOS is expected to be done in waitForAllOutputs()
            while (!mSawInputEOS && frameCount < frameLimit) {
                int outputBufferId = mCodec.dequeueOutputBuffer(outInfo, Q_DEQ_TIMEOUT_US);
                if (outputBufferId >= 0) {
                    dequeueOutput(outputBufferId, outInfo);
                } else if (outputBufferId == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                    mOutFormat = mCodec.getOutputFormat();
                    mSignalledOutFormatChanged = true;
                }
                int inputBufferId = mCodec.dequeueInputBuffer(Q_DEQ_TIMEOUT_US);
                if (inputBufferId != -1) {
                    enqueueInput(inputBufferId);
                    frameCount++;
                }
            }
        }
    }

    void queueEOS() throws InterruptedException {
        if (mIsCodecInAsyncMode) {
            while (!mAsyncHandle.hasSeenError() && !mSawInputEOS) {
                Pair<Integer, MediaCodec.BufferInfo> element = mAsyncHandle.getWork();
                if (element != null) {
                    int bufferID = element.first;
                    MediaCodec.BufferInfo info = element.second;
                    if (info != null) {
                        dequeueOutput(bufferID, info);
                    } else {
                        enqueueEOS(element.first);
                    }
                }
            }
        } else {
            MediaCodec.BufferInfo outInfo = new MediaCodec.BufferInfo();
            while (!mSawInputEOS) {
                int outputBufferId = mCodec.dequeueOutputBuffer(outInfo, Q_DEQ_TIMEOUT_US);
                if (outputBufferId >= 0) {
                    dequeueOutput(outputBufferId, outInfo);
                } else if (outputBufferId == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                    mOutFormat = mCodec.getOutputFormat();
                    mSignalledOutFormatChanged = true;
                }
                int inputBufferId = mCodec.dequeueInputBuffer(Q_DEQ_TIMEOUT_US);
                if (inputBufferId != -1) {
                    enqueueEOS(inputBufferId);
                }
            }
        }
    }

    void waitForAllOutputs() throws InterruptedException {
        if (mIsCodecInAsyncMode) {
            while (!mAsyncHandle.hasSeenError() && !mSawOutputEOS) {
                Pair<Integer, MediaCodec.BufferInfo> element = mAsyncHandle.getOutput();
                if (element != null) {
                    dequeueOutput(element.first, element.second);
                }
            }
        } else {
            MediaCodec.BufferInfo outInfo = new MediaCodec.BufferInfo();
            while (!mSawOutputEOS) {
                int outputBufferId = mCodec.dequeueOutputBuffer(outInfo, Q_DEQ_TIMEOUT_US);
                if (outputBufferId >= 0) {
                    dequeueOutput(outputBufferId, outInfo);
                } else if (outputBufferId == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                    mOutFormat = mCodec.getOutputFormat();
                    mSignalledOutFormatChanged = true;
                }
            }
        }
    }

    static ArrayList<String> selectCodecs(String mime, ArrayList<MediaFormat> formats,
            String[] features, boolean isEncoder) {
        MediaCodecList codecList = new MediaCodecList(MediaCodecList.REGULAR_CODECS);
        MediaCodecInfo[] codecInfos = codecList.getCodecInfos();
        ArrayList<String> listOfCodecs = new ArrayList<>();
        for (MediaCodecInfo codecInfo : codecInfos) {
            if (codecInfo.isEncoder() != isEncoder) continue;
            if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.Q && codecInfo.isAlias()) continue;
            String[] types = codecInfo.getSupportedTypes();
            for (String type : types) {
                if (type.equalsIgnoreCase(mime)) {
                    boolean isOk = true;
                    MediaCodecInfo.CodecCapabilities codecCapabilities =
                            codecInfo.getCapabilitiesForType(type);
                    if (formats != null) {
                        for (MediaFormat format : formats) {
                            if (!codecCapabilities.isFormatSupported(format)) {
                                isOk = false;
                                break;
                            }
                        }
                    }
                    if (features != null) {
                        for (String feature : features) {
                            if (!codecCapabilities.isFeatureSupported(feature)) {
                                isOk = false;
                                break;
                            }
                        }
                    }
                    if (isOk) listOfCodecs.add(codecInfo.getName());
                }
            }
        }
        return listOfCodecs;
    }

    static int getWidth(MediaFormat format) {
        int width = format.getInteger(MediaFormat.KEY_WIDTH, -1);
        if (format.containsKey(""crop-left"") && format.containsKey(""crop-right"")) {
            width = format.getInteger(""crop-right"") + 1 - format.getInteger(""crop-left"");
        }
        return width;
    }

    static int getHeight(MediaFormat format) {
        int height = format.getInteger(MediaFormat.KEY_HEIGHT, -1);
        if (format.containsKey(""crop-top"") && format.containsKey(""crop-bottom"")) {
            height = format.getInteger(""crop-bottom"") + 1 - format.getInteger(""crop-top"");
        }
        return height;
    }

    boolean isFormatSimilar(MediaFormat inpFormat, MediaFormat outFormat) {
        if (inpFormat == null || outFormat == null) return false;
        String inpMime = inpFormat.getString(MediaFormat.KEY_MIME);
        String outMime = outFormat.getString(MediaFormat.KEY_MIME);
        // not comparing input and output mimes because for a codec, mime is raw on one side and
        // encoded type on the other
        if (outMime.startsWith(""audio/"")) {
            return inpFormat.getInteger(MediaFormat.KEY_CHANNEL_COUNT, -1) ==
                    outFormat.getInteger(MediaFormat.KEY_CHANNEL_COUNT, -2) &&
                    inpFormat.getInteger(MediaFormat.KEY_SAMPLE_RATE, -1) ==
                            outFormat.getInteger(MediaFormat.KEY_SAMPLE_RATE, -2) &&
                    inpMime.startsWith(""audio/"");
        } else if (outMime.startsWith(""video/"")) {
            return getWidth(inpFormat) == getWidth(outFormat) &&
                    getHeight(inpFormat) == getHeight(outFormat) && inpMime.startsWith(""video/"");
        }
        return true;
    }

    PersistableBundle validateMetrics(String codec) {
        PersistableBundle metrics = mCodec.getMetrics();
        assertTrue(""metrics is null"", metrics != null);
        assertTrue(metrics.getString(MediaCodec.MetricsConstants.CODEC).equals(codec));
        if (mIsAudio) {
            assertTrue(metrics.getString(MediaCodec.MetricsConstants.MODE)
                    .equals(MediaCodec.MetricsConstants.MODE_AUDIO));
        } else {
            assertTrue(metrics.getString(MediaCodec.MetricsConstants.MODE)
                    .equals(MediaCodec.MetricsConstants.MODE_VIDEO));
        }
        return metrics;
    }

    PersistableBundle validateMetrics(String codec, MediaFormat format) {
        PersistableBundle metrics = validateMetrics(codec);
        if (!mIsAudio) {
            assertTrue(metrics.getInt(MediaCodec.MetricsConstants.WIDTH) == getWidth(format));
            assertTrue(metrics.getInt(MediaCodec.MetricsConstants.HEIGHT) == getHeight(format));
        }
        assertTrue(metrics.getInt(MediaCodec.MetricsConstants.SECURE) == 0);
        return metrics;
    }

    void validateColorAspects(MediaFormat fmt, int range, int standard, int transfer) {
        int colorRange = fmt.getInteger(MediaFormat.KEY_COLOR_RANGE, UNSPECIFIED);
        int colorStandard = fmt.getInteger(MediaFormat.KEY_COLOR_STANDARD, UNSPECIFIED);
        int colorTransfer = fmt.getInteger(MediaFormat.KEY_COLOR_TRANSFER, UNSPECIFIED);
        if (range > UNSPECIFIED) {
            assertEquals(""color range mismatch "", range, colorRange);
        }
        if (standard > UNSPECIFIED) {
            assertEquals(""color standard mismatch "", standard, colorStandard);
        }
        if (transfer > UNSPECIFIED) {
            assertEquals(""color transfer mismatch "", transfer, colorTransfer);
        }
    }

    public void setUpSurface(CodecTestActivity activity) throws InterruptedException {
        activity.waitTillSurfaceIsCreated();
        mSurface = activity.getSurface();
        assertTrue(""Surface created is null."", mSurface != null);
        assertTrue(""Surface created is invalid."", mSurface.isValid());
    }

    public void tearDownSurface() {
        if (mSurface != null) {
            mSurface.release();
            mSurface = null;
        }
    }

    @Before
    public void isCodecNameValid() {
        if (mCodecName != null && mCodecName.startsWith(INVALID_CODEC)) {
            fail(""no valid component available for current test "");
        }
    }
}

class CodecDecoderTestBase extends CodecTestBase {
    private static final String LOG_TAG = CodecDecoderTestBase.class.getSimpleName();

    String mMime;
    String mTestFile;
    boolean mIsInterlaced;

    ArrayList<ByteBuffer> mCsdBuffers;
    private int mCurrCsdIdx;

    private ByteBuffer flatBuffer = ByteBuffer.allocate(4 * Integer.BYTES);

    MediaExtractor mExtractor;

    CodecDecoderTestBase(String codecName, String mime, String testFile) {
        mCodecName = codecName;
        mMime = mime;
        mTestFile = testFile;
        mAsyncHandle = new CodecAsyncHandler();
        mCsdBuffers = new ArrayList<>();
        mIsAudio = mMime.startsWith(""audio/"");
    }

    MediaFormat setUpSource(String srcFile) throws IOException {
        return setUpSource(mInpPrefix, srcFile);
    }

    MediaFormat setUpSource(String prefix, String srcFile) throws IOException {
        mExtractor = new MediaExtractor();
        mExtractor.setDataSource(prefix + srcFile);
        for (int trackID = 0; trackID < mExtractor.getTrackCount(); trackID++) {
            MediaFormat format = mExtractor.getTrackFormat(trackID);
            if (mMime.equalsIgnoreCase(format.getString(MediaFormat.KEY_MIME))) {
                mExtractor.selectTrack(trackID);
                if (!mIsAudio) {
                    if (mSurface == null) {
                        // COLOR_FormatYUV420Flexible must be supported by all components
                        format.setInteger(MediaFormat.KEY_COLOR_FORMAT, COLOR_FormatYUV420Flexible);
                    } else {
                        format.setInteger(MediaFormat.KEY_COLOR_FORMAT, COLOR_FormatSurface);
                    }
                }
                // TODO: determine this from the extractor format when it becomes exposed.
                mIsInterlaced = srcFile.contains(""_interlaced_"");
                return format;
            }
        }
        fail(""No track with mime: "" + mMime + "" found in file: "" + srcFile);
        return null;
    }

    boolean hasCSD(MediaFormat format) {
        return format.containsKey(""csd-0"");
    }

    void flattenBufferInfo(MediaCodec.BufferInfo info, boolean isAudio) {
        if (isAudio) {
            flatBuffer.putInt(info.size);
        }
        flatBuffer.putInt(info.flags & ~MediaCodec.BUFFER_FLAG_END_OF_STREAM)
                .putLong(info.presentationTimeUs);
        flatBuffer.flip();
    }

    void enqueueCodecConfig(int bufferIndex) {
        ByteBuffer inputBuffer = mCodec.getInputBuffer(bufferIndex);
        ByteBuffer csdBuffer = mCsdBuffers.get(mCurrCsdIdx);
        inputBuffer.put((ByteBuffer) csdBuffer.rewind());
        mCodec.queueInputBuffer(bufferIndex, 0, csdBuffer.limit(), 0,
                MediaCodec.BUFFER_FLAG_CODEC_CONFIG);
        if (ENABLE_LOGS) {
            Log.v(LOG_TAG, ""queued csd: id: "" + bufferIndex + "" size: "" + csdBuffer.limit());
        }
    }

    void enqueueInput(int bufferIndex) {
        if (mExtractor.getSampleSize() < 0) {
            enqueueEOS(bufferIndex);
        } else {
            ByteBuffer inputBuffer = mCodec.getInputBuffer(bufferIndex);
            mExtractor.readSampleData(inputBuffer, 0);
            int size = (int) mExtractor.getSampleSize();
            long pts = mExtractor.getSampleTime();
            int extractorFlags = mExtractor.getSampleFlags();
            int codecFlags = 0;
            if ((extractorFlags & MediaExtractor.SAMPLE_FLAG_SYNC) != 0) {
                codecFlags |= MediaCodec.BUFFER_FLAG_KEY_FRAME;
            }
            if ((extractorFlags & MediaExtractor.SAMPLE_FLAG_PARTIAL_FRAME) != 0) {
                codecFlags |= MediaCodec.BUFFER_FLAG_PARTIAL_FRAME;
            }
            if (!mExtractor.advance() && mSignalEOSWithLastFrame) {
                codecFlags |= MediaCodec.BUFFER_FLAG_END_OF_STREAM;
                mSawInputEOS = true;
            }
            if (ENABLE_LOGS) {
                Log.v(LOG_TAG, ""input: id: "" + bufferIndex + "" size: "" + size + "" pts: "" + pts +
                        "" flags: "" + codecFlags);
            }
            mCodec.queueInputBuffer(bufferIndex, 0, size, pts, codecFlags);
            if (size > 0 && (codecFlags & (MediaCodec.BUFFER_FLAG_CODEC_CONFIG |
                    MediaCodec.BUFFER_FLAG_PARTIAL_FRAME)) == 0) {
                mOutputBuff.saveInPTS(pts);
                mInputCount++;
            }
        }
    }

    void enqueueInput(int bufferIndex, ByteBuffer buffer, MediaCodec.BufferInfo info) {
        ByteBuffer inputBuffer = mCodec.getInputBuffer(bufferIndex);
        buffer.position(info.offset);
        for (int i = 0; i < info.size; i++) {
            inputBuffer.put(buffer.get());
        }
        if (ENABLE_LOGS) {
            Log.v(LOG_TAG, ""input: id: "" + bufferIndex + "" flags: "" + info.flags + "" size: "" +
                    info.size + "" timestamp: "" + info.presentationTimeUs);
        }
        mCodec.queueInputBuffer(bufferIndex, 0, info.size, info.presentationTimeUs,
                info.flags);
        if (info.size > 0 && ((info.flags & MediaCodec.BUFFER_FLAG_CODEC_CONFIG) == 0) &&
                ((info.flags & MediaCodec.BUFFER_FLAG_PARTIAL_FRAME) == 0)) {
            mOutputBuff.saveInPTS(info.presentationTimeUs);
            mInputCount++;
        }
        if ((info.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
            mSawInputEOS = true;
        }
    }

    void dequeueOutput(int bufferIndex, MediaCodec.BufferInfo info) {
        if (info.size > 0 && mSaveToMem) {
            ByteBuffer buf = mCodec.getOutputBuffer(bufferIndex);
            flattenBufferInfo(info, mIsAudio);
            mOutputBuff.checksum(flatBuffer, flatBuffer.limit());
            if (mIsAudio) {
                mOutputBuff.checksum(buf, info.size);
                mOutputBuff.saveToMemory(buf, info);
            } else {
                // tests both getOutputImage and getOutputBuffer. Can do time division
                // multiplexing but lets allow it for now
                MediaFormat format = mCodec.getOutputFormat();
                int width = format.getInteger(MediaFormat.KEY_WIDTH);
                int height = format.getInteger(MediaFormat.KEY_HEIGHT);
                int stride = format.getInteger(MediaFormat.KEY_STRIDE);
                mOutputBuff.checksum(buf, info.size, width, height, stride);

                Image img = mCodec.getOutputImage(bufferIndex);
                assertTrue(img != null);
                mOutputBuff.checksum(img);
            }
        }
        if ((info.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
            mSawOutputEOS = true;
        }
        if (ENABLE_LOGS) {
            Log.v(LOG_TAG, ""output: id: "" + bufferIndex + "" flags: "" + info.flags + "" size: "" +
                    info.size + "" timestamp: "" + info.presentationTimeUs);
        }
        if (info.size > 0 && (info.flags & MediaCodec.BUFFER_FLAG_CODEC_CONFIG) == 0) {
            mOutputBuff.saveOutPTS(info.presentationTimeUs);
            mOutputCount++;
        }
        mCodec.releaseOutputBuffer(bufferIndex, false);
    }

    void doWork(ByteBuffer buffer, ArrayList<MediaCodec.BufferInfo> list)
            throws InterruptedException {
        int frameCount = 0;
        if (mIsCodecInAsyncMode) {
            // output processing after queuing EOS is done in waitForAllOutputs()
            while (!mAsyncHandle.hasSeenError() && !mSawInputEOS && frameCount < list.size()) {
                Pair<Integer, MediaCodec.BufferInfo> element = mAsyncHandle.getWork();
                if (element != null) {
                    int bufferID = element.first;
                    MediaCodec.BufferInfo info = element.second;
                    if (info != null) {
                        dequeueOutput(bufferID, info);
                    } else {
                        enqueueInput(bufferID, buffer, list.get(frameCount));
                        frameCount++;
                    }
                }
            }
        } else {
            MediaCodec.BufferInfo outInfo = new MediaCodec.BufferInfo();
            // output processing after queuing EOS is done in waitForAllOutputs()
            while (!mSawInputEOS && frameCount < list.size()) {
                int outputBufferId = mCodec.dequeueOutputBuffer(outInfo, Q_DEQ_TIMEOUT_US);
                if (outputBufferId >= 0) {
                    dequeueOutput(outputBufferId, outInfo);
                } else if (outputBufferId == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                    mOutFormat = mCodec.getOutputFormat();
                    mSignalledOutFormatChanged = true;
                }
                int inputBufferId = mCodec.dequeueInputBuffer(Q_DEQ_TIMEOUT_US);
                if (inputBufferId != -1) {
                    enqueueInput(inputBufferId, buffer, list.get(frameCount));
                    frameCount++;
                }
            }
        }
    }

    void queueCodecConfig() throws InterruptedException {
        if (mIsCodecInAsyncMode) {
            for (mCurrCsdIdx = 0; !mAsyncHandle.hasSeenError() && mCurrCsdIdx < mCsdBuffers.size();
                 mCurrCsdIdx++) {
                Pair<Integer, MediaCodec.BufferInfo> element = mAsyncHandle.getInput();
                if (element != null) {
                    enqueueCodecConfig(element.first);
                }
            }
        } else {
            for (mCurrCsdIdx = 0; mCurrCsdIdx < mCsdBuffers.size(); mCurrCsdIdx++) {
                enqueueCodecConfig(mCodec.dequeueInputBuffer(-1));
            }
        }
    }

    void decodeToMemory(String file, String decoder, long pts, int mode, int frameLimit)
            throws IOException, InterruptedException {
        mSaveToMem = true;
        mOutputBuff = new OutputManager();
        mCodec = MediaCodec.createByCodecName(decoder);
        MediaFormat format = setUpSource(file);
        configureCodec(format, false, true, false);
        mCodec.start();
        mExtractor.seekTo(pts, mode);
        doWork(frameLimit);
        queueEOS();
        waitForAllOutputs();
        mCodec.stop();
        mCodec.release();
        mExtractor.release();
        mSaveToMem = false;
    }

    @Override
    PersistableBundle validateMetrics(String decoder, MediaFormat format) {
        PersistableBundle metrics = super.validateMetrics(decoder, format);
        assertTrue(metrics.getString(MediaCodec.MetricsConstants.MIME_TYPE).equals(mMime));
        assertTrue(metrics.getInt(MediaCodec.MetricsConstants.ENCODER) == 0);
        return metrics;
    }

    void validateColorAspects(String decoder, String parent, String name, int range, int standard,
            int transfer, boolean ignoreColorBox)
            throws IOException, InterruptedException {
        mOutputBuff = new OutputManager();
        MediaFormat format = setUpSource(parent, name);
        if (ignoreColorBox) {
            format.removeKey(MediaFormat.KEY_COLOR_RANGE);
            format.removeKey(MediaFormat.KEY_COLOR_STANDARD);
            format.removeKey(MediaFormat.KEY_COLOR_TRANSFER);
        }
        if (decoder == null) {
            MediaCodecList codecList = new MediaCodecList(MediaCodecList.REGULAR_CODECS);
            decoder = codecList.findDecoderForFormat(format);
        }
        mCodec = MediaCodec.createByCodecName(decoder);
        configureCodec(format, true, true, false);
        mCodec.start();
        doWork(1);
        queueEOS();
        waitForAllOutputs();
        validateColorAspects(mCodec.getOutputFormat(), range, standard, transfer);
        mCodec.stop();
        mCodec.release();
        mExtractor.release();
    }
}

class CodecEncoderTestBase extends CodecTestBase {
    private static final String LOG_TAG = CodecEncoderTestBase.class.getSimpleName();

    // files are in WorkDir.getMediaDirString();
    private static final String mInputAudioFile = ""bbb_2ch_44kHz_s16le.raw"";
    private static final String mInputVideoFile = ""bbb_cif_yuv420p_30fps.yuv"";
    private final int INP_FRM_WIDTH = 352;
    private final int INP_FRM_HEIGHT = 288;

    final String mMime;
    final int[] mBitrates;
    final int[] mEncParamList1;
    final int[] mEncParamList2;

    final String mInputFile;
    byte[] mInputData;
    int mNumBytesSubmitted;
    long mInputOffsetPts;

    ArrayList<MediaFormat> mFormats;
    ArrayList<MediaCodec.BufferInfo> mInfoList;

    int mWidth, mHeight;
    int mFrameRate;
    int mMaxBFrames;
    int mChannels;
    int mSampleRate;

    CodecEncoderTestBase(String encoder, String mime, int[] bitrates, int[] encoderInfo1,
            int[] encoderInfo2) {
        mMime = mime;
        mCodecName = encoder;
        mBitrates = bitrates;
        mEncParamList1 = encoderInfo1;
        mEncParamList2 = encoderInfo2;
        mFormats = new ArrayList<>();
        mInfoList = new ArrayList<>();
        mWidth = INP_FRM_WIDTH;
        mHeight = INP_FRM_HEIGHT;
        if (mime.equals(MediaFormat.MIMETYPE_VIDEO_MPEG4)) mFrameRate = 12;
        else if (mime.equals(MediaFormat.MIMETYPE_VIDEO_H263)) mFrameRate = 12;
        else mFrameRate = 30;
        mMaxBFrames = 0;
        mChannels = 1;
        mSampleRate = 8000;
        mAsyncHandle = new CodecAsyncHandler();
        mIsAudio = mMime.startsWith(""audio/"");
        mInputFile = mIsAudio ? mInputAudioFile : mInputVideoFile;
    }

    /**
     * Selects encoder input color format in byte buffer mode. As of now ndk tests support only
     * 420p, 420sp. COLOR_FormatYUV420Flexible although can represent any form of yuv, it doesn't
     * work in ndk due to lack of AMediaCodec_GetInputImage()
     */
    static int findByteBufferColorFormat(String encoder, String mime) throws IOException {
        MediaCodec codec = MediaCodec.createByCodecName(encoder);
        MediaCodecInfo.CodecCapabilities cap = codec.getCodecInfo().getCapabilitiesForType(mime);
        int colorFormat = -1;
        for (int c : cap.colorFormats) {
            if (c == MediaCodecInfo.CodecCapabilities.COLOR_FormatYUV420SemiPlanar ||
                    c == MediaCodecInfo.CodecCapabilities.COLOR_FormatYUV420Planar) {
                Log.v(LOG_TAG, ""selecting color format: "" + c);
                colorFormat = c;
                break;
            }
        }
        codec.release();
        return colorFormat;
    }

    @Override
    void resetContext(boolean isAsync, boolean signalEOSWithLastFrame) {
        super.resetContext(isAsync, signalEOSWithLastFrame);
        mNumBytesSubmitted = 0;
        mInputOffsetPts = 0;
    }

    @Override
    void flushCodec() {
        super.flushCodec();
        if (mIsAudio) {
            mInputOffsetPts =
                    (mNumBytesSubmitted + 1024) * 1000000L / (2 * mChannels * mSampleRate);
        } else {
            mInputOffsetPts = (mInputCount + 5) * 1000000L / mFrameRate;
        }
        mPrevOutputPts = mInputOffsetPts - 1;
        mNumBytesSubmitted = 0;
    }

    void setUpSource(String srcFile) throws IOException {
        String inpPath = mInpPrefix + srcFile;
        try (FileInputStream fInp = new FileInputStream(inpPath)) {
            int size = (int) new File(inpPath).length();
            mInputData = new byte[size];
            fInp.read(mInputData, 0, size);
        }
    }

    void fillImage(Image image) {
        Assert.assertTrue(image.getFormat() == ImageFormat.YUV_420_888);
        int imageWidth = image.getWidth();
        int imageHeight = image.getHeight();
        Image.Plane[] planes = image.getPlanes();
        int offset = mNumBytesSubmitted;
        for (int i = 0; i < planes.length; ++i) {
            ByteBuffer buf = planes[i].getBuffer();
            int width = imageWidth;
            int height = imageHeight;
            int tileWidth = INP_FRM_WIDTH;
            int tileHeight = INP_FRM_HEIGHT;
            int rowStride = planes[i].getRowStride();
            int pixelStride = planes[i].getPixelStride();
            if (i != 0) {
                width = imageWidth / 2;
                height = imageHeight / 2;
                tileWidth = INP_FRM_WIDTH / 2;
                tileHeight = INP_FRM_HEIGHT / 2;
            }
            if (pixelStride == 1) {
                if (width == rowStride && width == tileWidth && height == tileHeight) {
                    buf.put(mInputData, offset, width * height);
                } else {
                    for (int z = 0; z < height; z += tileHeight) {
                        int rowsToCopy = Math.min(height - z, tileHeight);
                        for (int y = 0; y < rowsToCopy; y++) {
                            for (int x = 0; x < width; x += tileWidth) {
                                int colsToCopy = Math.min(width - x, tileWidth);
                                buf.position((z + y) * rowStride + x);
                                buf.put(mInputData, offset + y * tileWidth, colsToCopy);
                            }
                        }
                    }
                }
            } else {
                // do it pixel-by-pixel
                for (int z = 0; z < height; z += tileHeight) {
                    int rowsToCopy = Math.min(height - z, tileHeight);
                    for (int y = 0; y < rowsToCopy; y++) {
                        int lineOffset = (z + y) * rowStride;
                        for (int x = 0; x < width; x += tileWidth) {
                            int colsToCopy = Math.min(width - x, tileWidth);
                            for (int w = 0; w < colsToCopy; w++) {
                                buf.position(lineOffset + (x + w) * pixelStride);
                                buf.put(mInputData[offset + y * tileWidth + w]);
                            }
                        }
                    }
                }
            }
            offset += tileWidth * tileHeight;
        }
    }

    void fillByteBuffer(ByteBuffer inputBuffer) {
        int offset = 0, frmOffset = mNumBytesSubmitted;
        for (int plane = 0; plane < 3; plane++) {
            int width = mWidth;
            int height = mHeight;
            int tileWidth = INP_FRM_WIDTH;
            int tileHeight = INP_FRM_HEIGHT;
            if (plane != 0) {
                width = mWidth / 2;
                height = mHeight / 2;
                tileWidth = INP_FRM_WIDTH / 2;
                tileHeight = INP_FRM_HEIGHT / 2;
            }
            for (int k = 0; k < height; k += tileHeight) {
                int rowsToCopy = Math.min(height - k, tileHeight);
                for (int j = 0; j < rowsToCopy; j++) {
                    for (int i = 0; i < width; i += tileWidth) {
                        int colsToCopy = Math.min(width - i, tileWidth);
                        inputBuffer.position(offset + (k + j) * width + i);
                        inputBuffer.put(mInputData, frmOffset + j * tileWidth, colsToCopy);
                    }
                }
            }
            offset += width * height;
            frmOffset += tileWidth * tileHeight;
        }
    }

    void enqueueInput(int bufferIndex) {
        ByteBuffer inputBuffer = mCodec.getInputBuffer(bufferIndex);
        if (mNumBytesSubmitted >= mInputData.length) {
            enqueueEOS(bufferIndex);
        } else {
            int size;
            int flags = 0;
            long pts = mInputOffsetPts;
            if (mIsAudio) {
                pts += mNumBytesSubmitted * 1000000L / (2 * mChannels * mSampleRate);
                size = Math.min(inputBuffer.capacity(), mInputData.length - mNumBytesSubmitted);
                inputBuffer.put(mInputData, mNumBytesSubmitted, size);
                if (mNumBytesSubmitted + size >= mInputData.length && mSignalEOSWithLastFrame) {
                    flags |= MediaCodec.BUFFER_FLAG_END_OF_STREAM;
                    mSawInputEOS = true;
                }
                mNumBytesSubmitted += size;
            } else {
                pts += mInputCount * 1000000L / mFrameRate;
                size = mWidth * mHeight * 3 / 2;
                int frmSize = INP_FRM_WIDTH * INP_FRM_HEIGHT * 3 / 2;
                if (mNumBytesSubmitted + frmSize > mInputData.length) {
                    fail(""received partial frame to encode"");
                } else {
                    Image img = mCodec.getInputImage(bufferIndex);
                    if (img != null) {
                        fillImage(img);
                    } else {
                        if (mWidth == INP_FRM_WIDTH && mHeight == INP_FRM_HEIGHT) {
                            inputBuffer.put(mInputData, mNumBytesSubmitted, size);
                        } else {
                            fillByteBuffer(inputBuffer);
                        }
                    }
                }
                if (mNumBytesSubmitted + frmSize >= mInputData.length && mSignalEOSWithLastFrame) {
                    flags |= MediaCodec.BUFFER_FLAG_END_OF_STREAM;
                    mSawInputEOS = true;
                }
                mNumBytesSubmitted += frmSize;
            }
            if (ENABLE_LOGS) {
                Log.v(LOG_TAG, ""input: id: "" + bufferIndex + "" size: "" + size + "" pts: "" + pts +
                        "" flags: "" + flags);
            }
            mCodec.queueInputBuffer(bufferIndex, 0, size, pts, flags);
            mOutputBuff.saveInPTS(pts);
            mInputCount++;
        }
    }

    void dequeueOutput(int bufferIndex, MediaCodec.BufferInfo info) {
        if (ENABLE_LOGS) {
            Log.v(LOG_TAG, ""output: id: "" + bufferIndex + "" flags: "" + info.flags + "" size: "" +
                    info.size + "" timestamp: "" + info.presentationTimeUs);
        }
        if ((info.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
            mSawOutputEOS = true;
        }
        if (info.size > 0) {
            if (mSaveToMem) {
                MediaCodec.BufferInfo copy = new MediaCodec.BufferInfo();
                copy.set(mOutputBuff.getOutStreamSize(), info.size, info.presentationTimeUs,
                        info.flags);
                mInfoList.add(copy);

                ByteBuffer buf = mCodec.getOutputBuffer(bufferIndex);
                mOutputBuff.saveToMemory(buf, info);
            }
            if ((info.flags & MediaCodec.BUFFER_FLAG_CODEC_CONFIG) == 0) {
                mOutputBuff.saveOutPTS(info.presentationTimeUs);
                mOutputCount++;
            }
        }
        mCodec.releaseOutputBuffer(bufferIndex, false);
    }

    @Override
    PersistableBundle validateMetrics(String codec, MediaFormat format) {
        PersistableBundle metrics = super.validateMetrics(codec, format);
        assertTrue(metrics.getString(MediaCodec.MetricsConstants.MIME_TYPE).equals(mMime));
        assertTrue(metrics.getInt(MediaCodec.MetricsConstants.ENCODER) == 1);
        return metrics;
    }

    void setUpParams(int limit) {
        int count = 0;
        for (int bitrate : mBitrates) {
            if (mIsAudio) {
                for (int rate : mEncParamList1) {
                    for (int channels : mEncParamList2) {
                        MediaFormat format = new MediaFormat();
                        format.setString(MediaFormat.KEY_MIME, mMime);
                        if (mMime.equals(MediaFormat.MIMETYPE_AUDIO_FLAC)) {
                            format.setInteger(MediaFormat.KEY_FLAC_COMPRESSION_LEVEL, bitrate);
                        } else {
                            format.setInteger(MediaFormat.KEY_BIT_RATE, bitrate);
                        }
                        format.setInteger(MediaFormat.KEY_SAMPLE_RATE, rate);
                        format.setInteger(MediaFormat.KEY_CHANNEL_COUNT, channels);
                        mFormats.add(format);
                        count++;
                        if (count >= limit) return;
                    }
                }
            } else {
                assertTrue(""Wrong number of height, width parameters"",
                        mEncParamList1.length == mEncParamList2.length);
                for (int i = 0; i < mEncParamList1.length; i++) {
                    MediaFormat format = new MediaFormat();
                    format.setString(MediaFormat.KEY_MIME, mMime);
                    format.setInteger(MediaFormat.KEY_BIT_RATE, bitrate);
                    format.setInteger(MediaFormat.KEY_WIDTH, mEncParamList1[i]);
                    format.setInteger(MediaFormat.KEY_HEIGHT, mEncParamList2[i]);
                    format.setInteger(MediaFormat.KEY_FRAME_RATE, mFrameRate);
                    format.setInteger(MediaFormat.KEY_MAX_B_FRAMES, mMaxBFrames);
                    format.setFloat(MediaFormat.KEY_I_FRAME_INTERVAL, 1.0f);
                    format.setInteger(MediaFormat.KEY_COLOR_FORMAT,
                            MediaCodecInfo.CodecCapabilities.COLOR_FormatYUV420Flexible);
                    mFormats.add(format);
                    count++;
                    if (count >= limit) return;
                }
            }
        }
    }

    void encodeToMemory(String file, String encoder, int frameLimit, MediaFormat format,
            boolean saveToMem) throws IOException, InterruptedException {
        mSaveToMem = saveToMem;
        mOutputBuff = new OutputManager();
        mInfoList.clear();
        mCodec = MediaCodec.createByCodecName(encoder);
        setUpSource(file);
        configureCodec(format, false, true, true);
        if (mIsAudio) {
            mSampleRate = format.getInteger(MediaFormat.KEY_SAMPLE_RATE);
            mChannels = format.getInteger(MediaFormat.KEY_CHANNEL_COUNT);
        } else {
            mWidth = format.getInteger(MediaFormat.KEY_WIDTH);
            mHeight = format.getInteger(MediaFormat.KEY_HEIGHT);
        }
        mCodec.start();
        doWork(frameLimit);
        queueEOS();
        waitForAllOutputs();
        mCodec.stop();
        mCodec.release();
        mSaveToMem = false;
    }

    ByteBuffer decodeElementaryStream(String decoder, MediaFormat format,
            ByteBuffer elementaryStream, ArrayList<MediaCodec.BufferInfo> infos)
            throws IOException, InterruptedException {
        String mime = format.getString(MediaFormat.KEY_MIME);
        CodecDecoderTestBase cdtb = new CodecDecoderTestBase(decoder, mime, null);
        cdtb.mOutputBuff = new OutputManager();
        cdtb.mSaveToMem = true;
        cdtb.mCodec = MediaCodec.createByCodecName(decoder);
        cdtb.mCodec.configure(format, null, null, 0);
        cdtb.mCodec.start();
        cdtb.doWork(elementaryStream, infos);
        cdtb.queueEOS();
        cdtb.waitForAllOutputs();
        cdtb.mCodec.stop();
        cdtb.mCodec.release();
        return cdtb.mOutputBuff.getBuffer();
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaMetadataRetrieverTest"	"testID3v2Metadata"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaMetadataRetrieverTest.java"	""	"public void testID3v2Metadata() {
        setDataSourceFd(
                ""video_480x360_mp4_h264_500kbps_25fps_aac_stereo_128kbps_44100hz_id3v2.mp4"");

        assertEquals(""Title was other than expected"",
                ""Title"", mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_TITLE));

        assertEquals(""Artist was other than expected"",
                ""UTF16LE  "",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_ARTIST));

        assertEquals(""Album was other than expected"",
                ""Test album"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_ALBUM));

        assertNull(""Album artist was unexpectedly present"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_ALBUMARTIST));

        assertNull(""Author was unexpectedly present"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_AUTHOR));

        assertNull(""Composer was unexpectedly present"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_COMPOSER));

        assertEquals(""Track number was other than expected"",
                ""10"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_CD_TRACK_NUMBER));

        assertNull(""Disc number was unexpectedly present"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_DISC_NUMBER));

        assertNull(""Compilation was unexpectedly present"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_COMPILATION));

        assertEquals(""Year was other than expected"",
                ""2013"", mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_YEAR));

        assertEquals(""Date was other than expected"",
                ""19700101T000000.000Z"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_DATE));

        assertEquals(""Bitrate was other than expected"",
                ""499895"",  // = 624869 (file size in byte) * 8e6 / 10000000 (duration in us)
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_BITRATE));

        assertNull(""Capture frame rate was unexpectedly present"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_CAPTURE_FRAMERATE));

        assertEquals(""Duration was other than expected"",
                ""10000"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_DURATION));

        assertEquals(""Number of tracks was other than expected"",
                ""2"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_NUM_TRACKS));

        assertEquals(""Has audio was other than expected"",
                ""yes"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_HAS_AUDIO));

        assertEquals(""Has video was other than expected"",
                ""yes"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_HAS_VIDEO));

        assertEquals(""Video frame count was other than expected"",
                ""240"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_VIDEO_FRAME_COUNT));

        assertEquals(""Video height was other than expected"",
                ""360"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_VIDEO_HEIGHT));

        assertEquals(""Video width was other than expected"",
                ""480"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_VIDEO_WIDTH));

        assertEquals(""Video rotation was other than expected"",
                ""0"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_VIDEO_ROTATION));

        assertEquals(""Mime type was other than expected"",
                ""video/mp4"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_MIMETYPE));

        assertNull(""Location was unexpectedly present"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_LOCATION));

        assertNull(""EXIF length was unexpectedly present"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_EXIF_LENGTH));

        assertNull(""EXIF offset was unexpectedly present"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_EXIF_OFFSET));

        assertNull(""Writer was unexpectedly present"",
                mRetriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_WRITER));
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaMetadataRetrieverTest"	"testThumbnailH264"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaMetadataRetrieverTest.java"	""	"public void testThumbnailH264() {
        testThumbnail(
                ""bbb_s4_1280x720_mp4_h264_mp31_8mbps_30fps_aac_he_mono_40kbps_44100hz.mp4"",
                1280,
                720);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaMetadataRetrieverTest"	"testThumbnailH263"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaMetadataRetrieverTest.java"	""	"public void testThumbnailH263() {
        testThumbnail(""video_176x144_3gp_h263_56kbps_12fps_aac_mono_24kbps_11025hz.3gp"", 176, 144);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaMetadataRetrieverTest"	"testThumbnailMPEG4"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaMetadataRetrieverTest.java"	""	"public void testThumbnailMPEG4() {
        testThumbnail(
                ""video_1280x720_mp4_mpeg4_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                1280,
                720);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaMetadataRetrieverTest"	"testThumbnailHEVC"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaMetadataRetrieverTest.java"	""	"public void testThumbnailHEVC() {
        testThumbnail(
                ""bbb_s1_720x480_mp4_hevc_mp3_1600kbps_30fps_aac_he_6ch_240kbps_48000hz.mp4"",
                720,
                480);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaMetadataRetrieverTest"	"testGetScaledFrameAtTimeWithInvalidResolutions"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaMetadataRetrieverTest.java"	""	"public void testGetScaledFrameAtTimeWithInvalidResolutions() {
        String[] resources = {""binary_counter_320x240_30fps_600frames.mp4"",
                ""binary_counter_320x240_30fps_600frames_editlist.mp4"",
                ""bbb_s4_1280x720_mp4_h264_mp31_8mbps_30fps_aac_he_mono_40kbps_44100hz.mp4"",
                ""video_176x144_3gp_h263_56kbps_12fps_aac_mono_24kbps_11025hz.3gp"",
                ""video_1280x720_mp4_mpeg4_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                ""bbb_s1_640x360_webm_vp8_2mbps_30fps_vorbis_5ch_320kbps_48000hz.webm"",
                ""bbb_s1_640x360_webm_vp9_0p21_1600kbps_30fps_vorbis_stereo_128kbps_48000hz.webm"",
                ""bbb_s1_720x480_mp4_hevc_mp3_1600kbps_30fps_aac_he_6ch_240kbps_48000hz.mp4"",
                ""video_1280x720_vp9_hdr_static_3mbps.mkv"",
                ""video_1280x720_av1_hdr_static_3mbps.webm"",
                ""video_1280x720_hevc_hdr10_static_3mbps.mp4""};
        int[][] resolutions = {{0, 120}, {-1, 0}, {-1, 120}, {140, -1}, {-1, -1}};
        int[] options =
                {OPTION_CLOSEST, OPTION_CLOSEST_SYNC, OPTION_NEXT_SYNC, OPTION_PREVIOUS_SYNC};

        for (String res : resources) {
            setDataSourceFd(res);
            if (!MediaUtils.hasCodecForResourceAndDomain(res, ""video/"")
                    && mPackageManager.hasSystemFeature(PackageManager.FEATURE_WATCH)) {
                MediaUtils.skipTest(""no video codecs for resource on watch"");
                continue;
            }

            for (int i = 0; i < resolutions.length; i++) {
                int width = resolutions[i][0];
                int height = resolutions[i][1];
                for (int option : options) {
                    try {
                        Bitmap bitmap = mRetriever.getScaledFrameAtTime(
                                2066666 /*timeUs*/, option, width, height);
                        fail(""Failed to receive exception"");
                    } catch (IllegalArgumentException e) {
                        // Expect exception
                    }
                }
            }
        }
    }

    private void testGetScaledFrameAtTime(int scaleToWidth, int scaleToHeight,
            int expectedWidth, int expectedHeight, Bitmap.Config config) {
        if (!MediaUtils.check(mIsAtLeastR, ""test needs Android 11"")) return;
        MediaMetadataRetriever.BitmapParams params = null;
        Bitmap bitmap = null;
        if (config != null) {
            params = new MediaMetadataRetriever.BitmapParams();
            params.setPreferredConfig(config);
            bitmap = mRetriever.getScaledFrameAtTime(
                    2066666 /*timeUs */, OPTION_CLOSEST, scaleToWidth, scaleToHeight, params);
        } else {
            bitmap = mRetriever.getScaledFrameAtTime(
                    2066666 /*timeUs */, OPTION_CLOSEST, scaleToWidth, scaleToHeight);
        }
        if (bitmap == null) {
            fail(""Failed to get scaled bitmap"");
        }
        if (SAVE_BITMAP_OUTPUT) {
            CodecUtils.saveBitmapToFile(bitmap, String.format(""test_%dx%d.jpg"",
                    expectedWidth, expectedHeight));
        }
        if (config != null) {
            assertEquals(""Actual config is wrong"", config, params.getActualConfig());
        }
        assertEquals(""Bitmap width is wrong"", expectedWidth, bitmap.getWidth());
        assertEquals(""Bitmap height is wrong"", expectedHeight, bitmap.getHeight());
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediapc.cts.FrameDropTestBase"	"isSPerfClass"	"CtsMediaPerformanceClassTestCases"	"/home/gpoor/cts-12-source/cts/tests/mediapc/src/android/mediapc/cts/FrameDropTestBase.java"	""	"public void test/*
 *.
 */

package android.mediapc.cts;

import android.media.MediaFormat;
import android.util.Log;
import android.view.Surface;

import androidx.test.rule.ActivityTestRule;

import org.junit.After;
import org.junit.Before;
import org.junit.Rule;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import static android.mediapc.cts.CodecTestBase.selectCodecs;
import static android.mediapc.cts.CodecTestBase.selectHardwareCodecs;
import static org.junit.Assert.assertFalse;
import static org.junit.Assert.assertTrue;
import static org.junit.Assume.assumeFalse;
import static org.junit.Assume.assumeTrue;

public class FrameDropTestBase {
    private static final String LOG_TAG = FrameDropTestBase.class.getSimpleName();
    static final boolean[] boolStates = {false, true};
    static final String AVC = MediaFormat.MIMETYPE_VIDEO_AVC;
    static final String HEVC = MediaFormat.MIMETYPE_VIDEO_HEVC;
    static final String VP8 = MediaFormat.MIMETYPE_VIDEO_VP8;
    static final String VP9 = MediaFormat.MIMETYPE_VIDEO_VP9;
    static final String AV1 = MediaFormat.MIMETYPE_VIDEO_AV1;
    static final String AAC = MediaFormat.MIMETYPE_AUDIO_AAC;
    static final String AAC_LOAD_FILE_NAME = ""bbb_1c_128kbps_aac_audio.mp4"";
    static final String AVC_LOAD_FILE_NAME = ""bbb_1280x720_3mbps_30fps_avc.mp4"";
    static final long DECODE_31S = 31000; // In ms
    static final int MAX_ADAPTIVE_PLAYBACK_FRAME_DROP = 0;
    static final int FRAME_RATE = Utils.isSPerfClass() ? 60 : 30;
    static final int MAX_FRAME_DROP_FOR_30S;

    final String mMime;
    final String mDecoderName;
    final boolean mIsAsync;
    Surface mSurface;

    private LoadStatus mLoadStatus = null;
    private Thread mTranscodeLoadThread = null;
    private Thread mAudioPlaybackLoadThread = null;
    private Exception mTranscodeLoadException = null;
    private Exception mAudioPlaybackLoadException = null;

    static String AVC_DECODER_NAME;
    static String AVC_ENCODER_NAME;
    static String AAC_DECODER_NAME;
    static Map<String, String> m540pTestFiles = new HashMap<>();
    static Map<String, String> m1080pTestFiles = new HashMap<>();
    static {
        if (Utils.isSPerfClass()) {
            // Two frame drops per 10 seconds at 60 fps is 6 drops per 30 seconds
            MAX_FRAME_DROP_FOR_30S = 6;
            m540pTestFiles.put(AVC, ""bbb_960x540_3mbps_60fps_avc.mp4"");
            m540pTestFiles.put(HEVC, ""bbb_960x540_3mbps_60fps_hevc.mp4"");
            m540pTestFiles.put(VP8, ""bbb_960x540_3mbps_60fps_vp8.webm"");
            m540pTestFiles.put(VP9, ""bbb_960x540_3mbps_60fps_vp9.webm"");
            m540pTestFiles.put(AV1, ""bbb_960x540_3mbps_60fps_av1.mp4"");

            m1080pTestFiles.put(AVC, ""bbb_1920x1080_8mbps_60fps_avc.mp4"");
            m1080pTestFiles.put(HEVC, ""bbb_1920x1080_6mbps_60fps_hevc.mp4"");
            m1080pTestFiles.put(VP8, ""bbb_1920x1080_8mbps_60fps_vp8.webm"");
            m1080pTestFiles.put(VP9, ""bbb_1920x1080_6mbps_60fps_vp9.webm"");
            m1080pTestFiles.put(AV1, ""bbb_1920x1080_6mbps_60fps_av1.mp4"");
        } else {
            // One frame drops per 10 seconds at 30 fps is 3 drops per 30 seconds
            MAX_FRAME_DROP_FOR_30S = 3;
            m540pTestFiles.put(AVC, ""bbb_960x540_2mbps_30fps_avc.mp4"");
            m540pTestFiles.put(HEVC, ""bbb_960x540_2mbps_30fps_hevc.mp4"");
            m540pTestFiles.put(VP8, ""bbb_960x540_2mbps_30fps_vp8.webm"");
            m540pTestFiles.put(VP9, ""bbb_960x540_2mbps_30fps_vp9.webm"");
            m540pTestFiles.put(AV1, ""bbb_960x540_2mbps_30fps_av1.mp4"");

            m1080pTestFiles.put(AVC, ""bbb_1920x1080_6mbps_30fps_avc.mp4"");
            m1080pTestFiles.put(HEVC, ""bbb_1920x1080_4mbps_30fps_hevc.mp4"");
            m1080pTestFiles.put(VP8, ""bbb_1920x1080_6mbps_30fps_vp8.webm"");
            m1080pTestFiles.put(VP9, ""bbb_1920x1080_4mbps_30fps_vp9.webm"");
            m1080pTestFiles.put(AV1, ""bbb_1920x1080_4mbps_30fps_av1.mp4"");
        }
    }

    @Before
    public void setUp() throws Exception {
        Utils.assumeDeviceMeetsPerformanceClassPreconditions();

        ArrayList<String> listOfAvcHwDecoders = selectHardwareCodecs(AVC, null, null, false);
        assumeFalse(""Test requires h/w avc decoder"", listOfAvcHwDecoders.isEmpty());
        AVC_DECODER_NAME = listOfAvcHwDecoders.get(0);

        ArrayList<String> listOfAvcHwEncoders = selectHardwareCodecs(AVC, null, null, true);
        assumeFalse(""Test requires h/w avc encoder"", listOfAvcHwEncoders.isEmpty());
        AVC_ENCODER_NAME = listOfAvcHwEncoders.get(0);

        ArrayList<String> listOfAacDecoders = selectCodecs(AAC, null, null, false);
        assertFalse(""Test requires aac decoder"", listOfAacDecoders.isEmpty());
        AAC_DECODER_NAME = listOfAacDecoders.get(0);

        createSurface();
        startLoad();
    }

    @After
    public void tearDown() throws Exception {
        stopLoad();
        releaseSurface();
    }

    @Rule
    public ActivityTestRule<TestActivity> mActivityRule =
            new ActivityTestRule<>(TestActivity.class);

    public FrameDropTestBase(String mimeType, String decoderName, boolean isAsync) {
        mMime = mimeType;
        mDecoderName = decoderName;
        mIsAsync = isAsync;
    }

    // Returns the list of objects with mimeTypes and their hardware decoders supporting the
    // given features combining with sync and async modes.
    static List<Object[]> prepareArgumentsList(String[] features) {
        final List<Object[]> argsList = new ArrayList<>();
        final String[] mimesList = new String[] {AVC, HEVC, VP8, VP9, AV1};
        for (String mime : mimesList) {
            MediaFormat format = MediaFormat.createVideoFormat(mime, 1920, 1080);
            format.setInteger(MediaFormat.KEY_FRAME_RATE, FRAME_RATE);
            ArrayList<MediaFormat> formats = new ArrayList<>();
            formats.add(format);
            ArrayList<String> listOfDecoders =
                    selectHardwareCodecs(mime, formats, features, false);
            for (String decoder : listOfDecoders) {
                for (boolean isAsync : boolStates) {
                    argsList.add(new Object[]{mime, decoder, isAsync});
                }
            }
        }
        return argsList;
    }

    private void createSurface() throws InterruptedException {
        mActivityRule.getActivity().waitTillSurfaceIsCreated();
        mSurface = mActivityRule.getActivity().getSurface();
        assertTrue(""Surface created is null."", mSurface != null);
        assertTrue(""Surface created is invalid."", mSurface.isValid());
        // As we display 1920x1080 and 960x540 only which are of same aspect ratio, we will
        // be setting screen params to 1920x1080
        mActivityRule.getActivity().setScreenParams(1920, 1080, true);
    }

    private void releaseSurface() {
        if (mSurface != null) {
            mSurface.release();
            mSurface = null;
        }
    }

    private Thread createTranscodeLoad() {
        Thread transcodeLoadThread = new Thread(() -> {
            try {
                TranscodeLoad transcodeLoad = new TranscodeLoad(AVC, AVC_LOAD_FILE_NAME,
                        AVC_DECODER_NAME, AVC_ENCODER_NAME, mLoadStatus);
                transcodeLoad.doTranscode();
            } catch (Exception e) {
                mTranscodeLoadException = e;
            }
        });
        return transcodeLoadThread;
    }

    private Thread createAudioPlaybackLoad() {
        Thread audioPlaybackLoadThread = new Thread(() -> {
            try {
                AudioPlaybackLoad audioPlaybackLoad = new AudioPlaybackLoad(AAC, AAC_LOAD_FILE_NAME,
                        AAC_DECODER_NAME, mLoadStatus);
                audioPlaybackLoad.doDecodeAndPlayback();
            } catch (Exception e) {
                mAudioPlaybackLoadException = e;
            }
        });
        return audioPlaybackLoadThread;
    }

    private void startLoad() {
        // TODO: b/183671436
        // Start Transcode load (Decoder(720p) + Encoder(720p))
        mLoadStatus = new LoadStatus();
        mTranscodeLoadThread = createTranscodeLoad();
        mTranscodeLoadThread.start();
        // Start 128kbps AAC audio playback
        mAudioPlaybackLoadThread = createAudioPlaybackLoad();
        mAudioPlaybackLoadThread.start();
    }

    private void stopLoad() throws Exception {
        if (mLoadStatus != null) {
            mLoadStatus.setLoadFinished();
            mLoadStatus = null;
        }
        if (mTranscodeLoadThread != null) {
            mTranscodeLoadThread.join();
            mTranscodeLoadThread = null;
        }
        if (mAudioPlaybackLoadThread != null) {
            mAudioPlaybackLoadThread.join();
            mAudioPlaybackLoadThread = null;
        }
        if (mTranscodeLoadException != null) throw mTranscodeLoadException;
        if (mAudioPlaybackLoadException != null) throw mAudioPlaybackLoadException;
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.bokeh.CameraBokehActivity"	"setPassFailButtonClickListeners"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/bokeh/CameraBokehActivity.java"	""	"public void test/*
 *.
 */
package com.android.cts.verifier.camera.bokeh;

import com.android.cts.verifier.PassFailButtons;
import com.android.cts.verifier.R;

import com.android.ex.camera2.blocking.BlockingCameraManager;
import com.android.ex.camera2.blocking.BlockingCameraManager.BlockingOpenException;
import com.android.ex.camera2.blocking.BlockingStateCallback;
import com.android.ex.camera2.blocking.BlockingSessionCallback;

import android.app.AlertDialog;
import android.content.res.Configuration;
import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.graphics.Color;
import android.graphics.ColorFilter;
import android.graphics.ColorMatrixColorFilter;
import android.graphics.ImageFormat;
import android.graphics.Matrix;
import android.graphics.RectF;
import android.graphics.SurfaceTexture;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraCharacteristics.Key;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CameraMetadata;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.params.Capability;
import android.hardware.camera2.params.StreamConfigurationMap;
import android.hardware.camera2.TotalCaptureResult;
import android.media.Image;
import android.media.ImageReader;
import android.os.Bundle;
import android.os.Handler;
import android.os.HandlerThread;
import android.util.Log;
import android.util.Size;
import android.util.SparseArray;
import android.view.Menu;
import android.view.MenuItem;
import android.view.View;
import android.view.Surface;
import android.view.TextureView;
import android.widget.AdapterView;
import android.widget.ArrayAdapter;
import android.widget.Button;
import android.widget.ImageButton;
import android.widget.ImageView;
import android.widget.Spinner;
import android.widget.TextView;
import android.widget.Toast;
import android.content.Context;

import java.lang.Math;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Comparator;
import java.util.List;
import java.util.Optional;
import java.util.Set;
import java.util.TreeSet;

/**
 * Tests for manual verification of bokeh modes supported by the camera device.
 */
public class CameraBokehActivity extends PassFailButtons.Activity
        implements TextureView.SurfaceTextureListener,
                   ImageReader.OnImageAvailableListener {

    private static final String TAG = ""CameraBokehActivity"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);
    private static final int SESSION_READY_TIMEOUT_MS = 5000;
    private static final Size FULLHD = new Size(1920, 1080);
    private static final ColorMatrixColorFilter sJFIF_YUVToRGB_Filter =
            new ColorMatrixColorFilter(new float[] {
                        1f,        0f,    1.402f, 0f, -179.456f,
                        1f, -0.34414f, -0.71414f, 0f,   135.46f,
                        1f,    1.772f,        0f, 0f, -226.816f,
                        0f,        0f,        0f, 1f,        0f
                    });

    private TextureView mPreviewView;
    private SurfaceTexture mPreviewTexture;
    private Surface mPreviewSurface;
    private int mPreviewTexWidth, mPreviewTexHeight;

    private ImageView mImageView;
    private ColorFilter mCurrentColorFilter;

    private Spinner mCameraSpinner;
    private TextView mTestLabel;
    private TextView mPreviewLabel;
    private TextView mImageLabel;

    private CameraManager mCameraManager;
    private String[] mCameraIdList;
    private HandlerThread mCameraThread;
    private Handler mCameraHandler;
    private BlockingCameraManager mBlockingCameraManager;
    private CameraCharacteristics mCameraCharacteristics;
    private BlockingStateCallback mCameraListener;

    private BlockingSessionCallback mSessionListener;
    private CaptureRequest.Builder mPreviewRequestBuilder;
    private CaptureRequest mPreviewRequest;
    private CaptureRequest.Builder mStillCaptureRequestBuilder;
    private CaptureRequest mStillCaptureRequest;

    private HashMap<String, ArrayList<Capability>> mTestCases = new HashMap<>();
    private int mCurrentCameraIndex = -1;
    private String mCameraId;
    private CameraCaptureSession mCaptureSession;
    private CameraDevice mCameraDevice;

    SizeComparator mSizeComparator = new SizeComparator();

    private Size mPreviewSize;
    private Size mJpegSize;
    private ImageReader mJpegImageReader;
    private ImageReader mYuvImageReader;

    private SparseArray<String> mModeNames;

    private CameraCombination mNextCombination;
    private Size mMaxBokehStreamingSize;

    private Button mNextButton;

    private final TreeSet<CameraCombination> mTestedCombinations = new TreeSet<>(COMPARATOR);
    private final TreeSet<CameraCombination> mUntestedCombinations = new TreeSet<>(COMPARATOR);
    private final TreeSet<String> mUntestedCameras = new TreeSet<>();

    // Menu to show the test progress
    private static final int MENU_ID_PROGRESS = Menu.FIRST + 1;

    private class CameraCombination {
        private final int mCameraIndex;
        private final int mMode;
        private final Size mPreviewSize;
        private final boolean mIsStillCapture;
        private final String mCameraId;
        private final String mModeName;

        private CameraCombination(int cameraIndex, int mode,
                int streamingWidth, int streamingHeight,
                String cameraId, String modeName,
                boolean isStillCapture) {
            this.mCameraIndex = cameraIndex;
            this.mMode = mode;
            this.mPreviewSize = new Size(streamingWidth, streamingHeight);
            this.mCameraId = cameraId;
            this.mModeName = modeName;
            this.mIsStillCapture = isStillCapture;
        }

        @Override
        public String toString() {
            return String.format(""Camera %s, mode %s, intent %s"",
                    mCameraId, mModeName, mIsStillCapture ? ""PREVIEW"" : ""STILL_CAPTURE"");
        }
    }

    private static final Comparator<CameraCombination> COMPARATOR =
        Comparator.<CameraCombination, Integer>comparing(c -> c.mCameraIndex)
            .thenComparing(c -> c.mMode)
            .thenComparing(c -> c.mIsStillCapture);

    private CameraCaptureSession.CaptureCallback mCaptureCallback =
            new CameraCaptureSession.CaptureCallback() {
        @Override
        public void onCaptureProgressed(CameraCaptureSession session,
                                        CaptureRequest request,
                                        CaptureResult partialResult) {
            // Don't need to do anything here.
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session,
                                       CaptureRequest request,
                                       TotalCaptureResult result) {
            // Don't need to do anything here.
        }
    };

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        setContentView(R.layout.cb_main);

        setPassFailButtonClickListeners();

        mPreviewView = (TextureView) findViewById(R.id.preview_view);
        mImageView = (ImageView) findViewById(R.id.image_view);

        mPreviewView.setSurfaceTextureListener(this);

        mCameraManager = (CameraManager) getSystemService(Context.CAMERA_SERVICE);
        try {
            mCameraIdList = mCameraManager.getCameraIdList();
            for (String id : mCameraIdList) {
                CameraCharacteristics characteristics =
                        mCameraManager.getCameraCharacteristics(id);
                Key<Capability[]> key =
                        CameraCharacteristics.CONTROL_AVAILABLE_EXTENDED_SCENE_MODE_CAPABILITIES;
                Capability[] extendedSceneModeCaps = characteristics.get(key);

                if (extendedSceneModeCaps == null) {
                    continue;
                }

                ArrayList<Capability> nonOffModes = new ArrayList<>();
                for (Capability cap : extendedSceneModeCaps) {
                    int mode = cap.getMode();
                    if (mode == CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE ||
                            mode == CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_CONTINUOUS) {
                        nonOffModes.add(cap);
                    }
                }

                if (nonOffModes.size() > 0) {
                    mUntestedCameras.add(""All combinations for Camera "" + id);
                    mTestCases.put(id, nonOffModes);
                }

            }
        } catch (CameraAccessException e) {
            e.printStackTrace();
        }

        // If no supported bokeh modes, mark the test as pass
        if (mTestCases.size() == 0) {
            setInfoResources(R.string.camera_bokeh_test, R.string.camera_bokeh_no_support, -1);
            setPassButtonEnabled(true);
        } else {
            setInfoResources(R.string.camera_bokeh_test, R.string.camera_bokeh_test_info, -1);
            // disable ""Pass"" button until all combinations are tested
            setPassButtonEnabled(false);
        }

        Set<String> cameraIdSet = mTestCases.keySet();
        String[] cameraNames = new String[cameraIdSet.size()];
        int i = 0;
        for (String id : cameraIdSet) {
            cameraNames[i++] = ""Camera "" + id;
        }
        mCameraSpinner = (Spinner) findViewById(R.id.cameras_selection);
        mCameraSpinner.setAdapter(
            new ArrayAdapter<String>(
                this, R.layout.camera_list_item, cameraNames));
        mCameraSpinner.setOnItemSelectedListener(mCameraSpinnerListener);

        mTestLabel = (TextView) findViewById(R.id.test_label);
        mPreviewLabel = (TextView) findViewById(R.id.preview_label);
        mImageLabel = (TextView) findViewById(R.id.image_label);

        // Must be kept in sync with camera bokeh mode manually
        mModeNames = new SparseArray(2);
        mModeNames.append(
                CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE, ""STILL_CAPTURE"");
        mModeNames.append(
                CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_CONTINUOUS, ""CONTINUOUS"");

        mNextButton = findViewById(R.id.next_button);
        mNextButton.setOnClickListener(v -> {
                if (mNextCombination != null) {
                    mUntestedCombinations.remove(mNextCombination);
                    mTestedCombinations.add(mNextCombination);
                }
                setUntestedCombination();

                if (mNextCombination != null) {
                    if (mNextCombination.mIsStillCapture) {
                        takePicture();
                    } else {
                        if (mCaptureSession != null) {
                            mCaptureSession.close();
                        }
                        startPreview();
                    }
                }
        });

        mBlockingCameraManager = new BlockingCameraManager(mCameraManager);
        mCameraListener = new BlockingStateCallback();
    }

    /**
     * Set an untested combination of resolution and bokeh mode for the current camera.
     * Triggered by next button click.
     */
    private void setUntestedCombination() {
        Optional<CameraCombination> combination = mUntestedCombinations.stream().filter(
            c -> c.mCameraIndex == mCurrentCameraIndex).findFirst();
        if (!combination.isPresent()) {
            Toast.makeText(this, ""All Camera "" + mCurrentCameraIndex + "" tests are done."",
                Toast.LENGTH_SHORT).show();
            mNextCombination = null;

            if (mUntestedCombinations.isEmpty() && mUntestedCameras.isEmpty()) {
                setPassButtonEnabled(true);
            }
            return;
        }

        // There is untested combination for the current camera, set the next untested combination.
        mNextCombination = combination.get();
        int nextMode = mNextCombination.mMode;
        ArrayList<Capability> bokehCaps = mTestCases.get(mCameraId);
        for (Capability cap : bokehCaps) {
            if (cap.getMode() == nextMode) {
                mMaxBokehStreamingSize = cap.getMaxStreamingSize();
            }
        }

        // Update bokeh mode and use case
        String testString = ""Mode: "" + mModeNames.get(mNextCombination.mMode);
        if (mNextCombination.mIsStillCapture) {
            testString += ""\nIntent: Capture"";
        } else {
            testString += ""\nIntent: Preview"";
        }
        testString += ""\n\nPress Next if the bokeh effect works as intended"";
        mTestLabel.setText(testString);

        // Update preview view and image view bokeh expectation
        boolean previewIsBokehCompatible =
                mSizeComparator.compare(mNextCombination.mPreviewSize, mMaxBokehStreamingSize) <= 0;
        String previewLabel = ""Normal preview"";
        if (previewIsBokehCompatible || mNextCombination.mIsStillCapture) {
            previewLabel += "" with bokeh"";
        }
        mPreviewLabel.setText(previewLabel);

        String imageLabel;
        if (mNextCombination.mIsStillCapture) {
            imageLabel = ""JPEG with bokeh"";
        } else {
            imageLabel = ""YUV"";
            if (previewIsBokehCompatible) {
                imageLabel += "" with bokeh"";
            }
        }
        mImageLabel.setText(imageLabel);
    }

    @Override
    public boolean onCreateOptionsMenu(Menu menu) {
        menu.add(Menu.NONE, MENU_ID_PROGRESS, Menu.NONE, ""Current Progress"");
        return super.onCreateOptionsMenu(menu);
    }

    @Override
    public boolean onOptionsItemSelected(MenuItem item) {
        boolean ret = true;
        switch (item.getItemId()) {
            case MENU_ID_PROGRESS:
                showCombinationsDialog();
                ret = true;
                break;
            default:
                ret = super.onOptionsItemSelected(item);
                break;
        }
        return ret;
    }

    private void showCombinationsDialog() {
        AlertDialog.Builder builder =
                new AlertDialog.Builder(CameraBokehActivity.this);
        builder.setMessage(getTestDetails())
                .setTitle(""Current Progress"")
                .setPositiveButton(""OK"", null);
        builder.show();
    }

    @Override
    public void onResume() {
        super.onResume();

        startBackgroundThread();

        int cameraIndex = mCameraSpinner.getSelectedItemPosition();
        if (cameraIndex >= 0) {
            setUpCamera(mCameraSpinner.getSelectedItemPosition());
        }
    }

    @Override
    public void onPause() {
        shutdownCamera();
        stopBackgroundThread();

        super.onPause();
    }

    @Override
    public String getTestDetails() {
        StringBuilder reportBuilder = new StringBuilder();
        reportBuilder.append(""Tested combinations:\n"");
        for (CameraCombination combination: mTestedCombinations) {
            reportBuilder.append(combination);
            reportBuilder.append(""\n"");
        }

        reportBuilder.append(""Untested cameras:\n"");
        for (String untestedCamera : mUntestedCameras) {
            reportBuilder.append(untestedCamera);
            reportBuilder.append(""\n"");
        }
        reportBuilder.append(""Untested combinations:\n"");
        for (CameraCombination combination: mUntestedCombinations) {
            reportBuilder.append(combination);
            reportBuilder.append(""\n"");
        }
        return reportBuilder.toString();
    }

    @Override
    public void onSurfaceTextureAvailable(SurfaceTexture surfaceTexture,
            int width, int height) {
        mPreviewTexture = surfaceTexture;
        mPreviewTexWidth = width;
        mPreviewTexHeight = height;

        mPreviewSurface = new Surface(mPreviewTexture);

        if (mCameraDevice != null) {
            startPreview();
        }
    }

    @Override
    public void onSurfaceTextureSizeChanged(SurfaceTexture surface, int width, int height) {
        // Ignored, Camera does all the work for us
        if (VERBOSE) {
            Log.v(TAG, ""onSurfaceTextureSizeChanged: "" + width + "" x "" + height);
        }
    }

    @Override
    public boolean onSurfaceTextureDestroyed(SurfaceTexture surface) {
        mPreviewTexture = null;
        return true;
    }

    @Override
    public void onSurfaceTextureUpdated(SurfaceTexture surface) {
        // Invoked every time there's a new Camera preview frame
    }

    @Override
    public void onImageAvailable(ImageReader reader) {
        Image img = null;
        try {
            img = reader.acquireNextImage();
            if (img == null) {
                Log.d(TAG, ""Invalid image!"");
                return;
            }
            final int format = img.getFormat();

            Size configuredSize = (format == ImageFormat.YUV_420_888 ? mPreviewSize : mJpegSize);
            Bitmap imgBitmap = null;
            if (format == ImageFormat.YUV_420_888) {
                ByteBuffer yBuffer = img.getPlanes()[0].getBuffer();
                ByteBuffer uBuffer = img.getPlanes()[1].getBuffer();
                ByteBuffer vBuffer = img.getPlanes()[2].getBuffer();
                yBuffer.rewind();
                uBuffer.rewind();
                vBuffer.rewind();
                int w = configuredSize.getWidth();
                int h = configuredSize.getHeight();
                int stride = img.getPlanes()[0].getRowStride();
                int uStride = img.getPlanes()[1].getRowStride();
                int vStride = img.getPlanes()[2].getRowStride();
                int uPStride = img.getPlanes()[1].getPixelStride();
                int vPStride = img.getPlanes()[2].getPixelStride();
                byte[] row = new byte[configuredSize.getWidth()];
                byte[] uRow = new byte[(configuredSize.getWidth()/2-1)*uPStride + 1];
                byte[] vRow = new byte[(configuredSize.getWidth()/2-1)*vPStride + 1];
                int[] imgArray = new int[w * h];
                for (int y = 0, j = 0, rowStart = 0, uRowStart = 0, vRowStart = 0; y < h;
                     y++, rowStart += stride) {
                    yBuffer.position(rowStart);
                    yBuffer.get(row);
                    if (y % 2 == 0) {
                        uBuffer.position(uRowStart);
                        uBuffer.get(uRow);
                        vBuffer.position(vRowStart);
                        vBuffer.get(vRow);
                        uRowStart += uStride;
                        vRowStart += vStride;
                    }
                    for (int x = 0, i = 0; x < w; x++) {
                        int yval = row[i] & 0xFF;
                        int uval = uRow[i/2 * uPStride] & 0xFF;
                        int vval = vRow[i/2 * vPStride] & 0xFF;
                        // Write YUV directly; the ImageView color filter will convert to RGB for us.
                        imgArray[j] = Color.rgb(yval, uval, vval);
                        i++;
                        j++;
                    }
                }
                img.close();
                imgBitmap = Bitmap.createBitmap(imgArray, w, h, Bitmap.Config.ARGB_8888);
            } else if (format == ImageFormat.JPEG) {
                ByteBuffer jpegBuffer = img.getPlanes()[0].getBuffer();
                jpegBuffer.rewind();
                byte[] jpegData = new byte[jpegBuffer.limit()];
                jpegBuffer.get(jpegData);
                imgBitmap = BitmapFactory.decodeByteArray(jpegData, 0, jpegData.length);
                img.close();
            } else {
                Log.i(TAG, ""Unsupported image format: "" + format);
            }
            if (imgBitmap != null) {
                final Bitmap bitmap = imgBitmap;
                runOnUiThread(new Runnable() {
                    @Override
                    public void run() {
                        if (format == ImageFormat.YUV_420_888 && (mCurrentColorFilter == null ||
                                !mCurrentColorFilter.equals(sJFIF_YUVToRGB_Filter))) {
                            mCurrentColorFilter = sJFIF_YUVToRGB_Filter;
                            mImageView.setColorFilter(mCurrentColorFilter);
                        } else if (format == ImageFormat.JPEG && mCurrentColorFilter != null &&
                                mCurrentColorFilter.equals(sJFIF_YUVToRGB_Filter)) {
                            mCurrentColorFilter = null;
                            mImageView.clearColorFilter();
                        }
                        mImageView.setImageBitmap(bitmap);
                    }
                });
            }
        } catch (java.lang.IllegalStateException e) {
            // Swallow exceptions
            e.printStackTrace();
        } finally {
            if (img != null) {
                img.close();
            }
        }
    }

    private AdapterView.OnItemSelectedListener mCameraSpinnerListener =
            new AdapterView.OnItemSelectedListener() {
                public void onItemSelected(AdapterView<?> parent,
                        View view, int pos, long id) {
                    if (mCurrentCameraIndex != pos) {
                        setUpCamera(pos);
                    }
                }

                public void onNothingSelected(AdapterView parent) {
                }
            };

    private class SizeComparator implements Comparator<Size> {
        @Override
        public int compare(Size lhs, Size rhs) {
            long lha = lhs.getWidth() * lhs.getHeight();
            long rha = rhs.getWidth() * rhs.getHeight();
            if (lha == rha) {
                lha = lhs.getWidth();
                rha = rhs.getWidth();
            }
            return (lha < rha) ? -1 : (lha > rha ? 1 : 0);
        }
    }

    private void setUpCamera(int index) {
        shutdownCamera();

        mCurrentCameraIndex = index;
        mCameraId = mCameraIdList[index];
        try {
            mCameraCharacteristics = mCameraManager.getCameraCharacteristics(mCameraId);
            mCameraDevice = mBlockingCameraManager.openCamera(mCameraId,
                    mCameraListener, mCameraHandler);
        } catch (CameraAccessException e) {
            e.printStackTrace();
        } catch (BlockingOpenException e) {
            e.printStackTrace();
        }

        // Update untested cameras
        mUntestedCameras.remove(""All combinations for Camera "" + mCameraId);

        StreamConfigurationMap config =
                mCameraCharacteristics.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        Size[] jpegSizes = config.getOutputSizes(ImageFormat.JPEG);
        Arrays.sort(jpegSizes, mSizeComparator);
        mJpegSize = jpegSizes[jpegSizes.length-1];

        Size[] yuvSizes = config.getOutputSizes(ImageFormat.YUV_420_888);
        Arrays.sort(yuvSizes, mSizeComparator);
        Size maxYuvSize = yuvSizes[yuvSizes.length-1];
        if (mSizeComparator.compare(maxYuvSize, FULLHD) > 1) {
            maxYuvSize = FULLHD;
        }

        // Update untested entries
        ArrayList<Capability> currentTestCase = mTestCases.get(mCameraId);
        for (Capability bokehCap : currentTestCase) {
            Size maxStreamingSize = bokehCap.getMaxStreamingSize();
            Size previewSize;
            if ((maxStreamingSize.getWidth() == 0 && maxStreamingSize.getHeight() == 0) ||
                    (mSizeComparator.compare(maxStreamingSize, maxYuvSize) > 0)) {
                previewSize = maxYuvSize;
            } else {
                previewSize = maxStreamingSize;
            }

            CameraCombination combination = new CameraCombination(
                    index, bokehCap.getMode(), previewSize.getWidth(),
                    previewSize.getHeight(), mCameraId,
                    mModeNames.get(bokehCap.getMode()),
                    /*isStillCapture*/false);

            if (!mTestedCombinations.contains(combination)) {
                mUntestedCombinations.add(combination);
            }

            // For BOKEH_MODE_STILL_CAPTURE, add 2 combinations: one streaming, one still capture.
            if (bokehCap.getMode() ==
                    CaptureRequest.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE) {
                CameraCombination combination2 = new CameraCombination(
                        index, bokehCap.getMode(), previewSize.getWidth(),
                        previewSize.getHeight(), mCameraId,
                        mModeNames.get(bokehCap.getMode()),
                        /*isStillCapture*/true);

                if (!mTestedCombinations.contains(combination2)) {
                    mUntestedCombinations.add(combination2);
                }
            }
        }

        mJpegImageReader = ImageReader.newInstance(
                mJpegSize.getWidth(), mJpegSize.getHeight(), ImageFormat.JPEG, 1);
        mJpegImageReader.setOnImageAvailableListener(this, mCameraHandler);

        setUntestedCombination();

        if (mPreviewTexture != null) {
            startPreview();
        }
    }

    private void shutdownCamera() {
        if (null != mCaptureSession) {
            mCaptureSession.close();
            mCaptureSession = null;
        }
        if (null != mCameraDevice) {
            mCameraDevice.close();
            mCameraDevice = null;
        }
        if (null != mJpegImageReader) {
            mJpegImageReader.close();
            mJpegImageReader = null;
        }
        if (null != mYuvImageReader) {
            mYuvImageReader.close();
            mYuvImageReader = null;
        }
    }

    private void configurePreviewTextureTransform() {
        int rotation = getWindowManager().getDefaultDisplay().getRotation();
        Configuration config = getResources().getConfiguration();
        int degrees = 0;
        switch (rotation) {
            case Surface.ROTATION_0: degrees = 0; break;
            case Surface.ROTATION_90: degrees = 90; break;
            case Surface.ROTATION_180: degrees = 180; break;
            case Surface.ROTATION_270: degrees = 270; break;
        }
        Matrix matrix = mPreviewView.getTransform(null);
        int deviceOrientation = Configuration.ORIENTATION_PORTRAIT;
        if ((degrees % 180 == 0 && config.orientation == Configuration.ORIENTATION_LANDSCAPE) ||
                (degrees % 180 == 90 && config.orientation == Configuration.ORIENTATION_PORTRAIT)) {
            deviceOrientation = Configuration.ORIENTATION_LANDSCAPE;
        }
        int effectiveWidth = mPreviewSize.getWidth();
        int effectiveHeight = mPreviewSize.getHeight();
        if (deviceOrientation == Configuration.ORIENTATION_PORTRAIT) {
            int temp = effectiveWidth;
            effectiveWidth = effectiveHeight;
            effectiveHeight = temp;
        }

        RectF viewRect = new RectF(0, 0, mPreviewTexWidth, mPreviewTexHeight);
        RectF bufferRect = new RectF(0, 0, effectiveWidth, effectiveHeight);
        float centerX = viewRect.centerX();
        float centerY = viewRect.centerY();
        bufferRect.offset(centerX - bufferRect.centerX(),
                centerY - bufferRect.centerY());

        matrix.setRectToRect(viewRect, bufferRect, Matrix.ScaleToFit.FILL);

        matrix.postRotate((360 - degrees) % 360, centerX, centerY);
        if ((degrees % 180) == 90) {
            int temp = effectiveWidth;
            effectiveWidth = effectiveHeight;
            effectiveHeight = temp;
        }
        // Scale to fit view, avoiding any crop
        float scale = Math.min(mPreviewTexWidth / (float) effectiveWidth,
                mPreviewTexHeight / (float) effectiveHeight);
        matrix.postScale(scale, scale, centerX, centerY);

        mPreviewView.setTransform(matrix);
    }
    /**
     * Starts a background thread and its {@link Handler}.
     */
    private void startBackgroundThread() {
        mCameraThread = new HandlerThread(""CameraBokehBackground"");
        mCameraThread.start();
        mCameraHandler = new Handler(mCameraThread.getLooper());
    }

    /**
     * Stops the background thread and its {@link Handler}.
     */
    private void stopBackgroundThread() {
        mCameraThread.quitSafely();
        try {
            mCameraThread.join();
            mCameraThread = null;
            mCameraHandler = null;
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }

    private void startPreview() {
        try {
            if (mPreviewSize == null || !mPreviewSize.equals(mNextCombination.mPreviewSize)) {
                mPreviewSize = mNextCombination.mPreviewSize;

                mYuvImageReader = ImageReader.newInstance(mPreviewSize.getWidth(),
                        mPreviewSize.getHeight(), ImageFormat.YUV_420_888, 1);
                mYuvImageReader.setOnImageAvailableListener(this, mCameraHandler);
            };

            mPreviewTexture.setDefaultBufferSize(mPreviewSize.getWidth(), mPreviewSize.getHeight());
            mPreviewRequestBuilder =
                    mCameraDevice.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
            mPreviewRequestBuilder.addTarget(mPreviewSurface);
            mPreviewRequestBuilder.addTarget(mYuvImageReader.getSurface());

            mStillCaptureRequestBuilder =
                    mCameraDevice.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
            mStillCaptureRequestBuilder.addTarget(mPreviewSurface);
            mStillCaptureRequestBuilder.addTarget(mJpegImageReader.getSurface());

            mSessionListener = new BlockingSessionCallback();
            List<Surface> outputSurfaces = new ArrayList<Surface>(/*capacity*/3);
            outputSurfaces.add(mPreviewSurface);
            outputSurfaces.add(mYuvImageReader.getSurface());
            outputSurfaces.add(mJpegImageReader.getSurface());
            mCameraDevice.createCaptureSession(outputSurfaces, mSessionListener, mCameraHandler);
            mCaptureSession = mSessionListener.waitAndGetSession(/*timeoutMs*/3000);

            configurePreviewTextureTransform();

            /* Set bokeh mode and start streaming */
            int bokehMode = mNextCombination.mMode;
            mPreviewRequestBuilder.set(CaptureRequest.CONTROL_EXTENDED_SCENE_MODE, bokehMode);
            mStillCaptureRequestBuilder.set(CaptureRequest.CONTROL_EXTENDED_SCENE_MODE, bokehMode);
            mPreviewRequest = mPreviewRequestBuilder.build();
            mStillCaptureRequest = mStillCaptureRequestBuilder.build();

            mCaptureSession.setRepeatingRequest(mPreviewRequest, mCaptureCallback, mCameraHandler);
        } catch (CameraAccessException e) {
            e.printStackTrace();
        }
    }

    private void takePicture() {
        try {
            mCaptureSession.stopRepeating();
            mSessionListener.getStateWaiter().waitForState(
                    BlockingSessionCallback.SESSION_READY, SESSION_READY_TIMEOUT_MS);

            mCaptureSession.capture(mStillCaptureRequest, mCaptureCallback, mCameraHandler);
        } catch (CameraAccessException e) {
            e.printStackTrace();
        }
    }

    private void setPassButtonEnabled(boolean enabled) {
        ImageButton pass_button = (ImageButton) findViewById(R.id.pass_button);
        pass_button.setEnabled(enabled);
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.mediaparser.cts.MediaParserTest"	"testGetParserNamesByMimeType"	"CtsMediaParserTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/mediaparser/src/android/media/mediaparser/cts/MediaParserTest.java"	""	"public void testGetParserNamesByMimeType() {
        // MimeTypes obtained from the W3C.
        assertParsers(MediaParser.PARSER_NAME_MATROSKA)
                .supportMimeTypes(
                        ""video/x-matroska"", ""audio/x-matroska"", ""video/x-webm"", ""audio/x-webm"");
        assertParsers(MediaParser.PARSER_NAME_MP4, MediaParser.PARSER_NAME_FMP4)
                .supportMimeTypes(""video/mp4"", ""audio/mp4"", ""application/mp4"");
        assertParsers(MediaParser.PARSER_NAME_MP3).supportMimeTypes(""audio/mpeg"");
        assertParsers(MediaParser.PARSER_NAME_ADTS).supportMimeTypes(""audio/aac"");
        assertParsers(MediaParser.PARSER_NAME_AC3).supportMimeTypes(""audio/ac3"");
        assertParsers(MediaParser.PARSER_NAME_TS).supportMimeTypes(""video/mp2t"", ""audio/mp2t"");
        assertParsers(MediaParser.PARSER_NAME_FLV).supportMimeTypes(""video/x-flv"");
        assertParsers(MediaParser.PARSER_NAME_OGG)
                .supportMimeTypes(""video/ogg"", ""audio/ogg"", ""application/ogg"");
        assertParsers(MediaParser.PARSER_NAME_PS).supportMimeTypes(""video/mp2p"", ""video/mp1s"");
        assertParsers(MediaParser.PARSER_NAME_WAV)
                .supportMimeTypes(""audio/vnd.wave"", ""audio/wav"", ""audio/wave"", ""audio/x-wav"");
        assertParsers(MediaParser.PARSER_NAME_AMR).supportMimeTypes(""audio/amr"");
        assertParsers(MediaParser.PARSER_NAME_AC4).supportMimeTypes(""audio/ac4"");
        assertParsers(MediaParser.PARSER_NAME_FLAC).supportMimeTypes(""audio/flac"", ""audio/x-flac"");
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.mediaparser.cts.MediaParserTest"	"testSupportsParameter"	"CtsMediaParserTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/mediaparser/src/android/media/mediaparser/cts/MediaParserTest.java"	""	"public void testSupportsParameter() {
        assertSupportFor(MediaParser.PARAMETER_ADTS_ENABLE_CBR_SEEKING);
        assertSupportFor(MediaParser.PARAMETER_AMR_ENABLE_CBR_SEEKING);
        assertSupportFor(MediaParser.PARAMETER_FLAC_DISABLE_ID3);
        assertSupportFor(MediaParser.PARAMETER_MP4_IGNORE_EDIT_LISTS);
        assertSupportFor(MediaParser.PARAMETER_MP4_IGNORE_TFDT_BOX);
        assertSupportFor(MediaParser.PARAMETER_MP4_TREAT_VIDEO_FRAMES_AS_KEYFRAMES);
        assertSupportFor(MediaParser.PARAMETER_MATROSKA_DISABLE_CUES_SEEKING);
        assertSupportFor(MediaParser.PARAMETER_MP3_DISABLE_ID3);
        assertSupportFor(MediaParser.PARAMETER_MP3_ENABLE_CBR_SEEKING);
        assertSupportFor(MediaParser.PARAMETER_MP3_ENABLE_INDEX_SEEKING);
        assertSupportFor(MediaParser.PARAMETER_TS_MODE);
        assertSupportFor(MediaParser.PARAMETER_TS_ALLOW_NON_IDR_AVC_KEYFRAMES);
        assertSupportFor(MediaParser.PARAMETER_TS_IGNORE_AAC_STREAM);
        assertSupportFor(MediaParser.PARAMETER_TS_IGNORE_AVC_STREAM);
        assertSupportFor(MediaParser.PARAMETER_TS_IGNORE_SPLICE_INFO_STREAM);
        assertSupportFor(MediaParser.PARAMETER_TS_DETECT_ACCESS_UNITS);
        assertSupportFor(MediaParser.PARAMETER_TS_ENABLE_HDMV_DTS_AUDIO_STREAMS);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.mediaparser.cts.MediaParserTest"	"testSetKnownParameters"	"CtsMediaParserTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/mediaparser/src/android/media/mediaparser/cts/MediaParserTest.java"	""	"public void testSetKnownParameters() {
        testValidAndInvalidValueForBooleanParameter(MediaParser.PARAMETER_ADTS_ENABLE_CBR_SEEKING);
        testValidAndInvalidValueForBooleanParameter(MediaParser.PARAMETER_AMR_ENABLE_CBR_SEEKING);
        testValidAndInvalidValueForBooleanParameter(MediaParser.PARAMETER_FLAC_DISABLE_ID3);
        testValidAndInvalidValueForBooleanParameter(MediaParser.PARAMETER_MP4_IGNORE_EDIT_LISTS);
        testValidAndInvalidValueForBooleanParameter(MediaParser.PARAMETER_MP4_IGNORE_TFDT_BOX);
        testValidAndInvalidValueForBooleanParameter(
                MediaParser.PARAMETER_MP4_TREAT_VIDEO_FRAMES_AS_KEYFRAMES);
        testValidAndInvalidValueForBooleanParameter(
                MediaParser.PARAMETER_MATROSKA_DISABLE_CUES_SEEKING);
        testValidAndInvalidValueForBooleanParameter(MediaParser.PARAMETER_MP3_DISABLE_ID3);
        testValidAndInvalidValueForBooleanParameter(MediaParser.PARAMETER_MP3_ENABLE_CBR_SEEKING);
        testValidAndInvalidValueForBooleanParameter(MediaParser.PARAMETER_MP3_ENABLE_INDEX_SEEKING);
        testValidAndInvalidValueForBooleanParameter(
                MediaParser.PARAMETER_TS_ALLOW_NON_IDR_AVC_KEYFRAMES);
        testValidAndInvalidValueForBooleanParameter(MediaParser.PARAMETER_TS_IGNORE_AAC_STREAM);
        testValidAndInvalidValueForBooleanParameter(MediaParser.PARAMETER_TS_IGNORE_AVC_STREAM);
        testValidAndInvalidValueForBooleanParameter(
                MediaParser.PARAMETER_TS_IGNORE_SPLICE_INFO_STREAM);
        testValidAndInvalidValueForBooleanParameter(MediaParser.PARAMETER_TS_DETECT_ACCESS_UNITS);
        testValidAndInvalidValueForBooleanParameter(
                MediaParser.PARAMETER_TS_ENABLE_HDMV_DTS_AUDIO_STREAMS);
        testParameterSetting(
                MediaParser.PARAMETER_TS_MODE, /* value= */ 1, /* valueIsIllegal= */ true);
        testParameterSetting(
                MediaParser.PARAMETER_TS_MODE,
                /* value= */ ""invalid_mode"",
                /* valueIsIllegal= */ true);
        testParameterSetting(
                MediaParser.PARAMETER_TS_MODE,
                /* value= */ ""single_pmt"",
                /* valueIsIllegal= */ false);
        testParameterSetting(
                MediaParser.PARAMETER_TS_MODE, /* value= */ ""hls"", /* valueIsIllegal= */ false);
        testParameterSetting(
                MediaParser.PARAMETER_TS_MODE,
                /* value= */ ""multi_pmt"",
                /* valueIsIllegal= */ false);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.app.cts.CloseSystemDialogsTest"	"testCloseSystemDialogsViaActivityManager_whenTestInstrumentedViaShell_isSent"	"CtsAppTestCases"	"/home/gpoor/cts-12-source/cts/tests/app/src/android/app/cts/CloseSystemDialogsTest.java"	""	"public void testCloseSystemDialogsViaActivityManager_whenTestInstrumentedViaShell_isSent()
            throws Exception {
        mService = getService(APP_SELF);

        mService.closeSystemDialogsViaActivityManager(REASON);

        assertThat(mFakeView.getNextCloseSystemDialogsCallReason(TIMEOUT_MS)).isEqualTo(REASON);
        assertCloseSystemDialogsReceived();
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.app.cts.CloseSystemDialogsTest"	"testCloseSystemDialogsViaActivityManager_whenRunningAsShell_isSent"	"CtsAppTestCases"	"/home/gpoor/cts-12-source/cts/tests/app/src/android/app/cts/CloseSystemDialogsTest.java"	""	"public void testCloseSystemDialogsViaActivityManager_whenRunningAsShell_isSent()
            throws Exception {
        mService = getService(APP_SELF);

        SystemUtil.runWithShellPermissionIdentity(
                () -> mService.closeSystemDialogsViaActivityManager(REASON));

        assertThat(mFakeView.getNextCloseSystemDialogsCallReason(TIMEOUT_MS)).isEqualTo(REASON);
        assertCloseSystemDialogsReceived();
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.app.cts.CloseSystemDialogsTest"	"testCloseSystemDialogsViaActivityManager_whenTargetSdkCurrent_isBlockedAndThrows"	"CtsAppTestCases"	"/home/gpoor/cts-12-source/cts/tests/app/src/android/app/cts/CloseSystemDialogsTest.java"	""	"public void testCloseSystemDialogsViaActivityManager_whenTargetSdkCurrent_isBlockedAndThrows()
            throws Exception {
        setTargetCurrent();
        mService = getService(APP_HELPER);

        assertThrows(SecurityException.class,
                () -> mService.closeSystemDialogsViaActivityManager(REASON));

        assertThat(mFakeView.getNextCloseSystemDialogsCallReason(TIMEOUT_MS)).isEqualTo(null);
        assertCloseSystemDialogsNotReceived();
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.app.cts.CloseSystemDialogsTest"	"testCloseSystemDialogsViaActivityManager_whenTargetSdk30_isBlockedButDoesNotThrow"	"CtsAppTestCases"	"/home/gpoor/cts-12-source/cts/tests/app/src/android/app/cts/CloseSystemDialogsTest.java"	""	"public void testCloseSystemDialogsViaActivityManager_whenTargetSdk30_isBlockedButDoesNotThrow()
            throws Exception {
        mService = getService(APP_HELPER);

        mService.closeSystemDialogsViaActivityManager(REASON);

        assertThat(mFakeView.getNextCloseSystemDialogsCallReason(TIMEOUT_MS)).isEqualTo(null);
        assertCloseSystemDialogsNotReceived();
    }

    private void setTargetCurrent() {
        // The helper app has targetSdk=30, opting-in to changes emulates targeting latest sdk.
        compat(APP_COMPAT_ENABLE, ActivityManager.LOCK_DOWN_CLOSE_SYSTEM_DIALOGS, APP_HELPER);
        compat(APP_COMPAT_ENABLE, ""NOTIFICATION_TRAMPOLINE_BLOCK"", APP_HELPER);
    }

    private void assertCloseSystemDialogsNotReceived() {
        // If both broadcasts are sent, they will be received in order here since they are both
        // registered receivers in the ""bg"" queue in system_server and belong to the same app.
        // This is guaranteed by a series of handlers that are the same in both cases and due to the
        // fact that the binder that system_server uses to call into the app is the same (since the
        // app is the same) and one-way calls on the same binder object are ordered.
        mSentinelReceived = new ConditionVariable(false);
        Intent intent = new Intent(ACTION_SENTINEL);
        intent.setPackage(mContext.getPackageName());
        mContext.sendBroadcast(intent);
        mSentinelReceived.block();
        assertThat(mCloseSystemDialogsReceived.isDone()).isFalse();
    }

    private void assertCloseSystemDialogsReceived() throws Exception {
        mCloseSystemDialogsReceived.get(TIMEOUT_MS, TimeUnit.MILLISECONDS);
        // No TimeoutException thrown
    }

    private ICloseSystemDialogsTestsService getService(String packageName) throws Exception {
        ICloseSystemDialogsTestsService service =
                ICloseSystemDialogsTestsService.Stub.asInterface(
                        connect(packageName).get(TIMEOUT_MS));
        assertTrue(""Can't call @hide methods"", service.waitUntilReady(TIMEOUT_MS));
        return service;
    }

    private FutureServiceConnection connect(String packageName) {
        if (mConnection != null) {
            return mConnection;
        }
        mConnection = new FutureServiceConnection();
        Intent intent = new Intent();
        intent.setComponent(ComponentName.createRelative(packageName, TEST_SERVICE));
        assertTrue(mContext.bindService(intent, mConnection, Context.BIND_AUTO_CREATE));
        return mConnection;
    }

    private String setHiddenApiPolicy(String policy) throws Exception {
        return SystemUtil.callWithShellPermissionIdentity(() -> {
            String previous = Settings.Global.getString(mResolver,
                    Settings.Global.HIDDEN_API_POLICY);
            Settings.Global.putString(mResolver, Settings.Global.HIDDEN_API_POLICY, policy);
            return previous;
        });
    }

    private void setAccessibilityService(String packageName, String service) throws Exception {
        setAccessibilityState(""1"", packageName + ""/"" + service);
    }

    private void setAccessibilityState(String enabled, String services) {
        mResetAccessibility = true;
        UiAutomation uiAutomation = mInstrumentation.getUiAutomation(
                FLAG_DONT_SUPPRESS_ACCESSIBILITY_SERVICES);
        SystemUtil.runWithShellPermissionIdentity(uiAutomation, () -> {
            mPreviousAccessibilityServices = Settings.Secure.getString(mResolver,
                    Settings.Secure.ENABLED_ACCESSIBILITY_SERVICES);
            mPreviousAccessibilityEnabled = Settings.Secure.getString(mResolver,
                    Settings.Secure.ACCESSIBILITY_ENABLED);
            Settings.Secure.putString(mResolver, Settings.Secure.ENABLED_ACCESSIBILITY_SERVICES,
                    services);
            Settings.Secure.putString(mResolver, Settings.Secure.ACCESSIBILITY_ENABLED, enabled);
        });
    }

    private static void enableUserFinal() {
        SystemUtil.runShellCommand(
                ""settings put global force_non_debuggable_final_build_for_compat 1"");
    }

    private static void resetUserFinal() {
        SystemUtil.runShellCommand(
                ""settings put global force_non_debuggable_final_build_for_compat 0"");
    }

    private static void compat(String command, String changeId, String packageName) {
        SystemUtil.runShellCommand(
                String.format(""am compat %s %s %s"", command, changeId, packageName));
    }

    private static void compat(String command, long changeId, String packageName) {
        compat(command, Long.toString(changeId), packageName);
    }

    private static Context getContextForSaw(Context context) {
        DisplayManager displayManager = context.getSystemService(DisplayManager.class);
        Display display = displayManager.getDisplay(DEFAULT_DISPLAY);
        Context displayContext = context.createDisplayContext(display);
        return displayContext.createWindowContext(TYPE_APPLICATION_OVERLAY, null);
    }

    private class IntentReceiver extends BroadcastReceiver {
        @Override
        public void onReceive(Context context, Intent intent) {
            switch (intent.getAction()) {
                case Intent.ACTION_CLOSE_SYSTEM_DIALOGS:
                    mCloseSystemDialogsReceived.complete(null);
                    break;
                case ACTION_SENTINEL:
                    mSentinelReceived.open();
                    break;
            }
        }
    }

    private class FutureReceiver extends ResultReceiver {
        private final CompletableFuture<Integer> mFuture;

        FutureReceiver(CompletableFuture<Integer> future) {
            super(mMainHandler);
            mFuture = future;
        }

        @Override
        protected void onReceiveResult(int resultCode, Bundle resultData) {
            mFuture.complete(resultCode);
        }
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraManagerTest"	"NoopCameraListener"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CameraManagerTest.java"	""	"/*
 *.
 */

package android.hardware.camera2.cts;

import static junit.framework.Assert.*;

import static org.mockito.Mockito.*;

import android.app.Instrumentation;
import android.app.NotificationManager;
import android.app.UiAutomation;
import android.content.pm.PackageManager;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraDevice.StateCallback;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.cts.Camera2ParameterizedTestCase;
import android.hardware.camera2.cts.CameraTestUtils.HandlerExecutor;
import android.hardware.camera2.cts.CameraTestUtils.MockStateCallback;
import android.hardware.camera2.cts.helpers.CameraErrorCollector;
import android.os.Build;
import android.os.Handler;
import android.os.HandlerThread;
import android.os.ParcelFileDescriptor;
import android.platform.test.annotations.AppModeFull;
import android.util.Log;
import android.util.Pair;

import androidx.test.InstrumentationRegistry;

import com.android.compatibility.common.util.PropertyUtil;
import com.android.ex.camera2.blocking.BlockingStateCallback;

import org.junit.Test;
import org.junit.runner.RunWith;
import org.junit.runners.Parameterized;
import org.mockito.ArgumentCaptor;
import org.mockito.InOrder;

import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.Executor;
import java.util.concurrent.LinkedBlockingQueue;

/**
 * <p>Basic test for CameraManager class.</p>
 */

@RunWith(Parameterized.class)
public class CameraManagerTest extends Camera2ParameterizedTestCase {
    private static final String TAG = ""CameraManagerTest"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);
    private static final int NUM_CAMERA_REOPENS = 10;
    private static final int AVAILABILITY_TIMEOUT_MS = 10;

    private PackageManager mPackageManager;
    private NoopCameraListener mListener;
    private HandlerThread mHandlerThread;
    private Handler mHandler;
    private BlockingStateCallback mCameraListener;
    private CameraErrorCollector mCollector;
    private Set<Set<String>> mConcurrentCameraIdCombinations;

    /** Load validation jni on initialization. */
    static {
        System.loadLibrary(""ctscamera2_jni"");
    }

    @Override
    public void setUp() throws Exception {
        super.setUp();
        mPackageManager = mContext.getPackageManager();
        assertNotNull(""Can't get package manager"", mPackageManager);
        mListener = new NoopCameraListener();

        /**
         * Workaround for mockito and JB-MR2 incompatibility
         *
         * Avoid java.lang.IllegalArgumentException: dexcache == null
         * https://code.google.com/p/dexmaker/issues/detail?id=2
         */
        System.setProperty(""dexmaker.dexcache"", mContext.getCacheDir().toString());

        mCameraListener = spy(new BlockingStateCallback());

        mHandlerThread = new HandlerThread(TAG);
        mHandlerThread.start();
        mHandler = new Handler(mHandlerThread.getLooper());
        mCollector = new CameraErrorCollector();
        mConcurrentCameraIdCombinations =
                CameraTestUtils.getConcurrentCameraIds(mCameraManager, mAdoptShellPerm);
    }

    @Override
    public void tearDown() throws Exception {
        mHandlerThread.quitSafely();
        mHandler = null;

        try {
            mCollector.verify();
        } catch (Throwable e) {
            // When new Exception(e) is used, exception info will be printed twice.
            throw new Exception(e.getMessage());
        } finally {
            super.tearDown();
        }
    }

    /**
     * Verifies that the reason is in the range of public-only codes.
     */
    private static int checkCameraAccessExceptionReason(CameraAccessException e) {
        int reason = e.getReason();

        switch (reason) {
            case CameraAccessException.CAMERA_DISABLED:
            case CameraAccessException.CAMERA_DISCONNECTED:
            case CameraAccessException.CAMERA_ERROR:
            case CameraAccessException.CAMERA_IN_USE:
            case CameraAccessException.MAX_CAMERAS_IN_USE:
                return reason;
        }

        fail(""Invalid CameraAccessException code: "" + reason);

        return -1; // unreachable
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraManagerTest"	"testCameraManagerOpenAllCameras"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CameraManagerTest.java"	""	"public void testCameraManagerOpenAllCameras() throws Exception {
        testCameraManagerOpenAllCameras(/*useExecutor*/ false);
        testCameraManagerOpenAllCameras(/*useExecutor*/ true);
    }

    private void testCameraManagerOpenAllCameras(boolean useExecutor) throws Exception {
        String[] ids = mCameraIdsUnderTest;
        assertNotNull(""Camera ids shouldn't be null"", ids);

        // Skip test if the device doesn't have multiple cameras.
        if (ids.length <= 1) {
            return;
        }

        final Executor executor = useExecutor ? new HandlerExecutor(mHandler) : null;
        List<CameraDevice> cameraList = new ArrayList<CameraDevice>();
        List<MockStateCallback> listenerList = new ArrayList<MockStateCallback>();
        List<BlockingStateCallback> blockingListenerList = new ArrayList<BlockingStateCallback>();
        try {
            for (int i = 0; i < ids.length; i++) {
                // Ignore state changes from other cameras
                MockStateCallback mockListener = MockStateCallback.mock();
                mCameraListener = new BlockingStateCallback(mockListener);

                /**
                 * Track whether or not we got a synchronous error from openCamera.
                 *
                 * A synchronous error must also be accompanied by an asynchronous
                 * StateCallback#onError callback.
                 */
                boolean expectingError = false;

                String cameraId = ids[i];
                try {
                    if (useExecutor) {
                        mCameraManager.openCamera(cameraId, executor, mCameraListener);
                    } else {
                        mCameraManager.openCamera(cameraId, mCameraListener, mHandler);
                    }
                } catch (CameraAccessException e) {
                    int reason = checkCameraAccessExceptionReason(e);
                    if (reason == CameraAccessException.CAMERA_DISCONNECTED ||
                            reason == CameraAccessException.CAMERA_DISABLED) {
                        // TODO: We should handle a Disabled camera by passing here and elsewhere
                        fail(""Camera must not be disconnected or disabled for this test"" + ids[i]);
                    } else {
                        expectingError = true;
                    }
                }

                List<Integer> expectedStates = new ArrayList<Integer>();
                expectedStates.add(BlockingStateCallback.STATE_OPENED);
                expectedStates.add(BlockingStateCallback.STATE_ERROR);
                int state = mCameraListener.waitForAnyOfStates(
                        expectedStates, CameraTestUtils.CAMERA_IDLE_TIMEOUT_MS);

                // It's possible that we got an asynchronous error transition only. This is ok.
                if (expectingError) {
                    assertEquals(""Throwing a CAMERA_ERROR exception must be accompanied with a "" +
                            ""StateCallback#onError callback"",
                            BlockingStateCallback.STATE_ERROR, state);
                }

                /**
                 * Two situations are considered passing:
                 * 1) The camera opened successfully.
                 *     => No error must be set.
                 * 2) The camera did not open because there were too many other cameras opened.
                 *     => Only MAX_CAMERAS_IN_USE error must be set.
                 *
                 * Any other situation is considered a failure.
                 *
                 * For simplicity we treat disconnecting asynchronously as a failure, so
                 * camera devices should not be physically unplugged during this test.
                 */

                CameraDevice camera;
                if (state == BlockingStateCallback.STATE_ERROR) {
                    // Camera did not open because too many other cameras were opened
                    // => onError called exactly once with a non-null camera
                    assertTrue(""At least one camera must be opened successfully"",
                            cameraList.size() > 0);

                    ArgumentCaptor<CameraDevice> argument =
                            ArgumentCaptor.forClass(CameraDevice.class);

                    verify(mockListener)
                            .onError(
                                    argument.capture(),
                                    eq(CameraDevice.StateCallback.ERROR_MAX_CAMERAS_IN_USE)
                            );
                    verifyNoMoreInteractions(mockListener);

                    camera = argument.getValue();
                    assertNotNull(""Expected a non-null camera for the error transition for ID: ""
                            + ids[i], camera);
                } else if (state == BlockingStateCallback.STATE_OPENED) {
                    // Camera opened successfully.
                    // => onOpened called exactly once
                    camera = verifyCameraStateOpened(cameraId,
                            mockListener);
                } else {
                    fail(""Unexpected state "" + state);
                    camera = null; // unreachable. but need this for java compiler
                }

                // Keep track of cameras so we can close it later
                cameraList.add(camera);
                listenerList.add(mockListener);
                blockingListenerList.add(mCameraListener);
            }
        } finally {
            for (int i = 0; i < cameraList.size(); i++) {
                // With conflicting devices, opening of one camera could result in the other camera
                // being disconnected. To handle such case, reset the mock before close.
                reset(listenerList.get(i));
                cameraList.get(i).close();
            }
            for (BlockingStateCallback blockingListener : blockingListenerList) {
                blockingListener.waitForState(
                        BlockingStateCallback.STATE_CLOSED,
                        CameraTestUtils.CAMERA_IDLE_TIMEOUT_MS);
            }
        }

        /*
         * Ensure that no state transitions have bled through from one camera to another
         * after closing the cameras.
         */
        int i = 0;
        for (MockStateCallback listener : listenerList) {
            CameraDevice camera = cameraList.get(i);

            verify(listener).onClosed(eq(camera));
            verifyNoMoreInteractions(listener);
            i++;
            // Only a #close can happen on the camera since we were done with it.
            // Also nothing else should've happened between the close and the open.
        }
    }

    /**
     * Verifies the camera in this listener was opened and then unconfigured exactly once.
     *
     * <p>This assumes that no other action to the camera has been done (e.g.
     * it hasn't been configured, or closed, or disconnected). Verification is
     * performed immediately without any timeouts.</p>
     *
     * <p>This checks that the state has previously changed first for opened and then unconfigured.
     * Any other state transitions will fail. A test failure is thrown if verification fails.</p>
     *
     * @param cameraId Camera identifier
     * @param listener Listener which was passed to {@link CameraManager#openCamera}
     *
     * @return The camera device (non-{@code null}).
     */
    private static CameraDevice verifyCameraStateOpened(String cameraId,
            MockStateCallback listener) {
        ArgumentCaptor<CameraDevice> argument =
                ArgumentCaptor.forClass(CameraDevice.class);
        InOrder inOrder = inOrder(listener);

        /**
         * State transitions (in that order):
         *  1) onOpened
         *
         * No other transitions must occur for successful #openCamera
         */
        inOrder.verify(listener)
                .onOpened(argument.capture());

        CameraDevice camera = argument.getValue();
        assertNotNull(
                String.format(""Failed to open camera device ID: %s"", cameraId),
                camera);

        // Do not use inOrder here since that would skip anything called before onOpened
        verifyNoMoreInteractions(listener);

        return camera;
    }

    /**
     * Test: that opening the same device multiple times and make sure the right
     * error state is set.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.EncoderTest"	"testAACEncoders"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/EncoderTest.java"	""	"public void testAACEncoders() {
        LinkedList<MediaFormat> formats = new LinkedList<MediaFormat>();

        final int kAACProfiles[] = {
            2 /* OMX_AUDIO_AACObjectLC */,
            5 /* OMX_AUDIO_AACObjectHE */,
            39 /* OMX_AUDIO_AACObjectELD */
        };

        final int kSampleRates[] = { 8000, 11025, 22050, 44100, 48000 };
        final int kBitRates[] = { 64000, 128000 };

        for (int k = 0; k < kAACProfiles.length; ++k) {
            for (int i = 0; i < kSampleRates.length; ++i) {
                if (kAACProfiles[k] == 5 && kSampleRates[i] < 22050) {
                    // Is this right? HE does not support sample rates < 22050Hz?
                    continue;
                }
                for (int j = 0; j < kBitRates.length; ++j) {
                    for (int ch = 1; ch <= 2; ++ch) {
                        MediaFormat format  = new MediaFormat();
                        format.setString(MediaFormat.KEY_MIME, MediaFormat.MIMETYPE_AUDIO_AAC);

                        format.setInteger(
                                MediaFormat.KEY_AAC_PROFILE, kAACProfiles[k]);

                        format.setInteger(
                                MediaFormat.KEY_SAMPLE_RATE, kSampleRates[i]);

                        format.setInteger(MediaFormat.KEY_CHANNEL_COUNT, ch);
                        format.setInteger(MediaFormat.KEY_BIT_RATE, kBitRates[j]);
                        formats.push(format);
                    }
                }
            }
        }

        testEncoderWithFormats(MediaFormat.MIMETYPE_AUDIO_AAC, formats);
    }

    private void testEncoderWithFormatsParallel(String mime, List<MediaFormat> formats,
            List<String> componentNames, int ThreadCount) {
        int testsStarted = 0;
        int totalDurationSeconds = 0;
        ExecutorService pool = Executors.newFixedThreadPool(ThreadCount);

        for (String componentName : componentNames) {
            for (MediaFormat format : formats) {
                assertEquals(mime, format.getString(MediaFormat.KEY_MIME));
                pool.execute(new EncoderRun(componentName, format));
                int sampleRate = format.getInteger(MediaFormat.KEY_SAMPLE_RATE);
                int channelCount = format.getInteger(MediaFormat.KEY_CHANNEL_COUNT);
                int bytesQueuedPerSecond = 2 * channelCount * sampleRate;
                int durationSeconds =
                        (kNumInputBytes + bytesQueuedPerSecond - 1) / bytesQueuedPerSecond;
                totalDurationSeconds += durationSeconds * kNumEncoderTestsPerRun;
                testsStarted++;
            }
        }
        try {
            pool.shutdown();
            Log.i(TAG, ""waiting up to "" + totalDurationSeconds + "" seconds for ""
                            + testsStarted + "" sub-tests to finish"");
            assertTrue(""timed out waiting for encoder threads"",
                    pool.awaitTermination(totalDurationSeconds, TimeUnit.SECONDS));
        } catch (InterruptedException e) {
            fail(""interrupted while waiting for encoder threads"");
        }
    }

    private void testEncoderWithFormats(
            String mime, List<MediaFormat> formatList) {
        MediaFormat[] formats = formatList.toArray(new MediaFormat[formatList.size()]);
        String[] componentNames = MediaUtils.getEncoderNames(formats);
        if (componentNames.length == 0) {
            MediaUtils.skipTest(""no encoders found for "" + Arrays.toString(formats));
            return;
        }

        final int ThreadPoolCount = 3;
        List<String>[] componentNamesGrouped = new List[ThreadPoolCount];
        for (int i = 0; i < ThreadPoolCount; i++) {
            componentNamesGrouped[i] = new ArrayList<>();
        }
        for (String componentName : componentNames) {
            MediaCodec codec = null;
            try {
                codec = MediaCodec.createByCodecName(componentName);
                MediaCodecInfo info = codec.getCodecInfo();
                MediaCodecInfo.CodecCapabilities cap = info.getCapabilitiesForType(mime);
                int instances = Math.min(cap.getMaxSupportedInstances(), ThreadPoolCount);
                assertTrue(instances >= 1 && instances <= ThreadPoolCount);
                componentNamesGrouped[instances - 1].add(componentName);
            } catch (Exception e) {
                fail(""codec '"" + componentName + ""' failed construction."");
            } finally {
                codec.release();
            }
        }
        for (int i = 0; i < ThreadPoolCount; i++) {
            if (componentNamesGrouped[i].size() > 0) {
                testEncoderWithFormatsParallel(mime, formatList, componentNamesGrouped[i], i + 1);
            }
        }
    }

    // See bug 25843966
    private static long[] mBadSeeds = {
            101833462733980l, // fail @ 23680 in all-random mode
            273262699095706l, // fail @ 58880 in all-random mode
            137295510492957l, // fail @ 35840 in zero-lead mode
            57821391502855l,  // fail @ 32000 in zero-lead mode
    };

    private int queueInputBuffer(
            MediaCodec codec, ByteBuffer[] inputBuffers, int index,
            InputStream istream, int mode, long timeUs, Random random) {
        ByteBuffer buffer = inputBuffers[index];
        buffer.rewind();
        int size = buffer.limit();

        if ((mode & MODE_RESOURCE) != 0 && istream != null) {
            while (buffer.hasRemaining()) {
                try {
                    int next = istream.read();
                    if (next < 0) {
                        break;
                    }
                    buffer.put((byte) next);
                } catch (Exception ex) {
                    Log.i(TAG, ""caught exception writing: "" + ex);
                    break;
                }
            }
        } else if ((mode & MODE_RANDOM) != 0) {
            if ((mode & MODE_SILENTLEAD) != 0) {
                buffer.putInt(0);
                buffer.putInt(0);
                buffer.putInt(0);
                buffer.putInt(0);
            }
            while (true) {
                try {
                    int next = random.nextInt();
                    buffer.putInt(random.nextInt());
                } catch (BufferOverflowException ex) {
                    break;
                }
            }
        } else {
            byte[] zeroes = new byte[size];
            buffer.put(zeroes);
        }

        if ((mode & MODE_QUIET) != 0) {
            int n = buffer.limit();
            for (int i = 0; i < n; i += 2) {
                short s = buffer.getShort(i);
                s /= 8;
                buffer.putShort(i, s);
            }
        }

        codec.queueInputBuffer(index, 0 /* offset */, size, timeUs, 0 /* flags */);

        return size;
    }

    private void dequeueOutputBuffer(
            MediaCodec codec, ByteBuffer[] outputBuffers,
            int index, MediaCodec.BufferInfo info) {
        codec.releaseOutputBuffer(index, false /* render */);
    }

    class EncoderRun implements Runnable {
        String mComponentName;
        MediaFormat mFormat;

        EncoderRun(String componentName, MediaFormat format) {
            mComponentName = componentName;
            mFormat = format;
        }
        @Override
        public void run() {
            try {
                testEncoder(mComponentName, mFormat);
            } catch (FileNotFoundException e) {
                e.printStackTrace();
                fail(""Received exception "" + e);
            }
        }
    }

    // Number of tests called in testEncoder(String componentName, MediaFormat format)
    private static int kNumEncoderTestsPerRun = 5 + mBadSeeds.length * 2;
    private void testEncoder(String componentName, MediaFormat format)
            throws FileNotFoundException {
        Log.i(TAG, ""testEncoder "" + componentName + ""/"" + format);
        // test with all zeroes/silence
        testEncoder(componentName, format, 0, null, MODE_SILENT);

        // test with pcm input file
        testEncoder(componentName, format, 0, ""okgoogle123_good.wav"", MODE_RESOURCE);
        testEncoder(componentName, format, 0, ""okgoogle123_good.wav"", MODE_RESOURCE | MODE_QUIET);
        testEncoder(componentName, format, 0, ""tones.wav"", MODE_RESOURCE);
        testEncoder(componentName, format, 0, ""tones.wav"", MODE_RESOURCE | MODE_QUIET);

        // test with random data, with and without a few leading zeroes
        for (int i = 0; i < mBadSeeds.length; i++) {
            testEncoder(componentName, format, mBadSeeds[i], null, MODE_RANDOM);
            testEncoder(componentName, format, mBadSeeds[i], null, MODE_RANDOM | MODE_SILENTLEAD);
        }
    }

    private void testEncoder(String componentName, MediaFormat format,
            long startSeed, final String res, int mode) throws FileNotFoundException {

        Log.i(TAG, ""testEncoder "" + componentName + ""/"" + mode + ""/"" + format);
        int sampleRate = format.getInteger(MediaFormat.KEY_SAMPLE_RATE);
        int channelCount = format.getInteger(MediaFormat.KEY_CHANNEL_COUNT);
        int inBitrate = sampleRate * channelCount * 16;  // bit/sec
        int outBitrate = format.getInteger(MediaFormat.KEY_BIT_RATE);

        MediaMuxer muxer = null;
        int muxidx = -1;
        if (sSaveResults) {
            try {
                String outFile = ""/data/local/tmp/transcoded-"" + componentName +
                        ""-"" + sampleRate + ""Hz-"" + channelCount + ""ch-"" + outBitrate +
                        ""bps-"" + mode + ""-"" + res + ""-"" + startSeed + ""-"" +
                        (android.os.Process.is64Bit() ? ""64bit"" : ""32bit"") + "".mp4"";
                new File(outFile).delete();
                muxer = new MediaMuxer(outFile, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
                // The track can't be added until we have the codec specific data
            } catch (Exception e) {
                Log.i(TAG, ""couldn't create muxer: "" + e);
            }
        }

        InputStream istream = null;
        if ((mode & MODE_RESOURCE) != 0) {
            Preconditions.assertTestFileExists(mInpPrefix + res);
            istream = new FileInputStream(mInpPrefix + res);
        }

        Random random = new Random(startSeed);
        MediaCodec codec;
        try {
            codec = MediaCodec.createByCodecName(componentName);
            String mime = format.getString(MediaFormat.KEY_MIME);
            MediaCodecInfo codecInfo = codec.getCodecInfo();
            MediaCodecInfo.CodecCapabilities caps = codecInfo.getCapabilitiesForType(mime);
            if (!caps.isFormatSupported(format)) {
                codec.release();
                codec = null;
                assertFalse(
                    ""Default codec doesn't support "" + format.toString(),
                    isDefaultCodec(componentName, mime));
                MediaUtils.skipTest(componentName + "" doesn't support "" + format.toString());
                return;
            }
        } catch (Exception e) {
            fail(""codec '"" + componentName + ""' failed construction."");
            return; /* does not get here, but avoids warning */
        }
        try {
            codec.configure(
                    format,
                    null /* surface */,
                    null /* crypto */,
                    MediaCodec.CONFIGURE_FLAG_ENCODE);
        } catch (IllegalStateException e) {
            fail(""codec '"" + componentName + ""' failed configuration."");
        }

        codec.start();
        ByteBuffer[] codecInputBuffers = codec.getInputBuffers();
        ByteBuffer[] codecOutputBuffers = codec.getOutputBuffers();

        int numBytesSubmitted = 0;
        boolean doneSubmittingInput = false;
        int numBytesDequeued = 0;

        while (true) {
            int index;

            if (!doneSubmittingInput) {
                index = codec.dequeueInputBuffer(kTimeoutUs /* timeoutUs */);

                if (index != MediaCodec.INFO_TRY_AGAIN_LATER) {
                    long timeUs =
                            (long)numBytesSubmitted * 1000000 / (2 * channelCount * sampleRate);
                    if (numBytesSubmitted >= kNumInputBytes) {
                        codec.queueInputBuffer(
                                index,
                                0 /* offset */,
                                0 /* size */,
                                timeUs,
                                MediaCodec.BUFFER_FLAG_END_OF_STREAM);

                        if (VERBOSE) {
                            Log.d(TAG, ""queued input EOS."");
                        }

                        doneSubmittingInput = true;
                    } else {
                        int size = queueInputBuffer(
                                codec, codecInputBuffers, index, istream, mode, timeUs, random);

                        numBytesSubmitted += size;

                        if (VERBOSE) {
                            Log.d(TAG, ""queued "" + size + "" bytes of input data."");
                        }
                    }
                }
            }

            MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
            index = codec.dequeueOutputBuffer(info, kTimeoutUs /* timeoutUs */);

            if (index == MediaCodec.INFO_TRY_AGAIN_LATER) {
            } else if (index == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
            } else if (index == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                codecOutputBuffers = codec.getOutputBuffers();
            } else {
                if (muxer != null) {
                    ByteBuffer buffer = codec.getOutputBuffer(index);
                    if (muxidx < 0) {
                        MediaFormat trackFormat = codec.getOutputFormat();
                        muxidx = muxer.addTrack(trackFormat);
                        muxer.start();
                    }
                    muxer.writeSampleData(muxidx, buffer, info);
                }

                dequeueOutputBuffer(codec, codecOutputBuffers, index, info);

                numBytesDequeued += info.size;

                if ((info.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                    if (VERBOSE) {
                        Log.d(TAG, ""dequeued output EOS."");
                    }
                    break;
                }

                if (VERBOSE) {
                    Log.d(TAG, ""dequeued "" + info.size + "" bytes of output data."");
                }
            }
        }

        if (VERBOSE) {
            Log.d(TAG, ""queued a total of "" + numBytesSubmitted + ""bytes, ""
                    + ""dequeued "" + numBytesDequeued + "" bytes."");
        }

        float desiredRatio = (float)outBitrate / (float)inBitrate;
        float actualRatio = (float)numBytesDequeued / (float)numBytesSubmitted;

        if (actualRatio < 0.9 * desiredRatio || actualRatio > 1.1 * desiredRatio) {
            Log.w(TAG, ""desiredRatio = "" + desiredRatio
                    + "", actualRatio = "" + actualRatio);
        }

        codec.release();
        codec = null;
        if (muxer != null) {
            muxer.stop();
            muxer.release();
            muxer = null;
        }
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.security.net.config.cts.DownloadManagerTest"	"testConfigTrustedCaAccepted"	"CtsNetSecConfigDownloadManagerTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/networksecurityconfig/networksecurityconfig-downloadmanager/src/android/security/net/config/cts/DownloadManagerTest.java"	""	"public void testConfigTrustedCaAccepted() throws Exception {
        SSLServerSocket serverSocket = bindTLSServer(R.raw.valid_chain, R.raw.test_key);
        runDownloadManagerTest(serverSocket, true);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.NativeDecoderTest"	"SKIP_testExtractor"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/NativeDecoderTest.java"	""	"public void test/*
 *.
 */

package android.media.cts;

import android.content.res.AssetFileDescriptor;
import android.media.MediaCodec;
import android.media.MediaCodec.BufferInfo;
import android.media.MediaExtractor;
import android.media.MediaFormat;
import android.media.MediaPlayer;
import android.media.cts.TestUtils.Monitor;
import android.net.Uri;
import android.os.Build;
import android.os.ParcelFileDescriptor;
import android.platform.test.annotations.AppModeFull;
import android.platform.test.annotations.Presubmit;
import android.platform.test.annotations.RequiresDevice;
import android.util.Log;
import android.view.Surface;
import android.webkit.cts.CtsTestServer;

import androidx.test.filters.SmallTest;

import com.android.compatibility.common.util.ApiLevelUtil;
import com.android.compatibility.common.util.MediaUtils;

import org.apache.http.Header;
import org.apache.http.HttpRequest;
import org.apache.http.impl.DefaultHttpServerConnection;
import org.apache.http.impl.io.SocketOutputBuffer;
import org.apache.http.io.SessionOutputBuffer;
import org.apache.http.params.HttpParams;
import org.apache.http.util.CharArrayBuffer;

import java.io.File;
import java.io.FileDescriptor;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.net.Socket;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.UUID;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;
import java.util.zip.Adler32;

@SmallTest
@RequiresDevice
@AppModeFull(reason = ""TODO: evaluate and port to instant"")
public class NativeDecoderTest extends MediaPlayerTestBase {
    private static final String TAG = ""DecoderTest"";

    private static final int RESET_MODE_NONE = 0;
    private static final int RESET_MODE_RECONFIGURE = 1;
    private static final int RESET_MODE_FLUSH = 2;
    private static final int RESET_MODE_EOS_FLUSH = 3;

    private static final String[] CSD_KEYS = new String[] { ""csd-0"", ""csd-1"" };

    private static final int CONFIG_MODE_NONE = 0;
    private static final int CONFIG_MODE_QUEUE = 1;

    private static boolean sIsAtLeastS = ApiLevelUtil.isAtLeast(Build.VERSION_CODES.S);

    static final String mInpPrefix = WorkDir.getMediaDirString();
    short[] mMasterBuffer;

    /** Load jni on initialization */
    static {
        Log.i(""@@@"", ""before loadlibrary"");
        System.loadLibrary(""ctsmediacodec_jni"");
        Log.i(""@@@"", ""after loadlibrary"");
    }

    @Override
    protected void setUp() throws Exception {
        super.setUp();

    }

    // check that native extractor behavior matches java extractor

    private void compareArrays(String message, int[] a1, int[] a2) {
        if (a1 == a2) {
            return;
        }

        assertNotNull(message + "": array 1 is null"", a1);
        assertNotNull(message + "": array 2 is null"", a2);

        assertEquals(message + "": arraylengths differ"", a1.length, a2.length);
        int length = a1.length;

        for (int i = 0; i < length; i++)
            if (a1[i] != a2[i]) {
                Log.i(""@@@@"", Arrays.toString(a1));
                Log.i(""@@@@"", Arrays.toString(a2));
                fail(message + "": at index "" + i);
            }
    }

    public void SKIP_testExtractor() throws Exception {
        // duplicate of CtsMediaV2TestCases:ExtractorTest$FunctionalityTest#testExtract where
        // checksum is computed over track format attributes, track buffer and buffer
        // info in both SDK and NDK side and checked for equality
        testExtractor(""sinesweepogg.ogg"");
        testExtractor(""sinesweepoggmkv.mkv"");
        testExtractor(""sinesweepoggmp4.mp4"");
        testExtractor(""sinesweepmp3lame.mp3"");
        testExtractor(""sinesweepmp3smpb.mp3"");
        testExtractor(""sinesweepopus.mkv"");
        testExtractor(""sinesweepopusmp4.mp4"");
        testExtractor(""sinesweepm4a.m4a"");
        testExtractor(""sinesweepflacmkv.mkv"");
        testExtractor(""sinesweepflac.flac"");
        testExtractor(""sinesweepflacmp4.mp4"");
        testExtractor(""sinesweepwav.wav"");

        testExtractor(""video_1280x720_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"");
        testExtractor(""bbb_s3_1280x720_webm_vp8_8mbps_60fps_opus_6ch_384kbps_48000hz.webm"");
        testExtractor(""bbb_s4_1280x720_webm_vp9_0p31_4mbps_30fps_opus_stereo_128kbps_48000hz.webm"");
        testExtractor(""video_1280x720_webm_av1_2000kbps_30fps_vorbis_stereo_128kbps_48000hz.webm"");
        testExtractor(""video_176x144_3gp_h263_300kbps_12fps_aac_mono_24kbps_11025hz.3gp"");
        testExtractor(""video_480x360_mp4_mpeg2_1500kbps_30fps_aac_stereo_128kbps_48000hz.mp4"");
        testExtractor(""video_480x360_mp4_mpeg4_860kbps_25fps_aac_stereo_128kbps_44100hz.mp4"");

        CtsTestServer foo = new CtsTestServer(mContext);
        testExtractor(foo.getAssetUrl(""noiseandchirps.ogg""), null, null);
        testExtractor(foo.getAssetUrl(""ringer.mp3""), null, null);
        testExtractor(foo.getRedirectingAssetUrl(""ringer.mp3""), null, null);

        String[] keys = new String[] {""header0"", ""header1""};
        String[] values = new String[] {""value0"", ""value1""};
        testExtractor(foo.getAssetUrl(""noiseandchirps.ogg""), keys, values);
        HttpRequest req = foo.getLastRequest(""noiseandchirps.ogg"");
        for (int i = 0; i < keys.length; i++) {
            String key = keys[i];
            String value = values[i];
            Header[] header = req.getHeaders(key);
            assertTrue(""expecting "" + key + "":"" + value + "", saw "" + Arrays.toString(header),
                    header.length == 1 && header[0].getValue().equals(value));
        }

        String[] emptyArray = new String[0];
        testExtractor(foo.getAssetUrl(""noiseandchirps.ogg""), emptyArray, emptyArray);
    }

    /**
     * |keys| and |values| should be arrays of the same length.
     *
     * If keys or values is null, test {@link MediaExtractor#setDataSource(String)}
     * and NDK counter part, i.e. set data source without headers.
     *
     * If keys or values is zero length, test {@link MediaExtractor#setDataSource(String, Map))}
     * and NDK counter part with null headers.
     *
     */
    private void testExtractor(String path, String[] keys, String[] values) throws Exception {
        int[] jsizes = getSampleSizes(path, keys, values);
        int[] nsizes = getSampleSizesNativePath(path, keys, values, /* testNativeSource = */ false);
        int[] nsizes2 = getSampleSizesNativePath(path, keys, values, /* testNativeSource = */ true);

        compareArrays(""different samplesizes"", jsizes, nsizes);
        compareArrays(""different samplesizes native source"", jsizes, nsizes2);
    }

    protected static AssetFileDescriptor getAssetFileDescriptorFor(final String res)
            throws FileNotFoundException {
        Preconditions.assertTestFileExists(mInpPrefix + res);
        File inpFile = new File(mInpPrefix + res);
        ParcelFileDescriptor parcelFD =
                ParcelFileDescriptor.open(inpFile, ParcelFileDescriptor.MODE_READ_ONLY);
        return new AssetFileDescriptor(parcelFD, 0, parcelFD.getStatSize());
    }

    private void testExtractor(final String res) throws Exception {
        AssetFileDescriptor fd = getAssetFileDescriptorFor(res);

        int[] jsizes = getSampleSizes(
                fd.getFileDescriptor(), fd.getStartOffset(), fd.getLength());
        int[] nsizes = getSampleSizesNative(
                fd.getParcelFileDescriptor().getFd(), fd.getStartOffset(), fd.getLength());

        fd.close();
        compareArrays(""different samples"", jsizes, nsizes);
    }

    private static int[] getSampleSizes(String path, String[] keys, String[] values) throws IOException {
        MediaExtractor ex = new MediaExtractor();
        if (keys == null || values == null) {
            ex.setDataSource(path);
        } else {
            Map<String, String> headers = null;
            int numheaders = Math.min(keys.length, values.length);
            for (int i = 0; i < numheaders; i++) {
                if (headers == null) {
                    headers = new HashMap<>();
                }
                String key = keys[i];
                String value = values[i];
                headers.put(key, value);
            }
            ex.setDataSource(path, headers);
        }

        return getSampleSizes(ex);
    }

    private static int[] getSampleSizes(FileDescriptor fd, long offset, long size)
            throws IOException {
        MediaExtractor ex = new MediaExtractor();
        ex.setDataSource(fd, offset, size);
        return getSampleSizes(ex);
    }

    private static int[] getSampleSizes(MediaExtractor ex) {
        ArrayList<Integer> foo = new ArrayList<Integer>();
        ByteBuffer buf = ByteBuffer.allocate(1024*1024);
        int numtracks = ex.getTrackCount();
        assertTrue(""no tracks"", numtracks > 0);
        foo.add(numtracks);
        for (int i = 0; i < numtracks; i++) {
            MediaFormat format = ex.getTrackFormat(i);
            String mime = format.getString(MediaFormat.KEY_MIME);
            if (mime.startsWith(""audio/"")) {
                foo.add(0);
                foo.add(format.getInteger(MediaFormat.KEY_SAMPLE_RATE));
                foo.add(format.getInteger(MediaFormat.KEY_CHANNEL_COUNT));
                foo.add((int)format.getLong(MediaFormat.KEY_DURATION));
            } else if (mime.startsWith(""video/"")) {
                foo.add(1);
                foo.add(format.getInteger(MediaFormat.KEY_WIDTH));
                foo.add(format.getInteger(MediaFormat.KEY_HEIGHT));
                foo.add((int)format.getLong(MediaFormat.KEY_DURATION));
            } else {
                fail(""unexpected mime type: "" + mime);
            }
            ex.selectTrack(i);
        }
        while(true) {
            int n = ex.readSampleData(buf, 0);
            if (n < 0) {
                break;
            }
            foo.add(n);
            foo.add(ex.getSampleTrackIndex());
            foo.add(ex.getSampleFlags());
            foo.add((int)ex.getSampleTime()); // just the low bits should be OK
            byte foobar[] = new byte[n];
            buf.get(foobar, 0, n);
            foo.add((int)adler32(foobar));
            ex.advance();
        }

        int [] ret = new int[foo.size()];
        for (int i = 0; i < ret.length; i++) {
            ret[i] = foo.get(i);
        }
        return ret;
    }

    private static native int[] getSampleSizesNative(int fd, long offset, long size);
    private static native int[] getSampleSizesNativePath(
            String path, String[] keys, String[] values, boolean testNativeSource);

    @Presubmit
    public void SKIP_testExtractorFileDurationNative() throws Exception {
        // duplicate of CtsMediaV2TestCases:ExtractorTest$FunctionalityTest#testExtract where
        // checksum is computed over track format attributes, track buffer and buffer
        // info in both SDK and NDK side and checked for equality. KEY_DURATION for each track is
        // part of the checksum.
        testExtractorFileDurationNative(
                ""video_1280x720_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"");
    }

    private void testExtractorFileDurationNative(final String res) throws Exception {
        AssetFileDescriptor fd = getAssetFileDescriptorFor(res);
        long durationUs = getExtractorFileDurationNative(
                fd.getParcelFileDescriptor().getFd(), fd.getStartOffset(), fd.getLength());

        MediaExtractor ex = new MediaExtractor();
        ex.setDataSource(fd.getFileDescriptor(), fd.getStartOffset(), fd.getLength());

        int numtracks = ex.getTrackCount();
        long aDurationUs = -1, vDurationUs = -1;
        for (int i = 0; i < numtracks; i++) {
            MediaFormat format = ex.getTrackFormat(i);
            String mime = format.getString(MediaFormat.KEY_MIME);
            if (mime.startsWith(""audio/"")) {
                aDurationUs = format.getLong(MediaFormat.KEY_DURATION);
            } else if (mime.startsWith(""video/"")) {
                vDurationUs = format.getLong(MediaFormat.KEY_DURATION);
            }
        }

        assertTrue(""duration inconsistency"",
                durationUs < 0 || durationUs >= aDurationUs && durationUs >= vDurationUs);

    }

    private static native long getExtractorFileDurationNative(int fd, long offset, long size);

    @Presubmit
    public void SKIP_testExtractorCachedDurationNative() throws Exception {
        // duplicate of CtsMediaV2TestCases:ExtractorTest$SetDataSourceTest#testDataSourceNative
        CtsTestServer foo = new CtsTestServer(mContext);
        String url = foo.getAssetUrl(""ringer.mp3"");
        long cachedDurationUs = getExtractorCachedDurationNative(url, /* testNativeSource = */ false);
        assertTrue(""cached duration negative"", cachedDurationUs >= 0);
        cachedDurationUs = getExtractorCachedDurationNative(url, /* testNativeSource = */ true);
        assertTrue(""cached duration negative native source"", cachedDurationUs >= 0);
    }

    private static native long getExtractorCachedDurationNative(String uri, boolean testNativeSource);

    public void SKIP_testDecoder() throws Exception {
        // duplicate of CtsMediaV2TestCases:CodecDecoderTest#testSimpleDecode where checksum  is
        // computed over decoded output in both SDK and NDK side and checked for equality.
        int testsRun =
            testDecoder(""sinesweepogg.ogg"") +
            testDecoder(""sinesweepoggmkv.mkv"") +
            testDecoder(""sinesweepoggmp4.mp4"") +
            testDecoder(""sinesweepmp3lame.mp3"") +
            testDecoder(""sinesweepmp3smpb.mp3"") +
            testDecoder(""sinesweepopus.mkv"") +
            testDecoder(""sinesweepopusmp4.mp4"") +
            testDecoder(""sinesweepm4a.m4a"") +
            testDecoder(""sinesweepflacmkv.mkv"") +
            testDecoder(""sinesweepflac.flac"") +
            testDecoder(""sinesweepflacmp4.mp4"") +
            testDecoder(""sinesweepwav.wav"") +
            testDecoder(""video_1280x720_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"") +
            testDecoder(""bbb_s1_640x360_webm_vp8_2mbps_30fps_vorbis_5ch_320kbps_48000hz.webm"") +
            testDecoder(""bbb_s1_640x360_webm_vp9_0p21_1600kbps_30fps_vorbis_stereo_128kbps_48000hz.webm"") +
            testDecoder(""video_176x144_3gp_h263_300kbps_12fps_aac_mono_24kbps_11025hz.3gp"") +
            testDecoder(""video_480x360_mp4_mpeg2_1500kbps_30fps_aac_stereo_128kbps_48000hz.mp4"");
            testDecoder(""video_480x360_mp4_mpeg4_860kbps_25fps_aac_stereo_128kbps_44100hz.mp4"");
        if (testsRun == 0) {
            MediaUtils.skipTest(""no decoders found"");
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.NativeDecoderTest"	"testDataSource"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/NativeDecoderTest.java"	""	"public void testDataSource() throws Exception {
        int testsRun = testDecoder(
                ""video_176x144_3gp_h263_300kbps_12fps_aac_mono_24kbps_11025hz.3gp"", /* wrapFd */
                true, /* useCallback */ false);
        if (testsRun == 0) {
            MediaUtils.skipTest(""no decoders found"");
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.NativeDecoderTest"	"testDataSourceAudioOnly"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/NativeDecoderTest.java"	""	"public void testDataSourceAudioOnly() throws Exception {
        int testsRun = testDecoder(
                ""loudsoftmp3.mp3"",
                /* wrapFd */ true, /* useCallback */ false) +
                testDecoder(
                        ""loudsoftaac.aac"",
                        /* wrapFd */ false, /* useCallback */ false);
        if (testsRun == 0) {
            MediaUtils.skipTest(""no decoders found"");
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.NativeDecoderTest"	"testDataSourceWithCallback"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/NativeDecoderTest.java"	""	"public void testDataSourceWithCallback() throws Exception {
        int testsRun = testDecoder(
                ""video_176x144_3gp_h263_300kbps_12fps_aac_mono_24kbps_11025hz.3gp"",/* wrapFd */
                true, /* useCallback */ true);
        if (testsRun == 0) {
            MediaUtils.skipTest(""no decoders found"");
        }
    }

    private int testDecoder(final String res) throws Exception {
        return testDecoder(res, /* wrapFd */ false, /* useCallback */ false);
    }

    private int testDecoder(final String res, boolean wrapFd, boolean useCallback)
            throws Exception {
        Preconditions.assertTestFileExists(mInpPrefix + res);
        if (!MediaUtils.hasCodecsForResource(mInpPrefix  + res)) {
            return 0; // skip
        }

        AssetFileDescriptor fd = getAssetFileDescriptorFor(res);

        int[] jdata1 = getDecodedData(
                fd.getFileDescriptor(), fd.getStartOffset(), fd.getLength());
        int[] jdata2 = getDecodedData(
                fd.getFileDescriptor(), fd.getStartOffset(), fd.getLength());
        int[] ndata1 = getDecodedDataNative(
                fd.getParcelFileDescriptor().getFd(), fd.getStartOffset(), fd.getLength(), wrapFd,
                useCallback);
        int[] ndata2 = getDecodedDataNative(
                fd.getParcelFileDescriptor().getFd(), fd.getStartOffset(), fd.getLength(), wrapFd,
                useCallback);

        fd.close();
        compareArrays(""inconsistent java decoder"", jdata1, jdata2);
        compareArrays(""inconsistent native decoder"", ndata1, ndata2);
        compareArrays(""different decoded data"", jdata1, ndata1);
        return 1;
    }

    private static int[] getDecodedData(FileDescriptor fd, long offset, long size)
            throws IOException {
        MediaExtractor ex = new MediaExtractor();
        ex.setDataSource(fd, offset, size);
        return getDecodedData(ex);
    }
    private static int[] getDecodedData(MediaExtractor ex) throws IOException {
        int numtracks = ex.getTrackCount();
        assertTrue(""no tracks"", numtracks > 0);
        ArrayList<Integer>[] trackdata = new ArrayList[numtracks];
        MediaCodec[] codec = new MediaCodec[numtracks];
        MediaFormat[] format = new MediaFormat[numtracks];
        ByteBuffer[][] inbuffers = new ByteBuffer[numtracks][];
        ByteBuffer[][] outbuffers = new ByteBuffer[numtracks][];
        for (int i = 0; i < numtracks; i++) {
            format[i] = ex.getTrackFormat(i);
            String mime = format[i].getString(MediaFormat.KEY_MIME);
            if (mime.startsWith(""audio/"") || mime.startsWith(""video/"")) {
                codec[i] = MediaCodec.createDecoderByType(mime);
                codec[i].configure(format[i], null, null, 0);
                codec[i].start();
                inbuffers[i] = codec[i].getInputBuffers();
                outbuffers[i] = codec[i].getOutputBuffers();
                trackdata[i] = new ArrayList<Integer>();
            } else {
                fail(""unexpected mime type: "" + mime);
            }
            ex.selectTrack(i);
        }

        boolean[] sawInputEOS = new boolean[numtracks];
        boolean[] sawOutputEOS = new boolean[numtracks];
        int eosCount = 0;
        BufferInfo info = new BufferInfo();
        while(eosCount < numtracks) {
            int t = ex.getSampleTrackIndex();
            if (t >= 0) {
                assertFalse(""saw input EOS twice"", sawInputEOS[t]);
                int bufidx = codec[t].dequeueInputBuffer(5000);
                if (bufidx >= 0) {
                    Log.i(""@@@@"", ""track "" + t + "" buffer "" + bufidx);
                    ByteBuffer buf = inbuffers[t][bufidx];
                    int sampleSize = ex.readSampleData(buf, 0);
                    Log.i(""@@@@"", ""read "" + sampleSize + "" @ "" + ex.getSampleTime());
                    if (sampleSize < 0) {
                        sampleSize = 0;
                        sawInputEOS[t] = true;
                        Log.i(""@@@@"", ""EOS"");
                        //break;
                    }
                    long presentationTimeUs = ex.getSampleTime();

                    codec[t].queueInputBuffer(bufidx, 0, sampleSize, presentationTimeUs,
                            sawInputEOS[t] ? MediaCodec.BUFFER_FLAG_END_OF_STREAM : 0);
                    ex.advance();
                }
            } else {
                Log.i(""@@@@"", ""no more input samples"");
                for (int tt = 0; tt < codec.length; tt++) {
                    if (!sawInputEOS[tt]) {
                        // we ran out of samples without ever signaling EOS to the codec,
                        // so do that now
                        int bufidx = codec[tt].dequeueInputBuffer(5000);
                        if (bufidx >= 0) {
                            codec[tt].queueInputBuffer(bufidx, 0, 0, 0,
                                    MediaCodec.BUFFER_FLAG_END_OF_STREAM);
                            sawInputEOS[tt] = true;
                        }
                    }
                }
            }

            // see if any of the codecs have data available
            for (int tt = 0; tt < codec.length; tt++) {
                if (!sawOutputEOS[tt]) {
                    int status = codec[tt].dequeueOutputBuffer(info, 1);
                    if (status >= 0) {
                        if ((info.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                            Log.i(""@@@@"", ""EOS on track "" + tt);
                            sawOutputEOS[tt] = true;
                            eosCount++;
                        }
                        Log.i(""@@@@"", ""got decoded buffer for track "" + tt + "", size "" + info.size);
                        if (info.size > 0) {
                            addSampleData(trackdata[tt], outbuffers[tt][status], info.size, format[tt]);
                        }
                        codec[tt].releaseOutputBuffer(status, false);
                    } else if (status == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                        Log.i(""@@@@"", ""output buffers changed for track "" + tt);
                        outbuffers[tt] = codec[tt].getOutputBuffers();
                    } else if (status == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                        format[tt] = codec[tt].getOutputFormat();
                        Log.i(""@@@@"", ""format changed for track "" + t + "": "" + format[tt].toString());
                    } else if (status == MediaCodec.INFO_TRY_AGAIN_LATER) {
                        Log.i(""@@@@"", ""no buffer right now for track "" + tt);
                    } else {
                        Log.i(""@@@@"", ""unexpected info code for track "" + tt + "": "" + status);
                    }
                } else {
                    Log.i(""@@@@"", ""already at EOS on track "" + tt);
                }
            }
        }

        int totalsize = 0;
        for (int i = 0; i < numtracks; i++) {
            totalsize += trackdata[i].size();
        }
        int[] trackbytes = new int[totalsize];
        int idx = 0;
        for (int i = 0; i < numtracks; i++) {
            ArrayList<Integer> src = trackdata[i];
            int tracksize = src.size();
            for (int j = 0; j < tracksize; j++) {
                trackbytes[idx++] = src.get(j);
            }
        }

        for (int i = 0; i < codec.length; i++) {
            codec[i].release();
        }

        return trackbytes;
    }

    static void addSampleData(ArrayList<Integer> dst,
            ByteBuffer buf, int size, MediaFormat format) throws IOException{

        Log.i(""@@@"", ""addsample "" + dst.size() + ""/"" + size);
        int width = format.getInteger(MediaFormat.KEY_WIDTH, size);
        int stride = format.getInteger(MediaFormat.KEY_STRIDE, width);
        int height = format.getInteger(MediaFormat.KEY_HEIGHT, 1);
        byte[] bb = new byte[width * height];
        int offset = buf.position();
        for (int i = 0; i < height; i++) {
            buf.position(i * stride + offset);
            buf.get(bb, i * width, width);
        }
        // bb is filled with data
        long sum = adler32(bb);
        dst.add( (int) (sum & 0xffffffff));
    }

    private final static Adler32 checksummer = new Adler32(); 
    // simple checksum computed over every decoded buffer
    static int adler32(byte[] input) {
        checksummer.reset();
        checksummer.update(input);
        int ret = (int) checksummer.getValue();
        Log.i(""@@@"", ""adler "" + input.length + ""/"" + ret);
        return ret;
    }

    private static native int[] getDecodedDataNative(int fd, long offset, long size, boolean wrapFd,
            boolean useCallback)
            throws IOException;

    public void SKIP_testVideoPlayback() throws Exception {
        // duplicate of
        // CtsMediaV2TestCases:CodecDecoderSurfaceTest#testSimpleDecodeToSurfaceNative[*]
        int testsRun =
            testVideoPlayback(
                    ""video_1280x720_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"") +
            testVideoPlayback(
                    ""bbb_s1_640x360_webm_vp8_2mbps_30fps_vorbis_5ch_320kbps_48000hz.webm"") +
            testVideoPlayback(
                    ""bbb_s1_640x360_webm_vp9_0p21_1600kbps_30fps_vorbis_stereo_128kbps_48000hz.webm"") +
            testVideoPlayback(
                    ""video_640x360_webm_av1_470kbps_30fps_vorbis_stereo_128kbps_48000hz.webm"") +
            testVideoPlayback(
                    ""video_176x144_3gp_h263_300kbps_12fps_aac_mono_24kbps_11025hz.3gp"") +
            testVideoPlayback(
                    ""video_176x144_mp4_mpeg2_105kbps_25fps_aac_stereo_128kbps_44100hz.mp4"") +
            testVideoPlayback(
                    ""video_480x360_mp4_mpeg4_860kbps_25fps_aac_stereo_128kbps_44100hz.mp4"");
        if (testsRun == 0) {
            MediaUtils.skipTest(""no decoders found"");
        }
    }

    private int testVideoPlayback(final String res) throws Exception {
        Preconditions.assertTestFileExists(mInpPrefix + res);
        if (!MediaUtils.checkCodecsForResource(mInpPrefix + res)) {
            return 0; // skip
        }

        AssetFileDescriptor fd = getAssetFileDescriptorFor(res);

        boolean ret = testPlaybackNative(mActivity.getSurfaceHolder().getSurface(),
                fd.getParcelFileDescriptor().getFd(), fd.getStartOffset(), fd.getLength());
        assertTrue(""native playback error"", ret);
        return 1;
    }

    private static native boolean testPlaybackNative(Surface surface,
            int fd, long startOffset, long length);

    @Presubmit
    @NonMediaMainlineTest"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.NativeDecoderTest"	"testMuxerAvc"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/NativeDecoderTest.java"	""	"public void testMuxerAvc() throws Exception {
        // IMPORTANT: this file must not have B-frames
        testMuxer(""video_1280x720_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"", false);
    }

    @NonMediaMainlineTest"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.NativeDecoderTest"	"testMuxerH263"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/NativeDecoderTest.java"	""	"public void testMuxerH263() throws Exception {
        // IMPORTANT: this file must not have B-frames
        testMuxer(""video_176x144_3gp_h263_300kbps_25fps_aac_stereo_128kbps_11025hz.3gp"", false);
    }

    @NonMediaMainlineTest"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.NativeDecoderTest"	"testMuxerVp9Hdr"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/NativeDecoderTest.java"	""	"public void testMuxerVp9Hdr() throws Exception {
        testMuxer(""video_256x144_webm_vp9_hdr_83kbps_24fps.webm"", true);
    }

    // We do not support MPEG-2 muxing as of yet
    public void SKIP_testMuxerMpeg2() throws Exception {
        // IMPORTANT: this file must not have B-frames
        testMuxer(""video_176x144_mp4_mpeg2_105kbps_25fps_aac_stereo_128kbps_44100hz.mp4"", false);
    }

    @NonMediaMainlineTest"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.NativeDecoderTest"	"testMuxerMpeg4"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/NativeDecoderTest.java"	""	"public void testMuxerMpeg4() throws Exception {
        // IMPORTANT: this file must not have B-frames
        testMuxer(""video_176x144_mp4_mpeg4_300kbps_25fps_aac_stereo_128kbps_44100hz.mp4"", false);
    }

    private void testMuxer(final String res, boolean webm) throws Exception {
        Preconditions.assertTestFileExists(mInpPrefix + res);
        if (!MediaUtils.checkCodecsForResource(mInpPrefix + res)) {
            return; // skip
        }

        AssetFileDescriptor infd = getAssetFileDescriptorFor(res);

        File base = mContext.getExternalFilesDir(null);
        String tmpFile = base.getPath() + ""/tmp.dat"";
        Log.i(""@@@"", ""using tmp file "" + tmpFile);
        new File(tmpFile).delete();
        ParcelFileDescriptor out = ParcelFileDescriptor.open(new File(tmpFile),
                ParcelFileDescriptor.MODE_READ_WRITE | ParcelFileDescriptor.MODE_CREATE);

        assertTrue(""muxer failed"", testMuxerNative(
                infd.getParcelFileDescriptor().getFd(), infd.getStartOffset(), infd.getLength(),
                out.getFd(), webm));

        // compare the original with the remuxed
        MediaExtractor org = new MediaExtractor();
        org.setDataSource(infd.getFileDescriptor(),
                infd.getStartOffset(), infd.getLength());

        MediaExtractor remux = new MediaExtractor();
        remux.setDataSource(out.getFileDescriptor());

        assertEquals(""mismatched numer of tracks"", org.getTrackCount(), remux.getTrackCount());
        // allow duration mismatch for webm files as ffmpeg does not consider the duration of the
        // last frame while libwebm (and our framework) does.
        final long maxDurationDiffUs = webm ? 50000 : 0; // 50ms for webm
        for (int i = 0; i < org.getTrackCount(); i++) {
            MediaFormat format1 = org.getTrackFormat(i);
            MediaFormat format2 = remux.getTrackFormat(i);
            Log.i(""@@@"", ""org: "" + format1);
            Log.i(""@@@"", ""remux: "" + format2);
            assertTrue(""different formats"", compareFormats(format1, format2, maxDurationDiffUs));
        }

        org.release();
        remux.release();

        Preconditions.assertTestFileExists(mInpPrefix + res);
        MediaPlayer player1 =
                MediaPlayer.create(mContext, Uri.fromFile(new File(mInpPrefix + res)));
        MediaPlayer player2 = MediaPlayer.create(mContext, Uri.parse(""file://"" + tmpFile));
        assertEquals(""duration is different"",
                     player1.getDuration(), player2.getDuration(), maxDurationDiffUs * 0.001);
        player1.release();
        player2.release();
        new File(tmpFile).delete();
    }

    private String hexString(ByteBuffer buf) {
        if (buf == null) {
            return ""(null)"";
        }
        final char digits[] =
            { '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f' };

        StringBuilder hex = new StringBuilder();
        for (int i = buf.position(); i < buf.limit(); ++i) {
            byte c = buf.get(i);
            hex.append(digits[(c >> 4) & 0xf]);
            hex.append(digits[c & 0xf]);
        }
        return hex.toString();
    }

    /** returns: null if key is in neither formats, true if they match and false otherwise */
    private Boolean compareByteBufferInFormats(MediaFormat f1, MediaFormat f2, String key) {
        ByteBuffer bufF1 = f1.containsKey(key) ? f1.getByteBuffer(key) : null;
        ByteBuffer bufF2 = f2.containsKey(key) ? f2.getByteBuffer(key) : null;
        if (bufF1 == null && bufF2 == null) {
            return null;
        }
        if (bufF1 == null || !bufF1.equals(bufF2)) {
            Log.i(""@@@"", ""org "" + key + "": "" + hexString(bufF1));
            Log.i(""@@@"", ""rmx "" + key + "": "" + hexString(bufF2));
            return false;
        }
        return true;
    }

    private boolean compareFormats(MediaFormat f1, MediaFormat f2, long maxDurationDiffUs) {
        final String KEY_DURATION = MediaFormat.KEY_DURATION;

        // allow some difference in durations
        if (maxDurationDiffUs > 0
                && f1.containsKey(KEY_DURATION) && f2.containsKey(KEY_DURATION)
                && Math.abs(f1.getLong(KEY_DURATION)
                        - f2.getLong(KEY_DURATION)) <= maxDurationDiffUs) {
            f2.setLong(KEY_DURATION, f1.getLong(KEY_DURATION));
        }

        // verify hdr-static-info
        if (Boolean.FALSE.equals(compareByteBufferInFormats(f1, f2, ""hdr-static-info""))) {
            return false;
        }

        // verify CSDs
        for (int i = 0;; ++i) {
            String key = ""csd-"" + i;
            Boolean match = compareByteBufferInFormats(f1, f2, key);
            if (match == null) {
                break;
            } else if (match == false) {
                return false;
            }
        }

        // before S, mpeg4 writers jammed a fixed SAR value into the output;
        // this was fixed in S
        if (!sIsAtLeastS) {
            if (f1.containsKey(MediaFormat.KEY_PIXEL_ASPECT_RATIO_HEIGHT)
                            && f2.containsKey(MediaFormat.KEY_PIXEL_ASPECT_RATIO_HEIGHT)) {
                f2.setInteger(MediaFormat.KEY_PIXEL_ASPECT_RATIO_HEIGHT,
                                f1.getInteger(MediaFormat.KEY_PIXEL_ASPECT_RATIO_HEIGHT));
            }
            if (f1.containsKey(MediaFormat.KEY_PIXEL_ASPECT_RATIO_WIDTH)
                            && f2.containsKey(MediaFormat.KEY_PIXEL_ASPECT_RATIO_WIDTH)) {
                f2.setInteger(MediaFormat.KEY_PIXEL_ASPECT_RATIO_WIDTH,
                                f1.getInteger(MediaFormat.KEY_PIXEL_ASPECT_RATIO_WIDTH));
            }
        }

        // look for f2 (the new) being a superset (>=) of f1 (the original)
        // ensure that all of our fields in f1 appear in f2 with the same
        // value. We allow f2 to contain extra fields.
        Set<String> keys = f1.getKeys();
        for (String key: keys) {
            if (key == null) {
                continue;
            }
            if (!f2.containsKey(key)) {
                return false;
            }
            int f1Type = f1.getValueTypeForKey(key);
            if (f1Type != f2.getValueTypeForKey(key)) {
                return false;
            }
            switch (f1Type) {
                case MediaFormat.TYPE_INTEGER:
                    int f1Int = f1.getInteger(key);
                    int f2Int = f2.getInteger(key);
                    if (f1Int != f2Int) {
                        return false;
                    }
                    break;
                case MediaFormat.TYPE_LONG:
                    long f1Long = f1.getLong(key);
                    long f2Long = f2.getLong(key);
                    if (f1Long != f2Long) {
                        return false;
                    }
                    break;
                case MediaFormat.TYPE_FLOAT:
                    float f1Float = f1.getFloat(key);
                    float f2Float = f2.getFloat(key);
                    if (f1Float != f2Float) {
                        return false;
                    }
                    break;
                case MediaFormat.TYPE_STRING:
                    String f1String = f1.getString(key);
                    String f2String = f2.getString(key);
                    if (!f1String.equals(f2String)) {
                        return false;
                    }
                    break;
                case MediaFormat.TYPE_BYTE_BUFFER:
                    ByteBuffer f1ByteBuffer = f1.getByteBuffer(key);
                    ByteBuffer f2ByteBuffer = f2.getByteBuffer(key);
                    if (!f1ByteBuffer.equals(f2ByteBuffer)) {
                        return false;
                    }
                    break;
                default:
                    return false;
            }
        }

        // repeat for getFeatures
        // (which we don't use in this test, but include for completeness)
        Set<String> features = f1.getFeatures();
        for (String key: features) {
            if (key == null) {
                continue;
            }
            if (!f2.containsKey(key)) {
                return false;
            }
            int f1Type = f1.getValueTypeForKey(key);
            if (f1Type != f2.getValueTypeForKey(key)) {
                return false;
            }
            switch (f1Type) {
                case MediaFormat.TYPE_INTEGER:
                    int f1Int = f1.getInteger(key);
                    int f2Int = f2.getInteger(key);
                    if (f1Int != f2Int) {
                        return false;
                    }
                    break;
                case MediaFormat.TYPE_LONG:
                    long f1Long = f1.getLong(key);
                    long f2Long = f2.getLong(key);
                    if (f1Long != f2Long) {
                        return false;
                    }
                    break;
                case MediaFormat.TYPE_FLOAT:
                    float f1Float = f1.getFloat(key);
                    float f2Float = f2.getFloat(key);
                    if (f1Float != f2Float) {
                        return false;
                    }
                    break;
                case MediaFormat.TYPE_STRING:
                    String f1String = f1.getString(key);
                    String f2String = f2.getString(key);
                    if (!f1String.equals(f2String)) {
                        return false;
                    }
                    break;
                case MediaFormat.TYPE_BYTE_BUFFER:
                    ByteBuffer f1ByteBuffer = f1.getByteBuffer(key);
                    ByteBuffer f2ByteBuffer = f2.getByteBuffer(key);
                    if (!f1ByteBuffer.equals(f2ByteBuffer)) {
                        return false;
                    }
                    break;
                default:
                    return false;
            }
        }

        // not otherwise disqualified
        return true;
    }

    private static native boolean testMuxerNative(int in, long inoffset, long insize,
            int out, boolean webm);

    @Presubmit"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaCodecTest"	"testAsyncStopAndReset"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaCodecTest.java"	""	"public void testAsyncStopAndReset() throws Exception, InterruptedException {
        testAsyncReset(true /* testStop */);
    }

    private void testAsyncReset(boolean testStop) throws Exception, InterruptedException {
        // Test video and audio 10x each
        for (int i = 0; i < 10; i++) {
            testAsyncReset(false /* audio */, (i % 2) == 0 /* swap */, testStop);
        }
        for (int i = 0; i < 10; i++) {
            testAsyncReset(true /* audio */, (i % 2) == 0 /* swap */, testStop);
        }
    }

    /*
     * This method simulates a race between flush (or stop) and reset() called from
     * two threads. Neither call should get stuck. This should be run multiple rounds.
     */
    private void testAsyncReset(boolean audio, boolean swap, final boolean testStop)
            throws Exception, InterruptedException {
        String mimeTypePrefix  = audio ? ""audio/"" : ""video/"";
        final MediaExtractor mediaExtractor = getMediaExtractorForMimeType(
                INPUT_RESOURCE, mimeTypePrefix);
        MediaFormat mediaFormat = mediaExtractor.getTrackFormat(
                mediaExtractor.getSampleTrackIndex());
        if (!MediaUtils.checkDecoderForFormat(mediaFormat)) {
            return; // skip
        }

        OutputSurface outputSurface = audio ? null : new OutputSurface(1, 1);
        final Surface surface = outputSurface == null ? null : outputSurface.getSurface();

        String mimeType = mediaFormat.getString(MediaFormat.KEY_MIME);
        final MediaCodec mediaCodec = MediaCodec.createDecoderByType(mimeType);

        try {
            mediaCodec.configure(mediaFormat, surface, null /* crypto */, 0 /* flags */);

            mediaCodec.start();

            assertTrue(runDecodeTillFirstOutput(mediaCodec, mediaExtractor));

            Thread flushingThread = new Thread(new Runnable() {
                @Override
                public void run() {
                    try {
                        if (testStop) {
                            mediaCodec.stop();
                        } else {
                            mediaCodec.flush();
                        }
                    } catch (IllegalStateException e) {
                        // This is okay, since we're simulating a race between flush and reset.
                        // If reset executed first, flush could fail.
                    }
                }
            });

            Thread resettingThread = new Thread(new Runnable() {
                @Override
                public void run() {
                    mediaCodec.reset();
                }
            });

            // start flushing (or stopping) and resetting in two threads
            if (swap) {
                flushingThread.start();
                resettingThread.start();
            } else {
                resettingThread.start();
                flushingThread.start();
            }

            // wait for at most 5 sec, and check if the thread exits properly
            flushingThread.join(5000);
            assertFalse(flushingThread.isAlive());

            resettingThread.join(5000);
            assertFalse(resettingThread.isAlive());
        } finally {
            if (mediaCodec != null) {
                mediaCodec.release();
            }
            if (mediaExtractor != null) {
                mediaExtractor.release();
            }
            if (outputSurface != null) {
                outputSurface.release();
            }
        }
    }

    private static class FlushThread extends Thread {
        final MediaCodec mEncoder;
        final CountDownLatch mBuffersExhausted;
        final CountDownLatch mCodecFlushed;

        FlushThread(MediaCodec encoder, CountDownLatch buffersExhausted,
                CountDownLatch codecFlushed) {
            mEncoder = encoder;
            mBuffersExhausted = buffersExhausted;
            mCodecFlushed = codecFlushed;
        }

        @Override
        public void run() {
            try {
                mBuffersExhausted.await();
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                Log.w(TAG, ""buffersExhausted wait interrupted; flushing immediately."", e);
            }
            mEncoder.flush();
            mCodecFlushed.countDown();
        }
    }

    private static class ReleaseAfterFlushCallback extends MediaCodec.Callback implements Runnable {
        final String mMime;
        final MediaCodec mEncoder;
        final CountDownLatch mBuffersExhausted, mCodecFlushed;
        final int mNumBuffersBeforeFlush;

        CountDownLatch mStopInput = new CountDownLatch(1);
        List<Integer> mInputBufferIndices = new ArrayList<>();
        List<Integer> mOutputBufferIndices = new ArrayList<>();

        ReleaseAfterFlushCallback(String mime,
                MediaCodec encoder,
                CountDownLatch buffersExhausted,
                CountDownLatch codecFlushed,
                int numBuffersBeforeFlush) {
            mMime = mime;
            mEncoder = encoder;
            mBuffersExhausted = buffersExhausted;
            mCodecFlushed = codecFlushed;
            mNumBuffersBeforeFlush = numBuffersBeforeFlush;
        }

        @Override
        public void onInputBufferAvailable(MediaCodec codec, int index) {
            assertTrue(""video onInputBufferAvailable "" + index, mMime.startsWith(""audio/""));
            synchronized (mInputBufferIndices) {
                mInputBufferIndices.add(index);
            };
        }

        @Override
        public void onOutputBufferAvailable(MediaCodec codec, int index, BufferInfo info) {
            mOutputBufferIndices.add(index);
            if (mOutputBufferIndices.size() == mNumBuffersBeforeFlush) {
                releaseAfterFlush(codec, mOutputBufferIndices, mBuffersExhausted, mCodecFlushed);
                mStopInput.countDown();
            }
        }

        @Override
        public void onError(MediaCodec codec, CodecException e) {
            Log.e(TAG, codec + "" onError"", e);
            fail(codec + "" onError "" + e.getMessage());
        }

        @Override
        public void onOutputFormatChanged(MediaCodec codec, MediaFormat format) {
            Log.v(TAG, codec + "" onOutputFormatChanged "" + format);
        }

        @Override
        public void run() {
            InputSurface inputSurface = null;
            try {
                inputSurface = initCodecAndSurface(mMime, mEncoder);
                do {
                    int inputIndex = -1;
                    if (inputSurface == null) {
                        // asynchronous audio codec
                        synchronized (mInputBufferIndices) {
                            if (mInputBufferIndices.isEmpty()) {
                                continue;
                            } else {
                                inputIndex = mInputBufferIndices.remove(0);
                            }
                        }
                    }
                    feedEncoder(mEncoder, inputSurface, inputIndex);
                } while (!mStopInput.await(TIMEOUT_USEC, TimeUnit.MICROSECONDS));
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                Log.w(TAG, ""mEncoder input frames interrupted/stopped"", e);
            } finally {
                cleanupCodecAndSurface(mEncoder, inputSurface);
            }
        }
    }

    private static void runReleaseAfterFlush(
            String mime,
            MediaCodec encoder,
            CountDownLatch buffersExhausted,
            CountDownLatch codecFlushed,
            AtomicInteger numBuffers) {
        InputSurface inputSurface = null;
        try {
            inputSurface = initCodecAndSurface(mime, encoder);
            List<Integer> outputBufferIndices = getOutputBufferIndices(encoder, inputSurface);
            if (numBuffers != null) {
                numBuffers.set(outputBufferIndices.size());
            }
            releaseAfterFlush(encoder, outputBufferIndices, buffersExhausted, codecFlushed);
        } finally {
            cleanupCodecAndSurface(encoder, inputSurface);
        }
    }

    private static InputSurface initCodecAndSurface(String mime, MediaCodec encoder) {
        MediaFormat format;
        InputSurface inputSurface = null;
        if (mime.startsWith(""audio/"")) {
            format = MediaFormat.createAudioFormat(mime, AUDIO_SAMPLE_RATE, AUDIO_CHANNEL_COUNT);
            format.setInteger(MediaFormat.KEY_AAC_PROFILE, AUDIO_AAC_PROFILE);
            format.setInteger(MediaFormat.KEY_BIT_RATE, AUDIO_BIT_RATE);
            encoder.configure(format, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE);
        } else if (MIME_TYPE.equals(mime)) {
            CodecInfo info = getAvcSupportedFormatInfo();
            format = MediaFormat.createVideoFormat(mime, info.mMaxW, info.mMaxH);
            format.setInteger(MediaFormat.KEY_COLOR_FORMAT,
                    MediaCodecInfo.CodecCapabilities.COLOR_FormatSurface);
            format.setInteger(MediaFormat.KEY_BIT_RATE, info.mBitRate);
            format.setInteger(MediaFormat.KEY_FRAME_RATE, info.mFps);
            format.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, IFRAME_INTERVAL);
            OutputSurface outputSurface = new OutputSurface(1, 1);
            encoder.configure(format, outputSurface.getSurface(), null, MediaCodec.CONFIGURE_FLAG_ENCODE);
            inputSurface = new InputSurface(encoder.createInputSurface());
            inputSurface.makeCurrent();
        } else {
            throw new IllegalArgumentException(""unsupported mime type: "" + mime);
        }
        encoder.start();
        return inputSurface;
    }

    private static void cleanupCodecAndSurface(MediaCodec encoder, InputSurface inputSurface) {
        if (encoder != null) {
            encoder.stop();
            encoder.release();
        }

        if (inputSurface != null) {
            inputSurface.release();
        }
    }

    private static List<Integer> getOutputBufferIndices(MediaCodec encoder, InputSurface inputSurface) {
        boolean feedMoreFrames;
        MediaCodec.BufferInfo bufferInfo = new MediaCodec.BufferInfo();
        List<Integer> indices = new ArrayList<>();
        do {
            feedMoreFrames = indices.isEmpty();
            feedEncoder(encoder, inputSurface, -1);
            // dequeue buffers until not available
            int index = encoder.dequeueOutputBuffer(bufferInfo, TIMEOUT_USEC);
            while (index >= 0) {
                indices.add(index);
                index = encoder.dequeueOutputBuffer(bufferInfo, TIMEOUT_USEC_SHORT);
            }
        } while (feedMoreFrames);
        assertFalse(indices.isEmpty());
        return indices;
    }

    /**
     * @param encoder audio/video encoder
     * @param inputSurface null for and only for audio encoders
     * @param inputIndex only used for audio; if -1 the function would attempt to dequeue from encoder;
     * do not use -1 for asynchronous encoders
     */
    private static void feedEncoder(MediaCodec encoder, InputSurface inputSurface, int inputIndex) {
        if (inputSurface == null) {
            // audio
            while (inputIndex == -1) {
                inputIndex = encoder.dequeueInputBuffer(TIMEOUT_USEC);
            }
            ByteBuffer inputBuffer = encoder.getInputBuffer(inputIndex);;
            for (int i = 0; i < inputBuffer.capacity() / 2; i++) {
                inputBuffer.putShort((short)i);
            }
            encoder.queueInputBuffer(inputIndex, 0, inputBuffer.limit(), 0, 0);
        } else {
            // video
            GLES20.glClearColor(0.0f, 0.5f, 0.0f, 1.0f);
            GLES20.glClear(GLES20.GL_COLOR_BUFFER_BIT);
            inputSurface.swapBuffers();
        }
    }

    private static void releaseAfterFlush(
            MediaCodec encoder,
            List<Integer> outputBufferIndices,
            CountDownLatch buffersExhausted,
            CountDownLatch codecFlushed) {
        if (buffersExhausted == null) {
            // flush from same thread
            encoder.flush();
        } else {
            assertNotNull(codecFlushed);
            buffersExhausted.countDown();
            try {
                codecFlushed.await();
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                Log.w(TAG, ""codecFlushed wait interrupted; releasing buffers immediately."", e);
            }
        }

        for (int index : outputBufferIndices) {
            try {
                encoder.releaseOutputBuffer(index, true);
                fail(""MediaCodec releaseOutputBuffer after flush() does not throw exception"");
            } catch (MediaCodec.CodecException e) {
                // Expected
            }
        }
    }

    /**
     * Tests:
     * <br> dequeueInputBuffer() fails when encoder configured with an input Surface
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaCodecTest"	"testDecodeShortInput"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaCodecTest.java"	""	"public void testDecodeShortInput() throws InterruptedException {
        // Input buffers from this input video are queued up to and including the video frame with
        // timestamp LAST_BUFFER_TIMESTAMP_US.
        final String INPUT_RESOURCE =
                ""video_480x360_mp4_h264_1350kbps_30fps_aac_stereo_192kbps_44100hz.mp4"";
        final long LAST_BUFFER_TIMESTAMP_US = 166666;

        // The test should fail if the decoder never produces output frames for the truncated input.
        // Time out decoding, as we have no way to query whether the decoder will produce output.
        final int DECODING_TIMEOUT_MS = 2000;

        final AtomicBoolean completed = new AtomicBoolean();
        Thread videoDecodingThread = new Thread(new Runnable() {
            @Override
            public void run() {
                completed.set(runDecodeShortInput(INPUT_RESOURCE, LAST_BUFFER_TIMESTAMP_US));
            }
        });
        videoDecodingThread.start();
        videoDecodingThread.join(DECODING_TIMEOUT_MS);
        if (!completed.get()) {
            throw new RuntimeException(""timed out decoding to end-of-stream"");
        }
    }

    private boolean runDecodeShortInput(final String inputResource, long lastBufferTimestampUs) {
        final int NO_BUFFER_INDEX = -1;

        OutputSurface outputSurface = null;
        MediaExtractor mediaExtractor = null;
        MediaCodec mediaCodec = null;
        try {
            outputSurface = new OutputSurface(1, 1);
            mediaExtractor = getMediaExtractorForMimeType(inputResource, ""video/"");
            MediaFormat mediaFormat =
                    mediaExtractor.getTrackFormat(mediaExtractor.getSampleTrackIndex());
            String mimeType = mediaFormat.getString(MediaFormat.KEY_MIME);
            if (!supportsCodec(mimeType, false)) {
                Log.i(TAG, ""No decoder found for mimeType= "" + MIME_TYPE);
                return true;
            }
            mediaCodec =
                    MediaCodec.createDecoderByType(mimeType);
            mediaCodec.configure(mediaFormat, outputSurface.getSurface(), null, 0);
            mediaCodec.start();
            boolean eos = false;
            boolean signaledEos = false;
            MediaCodec.BufferInfo outputBufferInfo = new MediaCodec.BufferInfo();
            int outputBufferIndex = NO_BUFFER_INDEX;
            while (!eos && !Thread.interrupted()) {
                // Try to feed more data into the codec.
                if (mediaExtractor.getSampleTrackIndex() != -1 && !signaledEos) {
                    int bufferIndex = mediaCodec.dequeueInputBuffer(0);
                    if (bufferIndex != NO_BUFFER_INDEX) {
                        ByteBuffer buffer = mediaCodec.getInputBuffers()[bufferIndex];
                        int size = mediaExtractor.readSampleData(buffer, 0);
                        long timestampUs = mediaExtractor.getSampleTime();
                        mediaExtractor.advance();
                        signaledEos = mediaExtractor.getSampleTrackIndex() == -1
                                || timestampUs == lastBufferTimestampUs;
                        mediaCodec.queueInputBuffer(bufferIndex,
                                0,
                                size,
                                timestampUs,
                                signaledEos ? MediaCodec.BUFFER_FLAG_END_OF_STREAM : 0);
                    }
                }

                // If we don't have an output buffer, try to get one now.
                if (outputBufferIndex == NO_BUFFER_INDEX) {
                    outputBufferIndex = mediaCodec.dequeueOutputBuffer(outputBufferInfo, 0);
                }

                if (outputBufferIndex == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED
                        || outputBufferIndex == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED
                        || outputBufferIndex == MediaCodec.INFO_TRY_AGAIN_LATER) {
                    outputBufferIndex = NO_BUFFER_INDEX;
                } else if (outputBufferIndex != NO_BUFFER_INDEX) {
                    eos = (outputBufferInfo.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0;

                    boolean render = outputBufferInfo.size > 0;
                    mediaCodec.releaseOutputBuffer(outputBufferIndex, render);
                    if (render) {
                        outputSurface.awaitNewImage();
                    }

                    outputBufferIndex = NO_BUFFER_INDEX;
                }
            }

            return eos;
        } catch (IOException e) {
            throw new RuntimeException(""error reading input resource"", e);
        } finally {
            if (mediaCodec != null) {
                mediaCodec.stop();
                mediaCodec.release();
            }
            if (mediaExtractor != null) {
                mediaExtractor.release();
            }
            if (outputSurface != null) {
                outputSurface.release();
            }
        }
    }

    /**
     * Tests creating two decoders for {@link #MIME_TYPE_AUDIO} at the same time.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaCodecTest"	"testCreateAudioDecoderAndEncoder"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaCodecTest.java"	""	"public void testCreateAudioDecoderAndEncoder() {
        if (!supportsCodec(MIME_TYPE_AUDIO, true)) {
            Log.i(TAG, ""No encoder found for mimeType= "" + MIME_TYPE_AUDIO);
            return;
        }

        if (!supportsCodec(MIME_TYPE_AUDIO, false)) {
            Log.i(TAG, ""No decoder found for mimeType= "" + MIME_TYPE_AUDIO);
            return;
        }

        final MediaFormat encoderFormat = MediaFormat.createAudioFormat(
                MIME_TYPE_AUDIO, AUDIO_SAMPLE_RATE, AUDIO_CHANNEL_COUNT);
        encoderFormat.setInteger(MediaFormat.KEY_AAC_PROFILE, AUDIO_AAC_PROFILE);
        encoderFormat.setInteger(MediaFormat.KEY_BIT_RATE, AUDIO_BIT_RATE);
        final MediaFormat decoderFormat = MediaFormat.createAudioFormat(
                MIME_TYPE_AUDIO, AUDIO_SAMPLE_RATE, AUDIO_CHANNEL_COUNT);

        MediaCodec audioEncoder = null;
        MediaCodec audioDecoder = null;
        try {
            try {
                audioEncoder = MediaCodec.createEncoderByType(MIME_TYPE_AUDIO);
            } catch (IOException e) {
                fail(""failed to create "" + MIME_TYPE_AUDIO + "" encoder"");
            }
            audioEncoder.configure(encoderFormat, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE);
            audioEncoder.start();

            try {
                audioDecoder = MediaCodec.createDecoderByType(MIME_TYPE_AUDIO);
            } catch (IOException e) {
                fail(""failed to create "" + MIME_TYPE_AUDIO + "" decoder"");
            }
            audioDecoder.configure(decoderFormat, null, null, 0);
            audioDecoder.start();
        } finally {
            if (audioDecoder != null) {
                try {
                    audioDecoder.stop();
                    audioDecoder.release();
                } catch (RuntimeException e) {
                    Log.w(TAG, ""exception stopping/releasing codec"", e);
                }
            }

            if (audioEncoder != null) {
                try {
                    audioEncoder.stop();
                    audioEncoder.release();
                } catch (RuntimeException e) {
                    Log.w(TAG, ""exception stopping/releasing codec"", e);
                }
            }
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaCodecTest"	"testCryptoInfoPattern"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaCodecTest.java"	""	"public void testCryptoInfoPattern() {
        CryptoInfo info = new CryptoInfo();
        Pattern pattern = new Pattern(1 /*blocksToEncrypt*/, 2 /*blocksToSkip*/);
        assertEquals(1, pattern.getEncryptBlocks());
        assertEquals(2, pattern.getSkipBlocks());
        pattern.set(3 /*blocksToEncrypt*/, 4 /*blocksToSkip*/);
        assertEquals(3, pattern.getEncryptBlocks());
        assertEquals(4, pattern.getSkipBlocks());
        info.setPattern(pattern);
        // Check that CryptoInfo does not leak access to the underlying pattern.
        if (mIsAtLeastS) {
            // getPattern() availability SDK>=S
            pattern.set(10, 10);
            info.getPattern().set(10, 10);
            assertSame(3, info.getPattern().getEncryptBlocks());
            assertSame(4, info.getPattern().getSkipBlocks());
        }
    }

    private static CodecInfo getAvcSupportedFormatInfo() {
        MediaCodecInfo mediaCodecInfo = selectCodec(MIME_TYPE);
        CodecCapabilities cap = mediaCodecInfo.getCapabilitiesForType(MIME_TYPE);
        if (cap == null) { // not supported
            return null;
        }
        CodecInfo info = new CodecInfo();
        int highestLevel = 0;
        for (CodecProfileLevel lvl : cap.profileLevels) {
            if (lvl.level > highestLevel) {
                highestLevel = lvl.level;
            }
        }
        int maxW = 0;
        int maxH = 0;
        int bitRate = 0;
        int fps = 0; // frame rate for the max resolution
        switch(highestLevel) {
            // Do not support Level 1 to 2.
            case CodecProfileLevel.AVCLevel1:
            case CodecProfileLevel.AVCLevel11:
            case CodecProfileLevel.AVCLevel12:
            case CodecProfileLevel.AVCLevel13:
            case CodecProfileLevel.AVCLevel1b:
            case CodecProfileLevel.AVCLevel2:
                return null;
            case CodecProfileLevel.AVCLevel21:
                maxW = 352;
                maxH = 576;
                bitRate = 4000000;
                fps = 25;
                break;
            case CodecProfileLevel.AVCLevel22:
                maxW = 720;
                maxH = 480;
                bitRate = 4000000;
                fps = 15;
                break;
            case CodecProfileLevel.AVCLevel3:
                maxW = 720;
                maxH = 480;
                bitRate = 10000000;
                fps = 30;
                break;
            case CodecProfileLevel.AVCLevel31:
                maxW = 1280;
                maxH = 720;
                bitRate = 14000000;
                fps = 30;
                break;
            case CodecProfileLevel.AVCLevel32:
                maxW = 1280;
                maxH = 720;
                bitRate = 20000000;
                fps = 60;
                break;
            case CodecProfileLevel.AVCLevel4: // only try up to 1080p
            default:
                maxW = 1920;
                maxH = 1080;
                bitRate = 20000000;
                fps = 30;
                break;
        }
        info.mMaxW = maxW;
        info.mMaxH = maxH;
        info.mFps = fps;
        info.mBitRate = bitRate;
        Log.i(TAG, ""AVC Level 0x"" + Integer.toHexString(highestLevel) + "" bit rate "" + bitRate +
                "" fps "" + info.mFps + "" w "" + maxW + "" h "" + maxH);

        return info;
    }

    private void runVideoEncoding(int numSwap, CodecInfo info) {
        MediaFormat format = MediaFormat.createVideoFormat(MIME_TYPE, info.mMaxW, info.mMaxH);
        format.setInteger(MediaFormat.KEY_COLOR_FORMAT,
                MediaCodecInfo.CodecCapabilities.COLOR_FormatSurface);
        format.setInteger(MediaFormat.KEY_BIT_RATE, info.mBitRate);
        format.setInteger(MediaFormat.KEY_FRAME_RATE, info.mFps);
        format.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, IFRAME_INTERVAL);
        MediaCodec encoder = null;
        InputSurface inputSurface = null;
        mVideoEncoderHadError = false;
        try {
            encoder = MediaCodec.createEncoderByType(MIME_TYPE);
            encoder.configure(format, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE);
            inputSurface = new InputSurface(encoder.createInputSurface());
            inputSurface.makeCurrent();
            MediaCodec.BufferInfo bufferInfo = new MediaCodec.BufferInfo();
            encoder.start();
            for (int i = 0; i < numSwap; i++) {
                GLES20.glClearColor(0.0f, 0.5f, 0.0f, 1.0f);
                GLES20.glClear(GLES20.GL_COLOR_BUFFER_BIT);
                inputSurface.swapBuffers();
                // dequeue buffers until not available
                int index = encoder.dequeueOutputBuffer(bufferInfo, TIMEOUT_USEC);
                while (index >= 0) {
                    encoder.releaseOutputBuffer(index, false);
                    // just throw away output
                    // allow shorter wait for 2nd round to move on quickly.
                    index = encoder.dequeueOutputBuffer(bufferInfo, TIMEOUT_USEC_SHORT);
                }
            }
            encoder.signalEndOfInputStream();
        } catch (Throwable e) {
            Log.w(TAG, ""runVideoEncoding got error: "" + e);
            mVideoEncoderHadError = true;
        } finally {
            if (encoder != null) {
                encoder.stop();
                encoder.release();
            }
            if (inputSurface != null) {
                inputSurface.release();
            }
        }
    }

    private void runAudioEncoding() {
        MediaFormat format = MediaFormat.createAudioFormat(MIME_TYPE_AUDIO, AUDIO_SAMPLE_RATE,
                AUDIO_CHANNEL_COUNT);
        format.setInteger(MediaFormat.KEY_AAC_PROFILE, AUDIO_AAC_PROFILE);
        format.setInteger(MediaFormat.KEY_BIT_RATE, AUDIO_BIT_RATE);
        MediaCodec encoder = null;
        mAudioEncoderHadError = false;
        try {
            encoder = MediaCodec.createEncoderByType(MIME_TYPE_AUDIO);
            encoder.configure(format, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE);
            MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
            encoder.start();
            ByteBuffer[] inputBuffers = encoder.getInputBuffers();
            ByteBuffer source = ByteBuffer.allocate(inputBuffers[0].capacity());
            for (int i = 0; i < source.capacity()/2; i++) {
                source.putShort((short)i);
            }
            source.rewind();
            int currentInputBufferIndex = 0;
            long encodingLatencySum = 0;
            int totalEncoded = 0;
            int numRepeat = 0;
            while (mVideoEncodingOngoing) {
                numRepeat++;
                int inputIndex = encoder.dequeueInputBuffer(TIMEOUT_USEC);
                while (inputIndex == -1) {
                    inputIndex = encoder.dequeueInputBuffer(TIMEOUT_USEC);
                }
                ByteBuffer inputBuffer = inputBuffers[inputIndex];
                inputBuffer.rewind();
                inputBuffer.put(source);
                long start = System.currentTimeMillis();
                totalEncoded += inputBuffers[inputIndex].limit();
                encoder.queueInputBuffer(inputIndex, 0, inputBuffer.limit(), 0, 0);
                source.rewind();
                int index = encoder.dequeueOutputBuffer(info, TIMEOUT_USEC);
                long end = System.currentTimeMillis();
                encodingLatencySum += (end - start);
                while (index >= 0) {
                    encoder.releaseOutputBuffer(index, false);
                    // just throw away output
                    // allow shorter wait for 2nd round to move on quickly.
                    index = encoder.dequeueOutputBuffer(info, TIMEOUT_USEC_SHORT);
                }
            }
            Log.w(TAG, ""Audio encoding average latency "" + encodingLatencySum / numRepeat +
                    "" ms for average write size "" + totalEncoded / numRepeat +
                    "" total latency "" + encodingLatencySum + "" ms for total bytes "" + totalEncoded);
        } catch (Throwable e) {
            Log.w(TAG, ""runAudioEncoding got error: "" + e);
            mAudioEncoderHadError = true;
        } finally {
            if (encoder != null) {
                encoder.stop();
                encoder.release();
            }
        }
    }

    /**
     * Creates a MediaFormat with the basic set of values.
     */
    private static MediaFormat createMediaFormat() {
        MediaFormat format = MediaFormat.createVideoFormat(MIME_TYPE, WIDTH, HEIGHT);
        format.setInteger(MediaFormat.KEY_COLOR_FORMAT,
                MediaCodecInfo.CodecCapabilities.COLOR_FormatSurface);
        format.setInteger(MediaFormat.KEY_BIT_RATE, BIT_RATE);
        format.setInteger(MediaFormat.KEY_FRAME_RATE, FRAME_RATE);
        format.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, IFRAME_INTERVAL);
        return format;
    }

    /**
     * Returns the first codec capable of encoding the specified MIME type, or null if no
     * match was found.
     */
    private static MediaCodecInfo selectCodec(String mimeType) {
        // FIXME: select codecs based on the complete use-case, not just the mime
        MediaCodecList mcl = new MediaCodecList(MediaCodecList.REGULAR_CODECS);
        for (MediaCodecInfo info : mcl.getCodecInfos()) {
            if (!info.isEncoder()) {
                continue;
            }

            String[] types = info.getSupportedTypes();
            for (int j = 0; j < types.length; j++) {
                if (types[j].equalsIgnoreCase(mimeType)) {
                    return info;
                }
            }
        }
        return null;
    }

    /**
     * Returns a color format that is supported by the codec and isn't COLOR_FormatSurface.  Throws
     * an exception if none found.
     */
    private static int findNonSurfaceColorFormat(MediaCodecInfo codecInfo, String mimeType) {
        MediaCodecInfo.CodecCapabilities capabilities = codecInfo.getCapabilitiesForType(mimeType);
        for (int i = 0; i < capabilities.colorFormats.length; i++) {
            int colorFormat = capabilities.colorFormats[i];
            if (colorFormat != MediaCodecInfo.CodecCapabilities.COLOR_FormatSurface) {
                return colorFormat;
            }
        }
        fail(""couldn't find a good color format for "" + codecInfo.getName() + "" / "" + MIME_TYPE);
        return 0;   // not reached
    }

    private MediaExtractor getMediaExtractorForMimeType(final String resource,
            String mimeTypePrefix) throws IOException {
        Preconditions.assertTestFileExists(mInpPrefix + resource);
        MediaExtractor mediaExtractor = new MediaExtractor();
        File inpFile = new File(mInpPrefix + resource);
        ParcelFileDescriptor parcelFD =
                ParcelFileDescriptor.open(inpFile, ParcelFileDescriptor.MODE_READ_ONLY);
        AssetFileDescriptor afd = new AssetFileDescriptor(parcelFD, 0, parcelFD.getStatSize());
        try {
            mediaExtractor.setDataSource(
                    afd.getFileDescriptor(), afd.getStartOffset(), afd.getLength());
        } finally {
            afd.close();
        }
        int trackIndex;
        for (trackIndex = 0; trackIndex < mediaExtractor.getTrackCount(); trackIndex++) {
            MediaFormat trackMediaFormat = mediaExtractor.getTrackFormat(trackIndex);
            if (trackMediaFormat.getString(MediaFormat.KEY_MIME).startsWith(mimeTypePrefix)) {
                mediaExtractor.selectTrack(trackIndex);
                break;
            }
        }
        if (trackIndex == mediaExtractor.getTrackCount()) {
            throw new IllegalStateException(""couldn't get a video track"");
        }

        return mediaExtractor;
    }

    private static boolean supportsCodec(String mimeType, boolean encoder) {
        MediaCodecList list = new MediaCodecList(MediaCodecList.ALL_CODECS);
        for (MediaCodecInfo info : list.getCodecInfos()) {
            if (encoder != info.isEncoder()) {
                continue;
            }

            for (String type : info.getSupportedTypes()) {
                if (type.equalsIgnoreCase(mimeType)) {
                    return true;
                }
            }
        }
        return false;
    }

    private static final UUID CLEARKEY_SCHEME_UUID =
            new UUID(0x1077efecc0b24d02L, 0xace33c1e52e2fb4bL);

    /**
     * Tests MediaCodec.CryptoException
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.location.cts.gnss.asn1.supl2.rrlp_messages.RRLP_Component"	"isTagImplicit"	"CtsLocationGnssTestCases"	"/home/gpoor/cts-12-source/cts/tests/location/location_gnss/src/android/location/cts/gnss/asn1/supl2/rrlp_messages/RRLP_Component.java"	""	"public void test/*
 *.
 */

package android.location.cts.asn1.supl2.rrlp_messages;

/*
 */


//
//
import android.location.cts.asn1.base.Asn1Choice;
import android.location.cts.asn1.base.Asn1Null;
import android.location.cts.asn1.base.Asn1Object;
import android.location.cts.asn1.base.Asn1Tag;
import android.location.cts.asn1.base.BitStream;
import android.location.cts.asn1.base.BitStreamReader;
import android.location.cts.asn1.base.ChoiceComponent;
import android.location.cts.asn1.supl2.rrlp_components.AssistanceData;
import android.location.cts.asn1.supl2.rrlp_components.MsrPosition_Req;
import android.location.cts.asn1.supl2.rrlp_components.MsrPosition_Rsp;
import android.location.cts.asn1.supl2.rrlp_components.PosCapability_Req;
import android.location.cts.asn1.supl2.rrlp_components.PosCapability_Rsp;
import android.location.cts.asn1.supl2.rrlp_components.ProtocolError;
import com.google.common.collect.ImmutableList;
import java.nio.ByteBuffer;
import java.util.Collection;
import java.util.HashMap;
import java.util.Map;
import javax.annotation.Nullable;


/**
 */
public  class RRLP_Component extends Asn1Choice {
  //

  private static final Asn1Tag TAG_RRLP_Component
      = Asn1Tag.fromClassAndNumber(-1, -1);

  private static final Map<Asn1Tag, Select> tagToSelection = new HashMap<>();

  private boolean extension;
  private ChoiceComponent selection;
  private Asn1Object element;

  static {
    for (Select select : Select.values()) {
      for (Asn1Tag tag : select.getPossibleFirstTags()) {
        Select select0;
        if ((select0 = tagToSelection.put(tag, select)) != null) {
          throw new IllegalStateException(
            ""RRLP_Component: "" + tag + "" maps to both "" + select0 + "" and "" + select);
        }
      }
    }
  }

  public RRLP_Component() {
    super();
  }

  @Override
  @Nullable
  protected Asn1Tag getTag() {
    return TAG_RRLP_Component;
  }

  @Override
  protected boolean isTagImplicit() {
    return true;
  }

  public static Collection<Asn1Tag> getPossibleFirstTags() {
    if (TAG_RRLP_Component != null) {
      return ImmutableList.of(TAG_RRLP_Component);
    } else {
      return tagToSelection.keySet();
    }
  }

  /**
   * Creates a new RRLP_Component from encoded stream.
   */
  public static RRLP_Component fromPerUnaligned(byte[] encodedBytes) {
    RRLP_Component result = new RRLP_Component();
    result.decodePerUnaligned(new BitStreamReader(encodedBytes));
    return result;
  }

  /**
   * Creates a new RRLP_Component from encoded stream.
   */
  public static RRLP_Component fromPerAligned(byte[] encodedBytes) {
    RRLP_Component result = new RRLP_Component();
    result.decodePerAligned(new BitStreamReader(encodedBytes));
    return result;
  }

  

  @Override protected boolean hasExtensionValue() {
    return extension;
  }

  @Override protected Integer getSelectionOrdinal() {
    return selection.ordinal();
  }

  @Nullable
  @Override
  protected ChoiceComponent getSelectedComponent() {
    return selection;
  }

  @Override protected int getOptionCount() {
    if (hasExtensionValue()) {
      return Extend.values().length;
    }
    return Select.values().length;
  }

  protected Asn1Object createAndSetValue(boolean isExtensionValue,
                                         int ordinal) {
    extension = isExtensionValue;
    if (isExtensionValue) {
      selection = Extend.values()[ordinal];
    } else {
      selection = Select.values()[ordinal];
    }
    element = selection.createElement();
    return element;
  }

  @Override protected ChoiceComponent createAndSetValue(Asn1Tag tag) {
    Select select = tagToSelection.get(tag);
    if (select == null) {
      throw new IllegalArgumentException(""Unknown selection tag: "" + tag);
    }
    element = select.createElement();
    selection = select;
    extension = false;
    return select;
  }

  @Override protected boolean isExtensible() {
    return true;
  }

  @Override protected Asn1Object getValue() {
    return element;
  }

  
  private static enum Select implements ChoiceComponent {
    
    $MsrPositionReq(Asn1Tag.fromClassAndNumber(2, 0),
        true) {
      @Override
      public Asn1Object createElement() {
        return new MsrPosition_Req();
      }

      @Override
      Collection<Asn1Tag> getPossibleFirstTags() {
        return tag == null ? MsrPosition_Req.getPossibleFirstTags() : ImmutableList.of(tag);
      }

      @Override
      String elementIndentedString(Asn1Object element, String indent) {
        return toString() + "" : "" + element.toIndentedString(indent);
      }
    },
    
    $MsrPositionRsp(Asn1Tag.fromClassAndNumber(2, 1),
        true) {
      @Override
      public Asn1Object createElement() {
        return new MsrPosition_Rsp();
      }

      @Override
      Collection<Asn1Tag> getPossibleFirstTags() {
        return tag == null ? MsrPosition_Rsp.getPossibleFirstTags() : ImmutableList.of(tag);
      }

      @Override
      String elementIndentedString(Asn1Object element, String indent) {
        return toString() + "" : "" + element.toIndentedString(indent);
      }
    },
    
    $AssistanceData(Asn1Tag.fromClassAndNumber(2, 2),
        true) {
      @Override
      public Asn1Object createElement() {
        return new AssistanceData();
      }

      @Override
      Collection<Asn1Tag> getPossibleFirstTags() {
        return tag == null ? AssistanceData.getPossibleFirstTags() : ImmutableList.of(tag);
      }

      @Override
      String elementIndentedString(Asn1Object element, String indent) {
        return toString() + "" : "" + element.toIndentedString(indent);
      }
    },
    
    $AssistanceDataAck(Asn1Tag.fromClassAndNumber(2, 3),
        true) {
      @Override
      public Asn1Object createElement() {
        return new RRLP_Component.assistanceDataAckType();
      }

      @Override
      Collection<Asn1Tag> getPossibleFirstTags() {
        return tag == null ? RRLP_Component.assistanceDataAckType.getPossibleFirstTags() : ImmutableList.of(tag);
      }

      @Override
      String elementIndentedString(Asn1Object element, String indent) {
        return toString() + "" : "" + element.toIndentedString(indent);
      }
    },
    
    $ProtocolError(Asn1Tag.fromClassAndNumber(2, 4),
        true) {
      @Override
      public Asn1Object createElement() {
        return new ProtocolError();
      }

      @Override
      Collection<Asn1Tag> getPossibleFirstTags() {
        return tag == null ? ProtocolError.getPossibleFirstTags() : ImmutableList.of(tag);
      }

      @Override
      String elementIndentedString(Asn1Object element, String indent) {
        return toString() + "" : "" + element.toIndentedString(indent);
      }
    },
    
    ;

    @Nullable final Asn1Tag tag;
    final boolean isImplicitTagging;

    Select(@Nullable Asn1Tag tag, boolean isImplicitTagging) {
      this.tag = tag;
      this.isImplicitTagging = isImplicitTagging;
    }

    @Override
    public Asn1Object createElement() {
      throw new IllegalStateException(""Select template error"");
    }

    @Override
    @Nullable
    public Asn1Tag getTag() {
      return tag;
    }

    @Override
    public boolean isImplicitTagging() {
      return isImplicitTagging;
    }

    abstract Collection<Asn1Tag> getPossibleFirstTags();

    abstract String elementIndentedString(Asn1Object element, String indent);
  }
  
  

  public boolean isMsrPositionReq() {
    return !hasExtensionValue() && Select.$MsrPositionReq == selection;
  }

  /**
   * @throws {@code IllegalStateException} if {@code !isMsrPositionReq}.
   */
  @SuppressWarnings(""unchecked"")
  public MsrPosition_Req getMsrPositionReq() {
    if (!isMsrPositionReq()) {
      throw new IllegalStateException(""RRLP_Component value not a MsrPositionReq"");
    }
    return (MsrPosition_Req) element;
  }

  public void setMsrPositionReq(MsrPosition_Req selected) {
    selection = Select.$MsrPositionReq;
    extension = false;
    element = selected;
  }

  public MsrPosition_Req setMsrPositionReqToNewInstance() {
      MsrPosition_Req element = new MsrPosition_Req();
      setMsrPositionReq(element);
      return element;
  }
  
  

  public boolean isMsrPositionRsp() {
    return !hasExtensionValue() && Select.$MsrPositionRsp == selection;
  }

  /**
   * @throws {@code IllegalStateException} if {@code !isMsrPositionRsp}.
   */
  @SuppressWarnings(""unchecked"")
  public MsrPosition_Rsp getMsrPositionRsp() {
    if (!isMsrPositionRsp()) {
      throw new IllegalStateException(""RRLP_Component value not a MsrPositionRsp"");
    }
    return (MsrPosition_Rsp) element;
  }

  public void setMsrPositionRsp(MsrPosition_Rsp selected) {
    selection = Select.$MsrPositionRsp;
    extension = false;
    element = selected;
  }

  public MsrPosition_Rsp setMsrPositionRspToNewInstance() {
      MsrPosition_Rsp element = new MsrPosition_Rsp();
      setMsrPositionRsp(element);
      return element;
  }
  
  

  public boolean isAssistanceData() {
    return !hasExtensionValue() && Select.$AssistanceData == selection;
  }

  /**
   * @throws {@code IllegalStateException} if {@code !isAssistanceData}.
   */
  @SuppressWarnings(""unchecked"")
  public AssistanceData getAssistanceData() {
    if (!isAssistanceData()) {
      throw new IllegalStateException(""RRLP_Component value not a AssistanceData"");
    }
    return (AssistanceData) element;
  }

  public void setAssistanceData(AssistanceData selected) {
    selection = Select.$AssistanceData;
    extension = false;
    element = selected;
  }

  public AssistanceData setAssistanceDataToNewInstance() {
      AssistanceData element = new AssistanceData();
      setAssistanceData(element);
      return element;
  }
  
/*
 */


//

/**
 */
public static class assistanceDataAckType extends Asn1Null {
  //

  private static final Asn1Tag TAG_assistanceDataAckType
      = Asn1Tag.fromClassAndNumber(-1, -1);

  public assistanceDataAckType() {
    super();
  }

  @Override
  @Nullable
  protected Asn1Tag getTag() {
    return TAG_assistanceDataAckType;
  }

  @Override
  protected boolean isTagImplicit() {
    return true;
  }

  public static Collection<Asn1Tag> getPossibleFirstTags() {
    if (TAG_assistanceDataAckType != null) {
      return ImmutableList.of(TAG_assistanceDataAckType);
    } else {
      return Asn1Null.getPossibleFirstTags();
    }
  }

  /**
   * Creates a new assistanceDataAckType from encoded stream.
   */
  public static assistanceDataAckType fromPerUnaligned(byte[] encodedBytes) {
    assistanceDataAckType result = new assistanceDataAckType();
    result.decodePerUnaligned(new BitStreamReader(encodedBytes));
    return result;
  }

  /**
   * Creates a new assistanceDataAckType from encoded stream.
   */
  public static assistanceDataAckType fromPerAligned(byte[] encodedBytes) {
    assistanceDataAckType result = new assistanceDataAckType();
    result.decodePerAligned(new BitStreamReader(encodedBytes));
    return result;
  }

  @Override public Iterable<BitStream> encodePerUnaligned() {
    return super.encodePerUnaligned();
  }

  @Override public Iterable<BitStream> encodePerAligned() {
    return super.encodePerAligned();
  }

  @Override public void decodePerUnaligned(BitStreamReader reader) {
    super.decodePerUnaligned(reader);
  }

  @Override public void decodePerAligned(BitStreamReader reader) {
    super.decodePerAligned(reader);
  }

  @Override public String toString() {
    return toIndentedString("""");
  }

  public String toIndentedString(String indent) {
    return ""assistanceDataAckType (null value);\n"";
  }
}


  public boolean isAssistanceDataAck() {
    return !hasExtensionValue() && Select.$AssistanceDataAck == selection;
  }

  /**
   * @throws {@code IllegalStateException} if {@code !isAssistanceDataAck}.
   */
  @SuppressWarnings(""unchecked"")
  public RRLP_Component.assistanceDataAckType getAssistanceDataAck() {
    if (!isAssistanceDataAck()) {
      throw new IllegalStateException(""RRLP_Component value not a AssistanceDataAck"");
    }
    return (RRLP_Component.assistanceDataAckType) element;
  }

  public void setAssistanceDataAck(RRLP_Component.assistanceDataAckType selected) {
    selection = Select.$AssistanceDataAck;
    extension = false;
    element = selected;
  }

  public RRLP_Component.assistanceDataAckType setAssistanceDataAckToNewInstance() {
      RRLP_Component.assistanceDataAckType element = new RRLP_Component.assistanceDataAckType();
      setAssistanceDataAck(element);
      return element;
  }
  
  

  public boolean isProtocolError() {
    return !hasExtensionValue() && Select.$ProtocolError == selection;
  }

  /**
   * @throws {@code IllegalStateException} if {@code !isProtocolError}.
   */
  @SuppressWarnings(""unchecked"")
  public ProtocolError getProtocolError() {
    if (!isProtocolError()) {
      throw new IllegalStateException(""RRLP_Component value not a ProtocolError"");
    }
    return (ProtocolError) element;
  }

  public void setProtocolError(ProtocolError selected) {
    selection = Select.$ProtocolError;
    extension = false;
    element = selected;
  }

  public ProtocolError setProtocolErrorToNewInstance() {
      ProtocolError element = new ProtocolError();
      setProtocolError(element);
      return element;
  }
  

  private static enum Extend implements ChoiceComponent {
    
    $PosCapabilityReq(Asn1Tag.fromClassAndNumber(2, 5),
        true) {
      @Override
      public Asn1Object createElement() {
        return new PosCapability_Req();
      }

      @Override
      @SuppressWarnings(""unchecked"")
      String elementIndentedString(Asn1Object element, String indent) {
        return toString() + "" : "" + ((PosCapability_Req) element).toIndentedString(indent);
      }
    },
    
    $PosCapabilityRsp(Asn1Tag.fromClassAndNumber(2, 6),
        true) {
      @Override
      public Asn1Object createElement() {
        return new PosCapability_Rsp();
      }

      @Override
      @SuppressWarnings(""unchecked"")
      String elementIndentedString(Asn1Object element, String indent) {
        return toString() + "" : "" + ((PosCapability_Rsp) element).toIndentedString(indent);
      }
    },
    
    ;
    @Nullable private final Asn1Tag tag;
    private final boolean isImplicitTagging;

    Extend(@Nullable Asn1Tag tag, boolean isImplicitTagging) {
      this.tag = tag;
      this.isImplicitTagging = isImplicitTagging;
    }

    public Asn1Object createElement() {
      throw new IllegalStateException(""Extend template error"");
    }

    @Override
    @Nullable
    public Asn1Tag getTag() {
      return tag;
    }

    @Override
    public boolean isImplicitTagging() {
      return isImplicitTagging;
    }

    String elementIndentedString(Asn1Object element, String indent) {
      throw new IllegalStateException(""Extend template error"");
    }
  }
  
  

  public boolean isExtensionPosCapabilityReq() {
    return hasExtensionValue() && Extend.$PosCapabilityReq == selection;
  }

  /**
   * @throws {@code IllegalStateException} if {@code !isPosCapabilityReq}.
   */
  @SuppressWarnings(""unchecked"")
  public PosCapability_Req getExtensionPosCapabilityReq() {
    if (!isExtensionPosCapabilityReq()) {
      throw new IllegalStateException(""RRLP_Component value not a PosCapabilityReq"");
    }
    return (PosCapability_Req) element;
  }

  public void setExtensionPosCapabilityReq(PosCapability_Req selected) {
    selection = Extend.$PosCapabilityReq;
    extension = true;
    element = selected;
  }

  public void setExtensionPosCapabilityReqToNewInstance() {
      PosCapability_Req element = new PosCapability_Req();
      setExtensionPosCapabilityReq(element);
  }
  
  

  public boolean isExtensionPosCapabilityRsp() {
    return hasExtensionValue() && Extend.$PosCapabilityRsp == selection;
  }

  /**
   * @throws {@code IllegalStateException} if {@code !isPosCapabilityRsp}.
   */
  @SuppressWarnings(""unchecked"")
  public PosCapability_Rsp getExtensionPosCapabilityRsp() {
    if (!isExtensionPosCapabilityRsp()) {
      throw new IllegalStateException(""RRLP_Component value not a PosCapabilityRsp"");
    }
    return (PosCapability_Rsp) element;
  }

  public void setExtensionPosCapabilityRsp(PosCapability_Rsp selected) {
    selection = Extend.$PosCapabilityRsp;
    extension = true;
    element = selected;
  }

  public void setExtensionPosCapabilityRspToNewInstance() {
      PosCapability_Rsp element = new PosCapability_Rsp();
      setExtensionPosCapabilityRsp(element);
  }
  

  @Override public Iterable<BitStream> encodePerUnaligned() {
    return super.encodePerUnaligned();
  }

  @Override public Iterable<BitStream> encodePerAligned() {
    return super.encodePerAligned();
  }

  @Override public void decodePerUnaligned(BitStreamReader reader) {
    super.decodePerUnaligned(reader);
  }

  @Override public void decodePerAligned(BitStreamReader reader) {
    super.decodePerAligned(reader);
  }

  @Override public String toString() {
    return toIndentedString("""");
  }

  private String elementIndentedString(String indent) {
    if (element == null) {
      return ""null;\n"";
    }
    if (extension) {
      return Extend.values()[selection.ordinal()]
          .elementIndentedString(element, indent + ""  "");
    } else {
      return Select.values()[selection.ordinal()]
          .elementIndentedString(element, indent + ""  "");
    }
  }

  public String toIndentedString(String indent) {
    return ""RRLP_Component = "" + elementIndentedString(indent) + indent + "";\n"";
  }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestAacDrc"	"testDecodeAacDrcLevelM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestAacDrc.java"	""	"public void testDecodeAacDrcLevelM4a() throws Exception {
        AudioParameter decParams = new AudioParameter();
        // full boost, full cut, target ref level: -23dBFS, heavy compression: no
        DrcParams drcParams = new DrcParams(127, 127, 92, 0);
        short[] decSamples = decodeToMemory(decParams, R.raw.sine_2ch_48khz_aot5_drclevel_mp4,
                -1, null, drcParams, null /*decoderName: use default decoder*/);
        DecoderTest decTester = new DecoderTest();
        decTester.checkEnergy(decSamples, decParams, 2, 0.70f);
    }

    /**
     * Verify correct decoding of MPEG-4 AAC with Dynamic Range Control (DRC) metadata.
     * Fully apply light compression DRC (default settings).
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestAacDrc"	"testDecodeAacDrcFullM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestAacDrc.java"	""	"public void testDecodeAacDrcFullM4a() throws Exception {
        AudioParameter decParams = new AudioParameter();
        short[] decSamples = decodeToMemory(decParams, R.raw.sine_2ch_48khz_aot5_drcfull_mp4,
                -1, null, null, null /*decoderName: use default decoder*/);
        DecoderTest decTester = new DecoderTest();
        decTester.checkEnergy(decSamples, decParams, 2, 0.80f);
    }

    /**
     * Verify correct decoding of MPEG-4 AAC with Dynamic Range Control (DRC) metadata.
     * Apply only half of the light compression DRC and normalize to -20dBFS output level.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestAacDrc"	"testDecodeAacDrcHalfM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestAacDrc.java"	""	"public void testDecodeAacDrcHalfM4a() throws Exception {
        AudioParameter decParams = new AudioParameter();
        // half boost, half cut, target ref level: -20dBFS, heavy compression: no
        DrcParams drcParams = new DrcParams(63, 63, 80, 0);
        short[] decSamples = decodeToMemory(decParams, R.raw.sine_2ch_48khz_aot2_drchalf_mp4,
                -1, null, drcParams, null /*decoderName: use default decoder*/);
        DecoderTest decTester = new DecoderTest();
        decTester.checkEnergy(decSamples, decParams, 2, 0.80f);
    }

    /**
     * Verify correct decoding of MPEG-4 AAC with Dynamic Range Control (DRC) metadata.
     * Disable light compression DRC to test if MediaFormat keys reach the decoder.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestAacDrc"	"testDecodeAacDrcOffM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestAacDrc.java"	""	"public void testDecodeAacDrcOffM4a() throws Exception {
        AudioParameter decParams = new AudioParameter();
        // no boost, no cut, target ref level: -16dBFS, heavy compression: no
        DrcParams drcParams = new DrcParams(0, 0, 64, 0);       // normalize to -16dBFS
        short[] decSamples = decodeToMemory(decParams, R.raw.sine_2ch_48khz_aot5_drcoff_mp4,
                -1, null, drcParams, null /*decoderName: use default decoder*/);
        DecoderTest decTester = new DecoderTest();
        decTester.checkEnergy(decSamples, decParams, 2, 0.80f);
    }

    /**
     * Verify correct decoding of MPEG-4 AAC with Dynamic Range Control (DRC) metadata.
     * Apply heavy compression gains and normalize to -16dBFS output level.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestAacDrc"	"testDecodeAacDrcHeavyM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestAacDrc.java"	""	"public void testDecodeAacDrcHeavyM4a() throws Exception {
        AudioParameter decParams = new AudioParameter();
        // full boost, full cut, target ref level: -16dBFS, heavy compression: yes
        DrcParams drcParams = new DrcParams(127, 127, 64, 1);
        short[] decSamples = decodeToMemory(decParams, R.raw.sine_2ch_48khz_aot2_drcheavy_mp4,
                -1, null, drcParams, null /*decoderName: use default decoder*/);
        DecoderTest decTester = new DecoderTest();
        decTester.checkEnergy(decSamples, decParams, 2, 0.80f);
    }

    /**
     * Test signal limiting (without clipping) of MPEG-4 AAC decoder with the help of DRC metadata.
     * Uses a two channel 248 Hz sine tone at 48 kHz sampling rate for input.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestAacDrc"	"testDecodeAacDrcClipM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestAacDrc.java"	""	"public void testDecodeAacDrcClipM4a() throws Exception {
        AudioParameter decParams = new AudioParameter();
        short[] decSamples = decodeToMemory(decParams, R.raw.sine_2ch_48khz_aot5_drcclip_mp4,
                -1, null, null, null /*decoderName: use default decoder*/);
        checkClipping(decSamples, decParams, 248.0f /* Hz */);
    }

    /**
     * Test if there is decoder internal clipping of MPEG-4 AAC decoder.
     * Uses a two channel 248 Hz sine tone at 48 kHz sampling rate for input.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestAacDrc"	"testDecodeAacInternalClipM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestAacDrc.java"	""	"public void testDecodeAacInternalClipM4a() throws Exception {
        if (!MediaUtils.check(sIsAndroidRAndAbove, ""Internal clipping fixed in Android R""))
                return;
        AudioParameter decParams = new AudioParameter();
        short[] decSamples = decodeToMemory(decParams, R.raw.sine_2ch_48khz_aot2_internalclip_mp4,
                -1, null, null, null /*decoderName: use default decoder*/);
        checkClipping(decSamples, decParams, 248.0f /* Hz */);
    }

    /**
     * Default decoder target level.
     * The actual default value used by the decoder can differ between platforms, or even devices,
     * but tests will measure energy relative to this value.
     */
    public static final int DEFAULT_DECODER_TARGET_LEVEL = 64; // -16.0 dBFs

    /**
     * Test USAC decoder with different target loudness levels
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestAacDrc"	"testDecodeUsacLoudnessM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestAacDrc.java"	""	"public void testDecodeUsacLoudnessM4a() throws Exception {
        Log.v(TAG, ""START testDecodeUsacLoudnessM4a"");

        ArrayList<String> aacDecoderNames = DecoderTestXheAac.initAacDecoderNames();
        assertTrue(""No AAC decoder found"", aacDecoderNames.size() > 0);

        for (String aacDecName : aacDecoderNames) {
            // test default loudness
            // decoderTargetLevel = 64 --> target output level = -16.0 dBFs
            try {
                checkUsacLoudness(DEFAULT_DECODER_TARGET_LEVEL, 1, 1.0f, aacDecName);
            } catch (Exception e) {
                Log.v(TAG, ""testDecodeUsacLoudnessM4a for default loudness failed for "" +
                        aacDecName);
                throw new RuntimeException(e);
            }

            // test loudness boost
            // decoderTargetLevel = 40 --> target output level = -10.0 dBFs
            // normFactor = 1/(10^(-6/10)) = 3.98f
            //   where ""-6"" is the difference between the default level (-16), and -10 for this test
            try {
                checkUsacLoudness(40, 1, (float)(1.0f/Math.pow(10.0f, -6.0f/10.0f)), aacDecName);
            } catch (Exception e) {
                Log.v(TAG, ""testDecodeUsacLoudnessM4a for loudness boost failed for "" + aacDecName);
                throw new RuntimeException(e);
            }

            // test loudness attenuation
            // decoderTargetLevel = 96 --> target output level = -24.0 dBFs
            // normFactor = 1/(10^(8/10)) = 0.15f
            //     where 8 is the difference between the default level (-16), and -24 for this test
            try {
                checkUsacLoudness(96, 0, (float)(1.0f/Math.pow(10.0f, 8.0f/10.0f)), aacDecName);
            } catch (Exception e) {
                Log.v(TAG, ""testDecodeUsacLoudnessM4a for loudness attenuation failed for ""
                        + aacDecName);
                throw new RuntimeException(e);
            }

            if (sIsAndroidRAndAbove) {
                // test loudness normalization off
                // decoderTargetLevel = -1 --> target output level = -19.0 dBFs (program loudness of
                // waveform)
                // normFactor = 1/(10^(3/10)) = 0.5f
                // where 3 is the difference between the default level (-16), and -19 for this test
                try {
                    checkUsacLoudness(-1, 0, (float) (1.0f / Math.pow(10.0f, 3.0f / 10.0f)),
                            aacDecName);
                } catch (Exception e) {
                    Log.v(TAG, ""testDecodeUsacLoudnessM4a for loudness attenuation failed for ""
                            + aacDecName);
                    throw new RuntimeException(e);
                }
            }
        }
    }

    /**
     * Verify that the correct output loudness values are returned by the MPEG-4 AAC decoder
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestAacDrc"	"testDecodeAacDrcOutputLoudnessM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestAacDrc.java"	""	"public void testDecodeAacDrcOutputLoudnessM4a() throws Exception {
        Log.v(TAG, ""START testDecodeAacDrcOutputLoudnessM4a"");

        ArrayList<String> aacDecoderNames = DecoderTestXheAac.initAacDecoderNames();
        assertTrue(""No AAC decoder found"", aacDecoderNames.size() > 0);

        for (String aacDecName : aacDecoderNames) {
            // test drc output loudness
            // testfile without loudness metadata and loudness normalization off
            // -> expected value: -1
            try {
                checkAacDrcOutputLoudness(
                        R.raw.noise_1ch_24khz_aot5_dr_sbr_sig1_mp4, -1, -1, aacDecName);
            } catch (Exception e) {
                Log.v(TAG, ""testDecodeUsacLoudnessM4a for default loudness failed for "" +
                        aacDecName);
                throw new RuntimeException(e);
            }
            // test drc output loudness
            // testfile without loudness metadata and loudness normalization on
            // -> expected value: -1
            try {
                checkAacDrcOutputLoudness(
                        R.raw.noise_1ch_24khz_aot5_dr_sbr_sig1_mp4, 70, -1, aacDecName);
            } catch (Exception e) {
                Log.v(TAG, ""testDecodeUsacLoudnessM4a for default loudness failed for "" +
                        aacDecName);
                throw new RuntimeException(e);
            }
            // test drc output loudness
            // testfile with MPEG-4 DRC loudness metadata and loudness normalization off
            // -> expected value: loudness metadata in bitstream (-16*-4 = 64)
            try {
                checkAacDrcOutputLoudness(
                        R.raw.sine_2ch_48khz_aot2_drchalf_mp4, -1, 64, aacDecName);
            } catch (Exception e) {
                Log.v(TAG, ""testDecodeUsacLoudnessM4a for default loudness failed for "" +
                        aacDecName);
                throw new RuntimeException(e);
            }
            // test drc output loudness
            // testfile with MPEG-4 DRC loudness metadata and loudness normalization off
            // -> expected value: loudness metadata in bitstream (-31*-4 = 124)
            try {
                checkAacDrcOutputLoudness(
                        R.raw.sine_2ch_48khz_aot5_drcclip_mp4, -1, 124, aacDecName);
            } catch (Exception e) {
                Log.v(TAG, ""testDecodeUsacLoudnessM4a for default loudness failed for "" +
                        aacDecName);
                throw new RuntimeException(e);
            }
            // test drc output loudness
            // testfile with MPEG-4 DRC loudness metadata and loudness normalization on
            // -> expected value: target loudness value (85)
            try {
                checkAacDrcOutputLoudness(
                        R.raw.sine_2ch_48khz_aot5_drcclip_mp4, 85, 85, aacDecName);
            } catch (Exception e) {
                Log.v(TAG, ""testDecodeUsacLoudnessM4a for default loudness failed for "" +
                        aacDecName);
                throw new RuntimeException(e);
            }
        }
    }

    /**
     *  Internal utilities
     */

    /**
     * The test routine performs a THD+N (Total Harmonic Distortion + Noise) analysis on a given
     * audio signal (decSamples). The THD+N value is defined here as harmonic distortion (+ noise)
     * RMS over full signal RMS.
     *
     * After the energy measurement of the unprocessed signal the routine creates and applies a
     * notch filter at the given frequency (sineFrequency). Afterwards the signal energy is
     * measured again. Then the THD+N value is calculated as the ratio of the filtered and the full
     * signal energy.
     *
     * The test passes if the THD+N value is lower than -60 dB. Otherwise it fails.
     *
     * @param decSamples the decoded audio samples to be tested
     * @param decParams the audio parameters of the given audio samples (decSamples)
     * @param sineFrequency frequency of the test signal tone used for testing
     * @throws RuntimeException
     */
    private void checkClipping(short[] decSamples, AudioParameter decParams, float sineFrequency)
            throws RuntimeException
    {
        final double threshold_clipping = -60.0; // dB
        final int numChannels = decParams.getNumChannels();
        final int startSample = 2 * 2048 * numChannels;          // exclude signal on- & offset to
        final int stopSample = decSamples.length - startSample;  // ... measure only the stationary
                                                                 // ... sine tone
        // get full energy of signal (all channels)
        double nrgFull = getEnergy(decSamples, startSample, stopSample);

        // create notch filter to suppress sine-tone at 248 Hz
        Biquad filter = new Biquad(sineFrequency, decParams.getSamplingRate());
        for (int channel = 0; channel < numChannels; channel++) {
            // apply notch-filter on buffer for each channel to filter out the sine tone.
            // only the harmonics (and noise) remain. */
            filter.apply(decSamples, channel, numChannels);
        }

        // get energy of harmonic distortion (signal without sine-tone)
        double nrgHd = getEnergy(decSamples, startSample, stopSample);

        // Total Harmonic Distortion + Noise, defined here as harmonic distortion (+ noise) RMS
        // over full signal RMS, given in dB
        double THDplusN = 10 * Math.log10(nrgHd / nrgFull);
        assertTrue(""signal has clipping samples"", THDplusN <= threshold_clipping);
    }

    /**
     * Measure the energy of a given signal over all channels within a given signal range.
     * @param signal audio signal samples
     * @param start start offset of the measuring range
     * @param stop stop sample which is the last sample of the measuring range
     * @return the signal energy in the given range
     */
    private double getEnergy(short[] signal, int start, int stop) {
        double nrg = 0.0;
        for (int sample = start; sample < stop; sample++) {
            double v = signal[sample];
            nrg += v * v;
        }
        return nrg;
    }

    // Notch filter implementation
    private class Biquad {
        // filter coefficients for biquad filter (2nd order IIR filter)
        float[] a;
        float[] b;
        // filter states
        float[] state_ff;
        float[] state_fb;

        protected float alpha = 0.95f;

        public Biquad(float f_notch, float f_s) {
            // Create filter coefficients of notch filter which suppresses a sine tone with f_notch
            // Hz at sampling frequency f_s. Zeros placed at unit circle at f_notch, poles placed
            // nearby the unit circle at f_notch.
            state_ff = new float[2];
            state_fb = new float[2];
            state_ff[0] = state_ff[1] = state_fb[0] = state_fb[1] = 0.0f;

            a = new float[3];
            b = new float[3];
            double omega = 2.0 * Math.PI * f_notch / f_s;
            a[0] = b[0] = b[2] = 1.0f;
            a[1] = -2.0f * alpha * (float)Math.cos(omega);
            a[2] = alpha * alpha;
            b[1] = -2.0f * (float)Math.cos(omega);
        }

        public void apply(short[] signal, int offset, int stride) {
            // reset states
            state_ff[0] = state_ff[1] = 0.0f;
            state_fb[0] = state_fb[1] = 0.0f;
            // process 2nd order IIR filter in Direct Form I
            float x_0, x_1, x_2, y_0, y_1, y_2;
            x_2 = state_ff[0];  // x[n-2]
            x_1 = state_ff[1];  // x[n-1]
            y_2 = state_fb[0];  // y[n-2]
            y_1 = state_fb[1];  // y[n-1]
            for (int sample = offset; sample < signal.length; sample += stride) {
                x_0 = signal[sample];
                y_0 = b[0] * x_0 + b[1] * x_1 + b[2] * x_2
                        - a[1] * y_1 - a[2] * y_2;
                x_2 = x_1;
                x_1 = x_0;
                y_2 = y_1;
                y_1 = y_0;
                signal[sample] = (short)y_0;
            }
            state_ff[0] = x_2;  // next x[n-2]
            state_ff[1] = x_1;  // next x[n-1]
            state_fb[0] = y_2;  // next y[n-2]
            state_fb[1] = y_1;  // next y[n-1]
        }
    }

    /**
     * USAC test DRC loudness
     */
    private void checkUsacLoudness(int decoderTargetLevel, int heavy, float normFactor,
            String decoderName) throws Exception {
        for (boolean runtimeChange : new boolean[] {false, true}) {
            if (runtimeChange && !sIsAndroidRAndAbove) {
                // changing decoder configuration after it has been initialized requires R and above
                continue;
            }
            AudioParameter decParams = new AudioParameter();
            DrcParams drcParams_def  = new DrcParams(127, 127, DEFAULT_DECODER_TARGET_LEVEL, 1);
            DrcParams drcParams_test = new DrcParams(127, 127, decoderTargetLevel, heavy);

            short[] decSamples_def = decodeToMemory(decParams,
                    R.raw.noise_2ch_48khz_aot42_19_lufs_mp4,
                    -1, null, drcParams_def, decoderName);
            short[] decSamples_test = decodeToMemory(decParams,
                    R.raw.noise_2ch_48khz_aot42_19_lufs_mp4,
                    -1, null, drcParams_test, decoderName, runtimeChange);

            DecoderTestXheAac decTesterXheAac = new DecoderTestXheAac();
            float[] nrg_def  = decTesterXheAac.checkEnergyUSAC(decSamples_def, decParams, 2, 1);
            float[] nrg_test = decTesterXheAac.checkEnergyUSAC(decSamples_test, decParams, 2, 1);

            float[] nrgThreshold = {2602510595620.0f, 2354652443657.0f};

            // Check default loudness behavior
            if (nrg_def[0] > nrgThreshold[0] || nrg_def[0] < nrgThreshold[1]) {
                throw new Exception(""Default loudness behavior not as expected"");
            }

            float nrgRatio = nrg_def[0]/nrg_test[0];

            // Check for loudness boost/attenuation if decoderTargetLevel deviates from default value
            // used in these tests (note that the default target level can change from platform
            // to platform, or device to device)
            if (decoderTargetLevel != -1) {
                if ((decoderTargetLevel < DEFAULT_DECODER_TARGET_LEVEL) // boosted loudness
                        && (nrg_def[0] > nrg_test[0])) {
                    throw new Exception(""Signal not attenuated"");
                }
                if ((decoderTargetLevel > DEFAULT_DECODER_TARGET_LEVEL) // attenuated loudness
                        && (nrg_def[0] < nrg_test[0])) {
                    throw new Exception(""Signal not boosted"");
                }
            }
            nrgRatio = nrgRatio * normFactor;

            // Check whether loudness behavior is as expected
            if (nrgRatio > 1.05f || nrgRatio < 0.95f ){
                throw new Exception(""Loudness behavior not as expected"");
            }
        }
    }

    /**
    * AAC test Output Loudness
    */
    private void checkAacDrcOutputLoudness(int testInput, int decoderTargetLevel, int expectedOutputLoudness, String decoderName) throws Exception {
        for (boolean runtimeChange : new boolean[] {false, true}) {
            AudioParameter decParams = new AudioParameter();
            DrcParams drcParams_test = new DrcParams(127, 127, decoderTargetLevel, 0, 6);

            // Check drc loudness preference
            short[] decSamples_test = decodeToMemory(decParams, testInput, -1, null,
                    drcParams_test, decoderName, runtimeChange, expectedOutputLoudness);
        }
    }


    /**
     *  Class handling all MPEG-4 and MPEG-D Dynamic Range Control (DRC) parameter relevant
     *  for testing
     */
    protected static class DrcParams {
        int mBoost;                          // scaling of boosting gains
        int mCut;                            // scaling of compressing gains
        int mDecoderTargetLevel;             // desired target output level (for normalization)
        int mHeavy;                          // en-/disable heavy compression
        int mEffectType;                     // MPEG-D DRC Effect Type
        int mAlbumMode;                      // MPEG-D DRC Album Mode

        public DrcParams() {
            mBoost = 127;               // no scaling
            mCut   = 127;               // no scaling
            mHeavy = 1;                 // enabled
        }

        public DrcParams(int boost, int cut, int decoderTargetLevel, int heavy) {
            mBoost = boost;
            mCut = cut;
            mDecoderTargetLevel = decoderTargetLevel;
            mHeavy = heavy;
        }

        public DrcParams(int boost, int cut, int decoderTargetLevel, int heavy, int effectType) {
            this(boost, cut, decoderTargetLevel, heavy);
            mEffectType = effectType;
        }

        public DrcParams(int boost, int cut, int decoderTargetLevel, int heavy, int effectType,
                int albumMode) {
            this(boost, cut, decoderTargetLevel, heavy, effectType);
            mAlbumMode = albumMode;
        }
    }


    // TODO: code is the same as in DecoderTest, differences are:
    //          - addition of application of DRC parameters
    //          - no need/use of resetMode, configMode
    //       Split method so code can be shared

    private short[] decodeToMemory(AudioParameter audioParams, int testinput, int eossample,
            List<Long> timestamps, DrcParams drcParams, String decoderName, boolean runtimeChange,
            int expectedOutputLoudness)
            throws IOException
    {
        String localTag = TAG + ""#decodeToMemory"";
        short [] decoded = new short[0];
        int decodedIdx = 0;

        AssetFileDescriptor testFd = mResources.openRawResourceFd(testinput);

        MediaExtractor extractor;
        MediaCodec codec;
        ByteBuffer[] codecInputBuffers;
        ByteBuffer[] codecOutputBuffers;

        extractor = new MediaExtractor();
        extractor.setDataSource(testFd.getFileDescriptor(), testFd.getStartOffset(),
                testFd.getLength());
        testFd.close();

        assertEquals(""wrong number of tracks"", 1, extractor.getTrackCount());
        MediaFormat format = extractor.getTrackFormat(0);
        String mime = format.getString(MediaFormat.KEY_MIME);
        assertTrue(""not an audio file"", mime.startsWith(""audio/""));

        MediaFormat configFormat = format;
        if (decoderName == null) {
            codec = MediaCodec.createDecoderByType(mime);
        } else {
            codec = MediaCodec.createByCodecName(decoderName);
        }

        // set DRC parameters
        if (drcParams != null) {
            configFormat.setInteger(MediaFormat.KEY_AAC_DRC_BOOST_FACTOR, drcParams.mBoost);
            configFormat.setInteger(MediaFormat.KEY_AAC_DRC_ATTENUATION_FACTOR, drcParams.mCut);
            if (!runtimeChange) {
                if (drcParams.mDecoderTargetLevel != 0) {
                    configFormat.setInteger(MediaFormat.KEY_AAC_DRC_TARGET_REFERENCE_LEVEL,
                            drcParams.mDecoderTargetLevel);
                }
            }
            configFormat.setInteger(MediaFormat.KEY_AAC_DRC_HEAVY_COMPRESSION, drcParams.mHeavy);
        }
        Log.v(localTag, ""configuring with "" + configFormat);
        codec.configure(configFormat, null /* surface */, null /* crypto */, 0 /* flags */);

        if (drcParams != null && sIsAndroidRAndAbove) { // querying output format requires R
            if(!runtimeChange) {
                // check if MediaCodec gives back correct drc parameters
                if (drcParams.mDecoderTargetLevel != 0) {
                    final int targetLevelFromCodec = DecoderTest.getOutputFormatInteger(codec,
                            MediaFormat.KEY_AAC_DRC_TARGET_REFERENCE_LEVEL);
                    if (targetLevelFromCodec != drcParams.mDecoderTargetLevel) {
                        fail(""DRC Target Ref Level received from MediaCodec is not the level set"");
                    }
                }
            }
        }

        codec.start();
        codecInputBuffers = codec.getInputBuffers();
        codecOutputBuffers = codec.getOutputBuffers();

        if (drcParams != null) {
            if (runtimeChange) {
                if (drcParams.mDecoderTargetLevel != 0) {
                    Bundle b = new Bundle();
                    b.putInt(MediaFormat.KEY_AAC_DRC_TARGET_REFERENCE_LEVEL,
                            drcParams.mDecoderTargetLevel);
                    codec.setParameters(b);
                }
            }
        }

        extractor.selectTrack(0);

        // start decoding
        final long kTimeOutUs = 5000;
        MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
        boolean sawInputEOS = false;
        boolean sawOutputEOS = false;
        int noOutputCounter = 0;
        int samplecounter = 0;
        while (!sawOutputEOS && noOutputCounter < 50) {
            noOutputCounter++;
            if (!sawInputEOS) {
                int inputBufIndex = codec.dequeueInputBuffer(kTimeOutUs);

                if (inputBufIndex >= 0) {
                    ByteBuffer dstBuf = codecInputBuffers[inputBufIndex];

                    int sampleSize =
                        extractor.readSampleData(dstBuf, 0 /* offset */);

                    long presentationTimeUs = 0;

                    if (sampleSize < 0 && eossample > 0) {
                        fail(""test is broken: never reached eos sample"");
                    }
                    if (sampleSize < 0) {
                        Log.d(TAG, ""saw input EOS."");
                        sawInputEOS = true;
                        sampleSize = 0;
                    } else {
                        if (samplecounter == eossample) {
                            sawInputEOS = true;
                        }
                        samplecounter++;
                        presentationTimeUs = extractor.getSampleTime();
                    }
                    codec.queueInputBuffer(
                            inputBufIndex,
                            0 /* offset */,
                            sampleSize,
                            presentationTimeUs,
                            sawInputEOS ? MediaCodec.BUFFER_FLAG_END_OF_STREAM : 0);

                    if (!sawInputEOS) {
                        extractor.advance();
                    }
                }
            }

            int res = codec.dequeueOutputBuffer(info, kTimeOutUs);

            if (res >= 0) {
                //Log.d(TAG, ""got frame, size "" + info.size + ""/"" + info.presentationTimeUs);

                if (info.size > 0) {
                    noOutputCounter = 0;
                    if (timestamps != null) {
                        timestamps.add(info.presentationTimeUs);
                    }
                }

                int outputBufIndex = res;
                ByteBuffer buf = codecOutputBuffers[outputBufIndex];

                if (decodedIdx + (info.size / 2) >= decoded.length) {
                    decoded = Arrays.copyOf(decoded, decodedIdx + (info.size / 2));
                }

                buf.position(info.offset);
                for (int i = 0; i < info.size; i += 2) {
                    decoded[decodedIdx++] = buf.getShort();
                }

                codec.releaseOutputBuffer(outputBufIndex, false /* render */);

                if ((info.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                    Log.d(TAG, ""saw output EOS."");
                    sawOutputEOS = true;
                }
            } else if (res == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                codecOutputBuffers = codec.getOutputBuffers();

                Log.d(TAG, ""output buffers have changed."");
            } else if (res == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                MediaFormat oformat = codec.getOutputFormat();
                audioParams.setNumChannels(oformat.getInteger(MediaFormat.KEY_CHANNEL_COUNT));
                audioParams.setSamplingRate(oformat.getInteger(MediaFormat.KEY_SAMPLE_RATE));
                Log.d(TAG, ""output format has changed to "" + oformat);
            } else {
                Log.d(TAG, ""dequeueOutputBuffer returned "" + res);
            }
        }
        if (noOutputCounter >= 50) {
            fail(""decoder stopped outputing data"");
        }

        // check if MediaCodec gives back correct drc parameters (R and above)
        if (drcParams != null && sIsAndroidRAndAbove) {
            if (drcParams.mDecoderTargetLevel != 0) {
                final int targetLevelFromCodec = DecoderTest.getOutputFormatInteger(codec,
                        MediaFormat.KEY_AAC_DRC_TARGET_REFERENCE_LEVEL);
                if (targetLevelFromCodec != drcParams.mDecoderTargetLevel) {
                    fail(""DRC Target Ref Level received from MediaCodec is not the level set"");
                }
            }

            final int cutFromCodec = DecoderTest.getOutputFormatInteger(codec,
                    MediaFormat.KEY_AAC_DRC_ATTENUATION_FACTOR);
            assertEquals(""Attenuation factor received from MediaCodec differs from set:"",
                    drcParams.mCut, cutFromCodec);
            final int boostFromCodec = DecoderTest.getOutputFormatInteger(codec,
                    MediaFormat.KEY_AAC_DRC_BOOST_FACTOR);
            assertEquals(""Boost factor received from MediaCodec differs from set:"",
                    drcParams.mBoost, boostFromCodec);
        }

        // expectedOutputLoudness == -2 indicates that output loudness is not tested
        if (expectedOutputLoudness != -2 && sIsAndroidRAndAbove) {
            final int outputLoudnessFromCodec = DecoderTest.getOutputFormatInteger(codec,
                    MediaFormat.KEY_AAC_DRC_OUTPUT_LOUDNESS);
            if (outputLoudnessFromCodec != expectedOutputLoudness) {
                fail(""Received decoder output loudness is not the expected value"");
            }
        }

        codec.stop();
        codec.release();
        return decoded;
    }

    private short[] decodeToMemory(AudioParameter audioParams, int testinput,
            int eossample, List<Long> timestamps, DrcParams drcParams, String decoderName)
            throws IOException
    {
        final short[] decoded = decodeToMemory(audioParams, testinput, eossample, timestamps,
                drcParams, decoderName, false, -2);
        return decoded;
    }

    private short[] decodeToMemory(AudioParameter audioParams, int testinput,
            int eossample, List<Long> timestamps, DrcParams drcParams, String decoderName,
            boolean runtimeChange)
            throws IOException
    {
        final short[] decoded = decodeToMemory(audioParams, testinput, eossample, timestamps,
                drcParams, decoderName, runtimeChange, -2);
        return decoded;
    }

}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.ThumbnailUtilsTest"	"testCreateVideoThumbnail"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/ThumbnailUtilsTest.java"	""	"public void testCreateVideoThumbnail() throws Exception {
        final File file = stageFile(
                ""bbb_s1_720x480_mp4_h264_mp3_2mbps_30fps_aac_lc_5ch_320kbps_48000hz.mp4"",
                new File(mDir, ""cts.mp4""));
        for (Size size : TEST_SIZES) {
            assertSaneThumbnail(size, ThumbnailUtils.createVideoThumbnail(file, size, null));
        }
    }

    private static File stageFile(final String res, File file) throws IOException {
        final String mInpPrefix = WorkDir.getMediaDirString();
        Preconditions.assertTestFileExists(mInpPrefix + res);
        try (InputStream source = new FileInputStream(mInpPrefix + res);
                OutputStream target = new FileOutputStream(file)) {
            android.os.FileUtils.copy(source, target);
        }
        return file;
    }

    private static void deleteContents(File dir) {
        File[] files = dir.listFiles();
        if (files != null) {
            for (File file : files) {
                if (file.isDirectory()) {
                    deleteContents(file);
                }
                file.delete();
            }
        }
    }

    private static void assertSaneThumbnail(Size expected, Bitmap actualBitmap) {
        final Size actual = new Size(actualBitmap.getWidth(), actualBitmap.getHeight());
        final int maxWidth = (expected.getWidth() * 3) / 2;
        final int maxHeight = (expected.getHeight() * 3) / 2;
        if ((actual.getWidth() > maxWidth) || (actual.getHeight() > maxHeight)) {
            fail(""Actual "" + actual + "" differs too much from expected "" + expected);
        }
    }

    private static void assertColorMostlyInRange(@ColorInt int actual, @ColorInt int upperBound,
            @ColorInt int lowerBound) {
        assertTrue(Color.alpha(lowerBound) <= Color.alpha(actual)
                && Color.alpha(actual) <= Color.alpha(upperBound));
        assertTrue(Color.red(lowerBound) <= Color.red(actual)
                && Color.red(actual) <= Color.red(upperBound));
        assertTrue(Color.green(lowerBound) <= Color.green(actual)
                && Color.green(actual) <= Color.green(upperBound));
        assertTrue(Color.blue(lowerBound) <= Color.blue(actual)
                && Color.blue(actual) <= Color.blue(upperBound));
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.security.cts.MediaRecorderInfoLeakTest"	"test_cve_2016_2499"	"CtsSecurityTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/security/src/android/security/cts/MediaRecorderInfoLeakTest.java"	""	"public void test_cve_2016_2499() throws Exception {
        MediaRecorder mediaRecorder = null;
        long end = System.currentTimeMillis() + 600_000; // 10 minutes from now
        String TAG = ""MediaRecorderInfoLeakTest"";
        boolean recorderPrepare = false;
        int retryCount = 2;

        try {
            while(System.currentTimeMillis() < end) {
                recorderPrepare = false;
                for (int i = 0; i < retryCount; i++) {
                    try {
                        mediaRecorder = new MediaRecorder();
                        mediaRecorder.setAudioSource(MediaRecorder.AudioSource.DEFAULT);
                        mediaRecorder.setVideoSource(MediaRecorder.VideoSource.SURFACE);
                        mediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.THREE_GPP);
                        mediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.AAC);
                        mediaRecorder.setVideoEncoder(MediaRecorder.VideoEncoder.H263);
                        mediaRecorder.setVideoFrameRate(30);
                        mediaRecorder.setVideoSize(352, 288);
                        mediaRecorder.setOutputFile(
                                new File(getContext().getFilesDir(), ""record.output"").getPath());
                        mediaRecorder.prepare();
                        recorderPrepare = true;
                        break;
                    } catch (Exception ex) {
                        Log.w(TAG, ""Media Recorder Prepare Exception"" + ex.getMessage());
                        Thread.sleep(200);
                    } finally {
                        if (recorderPrepare == false && mediaRecorder != null){
                            mediaRecorder.release();
                        }
                    }
                }
                if(recorderPrepare){
                    int test = mediaRecorder.getMaxAmplitude();
                    mediaRecorder.reset();
                    mediaRecorder.release();
                    if(test != 0){
                        fail(""MediaRecorderInfoLeakTest failed"");
                    }
                } else {
                    fail(""Media Recorder prepare fail"");
                }
            }
        } catch (Exception e) {
            fail(""Media Recorder Exception"" + e.getMessage());
        } finally {
            if (mediaRecorder != null){
                mediaRecorder.release();
            }
        }
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.deviceinfo.CameraDeviceInfo"	"getAllCharacteristicsKeyNames"	""	"/home/gpoor/cts-12-source/cts/tools/cts-device-info/src/com/android/cts/deviceinfo/CameraDeviceInfo.java"	""	"public void test/*
 *.
 */
package com.android.cts.deviceinfo;

import android.content.Context;
import android.graphics.Rect;
import android.hardware.Camera;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.params.BlackLevelPattern;
import android.hardware.camera2.params.ColorSpaceTransform;
import android.hardware.camera2.params.StreamConfigurationMap;
import android.hardware.camera2.params.MultiResolutionStreamConfigurationMap;
import android.hardware.camera2.params.MultiResolutionStreamInfo;
import android.media.CamcorderProfile;
import android.os.Build;
import android.util.Log;
import android.util.Rational;
import android.util.Size;
import android.util.SizeF;
import android.util.Range;

import com.android.compatibility.common.deviceinfo.DeviceInfo;
import com.android.compatibility.common.util.DeviceInfoStore;

import java.lang.reflect.Array;
import java.lang.reflect.Field;
import java.lang.reflect.GenericArrayType;
import java.lang.reflect.Modifier;
import java.lang.reflect.ParameterizedType;
import java.lang.reflect.Type;
import java.util.ArrayList;
import java.util.Collection;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;

/**
 * Camera information collector.
 */
public final class CameraDeviceInfo extends DeviceInfo {
    private static final String TAG = ""CameraDeviceInfo"";

    private final static class CameraCharacteristicsStorer {
        private CameraManager mCameraManager;
        private DeviceInfoStore mStore;

        public CameraCharacteristicsStorer(CameraManager cameraManager, DeviceInfoStore store) {
            if (cameraManager == null || store == null) {
                throw new IllegalArgumentException(""can not create an CameraMetadataGetter object""
                        + "" with null CameraManager or null DeviceInfoStore"");
            }

            mCameraManager = cameraManager;
            mStore = store;
        }

        public void storeCameraInfo(String cameraId) throws Exception {
            try {
                CameraCharacteristics chars = mCameraManager.getCameraCharacteristics(cameraId);
                mStore.startGroup(); // per camera chars
                mStore.addResult(""cameraId"", cameraId);
                storeCameraChars(chars);
                mStore.endGroup(); // per camera chars
            } catch (CameraAccessException e) {
                Log.e(TAG,
                        ""Unable to get camera camera static info, skip this camera, error: ""
                                + e.getMessage());
            }
            return;
        }

        public void storePhysicalCameraInfo(String cameraId, List<String> logicalCameras)
                throws Exception {
            try {
                CameraCharacteristics chars = mCameraManager.getCameraCharacteristics(cameraId);
                mStore.startGroup(); // per camera chars
                mStore.addResult(""cameraId"", cameraId);
                mStore.addListResult(""parentLogicalCameraIds"", logicalCameras);
                storeCameraChars(chars);
                mStore.endGroup(); // per camera chars
            } catch (CameraAccessException e) {
                Log.e(TAG,
                        ""Unable to get camera camera static info, skip this camera, error: ""
                                + e.getMessage());
            }
            return;
        }

        private void storeRational(
                Rational rat, String protoName) throws Exception {
            if (protoName == null) {
                mStore.startGroup();
            } else {
                mStore.startGroup(protoName);
            }
            mStore.addResult(""numerator"", rat.getNumerator());
            mStore.addResult(""denominator"", rat.getDenominator());
            mStore.endGroup();
        }

        private void storeSize(
                Size size, String protoName) throws Exception {
            if (protoName == null) {
                mStore.startGroup();
            } else {
                mStore.startGroup(protoName);
            }
            mStore.addResult(""width"", size.getWidth());
            mStore.addResult(""height"", size.getHeight());
            mStore.endGroup();
        }

        private void storeSizeF(
                SizeF size, String protoName) throws Exception {
            if (protoName == null) {
                mStore.startGroup();
            } else {
                mStore.startGroup(protoName);
            }
            mStore.addResult(""width"", size.getWidth());
            mStore.addResult(""height"", size.getHeight());
            mStore.endGroup();
        }

        private void storeRect(
                Rect rect, String protoName) throws Exception {
            if (protoName == null) {
                mStore.startGroup();
            } else {
                mStore.startGroup(protoName);
            }
            mStore.addResult(""left"", rect.left);
            mStore.addResult(""right"", rect.right);
            mStore.addResult(""top"", rect.top);
            mStore.addResult(""bottom"", rect.bottom);
            mStore.endGroup();
        }

        private void storeStreamConfigurationMap(
                StreamConfigurationMap map, String protoName) throws Exception {
            if (protoName == null) {
                mStore.startGroup();
            } else {
                mStore.startGroup(protoName);
            }

            int fmts[] = map.getOutputFormats();
            if (fmts != null) {
                mStore.startArray(""availableStreamConfigurations"");
                for (int fi = 0; fi < Array.getLength(fmts); fi++) {
                    Size sizes[] = map.getOutputSizes(fmts[fi]);
                    if (sizes != null) {
                        for (int si = 0; si < Array.getLength(sizes); si++) {
                            mStore.startGroup();
                            mStore.addResult(""format"", fmts[fi]);
                            mStore.addResult(""width"", sizes[si].getWidth());
                            mStore.addResult(""height"", sizes[si].getHeight());
                            mStore.addResult(""input"", false);
                            mStore.addResult(""minFrameDuration"",
                                            map.getOutputMinFrameDuration(fmts[fi], sizes[si]));
                            mStore.endGroup();
                        }
                    }
                }
                mStore.endArray();
            }

            Size[] highSpeedVideoSizes = map.getHighSpeedVideoSizes();
            if (highSpeedVideoSizes != null && highSpeedVideoSizes.length > 0) {
                mStore.startArray(""availableHighSpeedVideoConfigurations"");
                for (int i = 0; i < highSpeedVideoSizes.length; i++) {
                    Range<Integer>[] fpsRanges = map.getHighSpeedVideoFpsRangesFor(
                            highSpeedVideoSizes[i]);
                    if (fpsRanges != null && fpsRanges.length > 0) {
                        for (int j = 0; j < fpsRanges.length; j++) {
                            mStore.startGroup();
                            mStore.addResult(""width"", highSpeedVideoSizes[i].getWidth());
                            mStore.addResult(""height"", highSpeedVideoSizes[i].getHeight());
                            mStore.addResult(""minFps"", fpsRanges[j].getLower());
                            mStore.addResult(""maxFps"", fpsRanges[j].getUpper());
                            mStore.endGroup();
                        }
                    }
                }
                mStore.endArray();
            }

            int inputFmts[] = map.getInputFormats();
            if (inputFmts != null && inputFmts.length > 0) {
                mStore.startArray(""availableInputConfigurations"");
                for (int i = 0 ; i < inputFmts.length; i++) {
                    Size[] inputSizes = map.getInputSizes(inputFmts[i]);
                    if (inputSizes != null && inputSizes.length > 0) {
                        for (int j = 0; j < inputSizes.length; j++) {
                            mStore.startGroup();
                            mStore.addResult(""inputFormat"", inputFmts[i]);
                            mStore.addResult(""inputWidth"", inputSizes[j].getWidth());
                            mStore.addResult(""inputHeight"", inputSizes[j].getHeight());
                            mStore.endGroup();
                        }
                    }
                }
                mStore.endArray();

                mStore.startArray(""availableInputOutputFormatsMap"");
                for (int i = 0 ; i < inputFmts.length; i++) {
                    int[] outputFmts = map.getValidOutputFormatsForInput(inputFmts[i]);
                    if (outputFmts != null && outputFmts.length > 0) {
                        for (int j = 0; j < outputFmts.length; j++) {
                            mStore.startGroup();
                            mStore.addResult(""inputFormat"", inputFmts[i]);
                            mStore.addResult(""outputFormat"", outputFmts[j]);
                            mStore.endGroup();
                        }
                    }
                }
                mStore.endArray();
            }

            mStore.endGroup();
        }

        private void storeRangeFloat(
                Range<Float> range, String protoName) throws Exception {
            if (protoName == null) {
                mStore.startGroup();
            } else {
                mStore.startGroup(protoName);
            }
            mStore.addResult(""lower"", range.getLower());
            mStore.addResult(""upper"", range.getUpper());
            mStore.endGroup();
        }

        private void storeRangeInt(
                Range<Integer> range, String protoName) throws Exception {
            if (protoName == null) {
                mStore.startGroup();
            } else {
                mStore.startGroup(protoName);
            }
            mStore.addResult(""lower"", range.getLower());
            mStore.addResult(""upper"", range.getUpper());
            mStore.endGroup();
        }

        private void storeRangeLong(
                Range<Long> range, String protoName) throws Exception {
            if (protoName == null) {
                mStore.startGroup();
            } else {
                mStore.startGroup(protoName);
            }
            mStore.addResult(""lower"", range.getLower());
            mStore.addResult(""upper"", range.getUpper());
            mStore.endGroup();
        }

        private void storeColorSpaceTransform(
                ColorSpaceTransform xform, String protoName) throws Exception {
            if (protoName == null) {
                mStore.startGroup();
            } else {
                mStore.startGroup(protoName);
            }

            mStore.startArray(""elements"");
            for (int row = 0; row < 3; row++) {
                for (int col = 0; col < 3; col++) {
                    storeRational((Rational) xform.getElement(col, row), null);
                }
            }
            mStore.endArray();
            mStore.endGroup();
        }

        private void storeBlackLevelPattern(
                BlackLevelPattern pat, String protoName) throws Exception {
            if (protoName == null) {
                mStore.startGroup();
            } else {
                mStore.startGroup(protoName);
            }
            int patVals[] = new int[4];
            pat.copyTo(patVals, 0);
            mStore.addArrayResult(""black_level_pattern"", patVals);
            mStore.endGroup();
        }

        private void storeMultiResStreamConfigurationMap(
                MultiResolutionStreamConfigurationMap map, String protoName) throws Exception {
            if (protoName == null) {
                mStore.startGroup();
            } else {
                mStore.startGroup(protoName);
            }

            mStore.startArray(""availableMultiResolutionConfigurations"");
            int[] fmts = map.getOutputFormats();
            if (fmts != null) {
                for (int fi = 0; fi < Array.getLength(fmts); fi++) {
                    Collection<MultiResolutionStreamInfo> streamInfo = map.getOutputInfo(fmts[fi]);
                    if (streamInfo != null) {
                        for (MultiResolutionStreamInfo oneStream : streamInfo) {
                            mStore.startGroup();
                            mStore.addResult(""format"", fmts[fi]);
                            mStore.addResult(""width"", oneStream.getWidth());
                            mStore.addResult(""height"", oneStream.getHeight());
                            mStore.addResult(""cameraId"", oneStream.getPhysicalCameraId());
                            mStore.addResult(""input"", false);
                            mStore.endGroup();
                        }
                    }
                }
            }

            int[] inputFmts = map.getInputFormats();
            if (inputFmts != null) {
                for (int fi = 0; fi < Array.getLength(inputFmts); fi++) {
                    Collection<MultiResolutionStreamInfo> streamInfo =
                            map.getInputInfo(inputFmts[fi]);
                    if (streamInfo != null) {
                        for (MultiResolutionStreamInfo oneStream : streamInfo) {
                            mStore.startGroup();
                            mStore.addResult(""format"", inputFmts[fi]);
                            mStore.addResult(""width"", oneStream.getWidth());
                            mStore.addResult(""height"", oneStream.getHeight());
                            mStore.addResult(""cameraId"", oneStream.getPhysicalCameraId());
                            mStore.addResult(""input"", true);
                            mStore.endGroup();
                        }
                    }
                }
            }
            mStore.endArray();
            mStore.endGroup();
        }

        private static String getKeyName(Object keyObj) {
            return ((CameraCharacteristics.Key) keyObj).getName();
        }

        private static Object getKeyValue(CameraCharacteristics chars, Object keyObj) {
            return chars.get((CameraCharacteristics.Key) keyObj);
        }

        private void storeEntry(Type keyType, Object keyObj,
                CameraCharacteristics chars) throws Exception {
            String keyName = getKeyName(keyObj);
            String protoName = keyName.replace('.', '_');
            Object keyValue = getKeyValue(chars, keyObj);
            if (keyValue == null) {
                return;
            }

            if (keyType == int.class || keyType == Integer.class) {
                mStore.addResult(protoName, (int) keyValue);
                return;
            } else if (keyType == float.class || keyType == Float.class) {
                mStore.addResult(protoName, (float) keyValue);
                return;
            } else if (keyType == long.class || keyType == Long.class) {
                mStore.addResult(protoName, (long) keyValue);
                return;
            } else if (keyType == double.class || keyType == Double.class) {
                mStore.addResult(protoName, (double) keyValue);
                return;
            } else if (keyType == boolean.class || keyType == Boolean.class) {
                mStore.addResult(protoName, (boolean) keyValue);
                return;
            } else if (keyType == byte.class || keyType == Byte.class) {
                // Infostore does not support byte, convert to int32 and save
                int intValue = (int) ((byte) keyValue);
                mStore.addResult(protoName, intValue);
                return;
            } else if (keyType == Rational.class) {
                storeRational((Rational) keyValue, protoName);
                return;
            } else if (keyType == Size.class) {
                storeSize((Size) keyValue, protoName);
                return;
            } else if (keyType == SizeF.class) {
                storeSizeF((SizeF) keyValue, protoName);
                return;
            } else if (keyType == Rect.class) {
                storeRect((Rect) keyValue, protoName);
                return;
            } else if (keyType == StreamConfigurationMap.class) {
                storeStreamConfigurationMap(
                        (StreamConfigurationMap) keyValue, protoName);
                return;
            } else if (keyType instanceof ParameterizedType &&
                    ((ParameterizedType) keyType).getRawType() == Range.class &&
                    ((ParameterizedType) keyType).getActualTypeArguments()[0] == Float.class) {
                storeRangeFloat((Range<Float>) keyValue, protoName);
                return;
            } else if (keyType instanceof ParameterizedType &&
                    ((ParameterizedType) keyType).getRawType() == Range.class &&
                    ((ParameterizedType) keyType).getActualTypeArguments()[0] == Integer.class) {
                storeRangeInt((Range<Integer>) keyValue, protoName);
                return;
            } else if (keyType instanceof ParameterizedType &&
                    ((ParameterizedType) keyType).getRawType() == Range.class &&
                    ((ParameterizedType) keyType).getActualTypeArguments()[0] == Long.class) {
                storeRangeLong((Range<Long>) keyValue, protoName);
                return;
            } else if (keyType == ColorSpaceTransform.class) {
                storeColorSpaceTransform((ColorSpaceTransform) keyValue, protoName);
                return;
            } else if (keyType == BlackLevelPattern.class) {
                storeBlackLevelPattern((BlackLevelPattern) keyValue, protoName);
                return;
            } else if (keyType == MultiResolutionStreamConfigurationMap.class) {
                storeMultiResStreamConfigurationMap(
                        (MultiResolutionStreamConfigurationMap) keyValue, protoName);
            } else {
                Log.w(TAG, ""Storing unsupported key type: "" + keyType +
                        "" for keyName: "" + keyName);
                return;
            }
        }

        private void storeArrayEntry(Type keyType, Object keyObj,
                CameraCharacteristics chars) throws Exception {
            String keyName = getKeyName(keyObj);
            String protoName = keyName.replace('.', '_');
            Object keyValue = getKeyValue(chars, keyObj);
            if (keyValue == null) {
                return;
            }

            int arrayLen = Array.getLength(keyValue);
            if (arrayLen == 0) {
                return;
            }
            Type elmtType = ((GenericArrayType) keyType).getGenericComponentType();

            if (elmtType == int.class) {
                mStore.addArrayResult(protoName, (int[]) keyValue);
                return;
            } else if (elmtType == float.class) {
                mStore.addArrayResult(protoName, (float[]) keyValue);
                return;
            } else if (elmtType == long.class) {
                mStore.addArrayResult(protoName, (long[]) keyValue);
                return;
            } else if (elmtType == double.class) {
                mStore.addArrayResult(protoName, (double[]) keyValue);
                return;
            } else if (elmtType == boolean.class) {
                mStore.addArrayResult(protoName, (boolean[]) keyValue);
                return;
            } else if (elmtType == byte.class) {
                // Infostore does not support byte, convert to int32 and save
                int[] intValues = new int[arrayLen];
                for (int i = 0; i < arrayLen; i++) {
                    intValues[i] = (int) ((byte) Array.get(keyValue, i));
                }
                mStore.addArrayResult(protoName, intValues);
                return;
            } else if (elmtType == Rational.class) {
                mStore.startArray(protoName);
                for (int i = 0; i < arrayLen; i++) {
                    storeRational((Rational) Array.get(keyValue, i), null);
                }
                mStore.endArray();
                return;
            } else if (elmtType == Size.class) {
                mStore.startArray(protoName);
                for (int i = 0; i < arrayLen; i++) {
                    storeSize((Size) Array.get(keyValue, i), null);
                }
                mStore.endArray();
                return;
            } else if (elmtType == Rect.class) {
                mStore.startArray(protoName);
                for (int i = 0; i < arrayLen; i++) {
                    storeRect((Rect) Array.get(keyValue, i), null);
                }
                mStore.endArray();
                return;
            } else if (elmtType instanceof ParameterizedType &&
                    ((ParameterizedType) elmtType).getRawType() == Range.class &&
                    ((ParameterizedType) elmtType).getActualTypeArguments()[0] == Integer.class) {
                mStore.startArray(protoName);
                for (int i = 0; i < arrayLen; i++) {
                    storeRangeInt((Range<Integer>) Array.get(keyValue, i), null);
                }
                mStore.endArray();
                return;
            } else if (elmtType == BlackLevelPattern.class) {
                mStore.startArray(protoName);
                for (int i = 0; i < arrayLen; i++) {
                    storeBlackLevelPattern((BlackLevelPattern) Array.get(keyValue, i), null);
                }
                mStore.endArray();
                return;
            } else {
                Log.w(TAG, ""Storing unsupported array type: "" + elmtType +
                        "" for keyName: "" + keyName);
                return;
            }
        }

        private void storeCameraChars(
                CameraCharacteristics chars) throws Exception {
            HashSet<String> charsKeyNames = getAllCharacteristicsKeyNames();
            Field[] allFields = chars.getClass().getDeclaredFields();
            for (Field field : allFields) {
                if (Modifier.isPublic(field.getModifiers()) &&
                        Modifier.isStatic(field.getModifiers()) &&
                        field.getType() == CameraCharacteristics.Key.class &&
                        field.getGenericType() instanceof ParameterizedType) {
                    ParameterizedType paramType = (ParameterizedType) field.getGenericType();
                    Type[] argTypes = paramType.getActualTypeArguments();
                    if (argTypes.length > 0) {
                        try {
                            Type keyType = argTypes[0];
                            Object keyObj = field.get(chars);
                            String keyName = getKeyName(keyObj);
                            if (charsKeyNames.contains(keyName)) {
                                if (keyType instanceof GenericArrayType) {
                                    storeArrayEntry(keyType, keyObj, chars);
                                } else {
                                    storeEntry(keyType, keyObj, chars);
                                }
                            }
                        } catch (IllegalAccessException e) {
                            throw new IllegalStateException(
                                    ""Access error for field: "" + field + "": "", e);
                        }
                    }
                }
            }
        }
    }


    @Override
    protected void collectDeviceInfo(DeviceInfoStore store) throws Exception {
        store.addResult(""profile_480p"", CamcorderProfile.hasProfile(CamcorderProfile.QUALITY_480P));
        store.addResult(""profile_720p"", CamcorderProfile.hasProfile(CamcorderProfile.QUALITY_720P));
        store.addResult(""profile_1080p"", CamcorderProfile.hasProfile(CamcorderProfile.QUALITY_1080P));
        store.addResult(""profile_cif"", CamcorderProfile.hasProfile(CamcorderProfile.QUALITY_CIF));
        store.addResult(""profile_qcif"", CamcorderProfile.hasProfile(CamcorderProfile.QUALITY_QCIF));
        store.addResult(""profile_qvga"", CamcorderProfile.hasProfile(CamcorderProfile.QUALITY_QVGA));

        CameraManager cameraManager = (CameraManager)
                getContext().getSystemService(Context.CAMERA_SERVICE);
        try {
            String[] cameraIdList = cameraManager.getCameraIdList();
            HashMap<String, ArrayList<String>> physicalLogicalIdMap =
                    new HashMap<String, ArrayList<String>>();
            store.addResult(""num_of_camera"", cameraIdList.length);
            if (cameraIdList.length > 0) {
                CameraCharacteristicsStorer charsStorer =
                        new CameraCharacteristicsStorer(cameraManager, store);
                store.startArray(""per_camera_info"");
                for (int i = 0; i < cameraIdList.length; i++) {
                    charsStorer.storeCameraInfo(cameraIdList[i]);

                    // Get the physical camera ids
                    CameraCharacteristics ch = cameraManager.getCameraCharacteristics(
                            cameraIdList[i]);
                    for (String physicalId : ch.getPhysicalCameraIds()) {
                        if (physicalLogicalIdMap.get(physicalId) == null) {
                            physicalLogicalIdMap.put(physicalId, new ArrayList<String>());
                        }
                        physicalLogicalIdMap.get(physicalId).add(cameraIdList[i]);
                    }
                }
                store.endArray(); // per_camera_info

                // Store characteristics for hidden physical camera ids
                for (int i = 0; i < cameraIdList.length; ++i) {
                    physicalLogicalIdMap.remove(cameraIdList[i]);
                }
                if (physicalLogicalIdMap.size() > 0) {
                    store.addResult(""num_of_hidden_physical_camera"", physicalLogicalIdMap.size());
                    store.startArray(""per_hidden_physical_camera_info"");
                    for (String physicalId : physicalLogicalIdMap.keySet()) {
                        charsStorer.storePhysicalCameraInfo(physicalId,
                                physicalLogicalIdMap.get(physicalId));
                    }
                    store.endArray(); // per_hidden_physical_camera_info
                }
            }
        } catch (CameraAccessException e) {
            Log.e(TAG,
                    ""Unable to get camera camera ID list, error: ""
                            + e.getMessage());
        }
    }

    /*@O~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~
     * The key entries below this point are generated from metadata
     * definitions in /system/media/camera/docs. Do not modify by hand or
     * modify the comment blocks at the start or end.
     *~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~*/

    private static HashSet<String> getAllCharacteristicsKeyNames() {
        HashSet<String> charsKeyNames = new HashSet<String>();
        charsKeyNames.add(CameraCharacteristics.COLOR_CORRECTION_AVAILABLE_ABERRATION_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_AE_AVAILABLE_ANTIBANDING_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_AE_AVAILABLE_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_AE_COMPENSATION_RANGE.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_AE_COMPENSATION_STEP.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_AF_AVAILABLE_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_AVAILABLE_EFFECTS.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_AVAILABLE_SCENE_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_AWB_AVAILABLE_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_MAX_REGIONS_AE.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_MAX_REGIONS_AWB.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_MAX_REGIONS_AF.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_AE_LOCK_AVAILABLE.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_AWB_LOCK_AVAILABLE.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_AVAILABLE_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_POST_RAW_SENSITIVITY_BOOST_RANGE.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_AVAILABLE_EXTENDED_SCENE_MODE_CAPABILITIES.getName());
        charsKeyNames.add(CameraCharacteristics.CONTROL_ZOOM_RATIO_RANGE.getName());
        charsKeyNames.add(CameraCharacteristics.EDGE_AVAILABLE_EDGE_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.FLASH_INFO_AVAILABLE.getName());
        charsKeyNames.add(CameraCharacteristics.HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.JPEG_AVAILABLE_THUMBNAIL_SIZES.getName());
        charsKeyNames.add(CameraCharacteristics.LENS_FACING.getName());
        charsKeyNames.add(CameraCharacteristics.LENS_POSE_REFERENCE.getName());
        charsKeyNames.add(CameraCharacteristics.LENS_DISTORTION_MAXIMUM_RESOLUTION.getName());
        charsKeyNames.add(CameraCharacteristics.LENS_INTRINSIC_CALIBRATION_MAXIMUM_RESOLUTION.getName());
        charsKeyNames.add(CameraCharacteristics.LENS_INFO_AVAILABLE_APERTURES.getName());
        charsKeyNames.add(CameraCharacteristics.LENS_INFO_AVAILABLE_FILTER_DENSITIES.getName());
        charsKeyNames.add(CameraCharacteristics.LENS_INFO_AVAILABLE_FOCAL_LENGTHS.getName());
        charsKeyNames.add(CameraCharacteristics.LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION.getName());
        charsKeyNames.add(CameraCharacteristics.LENS_INFO_HYPERFOCAL_DISTANCE.getName());
        charsKeyNames.add(CameraCharacteristics.LENS_INFO_MINIMUM_FOCUS_DISTANCE.getName());
        charsKeyNames.add(CameraCharacteristics.LENS_INFO_FOCUS_DISTANCE_CALIBRATION.getName());
        charsKeyNames.add(CameraCharacteristics.NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.REQUEST_MAX_NUM_OUTPUT_RAW.getName());
        charsKeyNames.add(CameraCharacteristics.REQUEST_MAX_NUM_OUTPUT_PROC.getName());
        charsKeyNames.add(CameraCharacteristics.REQUEST_MAX_NUM_OUTPUT_PROC_STALLING.getName());
        charsKeyNames.add(CameraCharacteristics.REQUEST_MAX_NUM_INPUT_STREAMS.getName());
        charsKeyNames.add(CameraCharacteristics.REQUEST_PIPELINE_MAX_DEPTH.getName());
        charsKeyNames.add(CameraCharacteristics.REQUEST_PARTIAL_RESULT_COUNT.getName());
        charsKeyNames.add(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES.getName());
        charsKeyNames.add(CameraCharacteristics.SCALER_AVAILABLE_MAX_DIGITAL_ZOOM.getName());
        charsKeyNames.add(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP.getName());
        charsKeyNames.add(CameraCharacteristics.SCALER_CROPPING_TYPE.getName());
        charsKeyNames.add(CameraCharacteristics.SCALER_MANDATORY_STREAM_COMBINATIONS.getName());
        charsKeyNames.add(CameraCharacteristics.SCALER_MANDATORY_CONCURRENT_STREAM_COMBINATIONS.getName());
        charsKeyNames.add(CameraCharacteristics.SCALER_AVAILABLE_ROTATE_AND_CROP_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.SCALER_DEFAULT_SECURE_IMAGE_SIZE.getName());
        charsKeyNames.add(CameraCharacteristics.SCALER_MULTI_RESOLUTION_STREAM_CONFIGURATION_MAP.getName());
        charsKeyNames.add(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP_MAXIMUM_RESOLUTION.getName());
        charsKeyNames.add(CameraCharacteristics.SCALER_MANDATORY_MAXIMUM_RESOLUTION_STREAM_COMBINATIONS.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_REFERENCE_ILLUMINANT1.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_REFERENCE_ILLUMINANT2.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_CALIBRATION_TRANSFORM1.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_CALIBRATION_TRANSFORM2.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_COLOR_TRANSFORM1.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_COLOR_TRANSFORM2.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_FORWARD_MATRIX1.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_FORWARD_MATRIX2.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_BLACK_LEVEL_PATTERN.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_MAX_ANALOG_SENSITIVITY.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_ORIENTATION.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_AVAILABLE_TEST_PATTERN_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_OPTICAL_BLACK_REGIONS.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_INFO_MAX_FRAME_DURATION.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_INFO_PHYSICAL_SIZE.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_INFO_PIXEL_ARRAY_SIZE.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_INFO_WHITE_LEVEL.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_INFO_TIMESTAMP_SOURCE.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_INFO_LENS_SHADING_APPLIED.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_INFO_PIXEL_ARRAY_SIZE_MAXIMUM_RESOLUTION.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION.getName());
        charsKeyNames.add(CameraCharacteristics.SENSOR_INFO_BINNING_FACTOR.getName());
        charsKeyNames.add(CameraCharacteristics.SHADING_AVAILABLE_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.STATISTICS_INFO_AVAILABLE_FACE_DETECT_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.STATISTICS_INFO_MAX_FACE_COUNT.getName());
        charsKeyNames.add(CameraCharacteristics.STATISTICS_INFO_AVAILABLE_HOT_PIXEL_MAP_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.STATISTICS_INFO_AVAILABLE_LENS_SHADING_MAP_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.STATISTICS_INFO_AVAILABLE_OIS_DATA_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.TONEMAP_MAX_CURVE_POINTS.getName());
        charsKeyNames.add(CameraCharacteristics.TONEMAP_AVAILABLE_TONE_MAP_MODES.getName());
        charsKeyNames.add(CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL.getName());
        charsKeyNames.add(CameraCharacteristics.INFO_VERSION.getName());
        charsKeyNames.add(CameraCharacteristics.SYNC_MAX_LATENCY.getName());
        charsKeyNames.add(CameraCharacteristics.REPROCESS_MAX_CAPTURE_STALL.getName());
        charsKeyNames.add(CameraCharacteristics.DEPTH_DEPTH_IS_EXCLUSIVE.getName());
        charsKeyNames.add(CameraCharacteristics.LOGICAL_MULTI_CAMERA_SENSOR_SYNC_TYPE.getName());
        charsKeyNames.add(CameraCharacteristics.DISTORTION_CORRECTION_AVAILABLE_MODES.getName());

        return charsKeyNames;
    }

    /*~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~
     * End generated code
     *~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~O@*/
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediapc.cts.EncoderInitializationLatencyTest"	"isRPerfClass"	"CtsMediaPerformanceClassTestCases"	"/home/gpoor/cts-12-source/cts/tests/mediapc/src/android/mediapc/cts/EncoderInitializationLatencyTest.java"	""	"/*
 *.
 */

package android.mediapc.cts;

import android.app.Instrumentation;
import android.content.Context;
import android.content.pm.PackageManager;
import android.media.MediaCodec;
import android.media.MediaCodecInfo;
import android.media.MediaCodecList;
import android.media.MediaFormat;
import android.media.MediaRecorder;
import android.os.Build;
import android.util.Log;
import android.util.Pair;
import android.view.Surface;

import androidx.test.filters.LargeTest;
import androidx.test.platform.app.InstrumentationRegistry;
import androidx.test.rule.ActivityTestRule;

import com.android.compatibility.common.util.CddTest;
import com.android.compatibility.common.util.ReportLog;
import com.android.compatibility.common.util.ReportLog.Metric;

import com.android.compatibility.common.util.DeviceReportLog;
import com.android.compatibility.common.util.ResultType;
import com.android.compatibility.common.util.ResultUnit;

import org.junit.After;
import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.junit.runners.Parameterized;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.List;

import static android.mediapc.cts.CodecTestBase.selectCodecs;
import static android.mediapc.cts.CodecTestBase.selectHardwareCodecs;
import static org.junit.Assert.assertNotNull;
import static org.junit.Assert.assertTrue;
import static org.junit.Assume.assumeFalse;
import static org.junit.Assume.assumeTrue;

/**
 * The following test class validates the codec initialization latency (time for codec create +
 * configure) for the audio encoders and hardware video encoders available in the device, under the
 * load condition (Transcode + MediaRecorder session Audio(Microphone) and 1080p Video(Camera)).
 */
@RunWith(Parameterized.class)
public class EncoderInitializationLatencyTest {
    private static final String LOG_TAG = EncoderInitializationLatencyTest.class.getSimpleName();
    private static final boolean[] boolStates = {false, true};
    private static final int MAX_AUDIOENC_INITIALIZATION_LATENCY_PC_R_MS = 50;
    private static final int MAX_VIDEOENC_INITIALIZATION_LATENCY_PC_R_MS = 65;
    private static final int MAX_AUDIOENC_INITIALIZATION_LATENCY_PC_S_MS = 40;
    private static final int MAX_VIDEOENC_INITIALIZATION_LATENCY_PC_S_MS = 50;

    private static final int MAX_AUDIOENC_INITIALIZATION_LATENCY_MS;
    private static final int MAX_VIDEOENC_INITIALIZATION_LATENCY_MS;
    private static final String AVC = MediaFormat.MIMETYPE_VIDEO_AVC;
    private static final String HEVC = MediaFormat.MIMETYPE_VIDEO_HEVC;
    private static final String AVC_TRANSCODE_FILE = ""bbb_1280x720_3mbps_30fps_avc.mp4"";
    private static String AVC_DECODER_NAME;
    private static String AVC_ENCODER_NAME;

    static {
        if (Utils.isRPerfClass()) {
            MAX_AUDIOENC_INITIALIZATION_LATENCY_MS = MAX_AUDIOENC_INITIALIZATION_LATENCY_PC_R_MS;
            MAX_VIDEOENC_INITIALIZATION_LATENCY_MS = MAX_VIDEOENC_INITIALIZATION_LATENCY_PC_R_MS;
        } else {
            // Performance class Build.VERSION_CODES.S and beyond
            MAX_AUDIOENC_INITIALIZATION_LATENCY_MS = MAX_AUDIOENC_INITIALIZATION_LATENCY_PC_S_MS;
            MAX_VIDEOENC_INITIALIZATION_LATENCY_MS = MAX_VIDEOENC_INITIALIZATION_LATENCY_PC_S_MS;
        }
    }

    private final String mMime;
    private final String mEncoderName;

    private LoadStatus mTranscodeLoadStatus = null;
    private Thread mTranscodeLoadThread = null;
    private MediaRecorder mMediaRecorderLoad = null;
    private File mTempRecordedFile = null;
    private Surface mSurface = null;
    private Exception mException = null;

    @Before
    public void setUp() throws Exception {
        Utils.assumeDeviceMeetsPerformanceClassPreconditions();

        ArrayList<String>  listOfAvcHwDecoders = selectHardwareCodecs(AVC, null, null, false);
        assumeFalse(""Test requires h/w avc decoder"", listOfAvcHwDecoders.isEmpty());
        AVC_DECODER_NAME = listOfAvcHwDecoders.get(0);

        ArrayList<String> listOfAvcHwEncoders = selectHardwareCodecs(AVC, null, null, true);
        assumeFalse(""Test requires h/w avc encoder"", listOfAvcHwEncoders.isEmpty());
        AVC_ENCODER_NAME = listOfAvcHwEncoders.get(0);

        Instrumentation instrumentation = InstrumentationRegistry.getInstrumentation();
        Context context = instrumentation.getTargetContext();
        PackageManager packageManager = context.getPackageManager();
        assertNotNull(packageManager.getSystemAvailableFeatures());
        assumeTrue(""The device doesn't have a camera"",
                packageManager.hasSystemFeature(PackageManager.FEATURE_CAMERA_ANY));
        assumeTrue(""The device doesn't have a microphone"",
                packageManager.hasSystemFeature(PackageManager.FEATURE_MICROPHONE));
        createSurface();
        startLoad();
    }

    @After
    public void tearDown() throws Exception {
        stopLoad();
        releaseSurface();
    }

    public EncoderInitializationLatencyTest(String mimeType, String encoderName) {
        mMime = mimeType;
        mEncoderName = encoderName;
    }

    @Rule
    public ActivityTestRule<TestActivity> mActivityRule =
            new ActivityTestRule<>(TestActivity.class);

    // Returns the list of all available hardware video encoders in the device.
    static ArrayList<String> getMimesOfAvailableHardwareVideoEncoders() {
        MediaCodecList codecList = new MediaCodecList(MediaCodecList.REGULAR_CODECS);
        MediaCodecInfo[] codecInfos = codecList.getCodecInfos();
        ArrayList<String> listOfMimes = new ArrayList<>();
        for (MediaCodecInfo codecInfo : codecInfos) {
            if (!codecInfo.isEncoder() || !codecInfo.isHardwareAccelerated()) continue;
            String[] types = codecInfo.getSupportedTypes();
            for (String type : types) {
                if (type.startsWith(""video/"") && !listOfMimes.contains(type)) {
                    listOfMimes.add(type);
                }
            }
        }
        return listOfMimes;
    }

    // Returns the list of all available audio encoders in the device.
    static ArrayList<String> getMimesOfAvailableAudioEncoders() {
        MediaCodecList codecList = new MediaCodecList(MediaCodecList.REGULAR_CODECS);
        MediaCodecInfo[] codecInfos = codecList.getCodecInfos();
        ArrayList<String> listOfMimes = new ArrayList<>();
        for (MediaCodecInfo codecInfo : codecInfos) {
            if (!codecInfo.isEncoder()) continue;
            String[] types = codecInfo.getSupportedTypes();
            for (String type : types) {
                if (type.startsWith(""audio/"") && !listOfMimes.contains(type)) {
                    listOfMimes.add(type);
                }
            }
        }
        return listOfMimes;
    }

    // Returns the list of parameters with mimetype and their encoder(for audio - all encoders,
    // video - hardware encoders).
    // Parameters {0}_{1} -- Mime_EncoderName
    @Parameterized.Parameters(name = ""{index}({0}_{1})"")
    public static Collection<Object[]> inputParams() {
        // Prepares the params list with the required Hardware video encoders and all available
        // audio encoders present in the device.
        final List<Object[]> argsList = new ArrayList<>();
        ArrayList<String> mimesList = getMimesOfAvailableHardwareVideoEncoders();
        mimesList.addAll(getMimesOfAvailableAudioEncoders());
        for (String mime : mimesList) {
            ArrayList<String> listOfEncoders;
            if (mime.startsWith(""audio/"")) {
                listOfEncoders = selectCodecs(mime, null, null, true);
            } else {
                listOfEncoders = selectHardwareCodecs(mime, null, null, true);
            }
            for (String encoder : listOfEncoders) {
                argsList.add(new Object[] {mime, encoder});
            }
        }
        return argsList;
    }

    private MediaRecorder createMediaRecorderLoad(Surface surface) throws Exception {
        MediaRecorder mediaRecorder = new MediaRecorder();
        mediaRecorder.setAudioSource(MediaRecorder.AudioSource.MIC);
        mediaRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);
        mediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.DEFAULT);
        mediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.AAC);
        mediaRecorder.setVideoEncoder(mMime.equalsIgnoreCase(HEVC) ?
                MediaRecorder.VideoEncoder.HEVC : MediaRecorder.VideoEncoder.H264);
        mediaRecorder.setOutputFile(mTempRecordedFile);
        mediaRecorder.setVideoSize(1920, 1080);
        mediaRecorder.setOrientationHint(0);
        mediaRecorder.setPreviewDisplay(surface);
        mediaRecorder.prepare();
        return mediaRecorder;
    }

    private void startLoad() throws Exception {
        // TODO: b/183671436
        // Create Transcode load (AVC Decoder(720p) + AVC Encoder(720p))
        mTranscodeLoadStatus = new LoadStatus();
        mTranscodeLoadThread = new Thread(() -> {
            try {
                TranscodeLoad transcodeLoad = new TranscodeLoad(AVC, AVC_TRANSCODE_FILE,
                        AVC_DECODER_NAME, AVC_ENCODER_NAME, mTranscodeLoadStatus);
                transcodeLoad.doTranscode();
            } catch (Exception e) {
                mException = e;
            }
        });
        // Create MediaRecorder Session - Audio (Microphone) + 1080p Video (Camera)
        // Create a temp file to dump the MediaRecorder output. Later it will be deleted.
        mTempRecordedFile = new File(WorkDir.getMediaDirString() + ""tempOut.mp4"");
        mTempRecordedFile.createNewFile();
        mMediaRecorderLoad = createMediaRecorderLoad(mSurface);
        // Start the Loads
        mTranscodeLoadThread.start();
        mMediaRecorderLoad.start();
    }

    private void stopLoad() throws Exception {
        if (mTranscodeLoadStatus != null) {
            mTranscodeLoadStatus.setLoadFinished();
            mTranscodeLoadStatus = null;
        }
        if (mTranscodeLoadThread != null) {
            mTranscodeLoadThread.join();
            mTranscodeLoadThread = null;
        }
        if (mMediaRecorderLoad != null) {
            // Note that a RuntimeException is intentionally thrown to the application, if no valid
            // audio/video data has been received when stop() is called. This happens if stop() is
            // called immediately after start(). So sleep for 1000ms inorder to make sure some
            // data has been received between start() and stop().
            Thread.sleep(1000);
            mMediaRecorderLoad.stop();
            mMediaRecorderLoad.release();
            mMediaRecorderLoad = null;
            if(mTempRecordedFile != null && mTempRecordedFile.exists()) {
                mTempRecordedFile.delete();
                mTempRecordedFile = null;
            }
        }
        if (mException != null) throw mException;
    }

    private void createSurface() throws InterruptedException {
        mActivityRule.getActivity().waitTillSurfaceIsCreated();
        mSurface = mActivityRule.getActivity().getSurface();
        assertTrue(""Surface created is null."", mSurface != null);
        assertTrue(""Surface created is invalid."", mSurface.isValid());
        mActivityRule.getActivity().setScreenParams(1920, 1080, true);
    }

    private void releaseSurface() {
        if (mSurface != null) {
            mSurface.release();
            mSurface = null;
        }
    }

    /**
     * This test validates that the initialization latency(time for codec create + configure)
     * for the audio encoders <= 30ms and for video encoders <= 40ms measuring 10 times in
     * succession(5 times alternating sync and async modes). This also logs the stats min, max, avg
     * of the encoder initialization latencies.
     */
    @LargeTest"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.AllocationTest"	"testAllocationFromCameraFlexibleYuv"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/AllocationTest.java"	""	"public void testAllocationFromCameraFlexibleYuv() throws Exception {

        /** number of frame (for streaming requests) to be verified. */
        final int NUM_FRAME_VERIFIED = 1;

        mCameraIterable.forEachCamera(new CameraBlock() {
            @Override
            public void run(CameraDevice camera) throws CameraAccessException {

                // Iterate over each size in the camera
                mSizeIterable.forEachSize(YUV_420_888, new SizeBlock() {
                    @Override
                    public void run(final Size size) throws CameraAccessException {
                        // Create a script graph that converts YUV to RGB
                        try (ScriptGraph scriptGraph = ScriptGraph.create()
                                .configureInputWithSurface(size, YUV_420_888)
                                .chainScript(ScriptYuvToRgb.class)
                                .buildGraph()) {

                            if (VERBOSE) Log.v(TAG, ""Prepared ScriptYuvToRgb for size "" + size);

                            // Run the graph against camera input and validate we get some input
                            CaptureRequest request =
                                    configureAndCreateRequestForSurface(scriptGraph.getInputSurface()).build();

                            // Block until we get 1 result, then iterate over the result
                            mResultIterable.forEachResultRepeating(
                                    request, NUM_FRAME_VERIFIED, new ResultBlock() {
                                @Override
                                public void run(CaptureResult result) throws CameraAccessException {
                                    scriptGraph.advanceInputWaiting();
                                    scriptGraph.execute();
                                    validateInputOutputNotZeroes(scriptGraph, size);
                                    scriptGraph.advanceInputAndDrop();
                                }
                            });

                            stopCapture();
                            if (VERBOSE) Log.v(TAG, ""Cleanup Renderscript cache"");
                            scriptGraph.close();
                            RenderScriptSingleton.clearContext();
                            RenderScriptSingleton.setContext(mContext);
                        }
                    }
                });
            }
        });
    }

    /**
     * Take two shots and ensure per-frame-control with exposure/gain is working correctly.
     *
     * <p>Takes a shot with very low ISO and exposure time. Expect it to be black.</p>
     *
     * <p>Take a shot with very high ISO and exposure time. Expect it to be white.</p>
     *
     * @throws Exception
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.AllocationTest"	"testBlackWhite"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/AllocationTest.java"	""	"public void testBlackWhite() throws CameraAccessException {

        /** low iso + low exposure (first shot) */
        final float THRESHOLD_LOW = 0.025f;
        /** high iso + high exposure (second shot) */
        final float THRESHOLD_HIGH = 0.975f;

        mCameraIterable.forEachCamera(/*fullHwLevel*/false, new CameraBlock() {
            @Override
            public void run(CameraDevice camera) throws CameraAccessException {
                final StaticMetadata staticInfo =
                        new StaticMetadata(mCameraManager.getCameraCharacteristics(camera.getId()));

                // This test requires PFC and manual sensor control
                if (!staticInfo.isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR) ||
                        !staticInfo.isPerFrameControlSupported()) {
                    return;
                }

                final Size maxSize = getMaxSize(
                        getSupportedSizeForFormat(YUV_420_888, camera.getId(), mCameraManager));

                try (ScriptGraph scriptGraph = createGraphForYuvCroppedMeans(maxSize)) {

                    CaptureRequest.Builder req =
                            configureAndCreateRequestForSurface(scriptGraph.getInputSurface());

                    // Take a shot with very low ISO and exposure time. Expect it to be black.
                    int minimumSensitivity = staticInfo.getSensitivityMinimumOrDefault();
                    long minimumExposure = staticInfo.getExposureMinimumOrDefault();
                    setManualCaptureRequest(req, minimumSensitivity, minimumExposure);

                    CaptureRequest lowIsoExposureShot = req.build();
                    captureSingleShotAndExecute(lowIsoExposureShot, scriptGraph);

                    float[] blackMeans = convertPixelYuvToRgb(scriptGraph.getOutputData());

                    // Take a shot with very high ISO and exposure time. Expect it to be white.
                    int maximumSensitivity = staticInfo.getSensitivityMaximumOrDefault();
                    long maximumExposure = staticInfo.getExposureMaximumOrDefault();
                    setManualCaptureRequest(req, maximumSensitivity, maximumExposure);

                    CaptureRequest highIsoExposureShot = req.build();
                    captureSingleShotAndExecute(highIsoExposureShot, scriptGraph);

                    float[] whiteMeans = convertPixelYuvToRgb(scriptGraph.getOutputData());

                    // Low iso + low exposure (first shot), just check and log the error.
                    for (int i = 0; i < blackMeans.length; ++i) {
                        if (blackMeans[i] >= THRESHOLD_LOW) {
                            Log.e(TAG,
                                    String.format(""Black means too high: (%s should be greater""
                                            + "" than %s; item index %d in %s)"", blackMeans[i],
                                            THRESHOLD_LOW, i,
                                            Arrays.toString(blackMeans)));
                        }
                    }

                    // High iso + high exposure (second shot), just check and log the error
                    for (int i = 0; i < whiteMeans.length; ++i) {
                        if (whiteMeans[i] <= THRESHOLD_HIGH) {
                            Log.e(TAG,
                                    String.format(""White means too low: (%s should be less than""
                                            + "" %s; item index %d in %s)"", whiteMeans[i],
                                            THRESHOLD_HIGH, i,
                                            Arrays.toString(whiteMeans)));
                        }
                    }
                }
            }
        });
    }

    /**
     * Test that the android.sensitivity.parameter is applied.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.AllocationTest"	"testParamSensitivity"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/AllocationTest.java"	""	"public void testParamSensitivity() throws CameraAccessException {
        final float THRESHOLD_MAX_MIN_DIFF = 0.3f;
        final float THRESHOLD_MAX_MIN_RATIO = 2.0f;
        final int NUM_STEPS = 5;
        final long EXPOSURE_TIME_NS = 2000000; // 2 ms
        final int RGB_CHANNELS = 3;

        mCameraIterable.forEachCamera(/*fullHwLevel*/false, new CameraBlock() {


            @Override
            public void run(CameraDevice camera) throws CameraAccessException {
                final StaticMetadata staticInfo =
                        new StaticMetadata(mCameraManager.getCameraCharacteristics(camera.getId()));
                // This test requires PFC and manual sensor control
                if (!staticInfo.isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR) ||
                        !staticInfo.isPerFrameControlSupported()) {
                    return;
                }

                final List<float[]> rgbMeans = new ArrayList<float[]>();
                final Size maxSize = getMaxSize(
                        getSupportedSizeForFormat(YUV_420_888, camera.getId(), mCameraManager));

                final int sensitivityMin = staticInfo.getSensitivityMinimumOrDefault();
                final int sensitivityMax = staticInfo.getSensitivityMaximumOrDefault();

                // List each sensitivity from min-max in NUM_STEPS increments
                int[] sensitivities = new int[NUM_STEPS];
                for (int i = 0; i < NUM_STEPS; ++i) {
                    int delta = (sensitivityMax - sensitivityMin) / (NUM_STEPS - 1);
                    sensitivities[i] = sensitivityMin + delta * i;
                }

                try (ScriptGraph scriptGraph = createGraphForYuvCroppedMeans(maxSize)) {

                    CaptureRequest.Builder req =
                            configureAndCreateRequestForSurface(scriptGraph.getInputSurface());

                    // Take burst shots with increasing sensitivity one after other.
                    for (int i = 0; i < NUM_STEPS; ++i) {
                        setManualCaptureRequest(req, sensitivities[i], EXPOSURE_TIME_NS);
                        captureSingleShotAndExecute(req.build(), scriptGraph);
                        float[] means = convertPixelYuvToRgb(scriptGraph.getOutputData());
                        rgbMeans.add(means);

                        if (VERBOSE) {
                            Log.v(TAG, ""testParamSensitivity - captured image "" + i +
                                    "" with RGB means: "" + Arrays.toString(means));
                        }
                    }

                    // Test that every consecutive image gets brighter.
                    for (int i = 0; i < rgbMeans.size() - 1; ++i) {
                        float[] curMeans = rgbMeans.get(i);
                        float[] nextMeans = rgbMeans.get(i+1);

                        float[] left = curMeans;
                        float[] right = nextMeans;
                        String leftString = Arrays.toString(left);
                        String rightString = Arrays.toString(right);

                        String msgHeader =
                                String.format(""Shot with sensitivity %d should not have higher "" +
                                ""average means than shot with sensitivity %d"",
                                sensitivities[i], sensitivities[i+1]);
                        for (int m = 0; m < left.length; ++m) {
                            String msg = String.format(
                                    ""%s: (%s should be less than or equal to %s; item index %d;""
                                    + "" left = %s; right = %s)"",
                                    msgHeader, left[m], right[m], m, leftString, rightString);
                            if (left[m] > right[m]) {
                                Log.e(TAG, msg);
                            }
                        }
                    }

                    // Test the min-max diff and ratios are within expected thresholds
                    float[] lastMeans = rgbMeans.get(NUM_STEPS - 1);
                    float[] firstMeans = rgbMeans.get(/*location*/0);
                    for (int i = 0; i < RGB_CHANNELS; ++i) {
                        if (lastMeans[i] - firstMeans[i] <= THRESHOLD_MAX_MIN_DIFF) {
                            Log.w(TAG, String.format(""Sensitivity max-min diff too small""
                                    + ""(max=%f, min=%f)"", lastMeans[i], firstMeans[i]));
                        }
                        if (lastMeans[i] / firstMeans[i] <= THRESHOLD_MAX_MIN_RATIO) {
                            Log.w(TAG, String.format(""Sensitivity max-min ratio too small""
                                    + ""(max=%f, min=%f)"", lastMeans[i], firstMeans[i]));
                        }
                    }
                }
            }
        });

    }

    /**
     * Common script graph for manual-capture based tests that determine the average pixel
     * values of a cropped sub-region.
     *
     * <p>Processing chain:
     *
     * <pre>
     * input:  YUV_420_888 surface
     * output: mean YUV value of a central section of the image,
     *         YUV 4:4:4 encoded as U8_3
     * steps:
     *      1) crop [0.45,0.45] - [0.55, 0.55]
     *      2) average columns
     *      3) average rows
     * </pre>
     * </p>
     */
    private static ScriptGraph createGraphForYuvCroppedMeans(final Size size) {
        ScriptGraph scriptGraph = ScriptGraph.create()
                .configureInputWithSurface(size, YUV_420_888)
                .configureScript(ScriptYuvCrop.class)
                    .set(ScriptYuvCrop.CROP_WINDOW,
                            new Patch(size, /*x*/0.45f, /*y*/0.45f, /*w*/0.1f, /*h*/0.1f).toRectF())
                    .buildScript()
                .chainScript(ScriptYuvMeans2dTo1d.class)
                .chainScript(ScriptYuvMeans1d.class)
                // TODO: Make a script for YUV 444 -> RGB 888 conversion
                .buildGraph();
        return scriptGraph;
    }

    /*
     * TODO: Refactor below code into separate classes and to not depend on AllocationTest
     * inner variables.
     *
     * TODO: add javadocs to below methods
     *
     * TODO: Figure out if there's some elegant way to compose these forEaches together, so that
     * the callers don't have to do a ton of nesting
     */

    interface CameraBlock {
        void run(CameraDevice camera) throws CameraAccessException;
    }

    class CameraIterable {
        public void forEachCamera(CameraBlock runnable)
                throws CameraAccessException {
            forEachCamera(/*fullHwLevel*/false, runnable);
        }

        public void forEachCamera(boolean fullHwLevel, CameraBlock runnable)
                throws CameraAccessException {
            assertNotNull(""No camera manager"", mCameraManager);
            assertNotNull(""No camera IDs"", mCameraIdsUnderTest);

            for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
                // Don't execute the runnable against non-FULL cameras if FULL is required
                CameraCharacteristics properties =
                        mCameraManager.getCameraCharacteristics(mCameraIdsUnderTest[i]);
                StaticMetadata staticInfo = new StaticMetadata(properties);
                if (fullHwLevel && !staticInfo.isHardwareLevelAtLeastFull()) {
                    Log.i(TAG, String.format(
                            ""Skipping this test for camera %s, needs FULL hw level"",
                            mCameraIdsUnderTest[i]));
                    continue;
                }
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, String.format(
                        ""Skipping this test for camera %s, does not support regular outputs"",
                        mCameraIdsUnderTest[i]));
                    continue;
                }
                // Open camera and execute test
                Log.i(TAG, ""Testing Camera "" + mCameraIdsUnderTest[i]);
                try {
                    openDevice(mCameraIdsUnderTest[i]);

                    runnable.run(mCamera);
                } finally {
                    closeDevice(mCameraIdsUnderTest[i]);
                }
            }
        }

        private void openDevice(String cameraId) {
            if (mCamera != null) {
                throw new IllegalStateException(""Already have open camera device"");
            }
            try {
                mCamera = openCamera(
                    mCameraManager, cameraId, mCameraListener, mHandler);
            } catch (CameraAccessException e) {
                fail(""Fail to open camera synchronously, "" + Log.getStackTraceString(e));
            } catch (BlockingOpenException e) {
                fail(""Fail to open camera asynchronously, "" + Log.getStackTraceString(e));
            }
            mCameraListener.waitForState(STATE_OPENED, CAMERA_OPEN_TIMEOUT_MS);
        }

        private void closeDevice(String cameraId) {
            if (mCamera != null) {
                mCamera.close();
                mCameraListener.waitForState(STATE_CLOSED, CAMERA_CLOSE_TIMEOUT_MS);
                mCamera = null;
            }
        }
    }

    interface SizeBlock {
        void run(Size size) throws CameraAccessException;
    }

    class SizeIterable {
        public void forEachSize(int format, SizeBlock runnable) throws CameraAccessException {
            assertNotNull(""No camera opened"", mCamera);
            assertNotNull(""No camera manager"", mCameraManager);

            CameraCharacteristics properties =
                    mCameraManager.getCameraCharacteristics(mCamera.getId());

            assertNotNull(""Can't get camera properties!"", properties);

            StreamConfigurationMap config =
                    properties.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            int[] availableOutputFormats = config.getOutputFormats();
            assertArrayNotEmpty(availableOutputFormats,
                    ""availableOutputFormats should not be empty"");
            Arrays.sort(availableOutputFormats);
            assertTrue(""Can't find the format "" + format + "" in supported formats "" +
                    Arrays.toString(availableOutputFormats),
                    Arrays.binarySearch(availableOutputFormats, format) >= 0);

            Size[] availableSizes = getSupportedSizeForFormat(format, mCamera.getId(),
                    mCameraManager);
            assertArrayNotEmpty(availableSizes, ""availableSizes should not be empty"");

            for (Size size : availableSizes) {

                if (VERBOSE) {
                    Log.v(TAG, ""Testing size "" + size.toString() +
                            "" for camera "" + mCamera.getId());
                }
                runnable.run(size);
            }
        }
    }

    interface ResultBlock {
        void run(CaptureResult result) throws CameraAccessException;
    }

    class ResultIterable {
        public void forEachResultOnce(CaptureRequest request, ResultBlock block)
                throws CameraAccessException {
            forEachResult(request, /*count*/1, /*repeating*/false, block);
        }

        public void forEachResultRepeating(CaptureRequest request, int count, ResultBlock block)
                throws CameraAccessException {
            forEachResult(request, count, /*repeating*/true, block);
        }

        public void forEachResult(CaptureRequest request, int count, boolean repeating,
                ResultBlock block) throws CameraAccessException {

            // TODO: start capture, i.e. configureOutputs

            SimpleCaptureCallback listener = new SimpleCaptureCallback();

            if (!repeating) {
                for (int i = 0; i < count; ++i) {
                    mSession.capture(request, listener, mHandler);
                }
            } else {
                mSession.setRepeatingRequest(request, listener, mHandler);
            }

            // Assume that the device is already IDLE.
            mSessionListener.getStateWaiter().waitForState(BlockingSessionCallback.SESSION_ACTIVE,
                    CAMERA_ACTIVE_TIMEOUT_MS);

            for (int i = 0; i < count; ++i) {
                if (VERBOSE) {
                    Log.v(TAG, String.format(""Testing with result %d of %d for camera %s"",
                            i, count, mCamera.getId()));
                }

                CaptureResult result = listener.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);
                block.run(result);
            }

            if (repeating) {
                mSession.stopRepeating();
                mSessionListener.getStateWaiter().waitForState(
                    BlockingSessionCallback.SESSION_READY, CAMERA_IDLE_TIMEOUT_MS);
            }

            // TODO: Make a Configure decorator or some such for configureOutputs
        }
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaMuxerTest"	"SKIP_testVideoAudio"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaMuxerTest.java"	""	"public void test/*
 *.
 */

package android.media.cts;

import android.content.Context;
import android.content.res.AssetFileDescriptor;
import android.media.MediaCodec.BufferInfo;
import android.media.MediaExtractor;
import android.media.MediaFormat;
import android.media.MediaMetadataRetriever;
import android.media.MediaMuxer;
import android.os.Build;
import android.os.ParcelFileDescriptor;
import android.platform.test.annotations.AppModeFull;
import android.test.AndroidTestCase;
import android.util.Log;

import com.android.compatibility.common.util.MediaUtils;

import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.RandomAccessFile;
import java.nio.ByteBuffer;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Vector;
import java.util.stream.IntStream;

@AppModeFull(reason = ""No interaction with system server"")
public class MediaMuxerTest extends AndroidTestCase {
    private static final String TAG = ""MediaMuxerTest"";
    private static final boolean VERBOSE = false;
    private static final int MAX_SAMPLE_SIZE = 256 * 1024;
    private static final float LATITUDE = 0.0000f;
    private static final float LONGITUDE  = -180.0f;
    private static final float BAD_LATITUDE = 91.0f;
    private static final float BAD_LONGITUDE = -181.0f;
    private static final float TOLERANCE = 0.0002f;
    private static final long OFFSET_TIME_US = 29 * 60 * 1000000L; // 29 minutes
    static final String mInpPrefix = WorkDir.getMediaDirString();
    private boolean mAndroid11 = Build.VERSION.SDK_INT >= Build.VERSION_CODES.R;

    @Override
    public void setContext(Context context) {
        super.setContext(context);
    }

    protected AssetFileDescriptor getAssetFileDescriptorFor(final String res)
            throws FileNotFoundException {
        Preconditions.assertTestFileExists(mInpPrefix + res);
        File inpFile = new File(mInpPrefix + res);
        ParcelFileDescriptor parcelFD =
                ParcelFileDescriptor.open(inpFile, ParcelFileDescriptor.MODE_READ_ONLY);
        return new AssetFileDescriptor(parcelFD, 0, parcelFD.getStatSize());
    }

    /**
     * Test: make sure the muxer handles both video and audio tracks correctly.
     */
    public void SKIP_testVideoAudio() throws Exception {
        // duplicate of CtsMediaV2TestCases:MuxerTest$TestMultiTrack#testMultiTrack[*]
        // numTracks @ {1, 1}
        final String source = ""video_176x144_3gp_h263_300kbps_25fps_aac_stereo_128kbps_11025hz.3gp"";
        String outputFilePath = File.createTempFile(""MediaMuxerTest_testAudioVideo"", "".mp4"")
                .getAbsolutePath();
        cloneAndVerify(source, outputFilePath, 2, 90, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
    }

    public void SKIP_testDualVideoTrack() throws Exception {
        // duplicate of CtsMediaV2TestCases:MuxerTest$TestMultiTrack#testMultiTrack[*]
        // numTracks @ {2, 0}
        final String source = ""video_176x144_h264_408kbps_30fps_352x288_h264_122kbps_30fps.mp4"";
        String outputFilePath = File.createTempFile(""MediaMuxerTest_testDualVideo"", "".mp4"")
                .getAbsolutePath();
        cloneAndVerify(source, outputFilePath, 2, 90, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
    }

    public void SKIP_testDualAudioTrack() throws Exception {
        // duplicate of CtsMediaV2TestCases:MuxerTest$TestMultiTrack#testMultiTrack[*]
        // numTracks @ {0, 2}
        if (!MediaUtils.check(mAndroid11, ""test needs Android 11"")) return;

        final String source = ""audio_aac_mono_70kbs_44100hz_aac_mono_70kbs_44100hz.mp4"";
        String outputFilePath = File.createTempFile(""MediaMuxerTest_testDualAudio"", "".mp4"")
                .getAbsolutePath();
        cloneAndVerify(source, outputFilePath, 2, 90, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
    }

    public void SKIP_testDualVideoAndAudioTrack() throws Exception {
        // duplicate of CtsMediaV2TestCases:MuxerTest$TestMultiTrack#testMultiTrack[*]
        // numTracks @ {2, 2}
        if (!MediaUtils.check(mAndroid11, ""test needs Android 11"")) return;

        final String source = ""video_h264_30fps_video_h264_30fps_aac_44100hz_aac_44100hz.mp4"";
        String outputFilePath = File.createTempFile(""MediaMuxerTest_testDualVideoAudio"", "".mp4"")
                .getAbsolutePath();
        cloneAndVerify(source, outputFilePath, 4, 90, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
    }

    /**
     * Test: make sure the muxer handles video, audio and non standard compliant metadata tracks
     * that generated before API29 correctly. This test will use extractor to extract the video
     * track, audio and the non standard compliant metadata track from the source file, then
     * remuxes them into a new file with standard compliant metadata track. Finally, it will check
     * to make sure the new file's metadata track matches the source file's metadata track for the
     * mime format and data payload.
     */
    public void SKIP_testVideoAudioMedatadataWithNonCompliantMetadataTrack() throws Exception {
        // duplicate of CtsMediaV2TestCases:MuxerTest$TestSimpleMux#testSimpleMux[application/gyro]
        // duplicate of CtsMediaV2TestCases:MuxerTest$TestSimpleMux#testSimpleMux[video/h263]
        // duplicate of CtsMediaV2TestCases:MuxerTest$TestSimpleMux#testSimpleMux[audio/aac]
        final String source =
                ""video_176x144_3gp_h263_300kbps_25fps_aac_stereo_128kbps_11025hz_metadata_gyro_non_compliant.3gp"";
        String outputFilePath = File.createTempFile(""MediaMuxerTest_testAudioVideoMetadata"", "".mp4"")
                .getAbsolutePath();
        cloneAndVerify(source, outputFilePath, 3, 90, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
    }

    /**
     * Test: make sure the muxer handles video, audio and standard compliant metadata tracks that
     * generated from API29 correctly. This test will use extractor to extract the video track,
     * audio and the standard compliant metadata track from the source file, then remuxes them
     * into a new file with standard compliant metadata track. Finally, it will check to make sure
     * the new file's metadata track matches the source file's metadata track for the mime format
     * and data payload.
     */
     public void SKIP_testVideoAudioMedatadataWithCompliantMetadataTrack() throws Exception {
         // duplicate of CtsMediaV2TestCases:MuxerTest$TestSimpleMux#testSimpleMux[application/gyro]
         // duplicate of CtsMediaV2TestCases:MuxerTest$TestSimpleMux#testSimpleMux[video/h263]
         // duplicate of CtsMediaV2TestCases:MuxerTest$TestSimpleMux#testSimpleMux[audio/aac]
         final String source =
                 ""video_176x144_3gp_h263_300kbps_25fps_aac_stereo_128kbps_11025hz_metadata_gyro_compliant.3gp"";
        String outputFilePath = File.createTempFile(""MediaMuxerTest_testAudioVideoMetadata"", "".mp4"")
                .getAbsolutePath();
        cloneAndVerify(source, outputFilePath, 3, 90, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
    }

    /**
     * Test: make sure the muxer handles audio track only file correctly.
     */
    public void SKIP_testAudioOnly() throws Exception {
        // duplicate of CtsMediaV2TestCases:MuxerTest$TestSimpleMux#testSimpleMux[audio/*]
        final String source = ""sinesweepm4a.m4a"";
        String outputFilePath = File.createTempFile(""MediaMuxerTest_testAudioOnly"", "".mp4"")
                .getAbsolutePath();
        cloneAndVerify(source, outputFilePath, 1, -1, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
    }

    /**
     * Test: make sure the muxer handles video track only file correctly.
     */
        public void SKIP_testVideoOnly() throws Exception {
        // duplicate of CtsMediaV2TestCases:MuxerTest$TestSimpleMux#testSimpleMux[video/*]
        final String source = ""video_only_176x144_3gp_h263_25fps.mp4"";
        String outputFilePath = File.createTempFile(""MediaMuxerTest_videoOnly"", "".mp4"")
                .getAbsolutePath();
        cloneAndVerify(source, outputFilePath, 1, 180, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaMuxerTest"	"testWebmOutput"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaMuxerTest.java"	""	"public void testWebmOutput() throws Exception {
        final String source =
                ""video_480x360_webm_vp9_333kbps_25fps_vorbis_stereo_128kbps_48000hz.webm"";
        String outputFilePath = File.createTempFile(""testWebmOutput"", "".webm"")
                .getAbsolutePath();
        cloneAndVerify(source, outputFilePath, 2, 90, MediaMuxer.OutputFormat.MUXER_OUTPUT_WEBM);
    }

    public void SKIP_testThreegppOutput() throws Exception {
        // duplicate of CtsMediaV2TestCases:MuxerTest$TestSimpleMux#testSimpleMux[*]
        final String source = ""video_176x144_3gp_h263_300kbps_12fps_aac_stereo_128kbps_22050hz.3gp"";
        String outputFilePath = File.createTempFile(""testThreegppOutput"", "".3gp"")
                .getAbsolutePath();
        cloneAndVerify(source, outputFilePath, 2, 90, MediaMuxer.OutputFormat.MUXER_OUTPUT_3GPP);
    }

    /**
     * Tests: make sure the muxer handles exceptions correctly.
     * <br> Throws exception b/c start() is not called.
     * <br> Throws exception b/c 2 video tracks were added.
     * <br> Throws exception b/c 2 audio tracks were added.
     * <br> Throws exception b/c 3 tracks were added.
     * <br> Throws exception b/c no tracks was added.
     * <br> Throws exception b/c a wrong format.
     */
    public void SKIP_testIllegalStateExceptions() throws IOException {
        // duplicate of CtsMediaV2TestCases:MuxerTest$TestMultiTrack#testMultiTrack[*] and
        // duplicate of CtsMediaV2TestCases:MuxerUnitTest$TestApi
        String outputFilePath = File.createTempFile(""MediaMuxerTest_testISEs"", "".mp4"")
                .getAbsolutePath();
        MediaMuxer muxer;

        // Throws exception b/c start() is not called.
        muxer = new MediaMuxer(outputFilePath, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
        muxer.addTrack(MediaFormat.createVideoFormat(MediaFormat.MIMETYPE_VIDEO_AVC, 480, 320));

        try {
            muxer.stop();
            fail(""should throw IllegalStateException."");
        } catch (IllegalStateException e) {
            // expected
        } finally {
            muxer.release();
        }

        // Should not throw exception when 2 video tracks were added.
        muxer = new MediaMuxer(outputFilePath, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
        muxer.addTrack(MediaFormat.createVideoFormat(MediaFormat.MIMETYPE_VIDEO_AVC, 480, 320));

        try {
            muxer.addTrack(MediaFormat.createVideoFormat(MediaFormat.MIMETYPE_VIDEO_AVC, 480, 320));
        } catch (IllegalStateException e) {
            fail(""should not throw IllegalStateException."");
        } finally {
            muxer.release();
        }

        // Should not throw exception when 2 audio tracks were added.
        muxer = new MediaMuxer(outputFilePath, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
        muxer.addTrack(MediaFormat.createAudioFormat(MediaFormat.MIMETYPE_AUDIO_AAC, 48000, 1));
        try {
            muxer.addTrack(MediaFormat.createAudioFormat(MediaFormat.MIMETYPE_AUDIO_AAC, 48000, 1));
        } catch (IllegalStateException e) {
            fail(""should not throw IllegalStateException."");
        } finally {
            muxer.release();
        }

        // Should not throw exception when 3 tracks were added.
        muxer = new MediaMuxer(outputFilePath, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
        muxer.addTrack(MediaFormat.createVideoFormat(MediaFormat.MIMETYPE_VIDEO_AVC, 480, 320));
        muxer.addTrack(MediaFormat.createAudioFormat(MediaFormat.MIMETYPE_AUDIO_AAC, 48000, 1));
        try {
            muxer.addTrack(MediaFormat.createVideoFormat(MediaFormat.MIMETYPE_VIDEO_AVC, 480, 320));
        } catch (IllegalStateException e) {
            fail(""should not throw IllegalStateException."");
        } finally {
            muxer.release();
        }

        // Throws exception b/c no tracks was added.
        muxer = new MediaMuxer(outputFilePath, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
        try {
            muxer.start();
            fail(""should throw IllegalStateException."");
        } catch (IllegalStateException e) {
            // expected
        } finally {
            muxer.release();
        }

        // Throws exception b/c a wrong format.
        muxer = new MediaMuxer(outputFilePath, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
        try {
            muxer.addTrack(MediaFormat.createVideoFormat(""vidoe/mp4"", 480, 320));
            fail(""should throw IllegalStateException."");
        } catch (IllegalStateException e) {
            // expected
        } finally {
            muxer.release();
        }

        // Test FileDescriptor Constructor expect sucess.
        RandomAccessFile file = null;
        try {
            file = new RandomAccessFile(outputFilePath, ""rws"");
            muxer = new MediaMuxer(file.getFD(), MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
            muxer.addTrack(MediaFormat.createVideoFormat(MediaFormat.MIMETYPE_VIDEO_AVC, 480, 320));
        } finally {
            file.close();
            muxer.release();
        }

        // Test FileDescriptor Constructor expect exception with read only mode.
        RandomAccessFile file2 = null;
        try {
            file2 = new RandomAccessFile(outputFilePath, ""r"");
            muxer = new MediaMuxer(file2.getFD(), MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
            fail(""should throw IOException."");
        } catch (IOException e) {
            // expected
        } finally {
            file2.close();
            // No need to release the muxer.
        }

        // Test FileDescriptor Constructor expect NO exception with write only mode.
        ParcelFileDescriptor out = null;
        try {
            out = ParcelFileDescriptor.open(new File(outputFilePath),
                    ParcelFileDescriptor.MODE_WRITE_ONLY | ParcelFileDescriptor.MODE_CREATE);
            muxer = new MediaMuxer(out.getFileDescriptor(), MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
        } catch (IllegalArgumentException e) {
            fail(""should not throw IllegalArgumentException."");
        } catch (IOException e) {
            fail(""should not throw IOException."");
        } finally {
            out.close();
            muxer.release();
        }

        new File(outputFilePath).delete();
    }

    /**
     * Test: makes sure if audio and video muxing using MPEG4Writer works well when there are frame
     * drops as in b/63590381 and b/64949961 while B Frames encoding is enabled.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaMuxerTest"	"testTimestampsStartOffsetNegativeAudioVideo"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaMuxerTest.java"	""	"public void testTimestampsStartOffsetNegativeAudioVideo() throws Exception {
        if (!MediaUtils.check(mAndroid11, ""test needs Android 11"")) return;

        Vector<Integer> startOffsetUsVect = new Vector<Integer>();
        // Video starts at 50000us.
        startOffsetUsVect.add(50000);
        // Audio starts at -23220us, multiple of duration of one frame (1024/44100hz)
        startOffsetUsVect.add(-23220);
        checkTimestampsWithStartOffsets(startOffsetUsVect);
    }

    /**
     * Test: makes sure if audio/video muxing using MPEG4Writer works with B Frames
     * when video and audio samples start after different times.
     */
    private void checkTimestampsAudioBVideoDiffStartOffsets(Vector<Integer> startOffsetUs)
            throws Exception {
        MPEG4CheckTimestampsAudioBVideoDiffStartOffsets(startOffsetUs);
        // TODO: uncomment webm testing once bugs related to timestamps in webmwriter are fixed.
        // WebMCheckTimestampsAudioBVideoDiffStartOffsets(startOffsetUsVect);
    }

    private void MPEG4CheckTimestampsAudioBVideoDiffStartOffsets(Vector<Integer> startOffsetUs)
            throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""MPEG4CheckTimestampsAudioBVideoDiffStartOffsets"");
        }
        final String source = ""video_h264_main_b_frames.mp4"";
        String outputFilePath = File.createTempFile(
            ""MediaMuxerTest_testTimestampsAudioBVideoDiffStartOffsets"", "".mp4"").getAbsolutePath();
        try {
            cloneMediaWithSamplesDropAndStartOffsets(source, outputFilePath,
                MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4, null, startOffsetUs);
            verifyTSWithSamplesDropAndStartOffset(
                    source, true /* has B frames */, outputFilePath, null, startOffsetUs);
        } finally {
            new File(outputFilePath).delete();
        }
    }

    /*
     * Check if timestamps are written consistently across all formats supported by MediaMuxer.
     */
    private void checkTimestampsWithStartOffsets(Vector<Integer> startOffsetUsVect)
            throws Exception {
        MPEG4CheckTimestampsWithStartOffsets(startOffsetUsVect);
        // TODO: uncomment webm testing once bugs related to timestamps in webmwriter are fixed.
        // WebMCheckTimestampsWithStartOffsets(startOffsetUsVect);
        // TODO: need to add other formats, OGG, AAC, AMR
    }

    /**
     * Make sure if audio/video muxing using MPEG4Writer works good with different start
     * offsets for audio and video.
     */
    private void MPEG4CheckTimestampsWithStartOffsets(Vector<Integer> startOffsetUsVect)
            throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""MPEG4CheckTimestampsWithStartOffsets"");
        }
        final String source = ""video_480x360_mp4_h264_500kbps_30fps_aac_stereo_128kbps_44100hz.mp4"";
        String outputFilePath =
            File.createTempFile(""MediaMuxerTest_MPEG4CheckTimestampsWithStartOffsets"", "".mp4"")
                .getAbsolutePath();
        try {
            cloneMediaWithSamplesDropAndStartOffsets(source, outputFilePath,
                    MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4, null, startOffsetUsVect);
            verifyTSWithSamplesDropAndStartOffset(
                    source, false /* no B frames */, outputFilePath, null, startOffsetUsVect);
        } finally {
            new File(outputFilePath).delete();
        }
    }

    /**
     * Make sure if audio/video muxing using WebMWriter works good with different start
     * offsets for audio and video.
     */
    private void WebMCheckTimestampsWithStartOffsets(Vector<Integer> startOffsetUsVect)
            throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""WebMCheckTimestampsWithStartOffsets"");
        }
        final String source =
                ""video_480x360_webm_vp9_333kbps_25fps_vorbis_stereo_128kbps_48000hz.webm"";
        String outputFilePath =
            File.createTempFile(""MediaMuxerTest_WebMCheckTimestampsWithStartOffsets"", "".webm"")
                .getAbsolutePath();
        try {
            cloneMediaWithSamplesDropAndStartOffsets(source, outputFilePath,
                    MediaMuxer.OutputFormat.MUXER_OUTPUT_WEBM, null, startOffsetUsVect);
            verifyTSWithSamplesDropAndStartOffset(
                    source, false /* no B frames */, outputFilePath, null, startOffsetUsVect);
        } finally {
            new File(outputFilePath).delete();
        }
    }

    /**
     * Clones a media file and then compares against the source file to make
     * sure they match.
     */
    private void cloneAndVerify(final String srcMedia, String outputMediaFile,
            int expectedTrackCount, int degrees, int fmt) throws IOException {
        try {
            cloneMediaUsingMuxer(srcMedia, outputMediaFile, expectedTrackCount,
                    degrees, fmt);
            if (fmt == MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4 ||
                    fmt == MediaMuxer.OutputFormat.MUXER_OUTPUT_3GPP) {
                verifyAttributesMatch(srcMedia, outputMediaFile, degrees);
                verifyLocationInFile(outputMediaFile);
            }
            // Verify timestamp of all samples.
            verifyTSWithSamplesDropAndStartOffset(
                    srcMedia, false /* no B frames */,outputMediaFile, null, null);
        } finally {
            new File(outputMediaFile).delete();
        }
    }

    /**
     * Using the MediaMuxer to clone a media file.
     */
    private void cloneMediaUsingMuxer(final String  srcMedia, String dstMediaPath,
            int expectedTrackCount, int degrees, int fmt)
            throws IOException {
        // Set up MediaExtractor to read from the source.
        AssetFileDescriptor srcFd = getAssetFileDescriptorFor(srcMedia);
        MediaExtractor extractor = new MediaExtractor();
        extractor.setDataSource(srcFd.getFileDescriptor(), srcFd.getStartOffset(),
                srcFd.getLength());

        int trackCount = extractor.getTrackCount();
        assertEquals(""wrong number of tracks"", expectedTrackCount, trackCount);

        // Set up MediaMuxer for the destination.
        MediaMuxer muxer;
        muxer = new MediaMuxer(dstMediaPath, fmt);

        // Set up the tracks.
        HashMap<Integer, Integer> indexMap = new HashMap<Integer, Integer>(trackCount);
        for (int i = 0; i < trackCount; i++) {
            extractor.selectTrack(i);
            MediaFormat format = extractor.getTrackFormat(i);
            int dstIndex = muxer.addTrack(format);
            indexMap.put(i, dstIndex);
        }

        // Copy the samples from MediaExtractor to MediaMuxer.
        boolean sawEOS = false;
        int bufferSize = MAX_SAMPLE_SIZE;
        int frameCount = 0;
        int offset = 100;

        ByteBuffer dstBuf = ByteBuffer.allocate(bufferSize);
        BufferInfo bufferInfo = new BufferInfo();

        if (degrees >= 0) {
            muxer.setOrientationHint(degrees);
        }

        if (fmt == MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4 ||
            fmt == MediaMuxer.OutputFormat.MUXER_OUTPUT_3GPP) {
            // Test setLocation out of bound cases
            try {
                muxer.setLocation(BAD_LATITUDE, LONGITUDE);
                fail(""setLocation succeeded with bad argument: ["" + BAD_LATITUDE + "","" + LONGITUDE
                    + ""]"");
            } catch (IllegalArgumentException e) {
                // Expected
            }
            try {
                muxer.setLocation(LATITUDE, BAD_LONGITUDE);
                fail(""setLocation succeeded with bad argument: ["" + LATITUDE + "","" + BAD_LONGITUDE
                    + ""]"");
            } catch (IllegalArgumentException e) {
                // Expected
            }

            muxer.setLocation(LATITUDE, LONGITUDE);
        }

        muxer.start();
        while (!sawEOS) {
            bufferInfo.offset = offset;
            bufferInfo.size = extractor.readSampleData(dstBuf, offset);

            if (bufferInfo.size < 0) {
                if (VERBOSE) {
                    Log.d(TAG, ""saw input EOS."");
                }
                sawEOS = true;
                bufferInfo.size = 0;
            } else {
                bufferInfo.presentationTimeUs = extractor.getSampleTime();
                bufferInfo.flags = extractor.getSampleFlags();
                int trackIndex = extractor.getSampleTrackIndex();

                muxer.writeSampleData(indexMap.get(trackIndex), dstBuf,
                        bufferInfo);
                extractor.advance();

                frameCount++;
                if (VERBOSE) {
                    Log.d(TAG, ""Frame ("" + frameCount + "") "" +
                            ""PresentationTimeUs:"" + bufferInfo.presentationTimeUs +
                            "" Flags:"" + bufferInfo.flags +
                            "" TrackIndex:"" + trackIndex +
                            "" Size(KB) "" + bufferInfo.size / 1024);
                }
            }
        }

        muxer.stop();
        muxer.release();
        extractor.release();
        srcFd.close();
        return;
    }

    /**
     * Compares some attributes using MediaMetadataRetriever to make sure the
     * cloned media file matches the source file.
     */
    private void verifyAttributesMatch(final String srcMedia, String testMediaPath,
            int degrees) throws IOException {
        AssetFileDescriptor testFd = getAssetFileDescriptorFor(srcMedia);

        MediaMetadataRetriever retrieverSrc = new MediaMetadataRetriever();
        retrieverSrc.setDataSource(testFd.getFileDescriptor(),
                testFd.getStartOffset(), testFd.getLength());

        MediaMetadataRetriever retrieverTest = new MediaMetadataRetriever();
        retrieverTest.setDataSource(testMediaPath);

        String testDegrees = retrieverTest.extractMetadata(
                MediaMetadataRetriever.METADATA_KEY_VIDEO_ROTATION);
        if (testDegrees != null) {
            assertEquals(""Different degrees"", degrees,
                    Integer.parseInt(testDegrees));
        }

        String heightSrc = retrieverSrc.extractMetadata(
                MediaMetadataRetriever.METADATA_KEY_VIDEO_HEIGHT);
        String heightTest = retrieverTest.extractMetadata(
                MediaMetadataRetriever.METADATA_KEY_VIDEO_HEIGHT);
        assertEquals(""Different height"", heightSrc,
                heightTest);

        String widthSrc = retrieverSrc.extractMetadata(
                MediaMetadataRetriever.METADATA_KEY_VIDEO_WIDTH);
        String widthTest = retrieverTest.extractMetadata(
                MediaMetadataRetriever.METADATA_KEY_VIDEO_WIDTH);
        assertEquals(""Different width"", widthSrc,
                widthTest);

        //TODO: need to check each individual track's duration also.
        String durationSrc = retrieverSrc.extractMetadata(
                MediaMetadataRetriever.METADATA_KEY_DURATION);
        String durationTest = retrieverTest.extractMetadata(
                MediaMetadataRetriever.METADATA_KEY_DURATION);
        assertEquals(""Different duration"", durationSrc,
                durationTest);

        retrieverSrc.release();
        retrieverTest.release();
        testFd.close();
    }

    private void verifyLocationInFile(String fileName) {
        MediaMetadataRetriever retriever = new MediaMetadataRetriever();
        retriever.setDataSource(fileName);
        String location = retriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_LOCATION);
        assertNotNull(""No location information found in file "" + fileName, location);


        // parsing String location and recover the location information in floats
        // Make sure the tolerance is very small - due to rounding errors.

        // Trim the trailing slash, if any.
        int lastIndex = location.lastIndexOf('/');
        if (lastIndex != -1) {
            location = location.substring(0, lastIndex);
        }

        // Get the position of the -/+ sign in location String, which indicates
        // the beginning of the longitude.
        int minusIndex = location.lastIndexOf('-');
        int plusIndex = location.lastIndexOf('+');

        assertTrue(""+ or - is not found or found only at the beginning ["" + location + ""]"",
                (minusIndex > 0 || plusIndex > 0));
        int index = Math.max(minusIndex, plusIndex);

        float latitude = Float.parseFloat(location.substring(0, index));
        float longitude = Float.parseFloat(location.substring(index));
        assertTrue(""Incorrect latitude: "" + latitude + "" ["" + location + ""]"",
                Math.abs(latitude - LATITUDE) <= TOLERANCE);
        assertTrue(""Incorrect longitude: "" + longitude + "" ["" + location + ""]"",
                Math.abs(longitude - LONGITUDE) <= TOLERANCE);
        retriever.release();
    }

    /**
     * Uses 2 MediaExtractor, seeking to the same position, reads the sample and
     * makes sure the samples match.
     */
    private void verifySamplesMatch(final String srcMedia, String testMediaPath, int seekToUs,
            long offsetTimeUs) throws IOException {
        AssetFileDescriptor testFd = getAssetFileDescriptorFor(srcMedia);
        MediaExtractor extractorSrc = new MediaExtractor();
        extractorSrc.setDataSource(testFd.getFileDescriptor(),
                testFd.getStartOffset(), testFd.getLength());
        int trackCount = extractorSrc.getTrackCount();
        final int videoTrackIndex = 0;

        MediaExtractor extractorTest = new MediaExtractor();
        extractorTest.setDataSource(testMediaPath);

        assertEquals(""wrong number of tracks"", trackCount,
                extractorTest.getTrackCount());

        // Make sure the format is the same and select them
        for (int i = 0; i < trackCount; i++) {
            MediaFormat formatSrc = extractorSrc.getTrackFormat(i);
            MediaFormat formatTest = extractorTest.getTrackFormat(i);

            String mimeIn = formatSrc.getString(MediaFormat.KEY_MIME);
            String mimeOut = formatTest.getString(MediaFormat.KEY_MIME);
            if (!(mimeIn.equals(mimeOut))) {
                fail(""format didn't match on track No."" + i +
                        formatSrc.toString() + ""\n"" + formatTest.toString());
            }
            extractorSrc.selectTrack(videoTrackIndex);
            extractorTest.selectTrack(videoTrackIndex);

            // Pick a time and try to compare the frame.
            extractorSrc.seekTo(seekToUs, MediaExtractor.SEEK_TO_CLOSEST_SYNC);
            extractorTest.seekTo(seekToUs + offsetTimeUs, MediaExtractor.SEEK_TO_CLOSEST_SYNC);

            int bufferSize = MAX_SAMPLE_SIZE;
            ByteBuffer byteBufSrc = ByteBuffer.allocate(bufferSize);
            ByteBuffer byteBufTest = ByteBuffer.allocate(bufferSize);

            int srcBufSize = extractorSrc.readSampleData(byteBufSrc, 0);
            int testBufSize = extractorTest.readSampleData(byteBufTest, 0);

            if (!(byteBufSrc.equals(byteBufTest))) {
                if (VERBOSE) {
                    Log.d(TAG,
                            ""srcTrackIndex:"" + extractorSrc.getSampleTrackIndex()
                                    + ""  testTrackIndex:"" + extractorTest.getSampleTrackIndex());
                    Log.d(TAG,
                            ""srcTSus:"" + extractorSrc.getSampleTime()
                                    + "" testTSus:"" + extractorTest.getSampleTime());
                    Log.d(TAG, ""srcBufSize:"" + srcBufSize + ""testBufSize:"" + testBufSize);
                }
                fail(""byteBuffer didn't match"");
            }
            extractorSrc.unselectTrack(i);
            extractorTest.unselectTrack(i);
        }
        extractorSrc.release();
        extractorTest.release();
        testFd.close();
    }

    /**
     * Using MediaMuxer and MediaExtractor to mux a media file from another file while skipping
     * some video frames as in the issues b/63590381 and b/64949961.
     */
    private void simulateVideoFramesDropIssuesAndMux(final String srcMedia, String dstMediaPath,
            int expectedTrackCount, int fmt) throws IOException {
        // Set up MediaExtractor to read from the source.
        AssetFileDescriptor srcFd = getAssetFileDescriptorFor(srcMedia);
        MediaExtractor extractor = new MediaExtractor();
        extractor.setDataSource(srcFd.getFileDescriptor(), srcFd.getStartOffset(),
            srcFd.getLength());

        int trackCount = extractor.getTrackCount();
        assertEquals(""wrong number of tracks"", expectedTrackCount, trackCount);

        // Set up MediaMuxer for the destination.
        MediaMuxer muxer;
        muxer = new MediaMuxer(dstMediaPath, fmt);

        // Set up the tracks.
        HashMap<Integer, Integer> indexMap = new HashMap<Integer, Integer>(trackCount);

        for (int i = 0; i < trackCount; i++) {
            extractor.selectTrack(i);
            MediaFormat format = extractor.getTrackFormat(i);
            int dstIndex = muxer.addTrack(format);
            indexMap.put(i, dstIndex);
        }

        // Copy the samples from MediaExtractor to MediaMuxer.
        boolean sawEOS = false;
        int bufferSize = MAX_SAMPLE_SIZE;
        int sampleCount = 0;
        int offset = 0;
        int videoSampleCount = 0;
        // Counting frame index values starting from 1
        final int muxAllTypeVideoFramesUntilIndex = 136; // I/P/B frames passed as it is until this
        final int muxAllTypeVideoFramesFromIndex = 171; // I/P/B frames passed as it is from this
        final int pFrameBeforeARandomBframeIndex = 137;
        final int bFrameAfterPFrameIndex = pFrameBeforeARandomBframeIndex+1;

        ByteBuffer dstBuf = ByteBuffer.allocate(bufferSize);
        BufferInfo bufferInfo = new BufferInfo();

        muxer.start();
        while (!sawEOS) {
            bufferInfo.offset = 0;
            bufferInfo.size = extractor.readSampleData(dstBuf, offset);
            if (bufferInfo.size < 0) {
                if (VERBOSE) {
                    Log.d(TAG, ""saw input EOS."");
                }
                sawEOS = true;
                bufferInfo.size = 0;
            } else {
                bufferInfo.presentationTimeUs = extractor.getSampleTime();
                bufferInfo.flags = extractor.getSampleFlags();
                int trackIndex = extractor.getSampleTrackIndex();
                // Video track at index 0, skip some video frames while muxing.
                if (trackIndex == 0) {
                    ++videoSampleCount;
                    if (VERBOSE) {
                        Log.v(TAG, ""videoSampleCount : "" + videoSampleCount);
                    }
                    if (videoSampleCount <= muxAllTypeVideoFramesUntilIndex
                            || videoSampleCount == bFrameAfterPFrameIndex) {
                        // Write frame as it is.
                        muxer.writeSampleData(indexMap.get(trackIndex), dstBuf, bufferInfo);
                    } else if (videoSampleCount == pFrameBeforeARandomBframeIndex
                            || videoSampleCount >= muxAllTypeVideoFramesFromIndex) {
                        // Adjust time stamp for this P frame to a few frames later, say ~5seconds
                        bufferInfo.presentationTimeUs += OFFSET_TIME_US;
                        muxer.writeSampleData(indexMap.get(trackIndex), dstBuf, bufferInfo);
                    } else {
                        // Skip frames after bFrameAfterPFrameIndex
                        // and before muxAllTypeVideoFramesFromIndex.
                        if (VERBOSE) {
                            Log.i(TAG, ""skipped this frame"");
                        }
                    }
                } else {
                    // write audio data as it is continuously
                    muxer.writeSampleData(indexMap.get(trackIndex), dstBuf, bufferInfo);
                }
                extractor.advance();
                sampleCount++;
                if (VERBOSE) {
                    Log.d(TAG, ""Frame ("" + sampleCount + "") "" +
                            ""PresentationTimeUs:"" + bufferInfo.presentationTimeUs +
                            "" Flags:"" + bufferInfo.flags +
                            "" TrackIndex:"" + trackIndex +
                            "" Size(bytes) "" + bufferInfo.size );
                }
            }
        }

        muxer.stop();
        muxer.release();
        extractor.release();
        srcFd.close();

        return;
    }

    /**
     * Uses two MediaExtractor's and checks whether timestamps of first few and another few
     *  from last sync frame matches
     */
    private void verifyAFewSamplesTimestamp(final String srcMedia, String testMediaPath)
            throws IOException {
        final int numFramesTSCheck = 10; // Num frames to be checked for its timestamps

        AssetFileDescriptor srcFd = getAssetFileDescriptorFor(srcMedia);
        MediaExtractor extractorSrc = new MediaExtractor();
        extractorSrc.setDataSource(srcFd.getFileDescriptor(),
            srcFd.getStartOffset(), srcFd.getLength());
        MediaExtractor extractorTest = new MediaExtractor();
        extractorTest.setDataSource(testMediaPath);

        int trackCount = extractorSrc.getTrackCount();
        for (int i = 0; i < trackCount; i++) {
            MediaFormat format = extractorSrc.getTrackFormat(i);
            extractorSrc.selectTrack(i);
            extractorTest.selectTrack(i);
            if (format.getString(MediaFormat.KEY_MIME).startsWith(""video/"")) {
                // Check time stamps for numFramesTSCheck frames from 33333us.
                checkNumFramesTimestamp(33333, 0, numFramesTSCheck, extractorSrc, extractorTest);
                // Check time stamps for numFramesTSCheck frames from 9333333 -
                // sync frame after framedrops at index 172 of video track.
                checkNumFramesTimestamp(
                        9333333, OFFSET_TIME_US, numFramesTSCheck, extractorSrc, extractorTest);
            } else if (format.getString(MediaFormat.KEY_MIME).startsWith(""audio/"")) {
                // Check timestamps for all audio frames. Test file has 427 audio frames.
                checkNumFramesTimestamp(0, 0, 427, extractorSrc, extractorTest);
            }
            extractorSrc.unselectTrack(i);
            extractorTest.unselectTrack(i);
        }

        extractorSrc.release();
        extractorTest.release();
        srcFd.close();
    }

    private void checkNumFramesTimestamp(long seekTimeUs, long offsetTimeUs, int numFrames,
            MediaExtractor extractorSrc, MediaExtractor extractorTest) {
        long srcSampleTimeUs = -1;
        long testSampleTimeUs = -1;
        extractorSrc.seekTo(seekTimeUs, MediaExtractor.SEEK_TO_CLOSEST_SYNC);
        extractorTest.seekTo(seekTimeUs + offsetTimeUs, MediaExtractor.SEEK_TO_CLOSEST_SYNC);
        while (numFrames-- > 0 ) {
            srcSampleTimeUs = extractorSrc.getSampleTime();
            testSampleTimeUs = extractorTest.getSampleTime();
            if (srcSampleTimeUs == -1 || testSampleTimeUs == -1) {
                fail(""either of tracks reached end of stream"");
            }
            if ((srcSampleTimeUs + offsetTimeUs) != testSampleTimeUs) {
                if (VERBOSE) {
                    Log.d(TAG, ""srcTrackIndex:"" + extractorSrc.getSampleTrackIndex() +
                        ""  testTrackIndex:"" + extractorTest.getSampleTrackIndex());
                    Log.d(TAG, ""srcTSus:"" + srcSampleTimeUs + "" testTSus:"" + testSampleTimeUs);
                }
                fail(""timestamps didn't match"");
            }
            extractorSrc.advance();
            extractorTest.advance();
        }
    }

    /**
     * Using MediaMuxer and MediaExtractor to mux a media file from another file while skipping
     * 0 or more video frames and desired start offsets for each track.
     * startOffsetUsVect : order of tracks is the same as in the input file
     */
    private void cloneMediaWithSamplesDropAndStartOffsets(final String srcMedia, String dstMediaPath,
            int fmt, HashSet<Integer> samplesDropSet, Vector<Integer> startOffsetUsVect)
            throws IOException {
        // Set up MediaExtractor to read from the source.
        AssetFileDescriptor srcFd = getAssetFileDescriptorFor(srcMedia);
        MediaExtractor extractor = new MediaExtractor();
        extractor.setDataSource(srcFd.getFileDescriptor(), srcFd.getStartOffset(),
            srcFd.getLength());

        int trackCount = extractor.getTrackCount();

        // Set up MediaMuxer for the destination.
        MediaMuxer muxer;
        muxer = new MediaMuxer(dstMediaPath, fmt);

        // Set up the tracks.
        HashMap<Integer, Integer> indexMap = new HashMap<Integer, Integer>(trackCount);

        int videoTrackIndex = 100;
        int videoStartOffsetUs = 0;
        int audioTrackIndex = 100;
        int audioStartOffsetUs = 0;
        for (int i = 0; i < trackCount; i++) {
            extractor.selectTrack(i);
            MediaFormat format = extractor.getTrackFormat(i);
            int dstIndex = muxer.addTrack(format);
            indexMap.put(i, dstIndex);
            if (format.getString(MediaFormat.KEY_MIME).startsWith(""video/"")) {
                videoTrackIndex = i;
                // Make sure there's an entry for video track.
                if (startOffsetUsVect != null && (videoTrackIndex < startOffsetUsVect.size())) {
                    videoStartOffsetUs = startOffsetUsVect.get(videoTrackIndex);
                }
            }
            if (format.getString(MediaFormat.KEY_MIME).startsWith(""audio/"")) {
                audioTrackIndex = i;
                // Make sure there's an entry for audio track.
                if (startOffsetUsVect != null && (audioTrackIndex < startOffsetUsVect.size())) {
                    audioStartOffsetUs = startOffsetUsVect.get(audioTrackIndex);
                }
            }
        }

        // Copy the samples from MediaExtractor to MediaMuxer.
        boolean sawEOS = false;
        int bufferSize = MAX_SAMPLE_SIZE;
        int sampleCount = 0;
        int offset = 0;
        int videoSampleCount = 0;

        ByteBuffer dstBuf = ByteBuffer.allocate(bufferSize);
        BufferInfo bufferInfo = new BufferInfo();

        muxer.start();
        while (!sawEOS) {
            bufferInfo.offset = 0;
            bufferInfo.size = extractor.readSampleData(dstBuf, offset);
            if (bufferInfo.size < 0) {
                if (VERBOSE) {
                    Log.d(TAG, ""saw input EOS."");
                }
                sawEOS = true;
                bufferInfo.size = 0;
            } else {
                bufferInfo.presentationTimeUs = extractor.getSampleTime();
                bufferInfo.flags = extractor.getSampleFlags();
                int trackIndex = extractor.getSampleTrackIndex();
                if (VERBOSE) {
                    Log.v(TAG, ""TrackIndex:"" + trackIndex + "" PresentationTimeUs:"" +
                                bufferInfo.presentationTimeUs + "" Flags:"" + bufferInfo.flags +
                                "" Size(bytes)"" + bufferInfo.size);
                }
                if (trackIndex == videoTrackIndex) {
                    ++videoSampleCount;
                    if (VERBOSE) {
                        Log.v(TAG, ""videoSampleCount : "" + videoSampleCount);
                    }
                    if (samplesDropSet == null || (!samplesDropSet.contains(videoSampleCount))) {
                        // Write video frame with start offset adjustment.
                        bufferInfo.presentationTimeUs += videoStartOffsetUs;
                        muxer.writeSampleData(indexMap.get(trackIndex), dstBuf, bufferInfo);
                    }
                    else {
                        if (VERBOSE) {
                            Log.v(TAG, ""skipped this frame"");
                        }
                    }
                } else {
                    // write audio sample with start offset adjustment.
                    bufferInfo.presentationTimeUs += audioStartOffsetUs;
                    muxer.writeSampleData(indexMap.get(trackIndex), dstBuf, bufferInfo);
                }
                extractor.advance();
                sampleCount++;
                if (VERBOSE) {
                    Log.i(TAG, ""Sample ("" + sampleCount + "")"" +
                            "" TrackIndex:"" + trackIndex +
                            "" PresentationTimeUs:"" + bufferInfo.presentationTimeUs +
                            "" Flags:"" + bufferInfo.flags +
                            "" Size(bytes)"" + bufferInfo.size );
                }
            }
        }

        muxer.stop();
        muxer.release();
        extractor.release();
        srcFd.close();

        return;
    }

    /*
     * Uses MediaExtractors and checks whether timestamps of all samples except in samplesDropSet
     *  and with start offsets adjustments for each track match.
     */
    private void verifyTSWithSamplesDropAndStartOffset(final String srcMedia, boolean hasBframes,
            String testMediaPath, HashSet<Integer> samplesDropSet,
            Vector<Integer> startOffsetUsVect) throws IOException {
        AssetFileDescriptor srcFd = getAssetFileDescriptorFor(srcMedia);
        MediaExtractor extractorSrc = new MediaExtractor();
        extractorSrc.setDataSource(srcFd.getFileDescriptor(),
            srcFd.getStartOffset(), srcFd.getLength());
        MediaExtractor extractorTest = new MediaExtractor();
        extractorTest.setDataSource(testMediaPath);

        int videoTrackIndex = -1;
        int videoStartOffsetUs = 0;
        int minStartOffsetUs = Integer.MAX_VALUE;
        int trackCount = extractorSrc.getTrackCount();

        /*
         * When all track's start offsets are positive, MPEG4Writer makes the start timestamp of the
         * earliest track as zero and adjusts all other tracks' timestamp accordingly.
         */
        // TODO: need to confirm if the above logic holds good with all others writers we support.
        if (startOffsetUsVect != null) {
            for (int startOffsetUs : startOffsetUsVect) {
                minStartOffsetUs = Math.min(startOffsetUs, minStartOffsetUs);
            }
        } else {
            minStartOffsetUs = 0;
        }

        if (minStartOffsetUs < 0) {
            /*
             * Atleast one of the start offsets were negative. We have some test cases with negative
             * offsets for audio, minStartOffset has to be reset as Writer won't adjust any of the
             * track's timestamps.
             */
            minStartOffsetUs = 0;
        }

        // Select video track.
        for (int i = 0; i < trackCount; i++) {
            MediaFormat format = extractorSrc.getTrackFormat(i);
            if (format.getString(MediaFormat.KEY_MIME).startsWith(""video/"")) {
                videoTrackIndex = i;
                if (startOffsetUsVect != null && videoTrackIndex < startOffsetUsVect.size()) {
                    videoStartOffsetUs = startOffsetUsVect.get(videoTrackIndex);
                }
                extractorSrc.selectTrack(videoTrackIndex);
                extractorTest.selectTrack(videoTrackIndex);
                checkVideoSamplesTimeStamps(extractorSrc, hasBframes, extractorTest, samplesDropSet,
                    videoStartOffsetUs - minStartOffsetUs);
                extractorSrc.unselectTrack(videoTrackIndex);
                extractorTest.unselectTrack(videoTrackIndex);
            }
        }

        int audioTrackIndex = -1;
        int audioSampleCount = 0;
        int audioStartOffsetUs = 0;
        //select audio track
        for (int i = 0; i < trackCount; i++) {
            MediaFormat format = extractorSrc.getTrackFormat(i);
            if (format.getString(MediaFormat.KEY_MIME).startsWith(""audio/"")) {
                audioTrackIndex = i;
                if (startOffsetUsVect != null && audioTrackIndex < startOffsetUsVect.size()) {
                    audioStartOffsetUs = startOffsetUsVect.get(audioTrackIndex);
                }
                extractorSrc.selectTrack(audioTrackIndex);
                extractorTest.selectTrack(audioTrackIndex);
                checkAudioSamplesTimestamps(
                        extractorSrc, extractorTest, audioStartOffsetUs - minStartOffsetUs);
            }
        }

        extractorSrc.release();
        extractorTest.release();
        srcFd.close();
    }

    // Check timestamps of all video samples.
    private void checkVideoSamplesTimeStamps(MediaExtractor extractorSrc, boolean hasBFrames,
            MediaExtractor extractorTest, HashSet<Integer> samplesDropSet, int videoStartOffsetUs) {
        long srcSampleTimeUs = -1;
        long testSampleTimeUs = -1;
        boolean srcAdvance = false;
        boolean testAdvance = false;
        int videoSampleCount = 0;

        extractorSrc.seekTo(0, MediaExtractor.SEEK_TO_CLOSEST_SYNC);
        extractorTest.seekTo(0, MediaExtractor.SEEK_TO_CLOSEST_SYNC);

        if (VERBOSE) {
            Log.v(TAG, ""srcTrackIndex:"" + extractorSrc.getSampleTrackIndex() +
                        ""  testTrackIndex:"" + extractorTest.getSampleTrackIndex());
            Log.v(TAG, ""videoStartOffsetUs:"" + videoStartOffsetUs);
        }

        do {
            ++videoSampleCount;
            srcSampleTimeUs = extractorSrc.getSampleTime();
            testSampleTimeUs = extractorTest.getSampleTime();
            if (VERBOSE) {
                Log.v(TAG, ""videoSampleCount:"" + videoSampleCount);
                Log.i(TAG, ""srcTSus:"" + srcSampleTimeUs + "" testTSus:"" + testSampleTimeUs);
            }
            if (samplesDropSet == null || !samplesDropSet.contains(videoSampleCount)) {
                if (srcSampleTimeUs == -1 || testSampleTimeUs == -1) {
                    if (VERBOSE) {
                        Log.v(TAG, ""srcUs:"" + srcSampleTimeUs + ""testUs:"" + testSampleTimeUs);
                    }
                    fail(""either source or test track reached end of stream"");
                }
                /* Stts values within 0.1ms(100us) difference are fudged to save too many
                 * stts entries in MPEG4Writer.
                 */
                else if (Math.abs(srcSampleTimeUs + videoStartOffsetUs - testSampleTimeUs) > 100) {
                    if (VERBOSE) {
                        Log.v(TAG, ""Fail:video timestamps didn't match"");
                        Log.v(TAG,
                            ""srcTrackIndex:"" + extractorSrc.getSampleTrackIndex()
                                + ""  testTrackIndex:"" + extractorTest.getSampleTrackIndex());
                        Log.v(TAG, ""srcTSus:"" + srcSampleTimeUs + "" testTSus:"" + testSampleTimeUs);
                  }
                    fail(""video timestamps didn't match"");
                }
                testAdvance = extractorTest.advance();
            }
            srcAdvance = extractorSrc.advance();
        } while (srcAdvance && testAdvance);
        if (srcAdvance != testAdvance) {
            if (VERBOSE) {
                Log.v(TAG, ""videoSampleCount:"" + videoSampleCount);
            }
            fail(""either video track has not reached its last sample"");
        }
    }

    private void checkAudioSamplesTimestamps(MediaExtractor extractorSrc,
                MediaExtractor extractorTest, int audioStartOffsetUs) {
        long srcSampleTimeUs = -1;
        long testSampleTimeUs = -1;
        boolean srcAdvance = false;
        boolean testAdvance = false;
        int audioSampleCount = 0;

        extractorSrc.seekTo(0, MediaExtractor.SEEK_TO_CLOSEST_SYNC);
        if (audioStartOffsetUs >= 0) {
            // Added edit list support for maintaining only the diff in start offsets of tracks.
            // TODO: Remove this once we add support for preserving absolute timestamps as well.
            extractorTest.seekTo(0, MediaExtractor.SEEK_TO_CLOSEST_SYNC);
        } else {
            extractorTest.seekTo(audioStartOffsetUs, MediaExtractor.SEEK_TO_CLOSEST_SYNC);
        }
        if (VERBOSE) {
            Log.v(TAG, ""audioStartOffsetUs:"" + audioStartOffsetUs);
            Log.v(TAG, ""srcTrackIndex:"" + extractorSrc.getSampleTrackIndex() +
                        ""  testTrackIndex:"" + extractorTest.getSampleTrackIndex());
        }
        // Check timestamps of all audio samples.
        do {
            ++audioSampleCount;
            srcSampleTimeUs = extractorSrc.getSampleTime();
            testSampleTimeUs = extractorTest.getSampleTime();
            if (VERBOSE) {
                Log.v(TAG, ""audioSampleCount:"" + audioSampleCount);
                Log.v(TAG, ""srcTSus:"" + srcSampleTimeUs + "" testTSus:"" + testSampleTimeUs);
            }

            if (srcSampleTimeUs == -1 || testSampleTimeUs == -1) {
                if (VERBOSE) {
                    Log.v(TAG, ""srcTSus:"" + srcSampleTimeUs + "" testTSus:"" + testSampleTimeUs);
                }
                fail(""either source or test track reached end of stream"");
            }
            // > 1us to ignore any round off errors.
            else if (Math.abs(srcSampleTimeUs + audioStartOffsetUs - testSampleTimeUs) > 1) {
                fail(""audio timestamps didn't match"");
            }
            testAdvance = extractorTest.advance();
            srcAdvance = extractorSrc.advance();
        } while (srcAdvance && testAdvance);
        if (srcAdvance != testAdvance) {
            fail(""either audio track has not reached its last sample"");
        }
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RecordingTest"	"testRecordingWithDifferentPreviewSizes"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RecordingTest.java"	""	"public void testRecordingWithDifferentPreviewSizes() throws Exception {
        if (!MediaUtils.checkCodecForDomain(true /* encoder */, ""video"")) {
            return; // skipped
        }
        mPersistentSurface = MediaCodec.createPersistentInputSurface();
        assertNotNull(""Failed to create persistent input surface!"", mPersistentSurface);

        try {
            doRecordingWithDifferentPreviewSizes();
        } finally {
            mPersistentSurface.release();
            mPersistentSurface = null;
        }
    }

    public void doRecordingWithDifferentPreviewSizes() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing recording with different preview sizes for camera "" +
                        mCameraIdsUnderTest[i]);
                StaticMetadata staticInfo = mAllStaticInfo.get(mCameraIdsUnderTest[i]);
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                if (staticInfo.isExternalCamera()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support CamcorderProfile, skipping"");
                    continue;
                }
                // Re-use the MediaRecorder object for the same camera device.
                mMediaRecorder = new MediaRecorder();
                openDevice(mCameraIdsUnderTest[i]);

                initSupportedVideoSize(mCameraIdsUnderTest[i]);

                Size maxPreviewSize = mOrderedPreviewSizes.get(0);
                List<Range<Integer> > fpsRanges = Arrays.asList(
                        mStaticInfo.getAeAvailableTargetFpsRangesChecked());
                int cameraId = Integer.valueOf(mCamera.getId());
                int maxVideoFrameRate = -1;
                for (int profileId : mCamcorderProfileList) {
                    if (!CamcorderProfile.hasProfile(cameraId, profileId)) {
                        continue;
                    }
                    CamcorderProfile profile = CamcorderProfile.get(cameraId, profileId);

                    Size videoSz = new Size(profile.videoFrameWidth, profile.videoFrameHeight);
                    Range<Integer> fpsRange = new Range(
                            profile.videoFrameRate, profile.videoFrameRate);
                    if (maxVideoFrameRate < profile.videoFrameRate) {
                        maxVideoFrameRate = profile.videoFrameRate;
                    }

                    if (allowedUnsupported(cameraId, profileId)) {
                        continue;
                    }

                    if (mStaticInfo.isHardwareLevelLegacy() &&
                            (videoSz.getWidth() > maxPreviewSize.getWidth() ||
                             videoSz.getHeight() > maxPreviewSize.getHeight())) {
                        // Skip. Legacy mode can only do recording up to max preview size
                        continue;
                    }
                    assertTrue(""Video size "" + videoSz.toString() + "" for profile ID "" + profileId +
                                    "" must be one of the camera device supported video size!"",
                                    mSupportedVideoSizes.contains(videoSz));
                    assertTrue(""Frame rate range "" + fpsRange + "" (for profile ID "" + profileId +
                            "") must be one of the camera device available FPS range!"",
                            fpsRanges.contains(fpsRange));

                    // Configure preview and recording surfaces.
                    mOutMediaFileName = mDebugFileNameBase + ""/test_video_surface_reconfig.mp4"";

                    // prepare preview surface by using video size.
                    List<Size> previewSizes = getPreviewSizesForVideo(videoSz,
                            profile.videoFrameRate);
                    if (previewSizes.size() <= 1) {
                        continue;
                    }

                    // 1. Do video recording using largest compatbile preview sizes
                    prepareRecordingWithProfile(profile);
                    updatePreviewSurface(previewSizes.get(0));
                    SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
                    startRecording(
                            /* useMediaRecorder */true, resultListener,
                            /*useVideoStab*/false, fpsRange, false);
                    SystemClock.sleep(RECORDING_DURATION_MS);
                    stopRecording(/* useMediaRecorder */true, /* useIntermediateSurface */false,
                            /* stopStreaming */false);

                    // 2. Reconfigure with the same recording surface, but switch to a smaller
                    // preview size.
                    prepareRecordingWithProfile(profile);
                    updatePreviewSurface(previewSizes.get(1));
                    SimpleCaptureCallback resultListener2 = new SimpleCaptureCallback();
                    startRecording(
                            /* useMediaRecorder */true, resultListener2,
                            /*useVideoStab*/false, fpsRange, false);
                    SystemClock.sleep(RECORDING_DURATION_MS);
                    stopRecording(/* useMediaRecorder */true);
                    break;
                }
            } finally {
                closeDevice();
                releaseRecorder();
            }
        }
    }

    /**
     * Test camera preview and video surface sharing for maximum supported size.
     */
    private void videoPreviewSurfaceSharingTestByCamera() throws Exception {
        for (Size sz : mOrderedPreviewSizes) {
            if (!isSupported(sz, VIDEO_FRAME_RATE, VIDEO_FRAME_RATE)) {
                continue;
            }

            if (VERBOSE) {
                Log.v(TAG, ""Testing camera recording with video size "" + sz.toString());
            }

            // Configure preview and recording surfaces.
            mOutMediaFileName = mDebugFileNameBase + ""/test_video_share.mp4"";
            if (DEBUG_DUMP) {
                mOutMediaFileName = mDebugFileNameBase + ""/test_video_share_"" + mCamera.getId() +
                    ""_"" + sz.toString() + "".mp4"";
            }

            // Allow external camera to use variable fps range
            Range<Integer> fpsRange = null;
            if (mStaticInfo.isExternalCamera()) {
                Range<Integer>[] availableFpsRange =
                        mStaticInfo.getAeAvailableTargetFpsRangesChecked();

                boolean foundRange = false;
                int minFps = 0;
                for (int i = 0; i < availableFpsRange.length; i += 1) {
                    if (minFps < availableFpsRange[i].getLower()
                            && VIDEO_FRAME_RATE == availableFpsRange[i].getUpper()) {
                        minFps = availableFpsRange[i].getLower();
                        foundRange = true;
                    }
                }
                assertTrue(""Cannot find FPS range for maxFps "" + VIDEO_FRAME_RATE, foundRange);
                fpsRange = Range.create(minFps, VIDEO_FRAME_RATE);
            }

            // Use AVC and AAC a/v compression format.
            prepareRecording(sz, VIDEO_FRAME_RATE, VIDEO_FRAME_RATE);

            // prepare preview surface by using video size.
            updatePreviewSurfaceWithVideo(sz, VIDEO_FRAME_RATE);

            // Start recording
            SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
            if (!startSharedRecording(/* useMediaRecorder */true, resultListener,
                    /*useVideoStab*/false, fpsRange)) {
                mMediaRecorder.reset();
                continue;
            }

            // Record certain duration.
            SystemClock.sleep(RECORDING_DURATION_MS);

            // Stop recording and preview
            stopRecording(/* useMediaRecorder */true);
            // Convert number of frames camera produced into the duration in unit of ms.
            float frameDurationMinMs = 1000.0f / VIDEO_FRAME_RATE;
            float durationMinMs = resultListener.getTotalNumFrames() * frameDurationMinMs;
            float durationMaxMs = durationMinMs;
            float frameDurationMaxMs = 0.f;
            if (fpsRange != null) {
                frameDurationMaxMs = 1000.0f / fpsRange.getLower();
                durationMaxMs = resultListener.getTotalNumFrames() * frameDurationMaxMs;
            }

            // Validation.
            validateRecording(sz, durationMinMs, durationMaxMs,
                    frameDurationMinMs, frameDurationMaxMs,
                    FRMDRP_RATE_TOLERANCE);

            break;
        }
    }

    /**
     * Test slow motion recording where capture rate (camera output) is different with
     * video (playback) frame rate for each camera if high speed recording is supported
     * by both camera and encoder.
     *
     * <p>
     * Normal recording use cases make the capture rate (camera output frame
     * rate) the same as the video (playback) frame rate. This guarantees that
     * the motions in the scene play at the normal speed. If the capture rate is
     * faster than video frame rate, for a given time duration, more number of
     * frames are captured than it can be played in the same time duration. This
     * generates ""slow motion"" effect during playback.
     * </p>
     */
    private void slowMotionRecording() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.i(TAG, ""Testing slow motion recording for camera "" + id);
                StaticMetadata staticInfo = mAllStaticInfo.get(id);
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                if (!staticInfo.isHighSpeedVideoSupported()) {
                    continue;
                }

                // Re-use the MediaRecorder object for the same camera device.
                mMediaRecorder = new MediaRecorder();
                openDevice(id);

                StreamConfigurationMap config =
                        mStaticInfo.getValueFromKeyNonNull(
                                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
                Size[] highSpeedVideoSizes = config.getHighSpeedVideoSizes();
                for (Size size : highSpeedVideoSizes) {
                    Range<Integer> fpsRange = getHighestHighSpeedFixedFpsRangeForSize(config, size);
                    mCollector.expectNotNull(""Unable to find the fixed frame rate fps range for "" +
                            ""size "" + size, fpsRange);
                    if (fpsRange == null) {
                        continue;
                    }

                    int captureRate = fpsRange.getLower();
                    int videoFramerate = captureRate / SLOWMO_SLOW_FACTOR;
                    // Skip the test if the highest recording FPS supported by CamcorderProfile
                    if (fpsRange.getUpper() > getFpsFromHighSpeedProfileForSize(size)) {
                        Log.w(TAG, ""high speed recording "" + size + ""@"" + captureRate + ""fps""
                                + "" is not supported by CamcorderProfile"");
                        continue;
                    }

                    mOutMediaFileName = mDebugFileNameBase + ""/test_slowMo_video.mp4"";
                    if (DEBUG_DUMP) {
                        mOutMediaFileName = mDebugFileNameBase + ""/test_slowMo_video_"" + id + ""_""
                                + size.toString() + "".mp4"";
                    }

                    prepareRecording(size, videoFramerate, captureRate);

                    // prepare preview surface by using video size.
                    updatePreviewSurfaceWithVideo(size, captureRate);

                    // Start recording
                    SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
                    startSlowMotionRecording(/*useMediaRecorder*/true, videoFramerate, captureRate,
                            fpsRange, resultListener, /*useHighSpeedSession*/false);

                    // Record certain duration.
                    SystemClock.sleep(RECORDING_DURATION_MS);

                    // Stop recording and preview
                    stopRecording(/*useMediaRecorder*/true);
                    // Convert number of frames camera produced into the duration in unit of ms.
                    float frameDurationMs = 1000.0f / videoFramerate;
                    float durationMs = resultListener.getTotalNumFrames() * frameDurationMs;

                    // Validation.
                    validateRecording(size, durationMs, frameDurationMs, FRMDRP_RATE_TOLERANCE);
                }

            } finally {
                closeDevice();
                releaseRecorder();
            }
        }
    }

    private void constrainedHighSpeedRecording() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.i(TAG, ""Testing constrained high speed recording for camera "" + id);

                if (!mAllStaticInfo.get(id).isConstrainedHighSpeedVideoSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" doesn't support high speed recording, skipping."");
                    continue;
                }

                // Re-use the MediaRecorder object for the same camera device.
                mMediaRecorder = new MediaRecorder();
                openDevice(id);

                StreamConfigurationMap config =
                        mStaticInfo.getValueFromKeyNonNull(
                                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
                Size[] highSpeedVideoSizes = config.getHighSpeedVideoSizes();
                for (Size size : highSpeedVideoSizes) {
                    List<Range<Integer>> fixedFpsRanges =
                            getHighSpeedFixedFpsRangeForSize(config, size);
                    mCollector.expectTrue(""Unable to find the fixed frame rate fps range for "" +
                            ""size "" + size, fixedFpsRanges.size() > 0);
                    // Test recording for each FPS range
                    for (Range<Integer> fpsRange : fixedFpsRanges) {
                        int captureRate = fpsRange.getLower();
                        final int VIDEO_FRAME_RATE = 30;
                        // Skip the test if the highest recording FPS supported by CamcorderProfile
                        if (fpsRange.getUpper() > getFpsFromHighSpeedProfileForSize(size)) {
                            Log.w(TAG, ""high speed recording "" + size + ""@"" + captureRate + ""fps""
                                    + "" is not supported by CamcorderProfile"");
                            continue;
                        }

                        SimpleCaptureCallback previewResultListener = new SimpleCaptureCallback();

                        // prepare preview surface by using video size.
                        updatePreviewSurfaceWithVideo(size, captureRate);

                        startConstrainedPreview(fpsRange, previewResultListener);

                        mOutMediaFileName = mDebugFileNameBase + ""/test_cslowMo_video_"" +
                            captureRate + ""fps_"" + id + ""_"" + size.toString() + "".mp4"";

                        prepareRecording(size, VIDEO_FRAME_RATE, captureRate);

                        SystemClock.sleep(PREVIEW_DURATION_MS);

                        stopCameraStreaming();

                        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
                        // Start recording
                        startSlowMotionRecording(/*useMediaRecorder*/true, VIDEO_FRAME_RATE,
                                captureRate, fpsRange, resultListener,
                                /*useHighSpeedSession*/true);

                        // Record certain duration.
                        SystemClock.sleep(RECORDING_DURATION_MS);

                        // Stop recording and preview
                        stopRecording(/*useMediaRecorder*/true);

                        startConstrainedPreview(fpsRange, previewResultListener);

                        // Convert number of frames camera produced into the duration in unit of ms.
                        float frameDurationMs = 1000.0f / VIDEO_FRAME_RATE;
                        float durationMs = resultListener.getTotalNumFrames() * frameDurationMs;

                        // Validation.
                        validateRecording(size, durationMs, frameDurationMs, FRMDRP_RATE_TOLERANCE);

                        SystemClock.sleep(PREVIEW_DURATION_MS);

                        stopCameraStreaming();
                    }
                }

            } finally {
                closeDevice();
                releaseRecorder();
            }
        }
    }

    /**
     * Get high speed FPS from CamcorderProfiles for a given size.
     *
     * @param size The size used to search the CamcorderProfiles for the FPS.
     * @return high speed video FPS, 0 if the given size is not supported by the CamcorderProfiles.
     */
    private int getFpsFromHighSpeedProfileForSize(Size size) {
        for (int quality = CamcorderProfile.QUALITY_HIGH_SPEED_480P;
                quality <= CamcorderProfile.QUALITY_HIGH_SPEED_2160P; quality++) {
            if (CamcorderProfile.hasProfile(quality)) {
                CamcorderProfile profile = CamcorderProfile.get(quality);
                if (size.equals(new Size(profile.videoFrameWidth, profile.videoFrameHeight))){
                    return profile.videoFrameRate;
                }
            }
        }

        return 0;
    }

    private Range<Integer> getHighestHighSpeedFixedFpsRangeForSize(StreamConfigurationMap config,
            Size size) {
        Range<Integer>[] availableFpsRanges = config.getHighSpeedVideoFpsRangesFor(size);
        Range<Integer> maxRange = availableFpsRanges[0];
        boolean foundRange = false;
        for (Range<Integer> range : availableFpsRanges) {
            if (range.getLower().equals(range.getUpper()) && range.getLower() >= maxRange.getLower()) {
                foundRange = true;
                maxRange = range;
            }
        }

        if (!foundRange) {
            return null;
        }
        return maxRange;
    }

    private List<Range<Integer>> getHighSpeedFixedFpsRangeForSize(StreamConfigurationMap config,
            Size size) {
        Range<Integer>[] availableFpsRanges = config.getHighSpeedVideoFpsRangesFor(size);
        List<Range<Integer>> fixedRanges = new ArrayList<Range<Integer>>();
        for (Range<Integer> range : availableFpsRanges) {
            if (range.getLower().equals(range.getUpper())) {
                fixedRanges.add(range);
            }
        }
        return fixedRanges;
    }

    private void startConstrainedPreview(Range<Integer> fpsRange,
            CameraCaptureSession.CaptureCallback listener) throws Exception {
        List<Surface> outputSurfaces = new ArrayList<Surface>(1);
        assertTrue(""Preview surface should be valid"", mPreviewSurface.isValid());
        outputSurfaces.add(mPreviewSurface);
        mSessionListener = new BlockingSessionCallback();

        List<CaptureRequest> slowMoRequests = null;
        CaptureRequest.Builder requestBuilder =
            mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
        requestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
        requestBuilder.addTarget(mPreviewSurface);
        CaptureRequest initialRequest = requestBuilder.build();
        CameraTestUtils.checkSessionConfigurationWithSurfaces(mCamera, mHandler,
                outputSurfaces, /*inputConfig*/ null, SessionConfiguration.SESSION_HIGH_SPEED,
                /*defaultSupport*/ true, ""Constrained session configuration query failed"");
        mSession = buildConstrainedCameraSession(mCamera, outputSurfaces, mSessionListener,
                mHandler, initialRequest);
        slowMoRequests = ((CameraConstrainedHighSpeedCaptureSession) mSession).
            createHighSpeedRequestList(initialRequest);

        mSession.setRepeatingBurst(slowMoRequests, listener, mHandler);
    }

    private void startSlowMotionRecording(boolean useMediaRecorder, int videoFrameRate,
            int captureRate, Range<Integer> fpsRange,
            CameraCaptureSession.CaptureCallback listener, boolean useHighSpeedSession)
            throws Exception {
        List<Surface> outputSurfaces = new ArrayList<Surface>(2);
        assertTrue(""Both preview and recording surfaces should be valid"",
                mPreviewSurface.isValid() && mRecordingSurface.isValid());
        outputSurfaces.add(mPreviewSurface);
        outputSurfaces.add(mRecordingSurface);
        // Video snapshot surface
        if (mReaderSurface != null) {
            outputSurfaces.add(mReaderSurface);
        }
        mSessionListener = new BlockingSessionCallback();

        // Create slow motion request list
        List<CaptureRequest> slowMoRequests = null;
        if (useHighSpeedSession) {
            CaptureRequest.Builder requestBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
            requestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
            requestBuilder.addTarget(mPreviewSurface);
            requestBuilder.addTarget(mRecordingSurface);
            CaptureRequest initialRequest = requestBuilder.build();
            mSession = buildConstrainedCameraSession(mCamera, outputSurfaces, mSessionListener,
                    mHandler, initialRequest);
            slowMoRequests = ((CameraConstrainedHighSpeedCaptureSession) mSession).
                    createHighSpeedRequestList(initialRequest);
        } else {
            CaptureRequest.Builder recordingRequestBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
            recordingRequestBuilder.set(CaptureRequest.CONTROL_MODE,
                    CaptureRequest.CONTROL_MODE_USE_SCENE_MODE);
            recordingRequestBuilder.set(CaptureRequest.CONTROL_SCENE_MODE,
                    CaptureRequest.CONTROL_SCENE_MODE_HIGH_SPEED_VIDEO);

            CaptureRequest.Builder recordingOnlyBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
            recordingOnlyBuilder.set(CaptureRequest.CONTROL_MODE,
                    CaptureRequest.CONTROL_MODE_USE_SCENE_MODE);
            recordingOnlyBuilder.set(CaptureRequest.CONTROL_SCENE_MODE,
                    CaptureRequest.CONTROL_SCENE_MODE_HIGH_SPEED_VIDEO);
            int slowMotionFactor = captureRate / videoFrameRate;

            // Make sure camera output frame rate is set to correct value.
            recordingRequestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
            recordingRequestBuilder.addTarget(mRecordingSurface);
            recordingRequestBuilder.addTarget(mPreviewSurface);
            recordingOnlyBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
            recordingOnlyBuilder.addTarget(mRecordingSurface);

            CaptureRequest initialRequest = recordingRequestBuilder.build();
            mSession = configureCameraSessionWithParameters(mCamera, outputSurfaces,
                    mSessionListener, mHandler, initialRequest);

            slowMoRequests = new ArrayList<CaptureRequest>();
            slowMoRequests.add(initialRequest);// Preview + recording.

            for (int i = 0; i < slowMotionFactor - 1; i++) {
                slowMoRequests.add(recordingOnlyBuilder.build()); // Recording only.
            }
        }

        mSession.setRepeatingBurst(slowMoRequests, listener, mHandler);

        if (useMediaRecorder) {
            mMediaRecorder.start();
        } else {
            // TODO: need implement MediaCodec path.
        }

    }

    private void basicRecordingTestByCamera(int[] camcorderProfileList, boolean useVideoStab)
            throws Exception {
        basicRecordingTestByCamera(camcorderProfileList, useVideoStab, false);
    }

    private void basicRecordingTestByCamera(int[] camcorderProfileList, boolean useVideoStab,
            boolean useIntermediateSurface) throws Exception {
        basicRecordingTestByCamera(camcorderProfileList, useVideoStab,
                useIntermediateSurface, false);
    }

    /**
     * Test camera recording by using each available CamcorderProfile for a
     * given camera. preview size is set to the video size.
     */
    private void basicRecordingTestByCamera(int[] camcorderProfileList, boolean useVideoStab,
            boolean useIntermediateSurface, boolean useEncoderProfiles) throws Exception {
        Size maxPreviewSize = mOrderedPreviewSizes.get(0);
        List<Range<Integer> > fpsRanges = Arrays.asList(
                mStaticInfo.getAeAvailableTargetFpsRangesChecked());
        int cameraId = Integer.valueOf(mCamera.getId());
        int maxVideoFrameRate = -1;

        // only validate recording for non-perf measurement runs
        boolean validateRecording = !isPerfMeasure();
        for (int profileId : camcorderProfileList) {
            if (!CamcorderProfile.hasProfile(cameraId, profileId)) {
                continue;
            }

            CamcorderProfile profile = CamcorderProfile.get(cameraId, profileId);
            Size videoSz = new Size(profile.videoFrameWidth, profile.videoFrameHeight);

            Range<Integer> fpsRange = new Range(profile.videoFrameRate, profile.videoFrameRate);
            if (maxVideoFrameRate < profile.videoFrameRate) {
                maxVideoFrameRate = profile.videoFrameRate;
            }

            if (allowedUnsupported(cameraId, profileId)) {
                continue;
            }

            if (mStaticInfo.isHardwareLevelLegacy() &&
                    (videoSz.getWidth() > maxPreviewSize.getWidth() ||
                     videoSz.getHeight() > maxPreviewSize.getHeight())) {
                // Skip. Legacy mode can only do recording up to max preview size
                continue;
            }
            assertTrue(""Video size "" + videoSz.toString() + "" for profile ID "" + profileId +
                            "" must be one of the camera device supported video size!"",
                            mSupportedVideoSizes.contains(videoSz));
            assertTrue(""Frame rate range "" + fpsRange + "" (for profile ID "" + profileId +
                    "") must be one of the camera device available FPS range!"",
                    fpsRanges.contains(fpsRange));


            if (useEncoderProfiles) {
                // Iterate through all video-audio codec combination
                EncoderProfiles profiles = CamcorderProfile.getAll(mCamera.getId(), profileId);
                for (EncoderProfiles.VideoProfile videoProfile : profiles.getVideoProfiles()) {
                    boolean hasAudioProfile = false;
                    for (EncoderProfiles.AudioProfile audioProfile : profiles.getAudioProfiles()) {
                        hasAudioProfile = true;
                        doBasicRecordingByProfile(profiles, videoProfile, audioProfile,
                                useVideoStab, useIntermediateSurface, validateRecording);
                        // Only measure the default video profile of the largest video
                        // recording size when measuring perf
                        if (isPerfMeasure()) {
                            break;
                        }
                    }
                    // Timelapse profiles do not have audio track
                    if (!hasAudioProfile) {
                        doBasicRecordingByProfile(profiles, videoProfile, /* audioProfile */null,
                                useVideoStab, useIntermediateSurface, validateRecording);
                    }
                }
            } else {
                doBasicRecordingByProfile(
                        profile, useVideoStab, useIntermediateSurface, validateRecording);
            }

            if (isPerfMeasure()) {
                // Only measure the largest video recording size when measuring perf
                break;
            }
        }
        if (maxVideoFrameRate != -1) {
            // At least one CamcorderProfile is present, check FPS
            assertTrue(""At least one CamcorderProfile must support >= 24 FPS"",
                    maxVideoFrameRate >= 24);
        }
    }

    private void doBasicRecordingByProfile(
            CamcorderProfile profile, boolean userVideoStab,
            boolean useIntermediateSurface, boolean validate) throws Exception {
        Size videoSz = new Size(profile.videoFrameWidth, profile.videoFrameHeight);
        int frameRate = profile.videoFrameRate;

        if (VERBOSE) {
            Log.v(TAG, ""Testing camera recording with video size "" + videoSz.toString());
        }

        // Configure preview and recording surfaces.
        mOutMediaFileName = mDebugFileNameBase + ""/test_video.mp4"";
        if (DEBUG_DUMP) {
            mOutMediaFileName = mDebugFileNameBase + ""/test_video_"" + mCamera.getId() + ""_""
                    + videoSz.toString() + "".mp4"";
        }

        setupMediaRecorder(profile);
        completeBasicRecording(videoSz, frameRate, userVideoStab, useIntermediateSurface, validate);
    }

    private void doBasicRecordingByProfile(
            EncoderProfiles profiles,
            EncoderProfiles.VideoProfile videoProfile, EncoderProfiles.AudioProfile audioProfile,
            boolean userVideoStab, boolean useIntermediateSurface, boolean validate)
                    throws Exception {
        Size videoSz = new Size(videoProfile.getWidth(), videoProfile.getHeight());
        int frameRate = videoProfile.getFrameRate();

        if (VERBOSE) {
            Log.v(TAG, ""Testing camera recording with video size "" + videoSz.toString() +
                  "", video codec "" + videoProfile.getMediaType() + "", and audio codec "" +
                  (audioProfile == null ? ""(null)"" : audioProfile.getMediaType()));
        }

        // Configure preview and recording surfaces.
        mOutMediaFileName = mDebugFileNameBase + ""/test_video.mp4"";
        if (DEBUG_DUMP) {
            mOutMediaFileName = mDebugFileNameBase + ""/test_video_"" + mCamera.getId() + ""_""
                    + videoSz.toString() + ""_"" + videoProfile.getCodec();
            if (audioProfile != null) {
                mOutMediaFileName += ""_"" + audioProfile.getCodec();
            }
            mOutMediaFileName += "".mp4"";
        }

        setupMediaRecorder(profiles, videoProfile, audioProfile);
        completeBasicRecording(videoSz, frameRate, userVideoStab, useIntermediateSurface, validate);
    }

    private void completeBasicRecording(
            Size videoSz, int frameRate, boolean useVideoStab,
            boolean useIntermediateSurface, boolean validate) throws Exception {
        prepareRecording(useIntermediateSurface);

        // prepare preview surface by using video size.
        updatePreviewSurfaceWithVideo(videoSz, frameRate);

        // Start recording
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        startRecording(/* useMediaRecorder */true, resultListener, useVideoStab,
                useIntermediateSurface);

        // Record certain duration.
        SystemClock.sleep(RECORDING_DURATION_MS);

        // Stop recording and preview
        stopRecording(/* useMediaRecorder */true, useIntermediateSurface,
                /* stopCameraStreaming */true);
        // Convert number of frames camera produced into the duration in unit of ms.
        float frameDurationMs = 1000.0f / frameRate;
        float durationMs = 0.f;
        if (useIntermediateSurface) {
            durationMs = mQueuer.getQueuedCount() * frameDurationMs;
        } else {
            durationMs = resultListener.getTotalNumFrames() * frameDurationMs;
        }

        if (VERBOSE) {
            Log.v(TAG, ""video frame rate: "" + frameRate +
                            "", num of frames produced: "" + resultListener.getTotalNumFrames());
        }

        if (validate) {
            validateRecording(videoSz, durationMs, frameDurationMs, FRMDRP_RATE_TOLERANCE);
        }
    }

    /**
     * Test camera recording for each supported video size by camera, preview
     * size is set to the video size.
     */
    private void recordingSizeTestByCamera() throws Exception {
        for (Size sz : mSupportedVideoSizes) {
            if (!isSupported(sz, VIDEO_FRAME_RATE, VIDEO_FRAME_RATE)) {
                continue;
            }

            if (VERBOSE) {
                Log.v(TAG, ""Testing camera recording with video size "" + sz.toString());
            }

            // Configure preview and recording surfaces.
            mOutMediaFileName = mDebugFileNameBase + ""/test_video.mp4"";
            if (DEBUG_DUMP) {
                mOutMediaFileName = mDebugFileNameBase + ""/test_video_"" + mCamera.getId() + ""_""
                        + sz.toString() + "".mp4"";
            }

            // Allow external camera to use variable fps range
            Range<Integer> fpsRange = null;
            if (mStaticInfo.isExternalCamera()) {
                Range<Integer>[] availableFpsRange =
                        mStaticInfo.getAeAvailableTargetFpsRangesChecked();

                boolean foundRange = false;
                int minFps = 0;
                for (int i = 0; i < availableFpsRange.length; i += 1) {
                    if (minFps < availableFpsRange[i].getLower()
                            && VIDEO_FRAME_RATE == availableFpsRange[i].getUpper()) {
                        minFps = availableFpsRange[i].getLower();
                        foundRange = true;
                    }
                }
                assertTrue(""Cannot find FPS range for maxFps "" + VIDEO_FRAME_RATE, foundRange);
                fpsRange = Range.create(minFps, VIDEO_FRAME_RATE);
            }

            // Use AVC and AAC a/v compression format.
            prepareRecording(sz, VIDEO_FRAME_RATE, VIDEO_FRAME_RATE);

            // prepare preview surface by using video size.
            updatePreviewSurfaceWithVideo(sz, VIDEO_FRAME_RATE);

            // Start recording
            SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
            startRecording(
                    /* useMediaRecorder */true, resultListener,
                    /*useVideoStab*/false, fpsRange, false);

            // Record certain duration.
            SystemClock.sleep(RECORDING_DURATION_MS);

            // Stop recording and preview
            stopRecording(/* useMediaRecorder */true);
            // Convert number of frames camera produced into the duration in unit of ms.
            float frameDurationMinMs = 1000.0f / VIDEO_FRAME_RATE;
            float durationMinMs = resultListener.getTotalNumFrames() * frameDurationMinMs;
            float durationMaxMs = durationMinMs;
            float frameDurationMaxMs = 0.f;
            if (fpsRange != null) {
                frameDurationMaxMs = 1000.0f / fpsRange.getLower();
                durationMaxMs = resultListener.getTotalNumFrames() * frameDurationMaxMs;
            }

            // Validation.
            validateRecording(sz, durationMinMs, durationMaxMs,
                    frameDurationMinMs, frameDurationMaxMs,
                    FRMDRP_RATE_TOLERANCE);
        }
    }

    /**
     * Initialize the supported video sizes.
     */
    private void initSupportedVideoSize(String cameraId)  throws Exception {
        int id = Integer.valueOf(cameraId);
        Size maxVideoSize = SIZE_BOUND_720P;
        if (CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_2160P)) {
            maxVideoSize = SIZE_BOUND_2160P;
        } else if (CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_QHD)) {
            maxVideoSize = SIZE_BOUND_QHD;
        } else if (CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_2K)) {
            maxVideoSize = SIZE_BOUND_2K;
        } else if (CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_1080P)) {
            maxVideoSize = SIZE_BOUND_1080P;
        }

        mSupportedVideoSizes =
                getSupportedVideoSizes(cameraId, mCameraManager, maxVideoSize);
    }

    /**
     * Simple wrapper to wrap normal/burst video snapshot tests
     */
    private void videoSnapshotHelper(boolean burstTest) throws Exception {
            for (String id : mCameraIdsUnderTest) {
                try {
                    Log.i(TAG, ""Testing video snapshot for camera "" + id);

                    StaticMetadata staticInfo = mAllStaticInfo.get(id);
                    if (!staticInfo.isColorOutputSupported()) {
                        Log.i(TAG, ""Camera "" + id +
                                "" does not support color outputs, skipping"");
                        continue;
                    }

                    if (staticInfo.isExternalCamera()) {
                        Log.i(TAG, ""Camera "" + id +
                                "" does not support CamcorderProfile, skipping"");
                        continue;
                    }

                    // Re-use the MediaRecorder object for the same camera device.
                    mMediaRecorder = new MediaRecorder();

                    openDevice(id);

                    initSupportedVideoSize(id);

                    videoSnapshotTestByCamera(burstTest);
                } finally {
                    closeDevice();
                    releaseRecorder();
                }
            }
    }

    /**
     * Returns {@code true} if the {@link CamcorderProfile} ID is allowed to be unsupported.
     *
     * <p>This only allows unsupported profiles when using the LEGACY mode of the Camera API.</p>
     *
     * @param profileId a {@link CamcorderProfile} ID to check.
     * @return {@code true} if supported.
     */
    private boolean allowedUnsupported(int cameraId, int profileId) {
        if (!mStaticInfo.isHardwareLevelLegacy()) {
            return false;
        }

        switch(profileId) {
            case CamcorderProfile.QUALITY_2160P:
            case CamcorderProfile.QUALITY_1080P:
            case CamcorderProfile.QUALITY_HIGH:
                return !CamcorderProfile.hasProfile(cameraId, profileId) ||
                        CamcorderProfile.get(cameraId, profileId).videoFrameWidth >= 1080;
        }
        return false;
    }

    /**
     * Test video snapshot for each  available CamcorderProfile for a given camera.
     *
     * <p>
     * Preview size is set to the video size. For the burst test, frame drop and jittering
     * is not checked.
     * </p>
     *
     * @param burstTest Perform burst capture or single capture. For burst capture
     *                  {@value #BURST_VIDEO_SNAPSHOT_NUM} capture requests will be sent.
     */
    private void videoSnapshotTestByCamera(boolean burstTest)
            throws Exception {
        final int NUM_SINGLE_SHOT_TEST = 5;
        final int FRAMEDROP_TOLERANCE = 8;
        final int FRAME_SIZE_15M = 15000000;
        final float FRAME_DROP_TOLERENCE_FACTOR = 1.5f;
        int kFrameDrop_Tolerence = FRAMEDROP_TOLERANCE;

        for (int profileId : mCamcorderProfileList) {
            int cameraId = Integer.valueOf(mCamera.getId());
            if (!CamcorderProfile.hasProfile(cameraId, profileId) ||
                    allowedUnsupported(cameraId, profileId)) {
                continue;
            }

            CamcorderProfile profile = CamcorderProfile.get(cameraId, profileId);
            Size QCIF = new Size(176, 144);
            Size FULL_HD = new Size(1920, 1080);
            Size videoSz = new Size(profile.videoFrameWidth, profile.videoFrameHeight);
            Size maxPreviewSize = mOrderedPreviewSizes.get(0);

            if (mStaticInfo.isHardwareLevelLegacy() &&
                    (videoSz.getWidth() > maxPreviewSize.getWidth() ||
                     videoSz.getHeight() > maxPreviewSize.getHeight())) {
                // Skip. Legacy mode can only do recording up to max preview size
                continue;
            }

            if (!mSupportedVideoSizes.contains(videoSz)) {
                mCollector.addMessage(""Video size "" + videoSz.toString() + "" for profile ID "" +
                        profileId + "" must be one of the camera device supported video size!"");
                continue;
            }

            // For LEGACY, find closest supported smaller or equal JPEG size to the current video
            // size; if no size is smaller than the video, pick the smallest JPEG size.  The assert
            // for video size above guarantees that for LIMITED or FULL, we select videoSz here.
            // Also check for minFrameDuration here to make sure jpeg stream won't slow down
            // video capture
            Size videoSnapshotSz = mOrderedStillSizes.get(mOrderedStillSizes.size() - 1);
            // Allow a bit tolerance so we don't fail for a few nano seconds of difference
            final float FRAME_DURATION_TOLERANCE = 0.01f;
            long videoFrameDuration = (long) (1e9 / profile.videoFrameRate *
                    (1.0 + FRAME_DURATION_TOLERANCE));
            HashMap<Size, Long> minFrameDurationMap = mStaticInfo.
                    getAvailableMinFrameDurationsForFormatChecked(ImageFormat.JPEG);
            for (int i = mOrderedStillSizes.size() - 2; i >= 0; i--) {
                Size candidateSize = mOrderedStillSizes.get(i);
                if (mStaticInfo.isHardwareLevelLegacy()) {
                    // Legacy level doesn't report min frame duration
                    if (candidateSize.getWidth() <= videoSz.getWidth() &&
                            candidateSize.getHeight() <= videoSz.getHeight()) {
                        videoSnapshotSz = candidateSize;
                    }
                } else {
                    Long jpegFrameDuration = minFrameDurationMap.get(candidateSize);
                    assertTrue(""Cannot find minimum frame duration for jpeg size "" + candidateSize,
                            jpegFrameDuration != null);
                    if (candidateSize.getWidth() <= videoSz.getWidth() &&
                            candidateSize.getHeight() <= videoSz.getHeight() &&
                            jpegFrameDuration <= videoFrameDuration) {
                        videoSnapshotSz = candidateSize;
                    }
                }
            }
            Size defaultvideoSnapshotSz = videoSnapshotSz;

            /**
             * Only test full res snapshot when below conditions are all true.
             * 1. Camera is at least a LIMITED device.
             * 2. video size is up to max preview size, which will be bounded by 1080p.
             * 3. Full resolution jpeg stream can keep up to video stream speed.
             *    When full res jpeg stream cannot keep up to video stream speed, search
             *    the largest jpeg size that can susptain video speed instead.
             */
            if (mStaticInfo.isHardwareLevelAtLeastLimited() &&
                    videoSz.getWidth() <= maxPreviewSize.getWidth() &&
                    videoSz.getHeight() <= maxPreviewSize.getHeight()) {
                for (Size jpegSize : mOrderedStillSizes) {
                    Long jpegFrameDuration = minFrameDurationMap.get(jpegSize);
                    assertTrue(""Cannot find minimum frame duration for jpeg size "" + jpegSize,
                            jpegFrameDuration != null);
                    if (jpegFrameDuration <= videoFrameDuration) {
                        videoSnapshotSz = jpegSize;
                        break;
                    }
                    if (jpegSize.equals(videoSz)) {
                        throw new AssertionFailedError(
                                ""Cannot find adequate video snapshot size for video size"" +
                                        videoSz);
                    }
                }
            }

            if (videoSnapshotSz.getWidth() * videoSnapshotSz.getHeight() > FRAME_SIZE_15M)
                kFrameDrop_Tolerence = (int)(FRAMEDROP_TOLERANCE * FRAME_DROP_TOLERENCE_FACTOR);

            createImageReader(
                    videoSnapshotSz, ImageFormat.JPEG,
                    MAX_VIDEO_SNAPSHOT_IMAGES, /*listener*/null);

            // Full or better devices should support whatever video snapshot size calculated above.
            // Limited devices may only be able to support the default one.
            if (mStaticInfo.isHardwareLevelLimited()) {
                List<Surface> outputs = new ArrayList<Surface>();
                outputs.add(mPreviewSurface);
                outputs.add(mRecordingSurface);
                outputs.add(mReaderSurface);
                boolean isSupported = isStreamConfigurationSupported(
                        mCamera, outputs, mSessionListener, mHandler);
                if (!isSupported) {
                    videoSnapshotSz = defaultvideoSnapshotSz;
                    createImageReader(
                            videoSnapshotSz, ImageFormat.JPEG,
                            MAX_VIDEO_SNAPSHOT_IMAGES, /*listener*/null);
                }
            }

            if (videoSz.equals(QCIF) &&
                    ((videoSnapshotSz.getWidth() > FULL_HD.getWidth()) ||
                     (videoSnapshotSz.getHeight() > FULL_HD.getHeight()))) {
                List<Surface> outputs = new ArrayList<Surface>();
                outputs.add(mPreviewSurface);
                outputs.add(mRecordingSurface);
                outputs.add(mReaderSurface);
                boolean isSupported = isStreamConfigurationSupported(
                        mCamera, outputs, mSessionListener, mHandler);
                if (!isSupported) {
                    videoSnapshotSz = defaultvideoSnapshotSz;
                    createImageReader(
                            videoSnapshotSz, ImageFormat.JPEG,
                            MAX_VIDEO_SNAPSHOT_IMAGES, /*listener*/null);
                }
            }

            Log.i(TAG, ""Testing video snapshot size "" + videoSnapshotSz +
                    "" for video size "" + videoSz);

            if (VERBOSE) {
                Log.v(TAG, ""Testing camera recording with video size "" + videoSz.toString());
            }

            // Configure preview and recording surfaces.
            mOutMediaFileName = mDebugFileNameBase + ""/test_video.mp4"";
            if (DEBUG_DUMP) {
                mOutMediaFileName = mDebugFileNameBase + ""/test_video_"" + cameraId + ""_""
                        + videoSz.toString() + "".mp4"";
            }

            int numTestIterations = burstTest ? 1 : NUM_SINGLE_SHOT_TEST;
            int totalDroppedFrames = 0;

            for (int numTested = 0; numTested < numTestIterations; numTested++) {
                prepareRecordingWithProfile(profile);

                // prepare video snapshot
                SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
                SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
                CaptureRequest.Builder videoSnapshotRequestBuilder =
                        mCamera.createCaptureRequest((mStaticInfo.isHardwareLevelLegacy()) ?
                                CameraDevice.TEMPLATE_RECORD :
                                CameraDevice.TEMPLATE_VIDEO_SNAPSHOT);

                // prepare preview surface by using video size.
                updatePreviewSurfaceWithVideo(videoSz, profile.videoFrameRate);

                prepareVideoSnapshot(videoSnapshotRequestBuilder, imageListener);
                Range<Integer> fpsRange = Range.create(profile.videoFrameRate,
                        profile.videoFrameRate);
                videoSnapshotRequestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE,
                        fpsRange);
                if (mStaticInfo.isVideoStabilizationSupported()) {
                    videoSnapshotRequestBuilder.set(CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE,
                            CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE_ON);
                }
                CaptureRequest request = videoSnapshotRequestBuilder.build();

                // Start recording
                startRecording(/* useMediaRecorder */true, resultListener,
                        /*useVideoStab*/mStaticInfo.isVideoStabilizationSupported());
                long startTime = SystemClock.elapsedRealtime();

                // Record certain duration.
                SystemClock.sleep(RECORDING_DURATION_MS / 2);

                // take video snapshot
                if (burstTest) {
                    List<CaptureRequest> requests =
                            new ArrayList<CaptureRequest>(BURST_VIDEO_SNAPSHOT_NUM);
                    for (int i = 0; i < BURST_VIDEO_SNAPSHOT_NUM; i++) {
                        requests.add(request);
                    }
                    mSession.captureBurst(requests, resultListener, mHandler);
                } else {
                    mSession.capture(request, resultListener, mHandler);
                }

                // make sure recording is still going after video snapshot
                SystemClock.sleep(RECORDING_DURATION_MS / 2);

                // Stop recording and preview
                float durationMs = (float) stopRecording(/* useMediaRecorder */true);
                // For non-burst test, use number of frames to also double check video frame rate.
                // Burst video snapshot is allowed to cause frame rate drop, so do not use number
                // of frames to estimate duration
                if (!burstTest) {
                    durationMs = resultListener.getTotalNumFrames() * 1000.0f /
                        profile.videoFrameRate;
                }

                float frameDurationMs = 1000.0f / profile.videoFrameRate;
                // Validation recorded video
                validateRecording(videoSz, durationMs,
                        frameDurationMs, VID_SNPSHT_FRMDRP_RATE_TOLERANCE);

                if (burstTest) {
                    for (int i = 0; i < BURST_VIDEO_SNAPSHOT_NUM; i++) {
                        Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                        validateVideoSnapshotCapture(image, videoSnapshotSz);
                        image.close();
                    }
                } else {
                    // validate video snapshot image
                    Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                    validateVideoSnapshotCapture(image, videoSnapshotSz);

                    // validate if there is framedrop around video snapshot
                    totalDroppedFrames +=  validateFrameDropAroundVideoSnapshot(
                            resultListener, image.getTimestamp());

                    //TODO: validate jittering. Should move to PTS
                    //validateJittering(resultListener);

                    image.close();
                }
            }

            if (!burstTest) {
                Log.w(TAG, String.format(""Camera %d Video size %s: Number of dropped frames "" +
                        ""detected in %d trials is %d frames."", cameraId, videoSz.toString(),
                        numTestIterations, totalDroppedFrames));
                mCollector.expectLessOrEqual(
                        String.format(
                                ""Camera %d Video size %s: Number of dropped frames %d must not""
                                + "" be larger than %d"",
                                cameraId, videoSz.toString(), totalDroppedFrames,
                                kFrameDrop_Tolerence),
                        kFrameDrop_Tolerence, totalDroppedFrames);
            }
            closeImageReader();
        }
    }

    /**
     * Configure video snapshot request according to the still capture size
     */
    private void prepareVideoSnapshot(
            CaptureRequest.Builder requestBuilder,
            ImageReader.OnImageAvailableListener imageListener)
            throws Exception {
        mReader.setOnImageAvailableListener(imageListener, mHandler);
        assertNotNull(""Recording surface must be non-null!"", mRecordingSurface);
        requestBuilder.addTarget(mRecordingSurface);
        assertNotNull(""Preview surface must be non-null!"", mPreviewSurface);
        requestBuilder.addTarget(mPreviewSurface);
        assertNotNull(""Reader surface must be non-null!"", mReaderSurface);
        requestBuilder.addTarget(mReaderSurface);
    }

    /**
     * Find compatible preview sizes for video size and framerate.
     *
     * <p>Preview size will be capped with max preview size.</p>
     *
     * @param videoSize The video size used for preview.
     * @param videoFrameRate The video frame rate
     */
    private List<Size> getPreviewSizesForVideo(Size videoSize, int videoFrameRate) {
        if (mOrderedPreviewSizes == null) {
            throw new IllegalStateException(""supported preview size list is not initialized yet"");
        }
        final float FRAME_DURATION_TOLERANCE = 0.01f;
        long videoFrameDuration = (long) (1e9 / videoFrameRate *
                (1.0 + FRAME_DURATION_TOLERANCE));
        HashMap<Size, Long> minFrameDurationMap = mStaticInfo.
                getAvailableMinFrameDurationsForFormatChecked(ImageFormat.PRIVATE);
        Size maxPreviewSize = mOrderedPreviewSizes.get(0);
        ArrayList<Size> previewSizes = new ArrayList<>();
        if (videoSize.getWidth() > maxPreviewSize.getWidth() ||
                videoSize.getHeight() > maxPreviewSize.getHeight()) {
            for (Size s : mOrderedPreviewSizes) {
                Long frameDuration = minFrameDurationMap.get(s);
                if (mStaticInfo.isHardwareLevelLegacy()) {
                    // Legacy doesn't report min frame duration
                    frameDuration = new Long(0);
                }
                assertTrue(""Cannot find minimum frame duration for private size"" + s,
                        frameDuration != null);
                if (frameDuration <= videoFrameDuration &&
                        s.getWidth() <= videoSize.getWidth() &&
                        s.getHeight() <= videoSize.getHeight()) {
                    Log.v(TAG, ""Add preview size "" + s.toString() + "" for video size "" +
                            videoSize.toString());
                    previewSizes.add(s);
                }
            }
        }

        if (previewSizes.isEmpty()) {
            previewSizes.add(videoSize);
        }

        return previewSizes;
    }

    /**
     * Update preview size with video size.
     *
     * <p>Preview size will be capped with max preview size.</p>
     *
     * @param videoSize The video size used for preview.
     * @param videoFrameRate The video frame rate
     *
     */
    private void updatePreviewSurfaceWithVideo(Size videoSize, int videoFrameRate) {
        List<Size> previewSizes = getPreviewSizesForVideo(videoSize, videoFrameRate);
        updatePreviewSurface(previewSizes.get(0));
    }

    private void prepareRecordingWithProfile(CamcorderProfile profile) throws Exception {
        prepareRecordingWithProfile(profile, false);
    }

    /**
     * Configure MediaRecorder recording session with CamcorderProfile, prepare
     * the recording surface.
     */
    private void prepareRecordingWithProfile(CamcorderProfile profile,
            boolean useIntermediateSurface) throws Exception {
        // Prepare MediaRecorder.
        setupMediaRecorder(profile);
        prepareRecording(useIntermediateSurface);
    }

    private void setupMediaRecorder(CamcorderProfile profile) throws Exception {
        // Set-up MediaRecorder.
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);
        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.SURFACE);
        mMediaRecorder.setProfile(profile);

        mVideoFrameRate = profile.videoFrameRate;
        mVideoSize = new Size(profile.videoFrameWidth, profile.videoFrameHeight);
    }

    private void setupMediaRecorder(
            EncoderProfiles profiles,
            EncoderProfiles.VideoProfile videoProfile,
            EncoderProfiles.AudioProfile audioProfile) throws Exception {
        // Set-up MediaRecorder.
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);
        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.SURFACE);
        mMediaRecorder.setOutputFormat(profiles.getRecommendedFileFormat());
        mMediaRecorder.setVideoProfile(videoProfile);
        if (audioProfile != null) {
            mMediaRecorder.setAudioProfile(audioProfile);
        }

        mVideoFrameRate = videoProfile.getFrameRate();
        mVideoSize = new Size(videoProfile.getWidth(), videoProfile.getHeight());
    }

    private void prepareRecording(boolean useIntermediateSurface) throws Exception {
        // Continue preparing MediaRecorder
        mMediaRecorder.setOutputFile(mOutMediaFileName);
        if (mPersistentSurface != null) {
            mMediaRecorder.setInputSurface(mPersistentSurface);
            mRecordingSurface = mPersistentSurface;
        }
        mMediaRecorder.prepare();
        if (mPersistentSurface == null) {
            mRecordingSurface = mMediaRecorder.getSurface();
        }
        assertNotNull(""Recording surface must be non-null!"", mRecordingSurface);

        if (useIntermediateSurface) {
            mIntermediateReader = ImageReader.newInstance(
                    mVideoSize.getWidth(), mVideoSize.getHeight(),
                    ImageFormat.PRIVATE, /*maxImages*/3, HardwareBuffer.USAGE_VIDEO_ENCODE);

            mIntermediateSurface = mIntermediateReader.getSurface();
            mIntermediateWriter = ImageWriter.newInstance(mRecordingSurface, /*maxImages*/3,
                    ImageFormat.PRIVATE);
            mQueuer = new ImageWriterQueuer(mIntermediateWriter);

            mIntermediateThread = new HandlerThread(TAG);
            mIntermediateThread.start();
            mIntermediateHandler = new Handler(mIntermediateThread.getLooper());
            mIntermediateReader.setOnImageAvailableListener(mQueuer, mIntermediateHandler);
        }
    }

    /**
     * Configure MediaRecorder recording session with CamcorderProfile, prepare
     * the recording surface. Use AVC for video compression, AAC for audio compression.
     * Both are required for android devices by android CDD.
     */
    private void prepareRecording(Size sz, int videoFrameRate, int captureRate)
            throws Exception {
        // Prepare MediaRecorder.
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);
        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.SURFACE);
        mMediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.THREE_GPP);
        mMediaRecorder.setOutputFile(mOutMediaFileName);
        mMediaRecorder.setVideoEncodingBitRate(getVideoBitRate(sz));
        mMediaRecorder.setVideoFrameRate(videoFrameRate);
        mMediaRecorder.setCaptureRate(captureRate);
        mMediaRecorder.setVideoSize(sz.getWidth(), sz.getHeight());
        mMediaRecorder.setVideoEncoder(MediaRecorder.VideoEncoder.H264);
        mMediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.AAC);
        if (mPersistentSurface != null) {
            mMediaRecorder.setInputSurface(mPersistentSurface);
            mRecordingSurface = mPersistentSurface;
        }
        mMediaRecorder.prepare();
        if (mPersistentSurface == null) {
            mRecordingSurface = mMediaRecorder.getSurface();
        }
        assertNotNull(""Recording surface must be non-null!"", mRecordingSurface);
        mVideoFrameRate = videoFrameRate;
        mVideoSize = sz;
    }

    private void startRecording(boolean useMediaRecorder,
            CameraCaptureSession.CaptureCallback listener, boolean useVideoStab) throws Exception {
        startRecording(useMediaRecorder, listener, useVideoStab, /*variableFpsRange*/null,
                /*useIntermediateSurface*/false);
    }

    private void startRecording(boolean useMediaRecorder,
            CameraCaptureSession.CaptureCallback listener, boolean useVideoStab,
            boolean useIntermediateSurface) throws Exception {
        startRecording(useMediaRecorder, listener, useVideoStab, /*variableFpsRange*/null,
                useIntermediateSurface);
    }

    private void startRecording(boolean useMediaRecorder,
            CameraCaptureSession.CaptureCallback listener, boolean useVideoStab,
            Range<Integer> variableFpsRange, boolean useIntermediateSurface) throws Exception {
        if (!mStaticInfo.isVideoStabilizationSupported() && useVideoStab) {
            throw new IllegalArgumentException(""Video stabilization is not supported"");
        }

        List<Surface> outputSurfaces = new ArrayList<Surface>(2);
        assertTrue(""Both preview and recording surfaces should be valid"",
                mPreviewSurface.isValid() && mRecordingSurface.isValid());
        outputSurfaces.add(mPreviewSurface);
        if (useIntermediateSurface) {
            outputSurfaces.add(mIntermediateSurface);
        } else {
            outputSurfaces.add(mRecordingSurface);
        }

        // Video snapshot surface
        if (mReaderSurface != null) {
            outputSurfaces.add(mReaderSurface);
        }
        mSessionListener = new BlockingSessionCallback();

        CaptureRequest.Builder recordingRequestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
        // Make sure camera output frame rate is set to correct value.
        Range<Integer> fpsRange = (variableFpsRange == null) ?
                Range.create(mVideoFrameRate, mVideoFrameRate) : variableFpsRange;

        recordingRequestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
        if (useVideoStab) {
            recordingRequestBuilder.set(CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE,
                    CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE_ON);
        }
        if (useIntermediateSurface) {
            recordingRequestBuilder.addTarget(mIntermediateSurface);
            if (mQueuer != null) {
                mQueuer.resetInvalidSurfaceFlag();
            }
        } else {
            recordingRequestBuilder.addTarget(mRecordingSurface);
        }
        recordingRequestBuilder.addTarget(mPreviewSurface);
        CaptureRequest recordingRequest = recordingRequestBuilder.build();
        mSession = configureCameraSessionWithParameters(mCamera, outputSurfaces, mSessionListener,
                mHandler, recordingRequest);
        mSession.setRepeatingRequest(recordingRequest, listener, mHandler);

        if (useMediaRecorder) {
            mMediaRecorder.start();
        } else {
            // TODO: need implement MediaCodec path.
        }
        mRecordingStartTime = SystemClock.elapsedRealtime();
    }

    /**
     * Start video recording with preview and video surfaces sharing the same
     * camera stream.
     *
     * @return true if success, false if sharing is not supported.
     */
    private boolean startSharedRecording(boolean useMediaRecorder,
            CameraCaptureSession.CaptureCallback listener, boolean useVideoStab,
            Range<Integer> variableFpsRange) throws Exception {
        if (!mStaticInfo.isVideoStabilizationSupported() && useVideoStab) {
            throw new IllegalArgumentException(""Video stabilization is not supported"");
        }

        List<OutputConfiguration> outputConfigs = new ArrayList<OutputConfiguration>(2);
        assertTrue(""Both preview and recording surfaces should be valid"",
                mPreviewSurface.isValid() && mRecordingSurface.isValid());
        OutputConfiguration sharedConfig = new OutputConfiguration(mPreviewSurface);
        sharedConfig.enableSurfaceSharing();
        sharedConfig.addSurface(mRecordingSurface);
        outputConfigs.add(sharedConfig);

        CaptureRequest.Builder recordingRequestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
        // Make sure camera output frame rate is set to correct value.
        Range<Integer> fpsRange = (variableFpsRange == null) ?
                Range.create(mVideoFrameRate, mVideoFrameRate) : variableFpsRange;
        recordingRequestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
        if (useVideoStab) {
            recordingRequestBuilder.set(CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE,
                    CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE_ON);
        }
        CaptureRequest recordingRequest = recordingRequestBuilder.build();

        mSessionListener = new BlockingSessionCallback();
        mSession = tryConfigureCameraSessionWithConfig(mCamera, outputConfigs, recordingRequest,
                mSessionListener, mHandler);

        if (mSession == null) {
            Log.i(TAG, ""Sharing between preview and video is not supported"");
            return false;
        }

        recordingRequestBuilder.addTarget(mRecordingSurface);
        recordingRequestBuilder.addTarget(mPreviewSurface);
        mSession.setRepeatingRequest(recordingRequestBuilder.build(), listener, mHandler);

        if (useMediaRecorder) {
            mMediaRecorder.start();
        } else {
            // TODO: need implement MediaCodec path.
        }
        mRecordingStartTime = SystemClock.elapsedRealtime();
        return true;
    }


    private void stopCameraStreaming() throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""Stopping camera streaming and waiting for idle"");
        }
        // Stop repeating, wait for captures to complete, and disconnect from
        // surfaces
        mSession.close();
        mSessionListener.getStateWaiter().waitForState(SESSION_CLOSED, SESSION_CLOSE_TIMEOUT_MS);
    }

    private int stopRecording(boolean useMediaRecorder) throws Exception {
        return stopRecording(useMediaRecorder, /*useIntermediateSurface*/false,
                /*stopStreaming*/true);
    }

    // Stop recording and return the estimated video duration in milliseconds.
    private int stopRecording(boolean useMediaRecorder, boolean useIntermediateSurface,
            boolean stopStreaming) throws Exception {
        long stopRecordingTime = SystemClock.elapsedRealtime();
        if (useMediaRecorder) {
            if (stopStreaming) {
                stopCameraStreaming();
            }
            if (useIntermediateSurface) {
                mIntermediateReader.setOnImageAvailableListener(null, null);
                mQueuer.expectInvalidSurface();
            }

            mMediaRecorder.stop();
            // Can reuse the MediaRecorder object after reset.
            mMediaRecorder.reset();
        } else {
            // TODO: need implement MediaCodec path.
        }

        if (useIntermediateSurface) {
            mIntermediateReader.close();
            mQueuer.close();
            mIntermediateWriter.close();
            mIntermediateSurface.release();
            mIntermediateReader = null;
            mIntermediateSurface = null;
            mIntermediateWriter = null;
            mIntermediateThread.quitSafely();
            mIntermediateHandler = null;
        }

        if (mPersistentSurface == null && mRecordingSurface != null) {
            mRecordingSurface.release();
            mRecordingSurface = null;
        }
        return (int) (stopRecordingTime - mRecordingStartTime);
    }

    private void releaseRecorder() {
        if (mMediaRecorder != null) {
            mMediaRecorder.release();
            mMediaRecorder = null;
        }
    }

    private void validateRecording(
            Size sz, float expectedDurationMs, float expectedFrameDurationMs,
            float frameDropTolerance) throws Exception {
        validateRecording(sz,
                expectedDurationMs,  /*fixed FPS recording*/0.f,
                expectedFrameDurationMs, /*fixed FPS recording*/0.f,
                frameDropTolerance);
    }

    private void validateRecording(
            Size sz,
            float expectedDurationMinMs,      // Min duration (maxFps)
            float expectedDurationMaxMs,      // Max duration (minFps). 0.f for fixed fps recording
            float expectedFrameDurationMinMs, // maxFps
            float expectedFrameDurationMaxMs, // minFps. 0.f for fixed fps recording
            float frameDropTolerance) throws Exception {
        File outFile = new File(mOutMediaFileName);
        assertTrue(""No video is recorded"", outFile.exists());
        float maxFrameDuration = expectedFrameDurationMinMs * (1.0f + FRAMEDURATION_MARGIN);
        if (expectedFrameDurationMaxMs > 0.f) {
            maxFrameDuration = expectedFrameDurationMaxMs * (1.0f + FRAMEDURATION_MARGIN);
        }

        if (expectedDurationMaxMs == 0.f) {
            expectedDurationMaxMs = expectedDurationMinMs;
        }

        MediaExtractor extractor = new MediaExtractor();
        try {
            extractor.setDataSource(mOutMediaFileName);
            long durationUs = 0;
            int width = -1, height = -1;
            int numTracks = extractor.getTrackCount();
            int selectedTrack = -1;
            final String VIDEO_MIME_TYPE = ""video"";
            for (int i = 0; i < numTracks; i++) {
                MediaFormat format = extractor.getTrackFormat(i);
                String mime = format.getString(MediaFormat.KEY_MIME);
                if (mime.contains(VIDEO_MIME_TYPE)) {
                    Log.i(TAG, ""video format is: "" + format.toString());
                    durationUs = format.getLong(MediaFormat.KEY_DURATION);
                    width = format.getInteger(MediaFormat.KEY_WIDTH);
                    height = format.getInteger(MediaFormat.KEY_HEIGHT);
                    selectedTrack = i;
                    extractor.selectTrack(i);
                    break;
                }
            }
            if (selectedTrack < 0) {
                throw new AssertionFailedError(
                        ""Cannot find video track!"");
            }

            Size videoSz = new Size(width, height);
            assertTrue(""Video size doesn't match, expected "" + sz.toString() +
                    "" got "" + videoSz.toString(), videoSz.equals(sz));
            float duration = (float) (durationUs / 1000);
            if (VERBOSE) {
                Log.v(TAG, String.format(""Video duration: recorded %fms, expected [%f,%f]ms"",
                                         duration, expectedDurationMinMs, expectedDurationMaxMs));
            }

            // Do rest of validation only for better-than-LEGACY devices
            if (mStaticInfo.isHardwareLevelLegacy()) return;

            // TODO: Don't skip this one for video snapshot on LEGACY
            assertTrue(String.format(
                    ""Camera %s: Video duration doesn't match: recorded %fms, expected [%f,%f]ms."",
                    mCamera.getId(), duration,
                    expectedDurationMinMs * (1.f - DURATION_MARGIN),
                    expectedDurationMaxMs * (1.f + DURATION_MARGIN)),
                    duration > expectedDurationMinMs * (1.f - DURATION_MARGIN) &&
                            duration < expectedDurationMaxMs * (1.f + DURATION_MARGIN));

            // Check for framedrop
            long lastSampleUs = 0;
            int frameDropCount = 0;
            int expectedFrameCount = (int) (expectedDurationMinMs / expectedFrameDurationMinMs);
            ArrayList<Long> timestamps = new ArrayList<Long>(expectedFrameCount);
            while (true) {
                timestamps.add(extractor.getSampleTime());
                if (!extractor.advance()) {
                    break;
                }
            }
            Collections.sort(timestamps);
            long prevSampleUs = timestamps.get(0);
            for (int i = 1; i < timestamps.size(); i++) {
                long currentSampleUs = timestamps.get(i);
                float frameDurationMs = (float) (currentSampleUs - prevSampleUs) / 1000;
                if (frameDurationMs > maxFrameDuration) {
                    Log.w(TAG, String.format(
                        ""Frame drop at %d: expectation %f, observed %f"",
                        i, expectedFrameDurationMinMs, frameDurationMs));
                    frameDropCount++;
                }
                prevSampleUs = currentSampleUs;
            }
            float frameDropRate = 100.f * frameDropCount / timestamps.size();
            Log.i(TAG, String.format(""Frame drop rate %d/%d (%f%%)"",
                frameDropCount, timestamps.size(), frameDropRate));
            assertTrue(String.format(
                    ""Camera %s: Video frame drop rate too high: %f%%, tolerance %f%%. "" +
                    ""Video size: %s, expectedDuration [%f,%f], expectedFrameDuration %f, "" +
                    ""frameDropCnt %d, frameCount %d"",
                    mCamera.getId(), frameDropRate, frameDropTolerance,
                    sz.toString(), expectedDurationMinMs, expectedDurationMaxMs,
                    expectedFrameDurationMinMs, frameDropCount, timestamps.size()),
                    frameDropRate < frameDropTolerance);
        } finally {
            extractor.release();
            if (!DEBUG_DUMP) {
                outFile.delete();
            }
        }
    }

    /**
     * Validate video snapshot capture image object validity and test.
     *
     * <p> Check for size, format and jpeg decoding</p>
     *
     * @param image The JPEG image to be verified.
     * @param size The JPEG capture size to be verified against.
     */
    private void validateVideoSnapshotCapture(Image image, Size size) {
        CameraTestUtils.validateImage(image, size.getWidth(), size.getHeight(),
                ImageFormat.JPEG, /*filePath*/null);
    }

    /**
     * Validate if video snapshot causes frame drop.
     * Here frame drop is defined as frame duration >= 2 * expected frame duration.
     * Return the estimated number of frames dropped during video snapshot
     */
    private int validateFrameDropAroundVideoSnapshot(
            SimpleCaptureCallback resultListener, long imageTimeStamp) {
        double expectedDurationMs = 1000.0 / mVideoFrameRate;
        CaptureResult prevResult = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
        long prevTS = getValueNotNull(prevResult, CaptureResult.SENSOR_TIMESTAMP);
        while (resultListener.hasMoreResults()) {
            CaptureResult currentResult =
                    resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            long currentTS = getValueNotNull(currentResult, CaptureResult.SENSOR_TIMESTAMP);
            if (currentTS == imageTimeStamp) {
                // validate the timestamp before and after, then return
                CaptureResult nextResult =
                        resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
                long nextTS = getValueNotNull(nextResult, CaptureResult.SENSOR_TIMESTAMP);
                double durationMs = (currentTS - prevTS) / 1000000.0;
                int totalFramesDropped = 0;

                // Snapshots in legacy mode pause the preview briefly.  Skip the duration
                // requirements for legacy mode unless this is fixed.
                if (!mStaticInfo.isHardwareLevelLegacy()) {
                    mCollector.expectTrue(
                            String.format(
                                    ""Video %dx%d Frame drop detected before video snapshot: "" +
                                            ""duration %.2fms (expected %.2fms)"",
                                    mVideoSize.getWidth(), mVideoSize.getHeight(),
                                    durationMs, expectedDurationMs
                            ),
                            durationMs <= (expectedDurationMs * MAX_NUM_FRAME_DROP_INTERVAL_ALLOWED)
                    );
                    // Log a warning is there is any frame drop detected.
                    if (durationMs >= expectedDurationMs * 2) {
                        Log.w(TAG, String.format(
                                ""Video %dx%d Frame drop detected before video snapshot: "" +
                                        ""duration %.2fms (expected %.2fms)"",
                                mVideoSize.getWidth(), mVideoSize.getHeight(),
                                durationMs, expectedDurationMs
                        ));
                    }

                    durationMs = (nextTS - currentTS) / 1000000.0;
                    mCollector.expectTrue(
                            String.format(
                                    ""Video %dx%d Frame drop detected after video snapshot: "" +
                                            ""duration %.2fms (expected %.2fms)"",
                                    mVideoSize.getWidth(), mVideoSize.getHeight(),
                                    durationMs, expectedDurationMs
                            ),
                            durationMs <= (expectedDurationMs * MAX_NUM_FRAME_DROP_INTERVAL_ALLOWED)
                    );
                    // Log a warning is there is any frame drop detected.
                    if (durationMs >= expectedDurationMs * 2) {
                        Log.w(TAG, String.format(
                                ""Video %dx%d Frame drop detected after video snapshot: "" +
                                        ""duration %fms (expected %fms)"",
                                mVideoSize.getWidth(), mVideoSize.getHeight(),
                                durationMs, expectedDurationMs
                        ));
                    }

                    double totalDurationMs = (nextTS - prevTS) / 1000000.0;
                    // Minus 2 for the expected 2 frames interval
                    totalFramesDropped = (int) (totalDurationMs / expectedDurationMs) - 2;
                    if (totalFramesDropped < 0) {
                        Log.w(TAG, ""totalFrameDropped is "" + totalFramesDropped +
                                "". Video frame rate might be too fast."");
                    }
                    totalFramesDropped = Math.max(0, totalFramesDropped);
                }
                return totalFramesDropped;
            }
            prevTS = currentTS;
        }
        throw new AssertionFailedError(
                ""Video snapshot timestamp does not match any of capture results!"");
    }

    /**
     * Validate frame jittering from the input simple listener's buffered results
     */
    private void validateJittering(SimpleCaptureCallback resultListener) {
        double expectedDurationMs = 1000.0 / mVideoFrameRate;
        CaptureResult prevResult = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
        long prevTS = getValueNotNull(prevResult, CaptureResult.SENSOR_TIMESTAMP);
        while (resultListener.hasMoreResults()) {
            CaptureResult currentResult =
                    resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            long currentTS = getValueNotNull(currentResult, CaptureResult.SENSOR_TIMESTAMP);
            double durationMs = (currentTS - prevTS) / 1000000.0;
            double durationError = Math.abs(durationMs - expectedDurationMs);
            long frameNumber = currentResult.getFrameNumber();
            mCollector.expectTrue(
                    String.format(
                            ""Resolution %dx%d Frame %d: jittering (%.2fms) exceeds bound [%.2fms,%.2fms]"",
                            mVideoSize.getWidth(), mVideoSize.getHeight(),
                            frameNumber, durationMs,
                            expectedDurationMs - FRAME_DURATION_ERROR_TOLERANCE_MS,
                            expectedDurationMs + FRAME_DURATION_ERROR_TOLERANCE_MS),
                    durationError <= FRAME_DURATION_ERROR_TOLERANCE_MS);
            prevTS = currentTS;
        }
    }

    /**
     * Calculate a video bit rate based on the size. The bit rate is scaled
     * based on ratio of video size to 1080p size.
     */
    private int getVideoBitRate(Size sz) {
        int rate = BIT_RATE_1080P;
        float scaleFactor = sz.getHeight() * sz.getWidth() / (float)(1920 * 1080);
        rate = (int)(rate * scaleFactor);

        // Clamp to the MIN, MAX range.
        return Math.max(BIT_RATE_MIN, Math.min(BIT_RATE_MAX, rate));
    }

    /**
     * Check if the encoder and camera are able to support this size and frame rate.
     * Assume the video compression format is AVC.
     */
    private boolean isSupported(Size sz, int captureRate, int encodingRate) throws Exception {
        // Check camera capability.
        if (!isSupportedByCamera(sz, captureRate)) {
            return false;
        }

        // Check encode capability.
        if (!isSupportedByAVCEncoder(sz, encodingRate)){
            return false;
        }

        if(VERBOSE) {
            Log.v(TAG, ""Both encoder and camera support "" + sz.toString() + ""@"" + encodingRate + ""@""
                    + getVideoBitRate(sz) / 1000 + ""Kbps"");
        }

        return true;
    }

    private boolean isSupportedByCamera(Size sz, int frameRate) {
        // Check if camera can support this sz and frame rate combination.
        StreamConfigurationMap config = mStaticInfo.
                getValueFromKeyNonNull(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);

        long minDuration = config.getOutputMinFrameDuration(MediaRecorder.class, sz);
        if (minDuration == 0) {
            return false;
        }

        int maxFrameRate = (int) (1e9f / minDuration);
        return maxFrameRate >= frameRate;
    }

    /**
     * Check if encoder can support this size and frame rate combination by querying
     * MediaCodec capability. Check is based on size and frame rate. Ignore the bit rate
     * as the bit rates targeted in this test are well below the bit rate max value specified
     * by AVC specification for certain level.
     */
    private static boolean isSupportedByAVCEncoder(Size sz, int frameRate) {
        MediaFormat format = MediaFormat.createVideoFormat(
                MediaFormat.MIMETYPE_VIDEO_AVC, sz.getWidth(), sz.getHeight());
        format.setInteger(MediaFormat.KEY_FRAME_RATE, frameRate);
        MediaCodecList mcl = new MediaCodecList(MediaCodecList.REGULAR_CODECS);
        return mcl.findEncoderForFormat(format) != null;
    }

    private static class ImageWriterQueuer implements ImageReader.OnImageAvailableListener {
        public ImageWriterQueuer(ImageWriter writer) {
            mWriter = writer;
        }

        public void resetInvalidSurfaceFlag() {
            synchronized (mLock) {
                mExpectInvalidSurface = false;
            }
        }

        // Indicate that the writer surface is about to get released
        // and become invalid.
        public void expectInvalidSurface() {
            // If we sync on 'mLock', we risk a possible deadlock
            // during 'mWriter.queueInputImage(image)' which is
            // called while the lock is held.
            mExpectInvalidSurface = true;
        }

        @Override
        public void onImageAvailable(ImageReader reader) {
            Image image = null;
            try {
                image = reader.acquireNextImage();
            } finally {
                synchronized (mLock) {
                    if (image != null && mWriter != null) {
                        try {
                            mWriter.queueInputImage(image);
                            mQueuedCount++;
                        } catch (IllegalStateException e) {
                            // Per API documentation ISE are possible
                            // in case the writer surface is not valid.
                            // Re-throw in case we have some other
                            // unexpected ISE.
                            if (mExpectInvalidSurface) {
                                Log.d(TAG, ""Invalid writer surface"");
                                image.close();
                            } else {
                                throw e;
                            }
                        }
                    } else if (image != null) {
                        image.close();
                    }
                }
            }
        }

        public int getQueuedCount() {
            synchronized (mLock) {
                return mQueuedCount;
            }
        }

        public void close() {
            synchronized (mLock) {
                mWriter = null;
            }
        }

        private Object      mLock = new Object();
        private ImageWriter mWriter = null;
        private int         mQueuedCount = 0;
        private boolean     mExpectInvalidSurface = false;
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.VideoDecoderPerfTest"	"currentTimeMillis"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/VideoDecoderPerfTest.java"	""	"public void test/*
 *.
 */

package android.media.cts;

import android.content.Context;
import android.media.MediaCodec;
import android.media.MediaCodecInfo.VideoCapabilities;
import android.media.MediaExtractor;
import android.media.MediaFormat;
import android.os.Build;
import android.os.Bundle;
import android.platform.test.annotations.AppModeFull;
import android.util.Log;
import android.util.Pair;
import android.text.TextUtils;
import android.view.Surface;
import androidx.test.platform.app.InstrumentationRegistry;

import com.android.compatibility.common.util.DeviceReportLog;
import com.android.compatibility.common.util.MediaPerfUtils;
import com.android.compatibility.common.util.MediaUtils;
import com.android.compatibility.common.util.ResultType;
import com.android.compatibility.common.util.ResultUnit;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.Arrays;
import java.util.LinkedList;
import java.util.Scanner;

@MediaHeavyPresubmitTest
@AppModeFull(reason = ""TODO: evaluate and port to instant"")
public class VideoDecoderPerfTest extends MediaPlayerTestBase {
    private static final String TAG = ""VideoDecoderPerfTest"";
    private static final String REPORT_LOG_NAME = ""CtsMediaTestCases"";
    private static final int TOTAL_FRAMES = 30000;
    private static final int MIN_FRAMES = 3000;
    private static final int MAX_TIME_MS = 120000;  // 2 minutes
    private static final int MAX_TEST_TIMEOUT_MS = 300000;  // 5 minutes
    private static final int MIN_TEST_MS = 10000;  // 10 seconds
    private static final int NUMBER_OF_REPEATS = 2;

    private static final String AVC = MediaFormat.MIMETYPE_VIDEO_AVC;
    private static final String H263 = MediaFormat.MIMETYPE_VIDEO_H263;
    private static final String HEVC = MediaFormat.MIMETYPE_VIDEO_HEVC;
    private static final String MPEG2 = MediaFormat.MIMETYPE_VIDEO_MPEG2;
    private static final String MPEG4 = MediaFormat.MIMETYPE_VIDEO_MPEG4;
    private static final String VP8 = MediaFormat.MIMETYPE_VIDEO_VP8;
    private static final String VP9 = MediaFormat.MIMETYPE_VIDEO_VP9;

    private static final boolean GOOG = true;
    private static final boolean OTHER = false;

    private static final int MAX_SIZE_SAMPLES_IN_MEMORY_BYTES = 12 << 20;  // 12MB
    // each sample contains the buffer and the PTS offset from the frame index
    LinkedList<Pair<ByteBuffer, Double>> mSamplesInMemory = new LinkedList<Pair<ByteBuffer, Double>>();
    private MediaFormat mDecInputFormat;
    private MediaFormat mDecOutputFormat;
    private int mBitrate;

    private boolean mSkipRateChecking = false;
    static final String mInpPrefix = WorkDir.getMediaDirString();

    @Override
    protected void setUp() throws Exception {
        super.setUp();
        Bundle bundle = InstrumentationRegistry.getArguments();
        mSkipRateChecking = TextUtils.equals(""true"", bundle.getString(""mts-media""));
    }

    @Override
    protected void tearDown() throws Exception {
        super.tearDown();
    }

    private void decode(String name, final String resource, MediaFormat format) throws Exception {
        int width = format.getInteger(MediaFormat.KEY_WIDTH);
        int height = format.getInteger(MediaFormat.KEY_HEIGHT);
        String mime = format.getString(MediaFormat.KEY_MIME);

        // Ensure we can finish this test within the test timeout. Allow 25% slack (4/5).
        long maxTimeMs = Math.min(
                MAX_TEST_TIMEOUT_MS * 4 / 5 / NUMBER_OF_REPEATS, MAX_TIME_MS);
        // reduce test run on non-real device
        if (MediaUtils.onFrankenDevice()) {
            maxTimeMs /= 10;
        }
        double measuredFps[] = new double[NUMBER_OF_REPEATS];

        for (int i = 0; i < NUMBER_OF_REPEATS; ++i) {
            // Decode to Surface.
            Log.d(TAG, ""round #"" + i + "": "" + name + "" for "" + maxTimeMs + "" msecs to surface"");
            Surface s = getActivity().getSurfaceHolder().getSurface();
            // only verify the result for decode to surface case.
            measuredFps[i] = doDecode(name, resource, width, height, s, i, maxTimeMs);

            // We don't test decoding to buffer.
            // Log.d(TAG, ""round #"" + i + "" decode to buffer"");
            // doDecode(name, video, width, height, null, i, maxTimeMs);
        }

        String error =
            MediaPerfUtils.verifyAchievableFrameRates(name, mime, width, height, measuredFps);
        // Performance numbers only make sense on real devices, so skip on non-real devices
        if ((MediaUtils.onFrankenDevice() || mSkipRateChecking) && error != null) {
            // ensure there is data, but don't insist that it is correct
            assertFalse(error, error.startsWith(""Failed to get ""));
        } else {
            assertNull(error, error);
        }
        mSamplesInMemory.clear();
    }

    private double doDecode(String name, final String filename, int w, int h, Surface surface,
            int round, long maxTimeMs) throws Exception {
        final String video = mInpPrefix + filename;
        Preconditions.assertTestFileExists(video);
        MediaExtractor extractor = new MediaExtractor();
        extractor.setDataSource(video);
        extractor.selectTrack(0);
        int trackIndex = extractor.getSampleTrackIndex();
        MediaFormat format = extractor.getTrackFormat(trackIndex);
        String mime = format.getString(MediaFormat.KEY_MIME);

        // use frame rate to calculate PTS offset used for PTS scaling
        double frameRate = 0.; // default - 0 is used for using zero PTS offset
        if (format.containsKey(MediaFormat.KEY_FRAME_RATE)) {
            frameRate = format.getInteger(MediaFormat.KEY_FRAME_RATE);
        } else if (!mime.equals(MediaFormat.MIMETYPE_VIDEO_VP8)
                && !mime.equals(MediaFormat.MIMETYPE_VIDEO_VP9)) {
            fail(""need framerate info for video file"");
        }

        ByteBuffer[] codecInputBuffers;
        ByteBuffer[] codecOutputBuffers;

        if (mSamplesInMemory.size() == 0) {
            int totalMemory = 0;
            ByteBuffer tmpBuf = ByteBuffer.allocate(w * h * 3 / 2);
            int sampleSize = 0;
            int index = 0;
            long firstPTS = 0;
            double presentationOffset = 0.;
            while ((sampleSize = extractor.readSampleData(tmpBuf, 0 /* offset */)) > 0) {
                if (totalMemory + sampleSize > MAX_SIZE_SAMPLES_IN_MEMORY_BYTES) {
                    break;
                }
                if (mSamplesInMemory.size() == 0) {
                    firstPTS = extractor.getSampleTime();
                }
                ByteBuffer copied = ByteBuffer.allocate(sampleSize);
                copied.put(tmpBuf);
                if (frameRate > 0.) {
                    // presentation offset is an offset from the frame index
                    presentationOffset =
                        (extractor.getSampleTime() - firstPTS) * frameRate / 1e6 - index;
                }
                mSamplesInMemory.addLast(Pair.create(copied, presentationOffset));
                totalMemory += sampleSize;
                ++index;
                extractor.advance();
            }
            Log.d(TAG, mSamplesInMemory.size() + "" samples in memory for "" +
                    (totalMemory / 1024) + "" KB."");
            // bitrate normalized to 30fps
            mBitrate = (int)Math.round(totalMemory * 30. * 8. / mSamplesInMemory.size());
        }
        format.setInteger(MediaFormat.KEY_BIT_RATE, mBitrate);

        int sampleIndex = 0;

        extractor.release();

        MediaCodec codec = MediaCodec.createByCodecName(name);
        VideoCapabilities cap = codec.getCodecInfo().getCapabilitiesForType(mime).getVideoCapabilities();
        frameRate = cap.getSupportedFrameRatesFor(w, h).getUpper();
        codec.configure(format, surface, null /* crypto */, 0 /* flags */);
        codec.start();
        codecInputBuffers = codec.getInputBuffers();
        codecOutputBuffers = codec.getOutputBuffers();
        mDecInputFormat = codec.getInputFormat();

        // start decode loop
        MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();

        final long kTimeOutUs = 1000; // 1ms timeout
        double[] frameTimeUsDiff = new double[TOTAL_FRAMES - 1];
        long lastOutputTimeUs = 0;
        boolean sawInputEOS = false;
        boolean sawOutputEOS = false;
        int inputNum = 0;
        int outputNum = 0;
        long start = System.currentTimeMillis();
        while (!sawOutputEOS) {
            // handle input
            if (!sawInputEOS) {
                int inputBufIndex = codec.dequeueInputBuffer(kTimeOutUs);

                if (inputBufIndex >= 0) {
                    ByteBuffer dstBuf = codecInputBuffers[inputBufIndex];
                    // sample contains the buffer and the PTS offset normalized to frame index
                    Pair<ByteBuffer, Double> sample =
                            mSamplesInMemory.get(sampleIndex++ % mSamplesInMemory.size());
                    sample.first.rewind();
                    int sampleSize = sample.first.remaining();
                    dstBuf.put(sample.first);
                    // use max supported framerate to compute pts
                    long presentationTimeUs = (long)((inputNum + sample.second) * 1e6 / frameRate);

                    long elapsed = System.currentTimeMillis() - start;
                    sawInputEOS = ((++inputNum == TOTAL_FRAMES)
                                   || (elapsed > maxTimeMs)
                                   || (elapsed > MIN_TEST_MS && outputNum > MIN_FRAMES));
                    if (sawInputEOS) {
                        Log.d(TAG, ""saw input EOS (stop at sample)."");
                    }
                    codec.queueInputBuffer(
                            inputBufIndex,
                            0 /* offset */,
                            sampleSize,
                            presentationTimeUs,
                            sawInputEOS ? MediaCodec.BUFFER_FLAG_END_OF_STREAM : 0);
                } else {
                    assertEquals(
                            ""codec.dequeueInputBuffer() unrecognized return value: "" + inputBufIndex,
                            MediaCodec.INFO_TRY_AGAIN_LATER, inputBufIndex);
                }
            }

            // handle output
            int outputBufIndex = codec.dequeueOutputBuffer(info, kTimeOutUs);

            if (outputBufIndex >= 0) {
                if (info.size > 0) { // Disregard 0-sized buffers at the end.
                    long nowUs = (System.nanoTime() + 500) / 1000;
                    if (outputNum > 1) {
                        frameTimeUsDiff[outputNum - 1] = nowUs - lastOutputTimeUs;
                    }
                    lastOutputTimeUs = nowUs;
                    outputNum++;
                }
                codec.releaseOutputBuffer(outputBufIndex, false /* render */);
                if ((info.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                    Log.d(TAG, ""saw output EOS."");
                    sawOutputEOS = true;
                }
            } else if (outputBufIndex == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                codecOutputBuffers = codec.getOutputBuffers();
                Log.d(TAG, ""output buffers have changed."");
            } else if (outputBufIndex == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                mDecOutputFormat = codec.getOutputFormat();
                int width = mDecOutputFormat.getInteger(MediaFormat.KEY_WIDTH);
                int height = mDecOutputFormat.getInteger(MediaFormat.KEY_HEIGHT);
                Log.d(TAG, ""output resolution "" + width + ""x"" + height);
            } else {
                assertEquals(
                        ""codec.dequeueOutputBuffer() unrecognized return index: ""
                                + outputBufIndex,
                        MediaCodec.INFO_TRY_AGAIN_LATER, outputBufIndex);
            }
        }
        long finish = System.currentTimeMillis();
        int validDataNum = outputNum - 1;
        frameTimeUsDiff = Arrays.copyOf(frameTimeUsDiff, validDataNum);
        codec.stop();
        codec.release();

        Log.d(TAG, ""input num "" + inputNum + "" vs output num "" + outputNum);

        DeviceReportLog log = new DeviceReportLog(REPORT_LOG_NAME, ""video_decoder_performance"");
        String message = MediaPerfUtils.addPerformanceHeadersToLog(
                log, ""decoder stats: decodeTo="" + ((surface == null) ? ""buffer"" : ""surface""),
                round, name, format, mDecInputFormat, mDecOutputFormat);
        log.addValue(""video_res"", video, ResultType.NEUTRAL, ResultUnit.NONE);
        log.addValue(""decode_to"", surface == null ? ""buffer"" : ""surface"",
                ResultType.NEUTRAL, ResultUnit.NONE);

        double fps = outputNum / ((finish - start) / 1000.0);
        log.addValue(""average_fps"", fps, ResultType.HIGHER_BETTER, ResultUnit.FPS);

        MediaUtils.Stats stats = new MediaUtils.Stats(frameTimeUsDiff);
        fps = MediaPerfUtils.addPerformanceStatsToLog(log, stats, message);
        log.submit(getInstrumentation());
        return fps;
    }

    private MediaFormat[] getVideoTrackFormats(String... resources) throws Exception {
        MediaFormat[] formats = new MediaFormat[resources.length];
        for (int i = 0; i < resources.length; ++i) {
            Preconditions.assertTestFileExists(mInpPrefix + resources[i]);
            formats[i] = MediaUtils.getTrackFormatForResource(mInpPrefix + resources[i], ""video/"");
        }
        return formats;
    }

    private void count(final String[] resources, int numGoog, int numOther) throws Exception {
        MediaFormat[] formats = getVideoTrackFormats(resources);
        MediaUtils.verifyNumCodecs(numGoog,  false /* isEncoder */, true /* isGoog */,  formats);
        MediaUtils.verifyNumCodecs(numOther, false /* isEncoder */, false /* isGoog */, formats);
    }

    private void perf(final String[] resources, boolean isGoog, int ix)  throws Exception {
        MediaFormat[] formats = getVideoTrackFormats(resources);
        String[] decoders = MediaUtils.getDecoderNames(isGoog, formats);
        String kind = isGoog ? ""Google"" : ""non-Google"";
        if (decoders.length == 0) {
            MediaUtils.skipTest(""No "" + kind + "" decoders for "" + Arrays.toString(formats));
            return;
        } else if (ix >= decoders.length) {
            Log.i(TAG, ""No more "" + kind + "" decoders for "" + Arrays.toString(formats));
            return;
        }

        String decoderName = decoders[ix];

        // Decode/measure the first supported video resource
        for (int i = 0; i < resources.length; ++i) {
            if (MediaUtils.supports(decoderName, formats[i])) {
                decode(decoderName, resources[i], formats[i]);
                break;
            }
        }
    }

    // Poor man's Parametrized test as this test must still run on CTSv1 runner.

    // The count tests are to ensure this Cts test covers all decoders. Add further
    // tests and change the count if there can be more decoders.

    // AVC tests

    private static final String[] sAvcMedia0320x0240 = {
        ""bbb_s1_320x240_mp4_h264_mp2_800kbps_30fps_aac_lc_5ch_240kbps_44100hz.mp4"",
    };"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.VideoDecoderPerfTest"	"testAvcOther3Perf0320x0240"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/VideoDecoderPerfTest.java"	""	"public void testAvcOther3Perf0320x0240() throws Exception { perf(sAvcMedia0320x0240, OTHER, 3); }

    private static final String[] sAvcMedia0720x0480 = {
        ""bbb_s1_720x480_mp4_h264_mp3_2mbps_30fps_aac_lc_5ch_320kbps_48000hz.mp4"",
    };"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.VideoDecoderPerfTest"	"testAvcOther3Perf0720x0480"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/VideoDecoderPerfTest.java"	""	"public void testAvcOther3Perf0720x0480() throws Exception { perf(sAvcMedia0720x0480, OTHER, 3); }

    // prefer highest effective bitrate, then high profile
    private static final String[] sAvcMedia1280x0720 = {
        ""bbb_s4_1280x720_mp4_h264_mp31_8mbps_30fps_aac_he_mono_40kbps_44100hz.mp4"",
        ""bbb_s3_1280x720_mp4_h264_hp32_8mbps_60fps_aac_he_v2_stereo_48kbps_48000hz.mp4"",
        ""bbb_s3_1280x720_mp4_h264_mp32_8mbps_60fps_aac_he_v2_6ch_144kbps_44100hz.mp4"",
    };"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.VideoDecoderPerfTest"	"testAvcOther3Perf1280x0720"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/VideoDecoderPerfTest.java"	""	"public void testAvcOther3Perf1280x0720() throws Exception { perf(sAvcMedia1280x0720, OTHER, 3); }

    // prefer highest effective bitrate, then high profile
    private static final String[] sAvcMedia1920x1080 = {
        ""bbb_s4_1920x1080_wide_mp4_h264_hp4_20mbps_30fps_aac_lc_6ch_384kbps_44100hz.mp4"",
        ""bbb_s4_1920x1080_wide_mp4_h264_mp4_20mbps_30fps_aac_he_5ch_200kbps_44100hz.mp4"",
        ""bbb_s2_1920x1080_mp4_h264_hp42_20mbps_60fps_aac_lc_6ch_384kbps_48000hz.mp4"",
        ""bbb_s2_1920x1080_mp4_h264_mp42_20mbps_60fps_aac_he_v2_5ch_160kbps_48000hz.mp4"",
    };"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.VideoDecoderPerfTest"	"testAvcOther3Perf1920x1080"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/VideoDecoderPerfTest.java"	""	"public void testAvcOther3Perf1920x1080() throws Exception { perf(sAvcMedia1920x1080, OTHER, 3); }

    // H263 tests

    private static final String[] sH263Media0176x0144 = {
        ""video_176x144_3gp_h263_300kbps_12fps_aac_stereo_128kbps_22050hz.3gp"",
    };"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.VideoDecoderPerfTest"	"testH263Other1Perf0176x0144"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/VideoDecoderPerfTest.java"	""	"public void testH263Other1Perf0176x0144() throws Exception { perf(sH263Media0176x0144, OTHER, 1); }

    private static final String[] sH263Media0352x0288 = {
        ""video_352x288_3gp_h263_300kbps_12fps_aac_stereo_128kbps_22050hz.3gp"",
    };"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.VideoDecoderPerfTest"	"testH263Other1Perf0352x0288"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/VideoDecoderPerfTest.java"	""	"public void testH263Other1Perf0352x0288() throws Exception { perf(sH263Media0352x0288, OTHER, 1); }

    // No media for H263 704x576

    // No media for H263 1408x1152

    // HEVC tests

    private static final String[] sHevcMedia0352x0288 = {
        ""bbb_s1_352x288_mp4_hevc_mp2_600kbps_30fps_aac_he_stereo_96kbps_48000hz.mp4"",
    };"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.VideoDecoderPerfTest"	"testHevcOther3Perf0352x0288"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/VideoDecoderPerfTest.java"	""	"public void testHevcOther3Perf0352x0288() throws Exception { perf(sHevcMedia0352x0288, OTHER, 3); }

    private static final String[] sHevcMedia0640x0360 = {
        ""bbb_s1_640x360_mp4_hevc_mp21_1600kbps_30fps_aac_he_6ch_288kbps_44100hz.mp4"",
    };"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.VideoDecoderPerfTest"	"testHevcOther3Perf0640x0360"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/VideoDecoderPerfTest.java"	""	"public void testHevcOther3Perf0640x0360() throws Exception { perf(sHevcMedia0640x0360, OTHER, 3); }

    private static final String[] sHevcMedia0720x0480 = {
        ""bbb_s1_720x480_mp4_hevc_mp3_1600kbps_30fps_aac_he_6ch_240kbps_48000hz.mp4"",
    };"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.VideoDecoderPerfTest"	"testHevcOther3Perf0720x0480"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/VideoDecoderPerfTest.java"	""	"public void testHevcOther3Perf0720x0480() throws Exception { perf(sHevcMedia0720x0480, OTHER, 3); }

    private static final String[] sHevcMedia1280x0720 = {
        ""bbb_s4_1280x720_mp4_hevc_mp31_4mbps_30fps_aac_he_stereo_80kbps_32000hz.mp4"",
    };"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.VideoDecoderPerfTest"	"testHevcOther3Perf1280x0720"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/VideoDecoderPerfTest.java"	""	"public void testHevcOther3Perf1280x0720() throws Exception { perf(sHevcMedia1280x0720, OTHER, 3); }

    private static final String[] sHevcMedia1920x1080 = {
        ""bbb_s2_1920x1080_mp4_hevc_mp41_10mbps_60fps_aac_lc_6ch_384kbps_22050hz.mp4"",
    };"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.VideoDecoderPerfTest"	"testHevcOther3Perf1920x1080"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/VideoDecoderPerfTest.java"	""	"public void testHevcOther3Perf1920x1080() throws Exception { perf(sHevcMedia1920x1080, OTHER, 3); }

    // prefer highest effective bitrate
    private static final String[] sHevcMedia3840x2160 = {
        ""bbb_s4_3840x2160_mp4_hevc_mp5_20mbps_30fps_aac_lc_6ch_384kbps_24000hz.mp4"",
        ""bbb_s2_3840x2160_mp4_hevc_mp51_20mbps_60fps_aac_lc_6ch_384kbps_32000hz.mp4"",
    };"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.VideoDecoderPerfTest"	"testHevcOther3Perf3840x2160"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/VideoDecoderPerfTest.java"	""	"public void testHevcOther3Perf3840x2160() throws Exception { perf(sHevcMedia3840x2160, OTHER, 3); }

    // MPEG2 tests

    // No media for MPEG2 176x144

    // No media for MPEG2 352x288

    // No media for MPEG2 640x480

    // No media for MPEG2 1280x720

    // No media for MPEG2 1920x1080

    // MPEG4 tests

    private static final String[] sMpeg4Media0176x0144 = {
        ""video_176x144_mp4_mpeg4_300kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
    };"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.VideoDecoderPerfTest"	"testMpeg4Other3Perf0176x0144"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/VideoDecoderPerfTest.java"	""	"public void testMpeg4Other3Perf0176x0144() throws Exception { perf(sMpeg4Media0176x0144, OTHER, 3); }

    private static final String[] sMpeg4Media0480x0360 = {
        ""video_480x360_mp4_mpeg4_860kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
    };"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.VideoDecoderPerfTest"	"testMpeg4Other3Perf0480x0360"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/VideoDecoderPerfTest.java"	""	"public void testMpeg4Other3Perf0480x0360() throws Exception { perf(sMpeg4Media0480x0360, OTHER, 3); }

   // No media for MPEG4 640x480

    private static final String[] sMpeg4Media1280x0720 = {
        ""video_1280x720_mp4_mpeg4_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
    };"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.EncoderProfileLevelTest"	"testValidateProfileLevel"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/EncoderProfileLevelTest.java"	""	"(timeout = PER_TEST_TIMEOUT_LARGE_TEST_MS)
    public void testValidateProfileLevel() throws IOException, InterruptedException {
        int[] profiles = mProfileMap.get(mMime);
        assertTrue(""no profile entry found for mime"" + mMime, profiles != null);
        // cdd check initialization
        boolean cddSupportedMime = mProfileLevelCdd.get(mMime) != null;
        int[] profileCdd = new int[0];
        int levelCdd = 0;
        if (cddSupportedMime) {
            Pair<int[], Integer> cddProfileLevel = mProfileLevelCdd.get(mMime);
            profileCdd = cddProfileLevel.first;
            levelCdd = cddProfileLevel.second;
        }
        MediaFormat format = mConfigFormat;
        mOutputBuff = new OutputManager();
        setUpSource(mInputFile);
        mSaveToMem = true;
        String tempMuxedFile = File.createTempFile(""tmp"", "".out"").getAbsolutePath();
        {
            mCodec = MediaCodec.createByCodecName(mCodecName);
            MediaCodecInfo.CodecCapabilities codecCapabilities =
                    mCodec.getCodecInfo().getCapabilitiesForType(mMime);
            for (int profile : profiles) {
                format.setInteger(MediaFormat.KEY_PROFILE, profile);
                // for aac encoder, alongwith setting profile, also set aac-profile as some
                // encoders may only support one of the two keys
                if (mMime.equals(MediaFormat.MIMETYPE_AUDIO_AAC)) {
                    format.setInteger(MediaFormat.KEY_AAC_PROFILE, profile);
                }
                int level = mIsAudio ? 0 : getMinLevel(mMime, mWidth, mHeight,
                        format.getInteger(MediaFormat.KEY_FRAME_RATE),
                        format.getInteger(MediaFormat.KEY_BIT_RATE), profile);
                assertTrue(""no min level found for mime"" + mMime, level != -1);
                if (!mIsAudio) format.setInteger(MediaFormat.KEY_LEVEL, level);
                if (!codecCapabilities.isFormatSupported(format)) {
                    if (cddSupportedMime) {
                        boolean shallSupportProfileLevel = false;
                        if (mIsAudio) {
                            for (int cddProfile : profileCdd) {
                                if (profile == cddProfile) {
                                    shallSupportProfileLevel = true;
                                }
                            }
                        } else if (profile == profileCdd[0] && level == levelCdd) {
                            shallSupportProfileLevel = true;
                        }

                        // TODO (b/193173880) Check if there is at least one component that
                        // supports this profile and level combination
                        if (shallSupportProfileLevel) {
                            ArrayList<MediaFormat> formats = new ArrayList<>();
                            formats.add(format);
                            assertFalse(
                                    ""No components support cdd requirement profile level with \n ""
                                            + ""format :"" + format + "" for mime: "" + mMime,
                                    selectCodecs(mMime, formats, null, false).isEmpty());
                        }
                        Log.w(LOG_TAG,
                                ""Component: "" + mCodecName + "" doesn't support cdd format: "" +
                                        format);
                    }
                    continue;
                }
                mOutputBuff.reset();
                mInfoList.clear();
                configureCodec(format, false, true, true);
                mCodec.start();
                doWork(1);
                queueEOS();
                waitForAllOutputs();
                MediaFormat outFormat = mCodec.getOutputFormat();
                /* TODO(b/147348711) */
                if (false) mCodec.stop();
                else mCodec.reset();
                String log =
                        String.format(""format: %s \n codec: %s, mode: %s:: "", format, mCodecName,
                                ""sync"");
                assertFalse(log + "" unexpected error"", mAsyncHandle.hasSeenError());
                assertTrue(log + ""configured format and output format are not similar."" +
                                (ENABLE_LOGS ? ""\n output format:"" + outFormat : """"),
                        isFormatSimilar(format, outFormat));

                // TODO (b/151398466)
                if (mMime.equals(MediaFormat.MIMETYPE_AUDIO_AAC)) {
                    Assume.assumeTrue(""neither KEY_AAC_PROFILE nor KEY_PROFILE are present"",
                            outFormat.containsKey(MediaFormat.KEY_AAC_PROFILE) ||
                                    outFormat.containsKey(MediaFormat.KEY_PROFILE));
                } else {
                    Assume.assumeTrue(""KEY_PROFILE not present"",
                            outFormat.containsKey(MediaFormat.KEY_PROFILE));
                }
                Assume.assumeTrue(outFormat.containsKey(MediaFormat.KEY_LEVEL));
                // TODO (b/166300446) avc mime fails validation
                if (mMime.equals(MediaFormat.MIMETYPE_VIDEO_AVC)) {
                    Log.w(LOG_TAG, ""Skip validation after muxing for mime = "" + mMime);
                    continue;
                }
                // TODO (b/166305723) hevc mime fails validation
                if (mMime.equals(MediaFormat.MIMETYPE_VIDEO_HEVC)) {
                    Log.w(LOG_TAG, ""Skip validation after muxing for mime = "" + mMime);
                    continue;
                }
                // TODO (b/166300448) h263 and mpeg4 mimes fails validation
                if (mMime.equals(MediaFormat.MIMETYPE_VIDEO_H263) ||
                            mMime.equals(MediaFormat.MIMETYPE_VIDEO_MPEG4)) {
                    Log.w(LOG_TAG, ""Skip validation after muxing for mime = "" + mMime);
                    continue;
                }
                // TODO (b/184889671) aac for profile AACObjectHE fails validation
                // TODO (b/184890155) aac for profile AACObjectLD, AACObjectELD fails validation
                if (mMime.equals(MediaFormat.MIMETYPE_AUDIO_AAC) &&
                            profile != AACObjectLC) {
                    Log.w(LOG_TAG, ""Skip validation after muxing for mime = "" + mMime +
                            "" profile "" + profile);
                    continue;
                }

                for (int muxerFormat = MediaMuxer.OutputFormat.MUXER_OUTPUT_FIRST;
                     muxerFormat <= MediaMuxer.OutputFormat.MUXER_OUTPUT_LAST; muxerFormat++) {
                    if (!MuxerTest.isCodecContainerPairValid(mMime, muxerFormat)) continue;
                    ByteBuffer mBuff = mOutputBuff.getBuffer();
                    mMuxer = new MediaMuxer(tempMuxedFile, muxerFormat);
                    try {
                        mMuxer.addTrack(outFormat);
                        mMuxer.start();
                        for (int i = 0; i < mInfoList.size(); i++) {
                            mMuxer.writeSampleData(0, mBuff, mInfoList.get(i));
                        }
                        mMuxer.stop();
                    } catch (Exception e) {
                        fail(log + ""error! failed write to muxer format "" + muxerFormat);
                    } finally {
                        mMuxer.release();
                        mMuxer = null;
                    }
                    MediaExtractor extractor = new MediaExtractor();
                    extractor.setDataSource(tempMuxedFile);
                    assertEquals(""Should be only 1 track "", 1, extractor.getTrackCount());
                    MediaFormat extractedFormat = extractor.getTrackFormat(0);
                    assertTrue(log + ""\nmuxer input config = "" + outFormat +
                                       ""\ninput format and extracted format are not similar."" +
                                       ""\nextracted format:"" + extractedFormat +
                                       ""\ncontainer format = "" + muxerFormat,
                            isFormatSimilar(format, extractedFormat));
                    extractor.release();
                }
            }
            mCodec.release();
        }
        new File(tempMuxedFile).delete();
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.FlashlightTest"	"testTorchModeExceptions"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/FlashlightTest.java"	""	"public void testTorchModeExceptions() throws Exception {
        // cameraIdsToTestTorch = all available camera ID + non-existing camera id +
        //                        non-existing numeric camera id + null
        String[] cameraIdsToTestTorch = new String[mCameraIdsUnderTest.length + 3];
        System.arraycopy(mCameraIdsUnderTest, 0, cameraIdsToTestTorch, 0, mCameraIdsUnderTest.length);
        cameraIdsToTestTorch[mCameraIdsUnderTest.length] = generateNonexistingCameraId();
        cameraIdsToTestTorch[mCameraIdsUnderTest.length + 1] = generateNonexistingNumericCameraId();

        for (String idToOpen : mCameraIdsUnderTest) {
            openDevice(idToOpen);
            try {
                for (String id : cameraIdsToTestTorch) {
                    try {
                        mCameraManager.setTorchMode(id, true);
                        SystemClock.sleep(TORCH_DURATION_MS);
                        mCameraManager.setTorchMode(id, false);
                        if (!hasFlash(id)) {
                            fail(""exception should be thrown when turning on torch mode of a "" +
                                    ""camera without a flash"");
                        } else if (id.equals(idToOpen)) {
                            fail(""exception should be thrown when turning on torch mode of an "" +
                                    ""opened camera"");
                        }
                    } catch (CameraAccessException e) {
                        int reason = e.getReason();
                        if ((hasFlash(id) &&  id.equals(idToOpen) &&
                                    reason == CameraAccessException.CAMERA_IN_USE) ||
                            (hasFlash(id) && !id.equals(idToOpen) &&
                                    reason == CameraAccessException.MAX_CAMERAS_IN_USE)) {
                            continue;
                        }
                        fail(""("" + id + "") not expecting: "" + e.getMessage() + ""reason "" + reason);
                    } catch (IllegalArgumentException e) {
                        if (hasFlash(id)) {
                            fail(""not expecting IllegalArgumentException"");
                        }
                    }
                }
            } finally {
                closeDevice(idToOpen);
            }
        }
    }

    private boolean hasFlash(String cameraId) {
        return mFlashCameraIdList.contains(cameraId);
    }

    // make sure the torch status is off.
    private void resetTorchModeStatus(String cameraId) throws Exception {
        TorchCallbackListener torchListener = new TorchCallbackListener(cameraId);

        mCameraManager.registerTorchCallback(torchListener, mHandler);
        mCameraManager.setTorchMode(cameraId, true);
        mCameraManager.setTorchMode(cameraId, false);

        torchListener.waitOnStatusChange(TorchCallbackListener.STATUS_ON);
        torchListener.waitOnStatusChange(TorchCallbackListener.STATUS_OFF);

        mCameraManager.unregisterTorchCallback(torchListener);
    }

    private String generateNonexistingCameraId() {
        String nonExisting = ""none_existing_camera"";
        for (String id : mCameraIdsUnderTest) {
            if (Arrays.asList(mCameraIdsUnderTest).contains(nonExisting)) {
                nonExisting += id;
            } else {
                break;
            }
        }
        return nonExisting;
    }

    // return a non-existing and non-negative numeric camera id.
    private String generateNonexistingNumericCameraId() throws Exception {
        // We don't rely on mCameraIdsUnderTest to generate a non existing camera id since
        // mCameraIdsUnderTest doesn't give us an accurate reflection of which camera ids actually
        // exist. It just tells us the ones we're testing right now.
        String[] allCameraIds = mCameraManager.getCameraIdListNoLazy();
        int[] numericCameraIds = new int[allCameraIds.length];
        int size = 0;

        for (String cameraId : allCameraIds) {
            try {
                int value = Integer.parseInt(cameraId);
                if (value >= 0) {
                    numericCameraIds[size++] = value;
                }
            } catch (Throwable e) {
                // do nothing if camera id isn't an integer
            }
        }

        if (size == 0) {
            return ""0"";
        }

        Arrays.sort(numericCameraIds, 0, size);
        if (numericCameraIds[0] != 0) {
            return ""0"";
        }

        for (int i = 0; i < size - 1; i++) {
            if (numericCameraIds[i] + 1 < numericCameraIds[i + 1]) {
                return String.valueOf(numericCameraIds[i] + 1);
            }
        }

        if (numericCameraIds[size - 1] != Integer.MAX_VALUE) {
            return String.valueOf(numericCameraIds[size - 1] + 1);
        }

        fail(""cannot find a non-existing and non-negative numeric camera id"");
        return null;
    }

    private final class TorchCallbackListener extends CameraManager.TorchCallback {
        private static final String TAG = ""TorchCallbackListener"";
        private static final int STATUS_WAIT_TIMEOUT_MS = 3000;
        private static final int QUEUE_CAPACITY = 100;

        private String mCameraId;
        private ArrayBlockingQueue<Integer> mStatusQueue =
                new ArrayBlockingQueue<Integer>(QUEUE_CAPACITY);

        public static final int STATUS_UNAVAILABLE = 0;
        public static final int STATUS_OFF = 1;
        public static final int STATUS_ON = 2;

        public TorchCallbackListener(String cameraId) {
            // only care about events for this camera id.
            mCameraId = cameraId;
        }

        public void waitOnStatusChange(int status) throws Exception {
            while (true) {
                Integer s = mStatusQueue.poll(STATUS_WAIT_TIMEOUT_MS, TimeUnit.MILLISECONDS);
                if (s == null) {
                    fail(""waiting for status "" + status + "" timed out"");
                } else if (s.intValue() == status) {
                    return;
                }
            }
        }

        @Override
        public void onTorchModeUnavailable(String cameraId) {
            if (cameraId.equals(mCameraId)) {
                Integer s = new Integer(STATUS_UNAVAILABLE);
                try {
                    mStatusQueue.put(s);
                } catch (Throwable e) {
                    fail(e.getMessage());
                }
            }
        }

        @Override
        public void onTorchModeChanged(String cameraId, boolean enabled) {
            if (cameraId.equals(mCameraId)) {
                Integer s;
                if (enabled) {
                    s = new Integer(STATUS_ON);
                } else {
                    s = new Integer(STATUS_OFF);
                }
                try {
                    mStatusQueue.put(s);
                } catch (Throwable e) {
                    fail(e.getMessage());
                }
            }
        }
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.telephony.cts.TelephonyRegistryManagerTest"	"testNotifyDataActivityChanged"	"CtsTelephonyTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/telephony/current/src/android/telephony/cts/TelephonyRegistryManagerTest.java"	""	"public void testNotifyDataActivityChanged() throws Exception {
        Context context = InstrumentationRegistry.getContext();

        LinkedBlockingQueue<Integer> queue = new LinkedBlockingQueue<>(1);
        PhoneStateListener psl = new PhoneStateListener(context.getMainExecutor()) {
            @Override
            public void onDataActivity(int activity) {
                queue.offer(activity);
            }
        };
        TelephonyManager tm = context.getSystemService(TelephonyManager.class);
        tm.listen(psl, PhoneStateListener.LISTEN_DATA_ACTIVITY);
        // clear the initial result from registering the listener.
        queue.poll(TIMEOUT_MILLIS, TimeUnit.MILLISECONDS);

        int testValue = TelephonyManager.DATA_ACTIVITY_DORMANT;
        ShellIdentityUtils.invokeMethodWithShellPermissionsNoReturn(mTelephonyRegistryMgr,
                (trm) -> trm.notifyDataActivityChanged(
                        SubscriptionManager.getDefaultSubscriptionId(),
                        testValue));

        int result = queue.poll(TIMEOUT_MILLIS, TimeUnit.MILLISECONDS);
        assertEquals(testValue, result);
    }

}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.IdleUidTest"	"testCameraAccessForIdleUid"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/IdleUidTest.java"	""	"public void testCameraAccessForIdleUid() throws Exception {
        for (String cameraId : mCameraIdsUnderTest) {
            testCameraAccessForIdleUidByCamera(cameraId,
                    new Handler(sCallbackThread.getLooper()));
        }
    }

    /**
     * Tests that a UID loses access to the camera if it becomes inactive.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.IdleUidTest"	"testCameraAccessBecomingInactiveUid"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/IdleUidTest.java"	""	"public void testCameraAccessBecomingInactiveUid() throws Exception {
        for (String cameraId : mCameraIdsUnderTest) {
            testCameraAccessBecomingInactiveUidByCamera(cameraId,
                    new Handler(sCallbackThread.getLooper()));
        }
    }

    private void testCameraAccessForIdleUidByCamera(String cameraId, Handler handler)
            throws Exception {
        // Can access camera from an active UID.
        assertCameraAccess(mCameraManager, cameraId, true, handler);

        // Make our UID idle
        makeMyPackageIdle();
        try {
            // Can not access camera from an idle UID.
            assertCameraAccess(mCameraManager, cameraId, false, handler);
        } finally {
            // Restore our UID as active
            makeMyPackageActive();
        }

        // Can access camera from an active UID.
        assertCameraAccess(mCameraManager, cameraId, true, handler);
    }

    private static void assertCameraAccess(CameraManager cameraManager,
            String cameraId, boolean hasAccess, Handler handler) {
        // Mock the callback used to observe camera state.
        final CameraDevice.StateCallback callback = mock(CameraDevice.StateCallback.class);

        // Open the camera
        try {
            cameraManager.openCamera(cameraId, callback, handler);
        } catch (CameraAccessException e) {
            if (hasAccess) {
                fail(""Unexpected exception"" + e);
            } else {
                assertThat(e.getReason()).isSameInstanceAs(CameraAccessException.CAMERA_DISABLED);
            }
        }

        // Verify access
        final ArgumentCaptor<CameraDevice> captor = ArgumentCaptor.forClass(CameraDevice.class);
        try {
            if (hasAccess) {
                // The camera should open fine as we are in the foreground
                verify(callback, timeout(CAMERA_OPERATION_TIMEOUT_MILLIS)
                        .times(1)).onOpened(captor.capture());
                verifyNoMoreInteractions(callback);
            } else {
                // The camera should not open as we are in the background
                verify(callback, timeout(CAMERA_OPERATION_TIMEOUT_MILLIS)
                        .times(1)).onError(captor.capture(),
                        eq(CameraDevice.StateCallback.ERROR_CAMERA_DISABLED));
                verifyNoMoreInteractions(callback);
            }
        } finally {
            final CameraDevice cameraDevice = captor.getValue();
            assertThat(cameraDevice).isNotNull();
            cameraDevice.close();
        }
    }

    private void testCameraAccessBecomingInactiveUidByCamera(String cameraId, Handler handler)
            throws Exception {
        // Mock the callback used to observe camera state.
        final CameraDevice.StateCallback callback = mock(CameraDevice.StateCallback.class);

        // Open the camera
        try {
            mCameraManager.openCamera(cameraId, callback, handler);
        } catch (CameraAccessException e) {
            fail(""Unexpected exception"" + e);
        }

        // Verify access
        final ArgumentCaptor<CameraDevice> captor = ArgumentCaptor.forClass(CameraDevice.class);
        try {
            // The camera should open fine as we are in the foreground
            verify(callback, timeout(CAMERA_OPERATION_TIMEOUT_MILLIS)
                    .times(1)).onOpened(captor.capture());
            verifyNoMoreInteractions(callback);

            // Ready for a new verification
            reset(callback);

            // Now we are moving in the background
            makeMyPackageIdle();

            // The camera should be closed if the UID became idle
            verify(callback, timeout(CAMERA_OPERATION_TIMEOUT_MILLIS)
                    .times(1)).onError(captor.capture(),
                    eq(CameraDevice.StateCallback.ERROR_CAMERA_DISABLED));
            verifyNoMoreInteractions(callback);
        } finally {
            // Restore to active state
            makeMyPackageActive();

            final CameraDevice cameraDevice = captor.getValue();
            assertThat(cameraDevice).isNotNull();
            cameraDevice.close();
        }
    }

    private static void makeMyPackageActive() throws IOException {
        final String command = ""cmd media.camera reset-uid-state ""
                +  InstrumentationRegistry.getTargetContext().getPackageName()
                        + "" --user "" + Process.myUserHandle().getIdentifier();
        SystemUtil.runShellCommand(InstrumentationRegistry.getInstrumentation(), command);
    }

    private static void makeMyPackageIdle() throws IOException {
        final String command = ""cmd media.camera set-uid-state ""
                + InstrumentationRegistry.getTargetContext().getPackageName() + "" idle""
                        + "" --user "" + Process.myUserHandle().getIdentifier();
        SystemUtil.runShellCommand(InstrumentationRegistry.getInstrumentation(), command);
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.helpers.Camera2Focuser"	"createCaptureListener"	""	"/home/gpoor/cts-12-source/cts/tests/camera/utils/src/android/hardware/camera2/cts/helpers/Camera2Focuser.java"	""	"public void test/*
 *.
 */

package android.hardware.camera2.cts.helpers;

import android.graphics.Rect;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraCaptureSession.CaptureCallback;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.params.MeteringRectangle;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.TotalCaptureResult;
import android.os.Handler;
import android.util.Log;
import android.view.Surface;

import com.android.ex.camera2.pos.AutoFocusStateMachine;
import com.android.ex.camera2.pos.AutoFocusStateMachine.AutoFocusStateListener;

/**
 * A focuser utility class to assist camera to do auto focus.
 * <p>
 * This class need create repeating request and single request to do auto focus.
 * The repeating request is used to get the auto focus states; the single
 * request is used to trigger the auto focus. This class assumes the camera device
 * supports auto-focus. Don't use this class if the camera device doesn't have focuser
 * unit.
 * </p>
 */
public class Camera2Focuser implements AutoFocusStateListener {
    private static final String TAG = ""Focuser"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);

    private final AutoFocusStateMachine mAutoFocus = new AutoFocusStateMachine(this);
    private final Handler mHandler;
    private final AutoFocusListener mAutoFocusListener;
    private final CameraDevice mCamera;
    private final CameraCaptureSession mSession;
    private final Surface mRequestSurface;
    private final StaticMetadata mStaticInfo;

    private int mAfRun = 0;
    private MeteringRectangle[] mAfRegions;
    private boolean mLocked = false;
    private boolean mSuccess = false;
    private CaptureRequest.Builder mRepeatingBuilder;

    /**
     * The callback interface to notify auto focus result.
     */
    public interface AutoFocusListener {
        /**
         * This callback is called when auto focus completes and locked.
         *
         * @param success true if focus was successful, false if otherwise
         */
        void onAutoFocusLocked(boolean success);
    }

    /**
     * Construct a focuser object, with given capture requestSurface, listener
     * and handler.
     * <p>
     * The focuser object will use camera and requestSurface to submit capture
     * request and receive focus state changes. The {@link AutoFocusListener} is
     * used to notify the auto focus callback.
     * </p>
     *
     * @param camera The camera device associated with this focuser
     * @param session The camera capture session associated with this focuser
     * @param requestSurface The surface to issue the capture request with
     * @param listener The auto focus listener to notify AF result
     * @param staticInfo The CameraCharacteristics of the camera device
     * @param handler The handler used to post auto focus callbacks
     * @throws CameraAccessException
     */
    public Camera2Focuser(CameraDevice camera, CameraCaptureSession session, Surface requestSurface,
            AutoFocusListener listener, CameraCharacteristics staticInfo, Handler handler)
            throws CameraAccessException {
        if (camera == null) {
            throw new IllegalArgumentException(""camera must not be null"");
        }
        if (session == null) {
            throw new IllegalArgumentException(""session must not be null"");
        }
        if (listener == null) {
            throw new IllegalArgumentException(""listener must not be null"");
        }
        if (handler == null) {
            throw new IllegalArgumentException(""handler must not be null"");
        }
        if (requestSurface == null) {
            throw new IllegalArgumentException(""requestSurface must not be null"");
        }
        if (staticInfo == null) {
            throw new IllegalArgumentException(""staticInfo must not be null"");
        }

        mCamera = camera;
        mSession = session;
        mRequestSurface = requestSurface;
        mAutoFocusListener = listener;
        mStaticInfo = new StaticMetadata(staticInfo,
                StaticMetadata.CheckLevel.ASSERT, /*collector*/null);
        mHandler = handler;

        if (!mStaticInfo.hasFocuser()) {
            throw new IllegalArgumentException(""this camera doesn't have a focuser"");
        }

        /**
         * Begin by always being in passive auto focus.
         */
        cancelAutoFocus();
    }

    @Override
    public synchronized void onAutoFocusSuccess(CaptureResult result, boolean locked) {
        mSuccess = true;
        mLocked = locked;

        if (locked) {
            dispatchAutoFocusStatusLocked(/*success*/true);
        }
    }

    @Override
    public synchronized void onAutoFocusFail(CaptureResult result, boolean locked) {
        mSuccess = false;
        mLocked = locked;

        if (locked) {
            dispatchAutoFocusStatusLocked(/*success*/false);
        }
    }

    @Override
    public synchronized void onAutoFocusScan(CaptureResult result) {
        mSuccess = false;
        mLocked = false;
    }

    @Override
    public synchronized void onAutoFocusInactive(CaptureResult result) {
        mSuccess = false;
        mLocked = false;
    }

    /**
     * Start a active auto focus scan based on the given regions.
     *
     * <p>This is usually used for touch for focus, it can make the auto-focus converge based
     * on some particular region aggressively. But it is usually slow as a full active scan
     * is initiated. After the auto focus is converged, the {@link cancelAutoFocus} must be called
     * to resume the continuous auto-focus.</p>
     *
     * @param afRegions The AF regions used by focuser auto focus, full active
     * array size is used if afRegions is null.
     * @throws CameraAccessException
     */
    public synchronized void touchForAutoFocus(MeteringRectangle[] afRegions)
            throws CameraAccessException {
        startAutoFocusLocked(/*active*/true, afRegions);
    }

    /**
     * Start auto focus scan.
     * <p>
     * Start an auto focus scan if it was not done yet. If AF passively focused,
     * lock it. If AF is already locked, return. Otherwise, initiate a full
     * active scan. This is suitable for still capture: focus should need to be
     * accurate, but the AF latency also need to be as short as possible.
     * </p>
     *
     * @param afRegions The AF regions used by focuser auto focus, full active
     *            array size is used if afRegions is null.
     * @throws CameraAccessException
     */
    public synchronized void startAutoFocus(MeteringRectangle[] afRegions)
            throws CameraAccessException {
        startAutoFocusLocked(/*forceActive*/false, afRegions);
    }

    /**
     * Cancel ongoing auto focus, unlock the auto-focus if it was locked, and
     * resume to passive continuous auto focus.
     *
     * @throws CameraAccessException
     */
    public synchronized void cancelAutoFocus() throws CameraAccessException {
        mSuccess = false;
        mLocked = false;

        // reset the AF regions:
        setAfRegions(null);

        // Create request builders, the af regions are automatically updated.
        mRepeatingBuilder = createRequestBuilder();
        CaptureRequest.Builder requestBuilder = createRequestBuilder();
        mAutoFocus.setPassiveAutoFocus(/*picture*/true, mRepeatingBuilder);
        mAutoFocus.unlockAutoFocus(mRepeatingBuilder, requestBuilder);
        CaptureCallback listener = createCaptureListener();
        mSession.setRepeatingRequest(mRepeatingBuilder.build(), listener, mHandler);
        mSession.capture(requestBuilder.build(), listener, mHandler);
    }

    /**
     * Get current AF mode.
     * @return current AF mode
     * @throws IllegalStateException if there auto focus is not running.
     */
    public synchronized int getCurrentAfMode() {
        if (mRepeatingBuilder == null) {
            throw new IllegalStateException(""Auto focus is not running, unable to get AF mode"");
        }

        return mRepeatingBuilder.get(CaptureRequest.CONTROL_AF_MODE);
    }

    private void startAutoFocusLocked(
            boolean forceActive, MeteringRectangle[] afRegions) throws CameraAccessException {

        setAfRegions(afRegions);
        mAfRun++;

        // Create request builders, the af regions are automatically updated.
        mRepeatingBuilder = createRequestBuilder();
        CaptureRequest.Builder requestBuilder = createRequestBuilder();
        if (forceActive) {
            startAutoFocusFullActiveLocked();
        } else {
            // Not forcing a full active scan. If AF passively focused, lock it. If AF is already
            // locked, return. Otherwise, initiate a full active scan.
            if (mSuccess && mLocked) {
                dispatchAutoFocusStatusLocked(/*success*/true);
                return;
            } else if (mSuccess) {
                mAutoFocus.lockAutoFocus(mRepeatingBuilder, requestBuilder);
                CaptureCallback listener = createCaptureListener();
                mSession.setRepeatingRequest(mRepeatingBuilder.build(), listener, mHandler);
                mSession.capture(requestBuilder.build(), listener, mHandler);
            } else {
                startAutoFocusFullActiveLocked();
            }
        }
    }

    private void startAutoFocusFullActiveLocked() throws CameraAccessException {
        // Create request builders, the af regions are automatically updated.
        mRepeatingBuilder = createRequestBuilder();
        CaptureRequest.Builder requestBuilder = createRequestBuilder();
        mAutoFocus.setActiveAutoFocus(mRepeatingBuilder, requestBuilder);
        if (mRepeatingBuilder.get(CaptureRequest.CONTROL_AF_TRIGGER)
                != CaptureRequest.CONTROL_AF_TRIGGER_IDLE) {
            throw new AssertionError(""Wrong trigger set in repeating request"");
        }
        if (requestBuilder.get(CaptureRequest.CONTROL_AF_TRIGGER)
                != CaptureRequest.CONTROL_AF_TRIGGER_START) {
            throw new AssertionError(""Wrong trigger set in queued request"");
        }
        mAutoFocus.resetState();

        CaptureCallback listener = createCaptureListener();
        mSession.setRepeatingRequest(mRepeatingBuilder.build(), listener, mHandler);
        mSession.capture(requestBuilder.build(), listener, mHandler);
    }

    private void dispatchAutoFocusStatusLocked(final boolean success) {
        mHandler.post(new Runnable() {
            @Override
            public void run() {
                mAutoFocusListener.onAutoFocusLocked(success);
            }
        });
    }

    /**
     * Create request builder, set the af regions.
     * @throws CameraAccessException
     */
    private CaptureRequest.Builder createRequestBuilder() throws CameraAccessException {
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);

        requestBuilder.set(CaptureRequest.CONTROL_AF_REGIONS, mAfRegions);
        requestBuilder.addTarget(mRequestSurface);

        return requestBuilder;
    }

    /**
     * Set AF regions, fall back to default region if afRegions is null.
     *
     * @param afRegions The AF regions to set
     * @throws IllegalArgumentException if the region is malformed (length is 0).
     */
    private void setAfRegions(MeteringRectangle[] afRegions) {
        if (afRegions == null) {
            setDefaultAfRegions();
            return;
        }
        // Throw IAE if AF regions are malformed.
        if (afRegions.length == 0) {
            throw new IllegalArgumentException(""afRegions is malformed, length: 0"");
        }

        mAfRegions = afRegions;
    }

    /**
     * Set default AF region to full active array size.
     */
    private void setDefaultAfRegions() {
        // Initialize AF regions with all zeros, meaning that it is up to camera device to device
        // the regions used by AF.
        mAfRegions = new MeteringRectangle[] {
                new MeteringRectangle(0, 0, 0, 0, MeteringRectangle.METERING_WEIGHT_DONT_CARE)};
    }
    private CaptureCallback createCaptureListener() {

        int thisAfRun;
        synchronized (this) {
            thisAfRun = mAfRun;
        }

        final int finalAfRun = thisAfRun;

        return new CaptureCallback() {
            private long mLatestFrameCount = -1;

            @Override
            public void onCaptureProgressed(CameraCaptureSession session, CaptureRequest request,
                    CaptureResult result) {
                // In case of a partial result, send to focuser if necessary
                // 3A fields are present
                if (result.get(CaptureResult.CONTROL_AF_STATE) != null &&
                        result.get(CaptureResult.CONTROL_AF_MODE) != null) {
                    if (VERBOSE) {
                        Log.v(TAG, ""Focuser - got early AF state"");
                    }

                    dispatchToFocuser(result);
                }
            }

            @Override
            public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                    TotalCaptureResult result) {
                    dispatchToFocuser(result);
            }

            private void dispatchToFocuser(CaptureResult result) {
                int afRun;
                synchronized (Camera2Focuser.this) {
                    // In case of partial results, don't send AF update twice
                    long frameCount = result.getFrameNumber();
                    if (frameCount <= mLatestFrameCount) return;
                    mLatestFrameCount = frameCount;

                    afRun = mAfRun;
                }

                if (afRun != finalAfRun) {
                    if (VERBOSE) {
                        Log.w(TAG,
                                ""onCaptureCompleted - Ignoring results from previous AF run ""
                                + finalAfRun);
                    }
                    return;
                }

                mAutoFocus.onCaptureCompleted(result);
            }
        };
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.RoutingTest"	"test_mediaPlayer_incallMusicRoutingPermissions"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/RoutingTest.java"	""	"public void test_mediaPlayer_incallMusicRoutingPermissions() {
        if (!mContext.getPackageManager().hasSystemFeature(PackageManager.FEATURE_AUDIO_OUTPUT)) {
            // Can't do it so skip this test
            return;
        }

        // only apps with MODIFY_PHONE_STATE permission can route playback
        // to the uplink stream during a phone call, so this test makes sure that
        // audio is re-routed to default device when the permission is missing

        AudioDeviceInfo telephonyDevice = getTelephonyDeviceAndSetInCommunicationMode();
        if (telephonyDevice == null) {
            // Can't do it so skip this test
            return;
        }

        MediaPlayer mediaPlayer = null;

        try {
            mediaPlayer = allocMediaPlayer(telephonyDevice, false);
            assertEquals(AudioDeviceInfo.TYPE_TELEPHONY, mediaPlayer.getPreferredDevice().getType());
            mediaPlayer.start();
            // Sleep for 1s to ensure the underlying AudioTrack is created and started
            SystemClock.sleep(1000);
            telephonyDevice = mediaPlayer.getRoutedDevice();
            // 3 behaviors are accepted when permission to play to telephony device is rejected:
            // - indicate a null routed device
            // - fallback to another device for playback
            // - stop playback in error.
            assertTrue(telephonyDevice == null
                    || telephonyDevice.getType() != AudioDeviceInfo.TYPE_TELEPHONY
                    || !mediaPlayer.isPlaying());
        } finally {
            if (mediaPlayer != null) {
                mediaPlayer.stop();
                mediaPlayer.release();
            }
            mAudioManager.setMode(AudioManager.MODE_NORMAL);
        }
    }

    private MediaRecorder allocMediaRecorder() throws Exception {
        final String outputPath = new File(Environment.getExternalStorageDirectory(),
            ""record.out"").getAbsolutePath();
        mOutFile = new File(outputPath);
        MediaRecorder mediaRecorder = new MediaRecorder();
        mediaRecorder.setAudioSource(MediaRecorder.AudioSource.MIC);
        assertEquals(0, mediaRecorder.getMaxAmplitude());
        mediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.THREE_GPP);
        mediaRecorder.setOutputFile(outputPath);
        mediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.AAC);
        mediaRecorder.setAudioChannels(AudioFormat.CHANNEL_OUT_DEFAULT);
        mediaRecorder.setAudioSamplingRate(AUDIO_SAMPLE_RATE_HZ);
        mediaRecorder.setAudioEncodingBitRate(AUDIO_BIT_RATE_IN_BPS);
        mediaRecorder.setMaxFileSize(MAX_FILE_SIZE_BYTE);
        mediaRecorder.prepare();
        mediaRecorder.start();
        // Sleep a while to ensure the underlying AudioRecord is initialized.
        Thread.sleep(1000);
        return mediaRecorder;
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.RoutingTest"	"test_mediaRecorder_preferredDevice"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/RoutingTest.java"	""	"public void test_mediaRecorder_preferredDevice() throws Exception {
        if (!mContext.getPackageManager().hasSystemFeature(PackageManager.FEATURE_MICROPHONE)
                || !MediaUtils.hasEncoder(MediaFormat.MIMETYPE_AUDIO_AAC)) {
            MediaUtils.skipTest(""no audio codecs or microphone"");
            return;
        }

        MediaRecorder mediaRecorder = allocMediaRecorder();

        // None selected (new MediaPlayer), so check for default
        assertNull(mediaRecorder.getPreferredDevice());

        // resets to default
        assertTrue(mediaRecorder.setPreferredDevice(null));

        // test each device
        AudioDeviceInfo[] deviceList = mAudioManager.getDevices(AudioManager.GET_DEVICES_INPUTS);
        for (int index = 0; index < deviceList.length; index++) {
            if (!AVAILABLE_INPUT_DEVICES_TYPE.contains(deviceList[index].getType())) {
                // Only try to set devices whose type is contained in predefined set as preferred
                // device in case of permission denied when switching input device.
                continue;
            }
            assertTrue(mediaRecorder.setPreferredDevice(deviceList[index]));
            assertTrue(mediaRecorder.getPreferredDevice() == deviceList[index]);
        }

        // Check defaults again
        assertTrue(mediaRecorder.setPreferredDevice(null));
        assertNull(mediaRecorder.getPreferredDevice());
        Thread.sleep(RECORD_TIME_MS);

        mediaRecorder.stop();
        mediaRecorder.release();
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.RoutingTest"	"test_mediaRecorder_getRoutedDeviceId"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/RoutingTest.java"	""	"public void test_mediaRecorder_getRoutedDeviceId() throws Exception {
        if (!mContext.getPackageManager().hasSystemFeature(PackageManager.FEATURE_MICROPHONE)
            || !MediaUtils.hasEncoder(MediaFormat.MIMETYPE_AUDIO_AAC)) {
            MediaUtils.skipTest(""no audio codecs or microphone"");
            return;
        }

        MediaRecorder mediaRecorder = allocMediaRecorder();

        AudioDeviceInfo routedDevice = mediaRecorder.getRoutedDevice();
        assertNotNull(routedDevice); // we probably can't say anything more than this
        Thread.sleep(RECORD_TIME_MS);

        mediaRecorder.stop();
        mediaRecorder.release();
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.RoutingTest"	"test_mediaRecorder_RoutingListener"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/RoutingTest.java"	""	"public void test_mediaRecorder_RoutingListener() throws Exception {
        if (!mContext.getPackageManager().hasSystemFeature(PackageManager.FEATURE_MICROPHONE)
            || !MediaUtils.hasEncoder(MediaFormat.MIMETYPE_AUDIO_AAC)) {
            MediaUtils.skipTest(""no audio codecs or microphone"");
            return;
        }

        MediaRecorder mediaRecorder = allocMediaRecorder();

        // null listener
        mediaRecorder.addOnRoutingChangedListener(null, null);

        AudioRoutingListener listener = new AudioRoutingListener();
        AudioRoutingListener someOtherListener = new AudioRoutingListener();

        // add a listener
        mediaRecorder.addOnRoutingChangedListener(listener, null);

        // remove listeners we didn't add
        mediaRecorder.removeOnRoutingChangedListener(someOtherListener);
        // remove a valid listener
        mediaRecorder.removeOnRoutingChangedListener(listener);

        Looper myLooper = prepareIfNeededLooper();
        mediaRecorder.addOnRoutingChangedListener(listener, new Handler());
        mediaRecorder.removeOnRoutingChangedListener(listener);

        Thread.sleep(RECORD_TIME_MS);

        mediaRecorder.stop();
        mediaRecorder.release();
        if (myLooper != null) {
            myLooper.quit();
        }
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.multiprocess.camera.cts.MediaRecorderCameraActivity"	"isInstantApp"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/multiprocess/camera/cts/MediaRecorderCameraActivity.java"	""	"public void test/*
 *.
 */

package android.hardware.multiprocess.camera.cts;

import android.app.Activity;
import android.camera.cts.R;
import android.media.MediaRecorder;
import android.os.Bundle;
import android.util.Log;
import android.view.SurfaceHolder;
import android.view.SurfaceView;
import android.view.ViewGroup;

import java.io.File;

/**
 * Activity implementing basic access of camera using MediaRecorder API.
 *
 * <p />
 * This will log all errors to {@link android.hardware.multiprocess.camera.cts.ErrorLoggingService}.
 */
public class MediaRecorderCameraActivity extends Activity implements SurfaceHolder.Callback {
    private static final String TAG = ""MediaRecorderCameraActivity"";

    private static final int VIDEO_WIDTH = 640;
    private static final int VIDEO_HEIGHT = 480;
    private static final int LAYOUT_WIDTH = VIDEO_WIDTH;
    private static final int LAYOUT_HEIGHT = VIDEO_HEIGHT;

    private String mOutputPath;
    private File mOutFile;
    private SurfaceView mSurfaceView;
    private ErrorLoggingService.ErrorServiceConnection mErrorServiceConnection;
    private MediaRecorder mMediaRecorder;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        Log.i(TAG, ""onCreate called."");
        super.onCreate(savedInstanceState);

        setContentView(R.layout.surface_view);

        mErrorServiceConnection = new ErrorLoggingService.ErrorServiceConnection(this);
        mErrorServiceConnection.start();

        mMediaRecorder = new MediaRecorder();

        File filesDir = getPackageManager().isInstantApp()
                ? getFilesDir()
                : getExternalFilesDir(null);

        mOutputPath = new File(filesDir, ""record.out"").getAbsolutePath();
    }

    @Override
    protected void onResume() {
        Log.i(TAG, ""onResume called."");
        super.onResume();
        try {

            mSurfaceView = (SurfaceView)this.findViewById(R.id.surface_view);
            ViewGroup.LayoutParams lp = mSurfaceView.getLayoutParams();
            lp.width = LAYOUT_WIDTH;
            lp.height = LAYOUT_HEIGHT;
            mSurfaceView.setLayoutParams(lp);

            SurfaceHolder holder = mSurfaceView.getHolder();
            holder.setFixedSize(LAYOUT_WIDTH, LAYOUT_HEIGHT);
            holder.addCallback(this);

        } catch (Throwable e) {
            mErrorServiceConnection.logAsync(TestConstants.EVENT_CAMERA_ERROR, TAG +
                    "" camera exception during connection: "" + e);
            Log.e(TAG, ""Runtime error: "" + e);
        }
    }

    @Override
    protected void onPause() {
        Log.i(TAG, ""onPause called."");
        super.onPause();
    }

    @Override
    protected void onDestroy() {
        Log.i(TAG, ""onDestroy called."");
        super.onDestroy();
        if (mErrorServiceConnection != null) {
            mErrorServiceConnection.stop();
            mErrorServiceConnection = null;
        }

        if (mOutFile != null && mOutFile.exists()) {
            mOutFile.delete();
        }

        if (mMediaRecorder != null) {
            mMediaRecorder.stop();
            mMediaRecorder.release();
        }
    }

    @Override
    public void surfaceCreated(SurfaceHolder holder) {
    }

    @Override
    public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
        try {
            mOutFile = new File(mOutputPath);
            mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);
            mMediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.DEFAULT);
            mMediaRecorder.setPreviewDisplay(mSurfaceView.getHolder().getSurface());
            mMediaRecorder.setVideoEncoder(MediaRecorder.VideoEncoder.DEFAULT);
            mMediaRecorder.setVideoSize(VIDEO_WIDTH, VIDEO_HEIGHT);
            mMediaRecorder.setOutputFile(mOutputPath);
            mMediaRecorder.prepare();
            mMediaRecorder.start();

            mErrorServiceConnection.logAsync(TestConstants.EVENT_CAMERA_CONNECT,
                    TAG + "" camera connected"");
        } catch (Throwable e) {
            mErrorServiceConnection.logAsync(TestConstants.EVENT_CAMERA_ERROR, TAG +
                    "" camera exception during connection: "" + e);
            Log.e(TAG, ""Runtime error: "" + e);
        }
    }

    @Override
    public void surfaceDestroyed(SurfaceHolder holder) {
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaCodecCapabilitiesTest"	"testAllNonTunneledVideoCodecsSupportFlexibleYUV"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaCodecCapabilitiesTest.java"	""	"public void testAllNonTunneledVideoCodecsSupportFlexibleYUV() throws IOException {
        boolean skipped = true;
        for (MediaCodecInfo info : mAllInfos) {
            boolean isEncoder = info.isEncoder();
            for (String mime: info.getSupportedTypes()) {
                if (!isVideoMime(mime)) {
                    continue;
                }
                CodecCapabilities caps = info.getCapabilitiesForType(mime);
                if (caps.isFeatureRequired(CodecCapabilities.FEATURE_TunneledPlayback)
                        || caps.isFeatureRequired(CodecCapabilities.FEATURE_SecurePlayback)) {
                    continue;
                }
                skipped = false;
                boolean found = false;
                for (int c : caps.colorFormats) {
                    if (c == caps.COLOR_FormatYUV420Flexible) {
                        found = true;
                        break;
                    }
                }
                assertTrue(
                    info.getName() + "" does not advertise COLOR_FormatYUV420Flexible"",
                    found);

                MediaCodec codec = null;
                MediaFormat format = null;
                try {
                    Size size = getVideoSizeForTest(caps.getVideoCapabilities());
                    codec = MediaCodec.createByCodecName(info.getName());
                    format = createReasonableVideoFormat(
                            caps, mime, isEncoder, size.getWidth(), size.getHeight());
                    format.setInteger(
                            MediaFormat.KEY_COLOR_FORMAT,
                            caps.COLOR_FormatYUV420Flexible);

                    codec.configure(format, null /* surface */, null /* crypto */,
                            isEncoder ? codec.CONFIGURE_FLAG_ENCODE : 0);
                    MediaFormat configuredFormat =
                            isEncoder ? codec.getInputFormat() : codec.getOutputFormat();
                    Log.d(TAG, ""color format is "" + configuredFormat.getInteger(
                            MediaFormat.KEY_COLOR_FORMAT));
                    if (isEncoder) {
                        codec.start();
                        int ix = codec.dequeueInputBuffer(TIMEOUT_US);
                        assertNotNull(
                                info.getName() + "" encoder has non-flexYUV input buffer #"" + ix,
                                codec.getInputImage(ix));
                    } else {
                        // TODO: test these on various decoders (need test streams)
                    }
                } finally {
                    if (codec != null) {
                        codec.release();
                    }
                }
            }
        }
        if (skipped) {
            MediaUtils.skipTest(""no non-tunneled/non-secure video decoders found"");
        }
    }

    private static MediaFormat createMinFormat(String mime, CodecCapabilities caps) {
        MediaFormat format;
        if (caps.getVideoCapabilities() != null) {
            VideoCapabilities vcaps = caps.getVideoCapabilities();
            int minWidth = vcaps.getSupportedWidths().getLower();
            int minHeight = vcaps.getSupportedHeightsFor(minWidth).getLower();
            int minBitrate = vcaps.getBitrateRange().getLower();
            int minFrameRate = Math.max(vcaps.getSupportedFrameRatesFor(minWidth, minHeight)
                    .getLower().intValue(), 1);
            format = MediaFormat.createVideoFormat(mime, minWidth, minHeight);
            format.setInteger(MediaFormat.KEY_COLOR_FORMAT, caps.colorFormats[0]);
            format.setInteger(MediaFormat.KEY_BIT_RATE, minBitrate);
            format.setInteger(MediaFormat.KEY_FRAME_RATE, minFrameRate);
            format.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, IFRAME_INTERVAL);
        } else {
            AudioCapabilities acaps = caps.getAudioCapabilities();
            int minSampleRate = acaps.getSupportedSampleRateRanges()[0].getLower();
            int minChannelCount = 1;
            int minBitrate = acaps.getBitrateRange().getLower();
            format = MediaFormat.createAudioFormat(mime, minSampleRate, minChannelCount);
            format.setInteger(MediaFormat.KEY_BIT_RATE, minBitrate);
        }

        return format;
    }

    private int getActualMax(
            boolean isEncoder, String name, String mime, CodecCapabilities caps, int max) {
        int flag = isEncoder ? MediaCodec.CONFIGURE_FLAG_ENCODE : 0;
        boolean memory_limited = false;
        MediaFormat format = createMinFormat(mime, caps);
        Log.d(TAG, ""Test format "" + format);
        Vector<MediaCodec> codecs = new Vector<MediaCodec>();
        MediaCodec codec = null;
        ActivityManager am = (ActivityManager)
                mContext.getSystemService(Context.ACTIVITY_SERVICE);
        ActivityManager.MemoryInfo outInfo = new ActivityManager.MemoryInfo();
        for (int i = 0; i < max; ++i) {
            try {
                Log.d(TAG, ""Create codec "" + name + "" #"" + i);
                codec = MediaCodec.createByCodecName(name);
                codec.configure(format, null, null, flag);
                codec.start();
                codecs.add(codec);
                codec = null;

                am.getMemoryInfo(outInfo);
                if (outInfo.lowMemory) {
                    Log.d(TAG, ""System is in low memory condition, stopping. max: "" + i);
                    memory_limited = true;
                    break;
                }
            } catch (IllegalArgumentException e) {
                fail(""Got unexpected IllegalArgumentException "" + e.getMessage());
            } catch (IOException e) {
                fail(""Got unexpected IOException "" + e.getMessage());
            } catch (MediaCodec.CodecException e) {
                // ERROR_INSUFFICIENT_RESOURCE is expected as the test keep creating codecs.
                // But other exception should be treated as failure.
                if (e.getErrorCode() == MediaCodec.CodecException.ERROR_INSUFFICIENT_RESOURCE) {
                    Log.d(TAG, ""Got CodecException with ERROR_INSUFFICIENT_RESOURCE."");
                    break;
                } else {
                    fail(""Unexpected CodecException "" + e.getDiagnosticInfo());
                }
            } finally {
                if (codec != null) {
                    Log.d(TAG, ""release codec"");
                    codec.release();
                    codec = null;
                }
            }
        }
        int actualMax = codecs.size();
        for (int i = 0; i < codecs.size(); ++i) {
            Log.d(TAG, ""release codec #"" + i);
            codecs.get(i).release();
        }
        codecs.clear();
        // encode both actual max and whether we ran out of memory
        if (memory_limited) {
            actualMax = -actualMax;
        }
        return actualMax;
    }

    private boolean knownTypes(String type) {
        return (type.equalsIgnoreCase(MediaFormat.MIMETYPE_AUDIO_AAC  ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_AUDIO_AC3      ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_AUDIO_AMR_NB   ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_AUDIO_AMR_WB   ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_AUDIO_EAC3     ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_AUDIO_FLAC     ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_AUDIO_G711_ALAW) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_AUDIO_G711_MLAW) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_AUDIO_MPEG     ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_AUDIO_MSGSM    ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_AUDIO_OPUS     ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_AUDIO_RAW      ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_AUDIO_VORBIS   ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_VIDEO_AV1      ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_VIDEO_AVC      ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_VIDEO_H263     ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_VIDEO_HEVC     ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_VIDEO_MPEG2    ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_VIDEO_MPEG4    ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_VIDEO_VP8      ) ||
            type.equalsIgnoreCase(MediaFormat.MIMETYPE_VIDEO_VP9      ));
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaCodecCapabilitiesTest"	"testIsSampleRateSupported"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaCodecCapabilitiesTest.java"	""	"public void testIsSampleRateSupported() throws IOException {
        if (!MediaUtils.checkDecoder(MediaFormat.MIMETYPE_AUDIO_AAC)) {
            return; // skip
        }
        // Chose AAC Decoder/MediaFormat.MIMETYPE_AUDIO_AAC randomly
        MediaCodec codec = MediaCodec.createDecoderByType(MediaFormat.MIMETYPE_AUDIO_AAC);
        MediaCodecInfo.AudioCapabilities audioCap = codec.getCodecInfo()
                    .getCapabilitiesForType(MediaFormat.MIMETYPE_AUDIO_AAC).getAudioCapabilities();
        final int[] validSampleRates = {8000, 16000, 22050, 44100};
        for(int sampleRate : validSampleRates) {
            Log.d(TAG, ""SampleRate = "" + sampleRate);
            assertTrue(""Expected True for isSampleRateSupported"",
                audioCap.isSampleRateSupported(sampleRate));
        }
        final int[] invalidSampleRates = {-1, 0, 1, Integer.MAX_VALUE};
        for(int sampleRate : invalidSampleRates) {
            Log.d(TAG, ""SampleRate = "" + sampleRate);
            assertFalse(""Expected False for isSampleRateSupported"",
                audioCap.isSampleRateSupported(sampleRate));
        }
        codec.release();
    }

    // API test coverage for MediaCodecInfo.EncoderCapabilities.getComplexityRange()"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaCodecCapabilitiesTest"	"testGetComplexityRange"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaCodecCapabilitiesTest.java"	""	"public void testGetComplexityRange() throws IOException {
        boolean skipTest = true;
        if (MediaUtils.hasEncoder(MediaFormat.MIMETYPE_AUDIO_AAC)) {
            // Chose AAC Encoder/MediaFormat.MIMETYPE_AUDIO_AAC randomly
            MediaCodec codec = MediaCodec.createEncoderByType(MediaFormat.MIMETYPE_AUDIO_AAC);
            Range<Integer> complexityRange =
                    codec.getCodecInfo()
                            .getCapabilitiesForType(MediaFormat.MIMETYPE_AUDIO_AAC)
                            .getEncoderCapabilities()
                            .getComplexityRange();
            Log.d(TAG, ""AAC ComplexityRange : "" + complexityRange.toString());
            assertTrue(""AAC ComplexityRange invalid low value"", complexityRange.getLower() >= 0);
            codec.release();
            skipTest = false;
        }
        if (MediaUtils.hasEncoder(MediaFormat.MIMETYPE_AUDIO_FLAC)) {
            // Repeat test with FLAC Encoder
            MediaCodec codec = MediaCodec.createEncoderByType(MediaFormat.MIMETYPE_AUDIO_FLAC);
            Range<Integer> complexityRange =
                    codec.getCodecInfo()
                            .getCapabilitiesForType(MediaFormat.MIMETYPE_AUDIO_FLAC)
                            .getEncoderCapabilities()
                            .getComplexityRange();
            Log.d(TAG, ""FLAC ComplexityRange : "" + complexityRange.toString());
            assertTrue(""FLAC ComplexityRange invalid low value"", complexityRange.getLower() >= 0);
            codec.release();
            skipTest = false;
        }
        if (skipTest) {
            MediaUtils.skipTest(TAG, ""AAC and FLAC encoders not present"");
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaRandomTest"	"testPlayerRandomActionH264"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaRandomTest.java"	""	"public void testPlayerRandomActionH264() throws Exception {
        testPlayerRandomAction(
                ""video_480x360_mp4_h264_500kbps_30fps_aac_stereo_128kbps_44100hz.mp4"");
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaRandomTest"	"testPlayerRandomActionHEVC"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaRandomTest.java"	""	"public void testPlayerRandomActionHEVC() throws Exception {
        testPlayerRandomAction(
                ""video_480x360_mp4_hevc_650kbps_30fps_aac_stereo_128kbps_48000hz.mp4"");
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaRandomTest"	"testPlayerRandomActionMpeg2"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaRandomTest.java"	""	"public void testPlayerRandomActionMpeg2() throws Exception {
        testPlayerRandomAction(
                ""video_480x360_mp4_mpeg2_1500kbps_30fps_aac_stereo_128kbps_48000hz.mp4"");
    }
    private void testPlayerRandomAction(final String res) throws Exception {
        Watchdog watchdog = new Watchdog(5000);
        try {
            mPlayer.setOnErrorListener(new MediaPlayer.OnErrorListener() {
                @Override
                public boolean onError(MediaPlayer mp, int what, int extra) {
                    if (mPlayer == mp &&
                        what == MediaPlayer.MEDIA_ERROR_SERVER_DIED) {
                        Log.e(TAG, ""mediaserver process died"");
                        mMediaServerDied = true;
                    }
                    return true;
                }
            });
            loadSource(res);
            mPlayer.setDisplay(mSurfaceHolder);
            mPlayer.prepare();
            mPlayer.start();

            long seed = System.currentTimeMillis();
            Log.v(TAG, ""seed = "" + seed);
            Random r = new Random(seed);

            watchdog.start();
            for (int i = 0; i < NUMBER_OF_PLAYER_RANDOM_ACTIONS; i++){
                watchdog.ping();
                assertTrue(!mMediaServerDied);

                mAction = (int)(r.nextInt() % 12);
                mParam = (int)(r.nextInt() % 1000000);
                try {
                    switch (mAction) {
                    case 0:
                        mPlayer.getCurrentPosition();
                        break;
                    case 1:
                        mPlayer.getDuration();
                        break;
                    case 2:
                        mPlayer.getVideoHeight();
                        break;
                    case 3:
                        mPlayer.getVideoWidth();
                       break;
                    case 4:
                        mPlayer.isPlaying();
                        break;
                    case 5:
                        mPlayer.pause();
                        break;
                    case 6:
                        // Don't add mPlayer.prepare() call here for two reasons:
                        // 1. calling prepare() is a bad idea since it is a blocking call, and
                        // 2. when prepare() is in progress, mediaserver died message will not be sent to apps
                        mPlayer.prepareAsync();
                        break;
                    case 7:
                        mPlayer.seekTo((int)(mParam));
                        break;
                    case 8:
                        mPlayer.setLooping(mParam % 2 == 0);
                        break;
                    case 9:
                        mPlayer.setVolume((mParam % 1000) / 500.0f,
                                     (mParam / 1000) / 500.0f);
                        break;
                    case 10:
                        mPlayer.start();
                        break;
                    case 11:
                        Thread.sleep(mParam % 20);
                        break;
                    }
                } catch (Exception e) {
                }
            }
            mPlayer.stop();
        } catch (Exception e) {
            Log.v(TAG, e.toString());
        } finally {
            watchdog.end();
            watchdog.join();
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraTestUtils"	"ImageDropperListener"	""	"/home/gpoor/cts-12-source/cts/tests/camera/utils/src/android/hardware/camera2/cts/CameraTestUtils.java"	""	"public void test/*
 *.
 */

package android.hardware.camera2.cts;

import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.graphics.ImageFormat;
import android.graphics.PointF;
import android.graphics.Rect;
import android.graphics.SurfaceTexture;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraConstrainedHighSpeedCaptureSession;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CameraMetadata;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CaptureFailure;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.MultiResolutionImageReader;
import android.hardware.camera2.cts.helpers.CameraErrorCollector;
import android.hardware.camera2.cts.helpers.StaticMetadata;
import android.hardware.camera2.params.InputConfiguration;
import android.hardware.camera2.TotalCaptureResult;
import android.hardware.cts.helpers.CameraUtils;
import android.hardware.camera2.params.MeteringRectangle;
import android.hardware.camera2.params.MandatoryStreamCombination;
import android.hardware.camera2.params.MandatoryStreamCombination.MandatoryStreamInformation;
import android.hardware.camera2.params.MultiResolutionStreamConfigurationMap;
import android.hardware.camera2.params.MultiResolutionStreamInfo;
import android.hardware.camera2.params.OutputConfiguration;
import android.hardware.camera2.params.SessionConfiguration;
import android.hardware.camera2.params.StreamConfigurationMap;
import android.location.Location;
import android.location.LocationManager;
import android.media.ExifInterface;
import android.media.Image;
import android.media.ImageReader;
import android.media.ImageWriter;
import android.media.Image.Plane;
import android.os.Build;
import android.os.ConditionVariable;
import android.os.Handler;
import android.util.Log;
import android.util.Pair;
import android.util.Size;
import android.util.Range;
import android.view.Display;
import android.view.Surface;
import android.view.WindowManager;

import com.android.ex.camera2.blocking.BlockingCameraManager;
import com.android.ex.camera2.blocking.BlockingCameraManager.BlockingOpenException;
import com.android.ex.camera2.blocking.BlockingSessionCallback;
import com.android.ex.camera2.blocking.BlockingStateCallback;
import com.android.ex.camera2.exceptions.TimeoutRuntimeException;

import junit.framework.Assert;

import org.mockito.Mockito;

import java.io.FileOutputStream;
import java.io.IOException;
import java.lang.reflect.Array;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.Comparator;
import java.util.Date;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.Executor;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.Semaphore;
import java.util.concurrent.TimeUnit;
import java.text.ParseException;
import java.text.SimpleDateFormat;

/**
 * A package private utility class for wrapping up the camera2 cts test common utility functions
 */
public class CameraTestUtils extends Assert {
    private static final String TAG = ""CameraTestUtils"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);
    private static final boolean DEBUG = Log.isLoggable(TAG, Log.DEBUG);
    public static final Size SIZE_BOUND_720P = new Size(1280, 720);
    public static final Size SIZE_BOUND_1080P = new Size(1920, 1088);
    public static final Size SIZE_BOUND_2K = new Size(2048, 1088);
    public static final Size SIZE_BOUND_QHD = new Size(2560, 1440);
    public static final Size SIZE_BOUND_2160P = new Size(3840, 2160);
    // Only test the preview size that is no larger than 1080p.
    public static final Size PREVIEW_SIZE_BOUND = SIZE_BOUND_1080P;
    // Default timeouts for reaching various states
    public static final int CAMERA_OPEN_TIMEOUT_MS = 3000;
    public static final int CAMERA_CLOSE_TIMEOUT_MS = 3000;
    public static final int CAMERA_IDLE_TIMEOUT_MS = 3000;
    public static final int CAMERA_ACTIVE_TIMEOUT_MS = 1000;
    public static final int CAMERA_BUSY_TIMEOUT_MS = 1000;
    public static final int CAMERA_UNCONFIGURED_TIMEOUT_MS = 1000;
    public static final int CAMERA_CONFIGURE_TIMEOUT_MS = 3000;
    public static final int CAPTURE_RESULT_TIMEOUT_MS = 3000;
    public static final int CAPTURE_IMAGE_TIMEOUT_MS = 3000;

    public static final int SESSION_CONFIGURE_TIMEOUT_MS = 3000;
    public static final int SESSION_CLOSE_TIMEOUT_MS = 3000;
    public static final int SESSION_READY_TIMEOUT_MS = 5000;
    public static final int SESSION_ACTIVE_TIMEOUT_MS = 1000;

    public static final int MAX_READER_IMAGES = 5;

    // Compensate for the loss of ""sensitivity"" and ""sensitivityBoost""
    public static final int MAX_ISO_MISMATCH = 3;

    public static final String OFFLINE_CAMERA_ID = ""offline_camera_id"";
    public static final String REPORT_LOG_NAME = ""CtsCameraTestCases"";

    private static final int EXIF_DATETIME_LENGTH = 19;
    private static final int EXIF_DATETIME_ERROR_MARGIN_SEC = 60;
    private static final float EXIF_FOCAL_LENGTH_ERROR_MARGIN = 0.001f;
    private static final float EXIF_EXPOSURE_TIME_ERROR_MARGIN_RATIO = 0.05f;
    private static final float EXIF_EXPOSURE_TIME_MIN_ERROR_MARGIN_SEC = 0.002f;
    private static final float EXIF_APERTURE_ERROR_MARGIN = 0.001f;

    private static final float ZOOM_RATIO_THRESHOLD = 0.01f;

    private static final Location sTestLocation0 = new Location(LocationManager.GPS_PROVIDER);
    private static final Location sTestLocation1 = new Location(LocationManager.GPS_PROVIDER);
    private static final Location sTestLocation2 = new Location(LocationManager.NETWORK_PROVIDER);

    static {
        sTestLocation0.setTime(1199145600000L);
        sTestLocation0.setLatitude(37.736071);
        sTestLocation0.setLongitude(-122.441983);
        sTestLocation0.setAltitude(21.0);

        sTestLocation1.setTime(1199145601000L);
        sTestLocation1.setLatitude(0.736071);
        sTestLocation1.setLongitude(0.441983);
        sTestLocation1.setAltitude(1.0);

        sTestLocation2.setTime(1199145602000L);
        sTestLocation2.setLatitude(-89.736071);
        sTestLocation2.setLongitude(-179.441983);
        sTestLocation2.setAltitude(100000.0);
    }

    // Exif test data vectors.
    public static final ExifTestData[] EXIF_TEST_DATA = {
            new ExifTestData(
                    /*gpsLocation*/ sTestLocation0,
                    /* orientation */90,
                    /* jpgQuality */(byte) 80,
                    /* thumbQuality */(byte) 75),
            new ExifTestData(
                    /*gpsLocation*/ sTestLocation1,
                    /* orientation */180,
                    /* jpgQuality */(byte) 90,
                    /* thumbQuality */(byte) 85),
            new ExifTestData(
                    /*gpsLocation*/ sTestLocation2,
                    /* orientation */270,
                    /* jpgQuality */(byte) 100,
                    /* thumbQuality */(byte) 100)
    };

    /**
     * Create an {@link android.media.ImageReader} object and get the surface.
     *
     * @param size The size of this ImageReader to be created.
     * @param format The format of this ImageReader to be created
     * @param maxNumImages The max number of images that can be acquired simultaneously.
     * @param listener The listener used by this ImageReader to notify callbacks.
     * @param handler The handler to use for any listener callbacks.
     */
    public static ImageReader makeImageReader(Size size, int format, int maxNumImages,
            ImageReader.OnImageAvailableListener listener, Handler handler) {
        ImageReader reader;
        reader = ImageReader.newInstance(size.getWidth(), size.getHeight(), format,
                maxNumImages);
        reader.setOnImageAvailableListener(listener, handler);
        if (VERBOSE) Log.v(TAG, ""Created ImageReader size "" + size);
        return reader;
    }

    /**
     * Create an ImageWriter and hook up the ImageListener.
     *
     * @param inputSurface The input surface of the ImageWriter.
     * @param maxImages The max number of Images that can be dequeued simultaneously.
     * @param listener The listener used by this ImageWriter to notify callbacks
     * @param handler The handler to post listener callbacks.
     * @return ImageWriter object created.
     */
    public static ImageWriter makeImageWriter(
            Surface inputSurface, int maxImages,
            ImageWriter.OnImageReleasedListener listener, Handler handler) {
        ImageWriter writer = ImageWriter.newInstance(inputSurface, maxImages);
        writer.setOnImageReleasedListener(listener, handler);
        return writer;
    }

    /**
     * Utility class to store the targets for mandatory stream combination test.
     */
    public static class StreamCombinationTargets {
        public List<SurfaceTexture> mPrivTargets = new ArrayList<>();
        public List<ImageReader> mJpegTargets = new ArrayList<>();
        public List<ImageReader> mYuvTargets = new ArrayList<>();
        public List<ImageReader> mY8Targets = new ArrayList<>();
        public List<ImageReader> mRawTargets = new ArrayList<>();
        public List<ImageReader> mHeicTargets = new ArrayList<>();
        public List<ImageReader> mDepth16Targets = new ArrayList<>();

        public List<MultiResolutionImageReader> mPrivMultiResTargets = new ArrayList<>();
        public List<MultiResolutionImageReader> mJpegMultiResTargets = new ArrayList<>();
        public List<MultiResolutionImageReader> mYuvMultiResTargets = new ArrayList<>();
        public List<MultiResolutionImageReader> mRawMultiResTargets = new ArrayList<>();

        public void close() {
            for (SurfaceTexture target : mPrivTargets) {
                target.release();
            }
            for (ImageReader target : mJpegTargets) {
                target.close();
            }
            for (ImageReader target : mYuvTargets) {
                target.close();
            }
            for (ImageReader target : mY8Targets) {
                target.close();
            }
            for (ImageReader target : mRawTargets) {
                target.close();
            }
            for (ImageReader target : mHeicTargets) {
                target.close();
            }
            for (ImageReader target : mDepth16Targets) {
                target.close();
            }

            for (MultiResolutionImageReader target : mPrivMultiResTargets) {
                target.close();
            }
            for (MultiResolutionImageReader target : mJpegMultiResTargets) {
                target.close();
            }
            for (MultiResolutionImageReader target : mYuvMultiResTargets) {
                target.close();
            }
            for (MultiResolutionImageReader target : mRawMultiResTargets) {
                target.close();
            }
        }
    }

    private static void configureTarget(StreamCombinationTargets targets,
            List<OutputConfiguration> outputConfigs, List<Surface> outputSurfaces,
            int format, Size targetSize, int numBuffers, String overridePhysicalCameraId,
            MultiResolutionStreamConfigurationMap multiResStreamConfig,
            boolean createMultiResiStreamConfig, ImageDropperListener listener, Handler handler) {
        if (createMultiResiStreamConfig) {
            Collection<MultiResolutionStreamInfo> multiResolutionStreams =
                    multiResStreamConfig.getOutputInfo(format);
            MultiResolutionImageReader multiResReader = new MultiResolutionImageReader(
                    multiResolutionStreams, format, numBuffers);
            multiResReader.setOnImageAvailableListener(listener, new HandlerExecutor(handler));
            Collection<OutputConfiguration> configs =
                    OutputConfiguration.createInstancesForMultiResolutionOutput(multiResReader);
            outputConfigs.addAll(configs);
            outputSurfaces.add(multiResReader.getSurface());
            switch (format) {
                case ImageFormat.PRIVATE:
                    targets.mPrivMultiResTargets.add(multiResReader);
                    break;
                case ImageFormat.JPEG:
                    targets.mJpegMultiResTargets.add(multiResReader);
                    break;
                case ImageFormat.YUV_420_888:
                    targets.mYuvMultiResTargets.add(multiResReader);
                    break;
                case ImageFormat.RAW_SENSOR:
                    targets.mRawMultiResTargets.add(multiResReader);
                    break;
                default:
                    fail(""Unknown/Unsupported output format "" + format);
            }
        } else {
            if (format == ImageFormat.PRIVATE) {
                SurfaceTexture target = new SurfaceTexture(/*random int*/1);
                target.setDefaultBufferSize(targetSize.getWidth(), targetSize.getHeight());
                OutputConfiguration config = new OutputConfiguration(new Surface(target));
                if (overridePhysicalCameraId != null) {
                    config.setPhysicalCameraId(overridePhysicalCameraId);
                }
                outputConfigs.add(config);
                outputSurfaces.add(config.getSurface());
                targets.mPrivTargets.add(target);
            } else {
                ImageReader target = ImageReader.newInstance(targetSize.getWidth(),
                        targetSize.getHeight(), format, numBuffers);
                target.setOnImageAvailableListener(listener, handler);
                OutputConfiguration config = new OutputConfiguration(target.getSurface());
                if (overridePhysicalCameraId != null) {
                    config.setPhysicalCameraId(overridePhysicalCameraId);
                }
                outputConfigs.add(config);
                outputSurfaces.add(config.getSurface());

                switch (format) {
                    case ImageFormat.JPEG:
                      targets.mJpegTargets.add(target);
                      break;
                    case ImageFormat.YUV_420_888:
                      targets.mYuvTargets.add(target);
                      break;
                    case ImageFormat.Y8:
                      targets.mY8Targets.add(target);
                      break;
                    case ImageFormat.RAW_SENSOR:
                      targets.mRawTargets.add(target);
                      break;
                    case ImageFormat.HEIC:
                      targets.mHeicTargets.add(target);
                      break;
                    case ImageFormat.DEPTH16:
                      targets.mDepth16Targets.add(target);
                      break;
                    default:
                      fail(""Unknown/Unsupported output format "" + format);
                }
            }
        }
    }

    public static void setupConfigurationTargets(List<MandatoryStreamInformation> streamsInfo,
            StreamCombinationTargets targets,
            List<OutputConfiguration> outputConfigs,
            List<Surface> outputSurfaces, int numBuffers,
            boolean substituteY8, boolean substituteHeic, String overridenPhysicalCameraId,
            MultiResolutionStreamConfigurationMap multiResStreamConfig, Handler handler) {
            List<Surface> uhSurfaces = new ArrayList<Surface>();
        setupConfigurationTargets(streamsInfo, targets, outputConfigs, outputSurfaces, uhSurfaces,
            numBuffers, substituteY8, substituteHeic, overridenPhysicalCameraId,
            multiResStreamConfig, handler);
    }

    public static void setupConfigurationTargets(List<MandatoryStreamInformation> streamsInfo,
            StreamCombinationTargets targets,
            List<OutputConfiguration> outputConfigs,
            List<Surface> outputSurfaces, List<Surface> uhSurfaces, int numBuffers,
            boolean substituteY8, boolean substituteHeic, String overridePhysicalCameraId,
            MultiResolutionStreamConfigurationMap multiResStreamConfig, Handler handler) {

        ImageDropperListener imageDropperListener = new ImageDropperListener();
        List<Surface> chosenSurfaces;
        for (MandatoryStreamInformation streamInfo : streamsInfo) {
            if (streamInfo.isInput()) {
                continue;
            }
            chosenSurfaces = outputSurfaces;
            if (streamInfo.isUltraHighResolution()) {
                chosenSurfaces = uhSurfaces;
            }
            int format = streamInfo.getFormat();
            if (substituteY8 && (format == ImageFormat.YUV_420_888)) {
                format = ImageFormat.Y8;
            } else if (substituteHeic && (format == ImageFormat.JPEG)) {
                format = ImageFormat.HEIC;
            }
            Size[] availableSizes = new Size[streamInfo.getAvailableSizes().size()];
            availableSizes = streamInfo.getAvailableSizes().toArray(availableSizes);
            Size targetSize = CameraTestUtils.getMaxSize(availableSizes);
            boolean createMultiResReader =
                    (multiResStreamConfig != null &&
                     !multiResStreamConfig.getOutputInfo(format).isEmpty() &&
                     streamInfo.isMaximumSize());
            switch (format) {
                case ImageFormat.PRIVATE:
                case ImageFormat.JPEG:
                case ImageFormat.YUV_420_888:
                case ImageFormat.Y8:
                case ImageFormat.HEIC:
                case ImageFormat.DEPTH16:
                {
                    configureTarget(targets, outputConfigs, chosenSurfaces, format,
                            targetSize, numBuffers, overridePhysicalCameraId, multiResStreamConfig,
                            createMultiResReader, imageDropperListener, handler);
                    break;
                }
                case ImageFormat.RAW_SENSOR: {
                    // targetSize could be null in the logical camera case where only
                    // physical camera supports RAW stream.
                    if (targetSize != null) {
                        configureTarget(targets, outputConfigs, chosenSurfaces, format,
                                targetSize, numBuffers, overridePhysicalCameraId,
                                multiResStreamConfig, createMultiResReader, imageDropperListener,
                                handler);
                    }
                    break;
                }
                default:
                    fail(""Unknown output format "" + format);
            }
        }
    }

    /**
     * Close pending images and clean up an {@link android.media.ImageReader} object.
     * @param reader an {@link android.media.ImageReader} to close.
     */
    public static void closeImageReader(ImageReader reader) {
        if (reader != null) {
            reader.close();
        }
    }

    /**
     * Close the pending images then close current active {@link ImageReader} objects.
     */
    public static void closeImageReaders(ImageReader[] readers) {
        if ((readers != null) && (readers.length > 0)) {
            for (ImageReader reader : readers) {
                CameraTestUtils.closeImageReader(reader);
            }
        }
    }

    /**
     * Close pending images and clean up an {@link android.media.ImageWriter} object.
     * @param writer an {@link android.media.ImageWriter} to close.
     */
    public static void closeImageWriter(ImageWriter writer) {
        if (writer != null) {
            writer.close();
        }
    }

    /**
     * Dummy listener that release the image immediately once it is available.
     *
     * <p>
     * It can be used for the case where we don't care the image data at all.
     * </p>
     */
    public static class ImageDropperListener implements ImageReader.OnImageAvailableListener {
        @Override
        public synchronized void onImageAvailable(ImageReader reader) {
            Image image = null;
            try {
                image = reader.acquireNextImage();
            } finally {
                if (image != null) {
                    image.close();
                    mImagesDropped++;
                }
            }
        }

        public synchronized int getImageCount() {
            return mImagesDropped;
        }

        public synchronized void resetImageCount() {
            mImagesDropped = 0;
        }

        private int mImagesDropped = 0;
    }

    /**
     * Image listener that release the image immediately after validating the image
     */
    public static class ImageVerifierListener implements ImageReader.OnImageAvailableListener {
        private Size mSize;
        private int mFormat;
        // Whether the parent ImageReader is valid or not. If the parent ImageReader
        // is destroyed, the acquired Image may become invalid.
        private boolean mReaderIsValid;

        public ImageVerifierListener(Size sz, int format) {
            mSize = sz;
            mFormat = format;
            mReaderIsValid = true;
        }

        public synchronized void onReaderDestroyed() {
            mReaderIsValid = false;
        }

        @Override
        public synchronized void onImageAvailable(ImageReader reader) {
            Image image = null;
            try {
                image = reader.acquireNextImage();
            } finally {
                if (image != null) {
                    // Should only do some quick validity checks in callback, as the ImageReader
                    // could be closed asynchronously, which will close all images acquired from
                    // this ImageReader.
                    checkImage(image, mSize.getWidth(), mSize.getHeight(), mFormat);
                    // checkAndroidImageFormat calls into underlying Image object, which could
                    // become invalid if the ImageReader is destroyed.
                    if (mReaderIsValid) {
                        checkAndroidImageFormat(image);
                    }
                    image.close();
                }
            }
        }
    }

    public static class SimpleImageReaderListener
            implements ImageReader.OnImageAvailableListener {
        private final LinkedBlockingQueue<Image> mQueue =
                new LinkedBlockingQueue<Image>();
        // Indicate whether this listener will drop images or not,
        // when the queued images reaches the reader maxImages
        private final boolean mAsyncMode;
        // maxImages held by the queue in async mode.
        private final int mMaxImages;

        /**
         * Create a synchronous SimpleImageReaderListener that queues the images
         * automatically when they are available, no image will be dropped. If
         * the caller doesn't call getImage(), the producer will eventually run
         * into buffer starvation.
         */
        public SimpleImageReaderListener() {
            mAsyncMode = false;
            mMaxImages = 0;
        }

        /**
         * Create a synchronous/asynchronous SimpleImageReaderListener that
         * queues the images automatically when they are available. For
         * asynchronous listener, image will be dropped if the queued images
         * reach to maxImages queued. If the caller doesn't call getImage(), the
         * producer will not be blocked. For synchronous listener, no image will
         * be dropped. If the caller doesn't call getImage(), the producer will
         * eventually run into buffer starvation.
         *
         * @param asyncMode If the listener is operating at asynchronous mode.
         * @param maxImages The max number of images held by this listener.
         */
        /**
         *
         * @param asyncMode
         */
        public SimpleImageReaderListener(boolean asyncMode, int maxImages) {
            mAsyncMode = asyncMode;
            mMaxImages = maxImages;
        }

        @Override
        public void onImageAvailable(ImageReader reader) {
            try {
                Image imge = reader.acquireNextImage();
                if (imge == null) {
                    return;
                }
                mQueue.put(imge);
                if (mAsyncMode && mQueue.size() >= mMaxImages) {
                    Image img = mQueue.poll();
                    img.close();
                }
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onImageAvailable"");
            }
        }

        /**
         * Get an image from the image reader.
         *
         * @param timeout Timeout value for the wait.
         * @return The image from the image reader.
         */
        public Image getImage(long timeout) throws InterruptedException {
            Image image = mQueue.poll(timeout, TimeUnit.MILLISECONDS);
            assertNotNull(""Wait for an image timed out in "" + timeout + ""ms"", image);
            return image;
        }

        /**
         * Drain the pending images held by this listener currently.
         *
         */
        public void drain() {
            while (!mQueue.isEmpty()) {
                Image image = mQueue.poll();
                assertNotNull(""Unable to get an image"", image);
                image.close();
            }
        }
    }

    public static class SimpleImageWriterListener implements ImageWriter.OnImageReleasedListener {
        private final Semaphore mImageReleasedSema = new Semaphore(0);
        private final ImageWriter mWriter;
        @Override
        public void onImageReleased(ImageWriter writer) {
            if (writer != mWriter) {
                return;
            }

            if (VERBOSE) {
                Log.v(TAG, ""Input image is released"");
            }
            mImageReleasedSema.release();
        }

        public SimpleImageWriterListener(ImageWriter writer) {
            if (writer == null) {
                throw new IllegalArgumentException(""writer cannot be null"");
            }
            mWriter = writer;
        }

        public void waitForImageReleased(long timeoutMs) throws InterruptedException {
            if (!mImageReleasedSema.tryAcquire(timeoutMs, TimeUnit.MILLISECONDS)) {
                fail(""wait for image available timed out after "" + timeoutMs + ""ms"");
            }
        }
    }

    public static class ImageAndMultiResStreamInfo {
        public final Image image;
        public final MultiResolutionStreamInfo streamInfo;

        public ImageAndMultiResStreamInfo(Image image, MultiResolutionStreamInfo streamInfo) {
            this.image = image;
            this.streamInfo = streamInfo;
        }
    }

    public static class SimpleMultiResolutionImageReaderListener
            implements ImageReader.OnImageAvailableListener {
        public SimpleMultiResolutionImageReaderListener(MultiResolutionImageReader owner,
                int maxBuffers, boolean acquireLatest) {
            mOwner = owner;
            mMaxBuffers = maxBuffers;
            mAcquireLatest = acquireLatest;
        }

        @Override
        public void onImageAvailable(ImageReader reader) {
            if (VERBOSE) Log.v(TAG, ""new image available"");

            if (mAcquireLatest) {
                mLastReader = reader;
                mImageAvailable.open();
            } else {
                if (mQueue.size() < mMaxBuffers) {
                    Image image = reader.acquireNextImage();
                    MultiResolutionStreamInfo multiResStreamInfo =
                            mOwner.getStreamInfoForImageReader(reader);
                    mQueue.offer(new ImageAndMultiResStreamInfo(image, multiResStreamInfo));
                }
            }
        }

        public ImageAndMultiResStreamInfo getAnyImageAndInfoAvailable(long timeoutMs)
                throws Exception {
            if (mAcquireLatest) {
                Image image = null;
                if (mImageAvailable.block(timeoutMs)) {
                    if (mLastReader != null) {
                        image = mLastReader.acquireLatestImage();
                        if (VERBOSE) Log.v(TAG, ""acquireLatestImage"");
                    } else {
                        fail(""invalid image reader"");
                    }
                    mImageAvailable.close();
                } else {
                    fail(""wait for image available time out after "" + timeoutMs + ""ms"");
                }
                return new ImageAndMultiResStreamInfo(image,
                        mOwner.getStreamInfoForImageReader(mLastReader));
            } else {
                ImageAndMultiResStreamInfo imageAndInfo = mQueue.poll(timeoutMs,
                        java.util.concurrent.TimeUnit.MILLISECONDS);
                if (imageAndInfo == null) {
                    fail(""wait for image available timed out after "" + timeoutMs + ""ms"");
                }
                return imageAndInfo;
            }
        }

        public void reset() {
            while (!mQueue.isEmpty()) {
                ImageAndMultiResStreamInfo imageAndInfo = mQueue.poll();
                assertNotNull(""Acquired image is not valid"", imageAndInfo.image);
                imageAndInfo.image.close();
            }
            mImageAvailable.close();
            mLastReader = null;
        }

        private LinkedBlockingQueue<ImageAndMultiResStreamInfo> mQueue =
                new LinkedBlockingQueue<ImageAndMultiResStreamInfo>();
        private final MultiResolutionImageReader mOwner;
        private final int mMaxBuffers;
        private final boolean mAcquireLatest;
        private ConditionVariable mImageAvailable = new ConditionVariable();
        private ImageReader mLastReader = null;
    }

    public static class SimpleCaptureCallback extends CameraCaptureSession.CaptureCallback {
        private final LinkedBlockingQueue<TotalCaptureResult> mQueue =
                new LinkedBlockingQueue<TotalCaptureResult>();
        private final LinkedBlockingQueue<CaptureFailure> mFailureQueue =
                new LinkedBlockingQueue<>();
        // (Surface, framenumber) pair for lost buffers
        private final LinkedBlockingQueue<Pair<Surface, Long>> mBufferLostQueue =
                new LinkedBlockingQueue<>();
        private final LinkedBlockingQueue<Integer> mAbortQueue =
                new LinkedBlockingQueue<>();
        // Pair<CaptureRequest, Long> is a pair of capture request and timestamp.
        private final LinkedBlockingQueue<Pair<CaptureRequest, Long>> mCaptureStartQueue =
                new LinkedBlockingQueue<>();
        // Pair<Int, Long> is a pair of sequence id and frame number
        private final LinkedBlockingQueue<Pair<Integer, Long>> mCaptureSequenceCompletedQueue =
                new LinkedBlockingQueue<>();

        private AtomicLong mNumFramesArrived = new AtomicLong(0);

        @Override
        public void onCaptureStarted(CameraCaptureSession session, CaptureRequest request,
                long timestamp, long frameNumber) {
            try {
                mCaptureStartQueue.put(new Pair(request, timestamp));
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureStarted"");
            }
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                TotalCaptureResult result) {
            try {
                mNumFramesArrived.incrementAndGet();
                mQueue.put(result);
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureCompleted"");
            }
        }

        @Override
        public void onCaptureFailed(CameraCaptureSession session, CaptureRequest request,
                CaptureFailure failure) {
            try {
                mFailureQueue.put(failure);
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureFailed"");
            }
        }

        @Override
        public void onCaptureSequenceAborted(CameraCaptureSession session, int sequenceId) {
            try {
                mAbortQueue.put(sequenceId);
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureAborted"");
            }
        }

        @Override
        public void onCaptureSequenceCompleted(CameraCaptureSession session, int sequenceId,
                long frameNumber) {
            try {
                mCaptureSequenceCompletedQueue.put(new Pair(sequenceId, frameNumber));
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureSequenceCompleted"");
            }
        }

        @Override
        public void onCaptureBufferLost(CameraCaptureSession session,
                CaptureRequest request, Surface target, long frameNumber) {
            try {
                mBufferLostQueue.put(new Pair<>(target, frameNumber));
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureBufferLost"");
            }
        }

        public long getTotalNumFrames() {
            return mNumFramesArrived.get();
        }

        public CaptureResult getCaptureResult(long timeout) {
            return getTotalCaptureResult(timeout);
        }

        public TotalCaptureResult getCaptureResult(long timeout, long timestamp) {
            try {
                long currentTs = -1L;
                TotalCaptureResult result;
                while (true) {
                    result = mQueue.poll(timeout, TimeUnit.MILLISECONDS);
                    if (result == null) {
                        throw new RuntimeException(
                                ""Wait for a capture result timed out in "" + timeout + ""ms"");
                    }
                    currentTs = result.get(CaptureResult.SENSOR_TIMESTAMP);
                    if (currentTs == timestamp) {
                        return result;
                    }
                }

            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }
        }

        public TotalCaptureResult getTotalCaptureResult(long timeout) {
            try {
                TotalCaptureResult result = mQueue.poll(timeout, TimeUnit.MILLISECONDS);
                assertNotNull(""Wait for a capture result timed out in "" + timeout + ""ms"", result);
                return result;
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }
        }

        /**
         * Get the {@link #CaptureResult capture result} for a given
         * {@link #CaptureRequest capture request}.
         *
         * @param myRequest The {@link #CaptureRequest capture request} whose
         *            corresponding {@link #CaptureResult capture result} was
         *            being waited for
         * @param numResultsWait Number of frames to wait for the capture result
         *            before timeout.
         * @throws TimeoutRuntimeException If more than numResultsWait results are
         *            seen before the result matching myRequest arrives, or each
         *            individual wait for result times out after
         *            {@value #CAPTURE_RESULT_TIMEOUT_MS}ms.
         */
        public CaptureResult getCaptureResultForRequest(CaptureRequest myRequest,
                int numResultsWait) {
            return getTotalCaptureResultForRequest(myRequest, numResultsWait);
        }

        /**
         * Get the {@link #TotalCaptureResult total capture result} for a given
         * {@link #CaptureRequest capture request}.
         *
         * @param myRequest The {@link #CaptureRequest capture request} whose
         *            corresponding {@link #TotalCaptureResult capture result} was
         *            being waited for
         * @param numResultsWait Number of frames to wait for the capture result
         *            before timeout.
         * @throws TimeoutRuntimeException If more than numResultsWait results are
         *            seen before the result matching myRequest arrives, or each
         *            individual wait for result times out after
         *            {@value #CAPTURE_RESULT_TIMEOUT_MS}ms.
         */
        public TotalCaptureResult getTotalCaptureResultForRequest(CaptureRequest myRequest,
                int numResultsWait) {
            ArrayList<CaptureRequest> captureRequests = new ArrayList<>(1);
            captureRequests.add(myRequest);
            return getTotalCaptureResultsForRequests(captureRequests, numResultsWait)[0];
        }

        /**
         * Get an array of {@link #TotalCaptureResult total capture results} for a given list of
         * {@link #CaptureRequest capture requests}. This can be used when the order of results
         * may not the same as the order of requests.
         *
         * @param captureRequests The list of {@link #CaptureRequest capture requests} whose
         *            corresponding {@link #TotalCaptureResult capture results} are
         *            being waited for.
         * @param numResultsWait Number of frames to wait for the capture results
         *            before timeout.
         * @throws TimeoutRuntimeException If more than numResultsWait results are
         *            seen before all the results matching captureRequests arrives.
         */
        public TotalCaptureResult[] getTotalCaptureResultsForRequests(
                List<CaptureRequest> captureRequests, int numResultsWait) {
            if (numResultsWait < 0) {
                throw new IllegalArgumentException(""numResultsWait must be no less than 0"");
            }
            if (captureRequests == null || captureRequests.size() == 0) {
                throw new IllegalArgumentException(""captureRequests must have at least 1 request."");
            }

            // Create a request -> a list of result indices map that it will wait for.
            HashMap<CaptureRequest, ArrayList<Integer>> remainingResultIndicesMap = new HashMap<>();
            for (int i = 0; i < captureRequests.size(); i++) {
                CaptureRequest request = captureRequests.get(i);
                ArrayList<Integer> indices = remainingResultIndicesMap.get(request);
                if (indices == null) {
                    indices = new ArrayList<>();
                    remainingResultIndicesMap.put(request, indices);
                }
                indices.add(i);
            }

            TotalCaptureResult[] results = new TotalCaptureResult[captureRequests.size()];
            int i = 0;
            do {
                TotalCaptureResult result = getTotalCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);
                CaptureRequest request = result.getRequest();
                ArrayList<Integer> indices = remainingResultIndicesMap.get(request);
                if (indices != null) {
                    results[indices.get(0)] = result;
                    indices.remove(0);

                    // Remove the entry if all results for this request has been fulfilled.
                    if (indices.isEmpty()) {
                        remainingResultIndicesMap.remove(request);
                    }
                }

                if (remainingResultIndicesMap.isEmpty()) {
                    return results;
                }
            } while (i++ < numResultsWait);

            throw new TimeoutRuntimeException(""Unable to get the expected capture result after ""
                    + ""waiting for "" + numResultsWait + "" results"");
        }

        /**
         * Get an array list of {@link #CaptureFailure capture failure} with maxNumFailures entries
         * at most. If it times out before maxNumFailures failures are received, return the failures
         * received so far.
         *
         * @param maxNumFailures The maximal number of failures to return. If it times out before
         *                       the maximal number of failures are received, return the received
         *                       failures so far.
         * @throws UnsupportedOperationException If an error happens while waiting on the failure.
         */
        public ArrayList<CaptureFailure> getCaptureFailures(long maxNumFailures) {
            ArrayList<CaptureFailure> failures = new ArrayList<>();
            try {
                for (int i = 0; i < maxNumFailures; i++) {
                    CaptureFailure failure = mFailureQueue.poll(CAPTURE_RESULT_TIMEOUT_MS,
                            TimeUnit.MILLISECONDS);
                    if (failure == null) {
                        // If waiting on a failure times out, return the failures so far.
                        break;
                    }
                    failures.add(failure);
                }
            }  catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }

            return failures;
        }

        /**
         * Get an array list of lost buffers with maxNumLost entries at most.
         * If it times out before maxNumLost buffer lost callbacks are received, return the
         * lost callbacks received so far.
         *
         * @param maxNumLost The maximal number of buffer lost failures to return. If it times out
         *                   before the maximal number of failures are received, return the received
         *                   buffer lost failures so far.
         * @throws UnsupportedOperationException If an error happens while waiting on the failure.
         */
        public ArrayList<Pair<Surface, Long>> getLostBuffers(long maxNumLost) {
            ArrayList<Pair<Surface, Long>> failures = new ArrayList<>();
            try {
                for (int i = 0; i < maxNumLost; i++) {
                    Pair<Surface, Long> failure = mBufferLostQueue.poll(CAPTURE_RESULT_TIMEOUT_MS,
                            TimeUnit.MILLISECONDS);
                    if (failure == null) {
                        // If waiting on a failure times out, return the failures so far.
                        break;
                    }
                    failures.add(failure);
                }
            }  catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }

            return failures;
        }

        /**
         * Get an array list of aborted capture sequence ids with maxNumAborts entries
         * at most. If it times out before maxNumAborts are received, return the aborted sequences
         * received so far.
         *
         * @param maxNumAborts The maximal number of aborted sequences to return. If it times out
         *                     before the maximal number of aborts are received, return the received
         *                     failed sequences so far.
         * @throws UnsupportedOperationException If an error happens while waiting on the failed
         *                                       sequences.
         */
        public ArrayList<Integer> geAbortedSequences(long maxNumAborts) {
            ArrayList<Integer> abortList = new ArrayList<>();
            try {
                for (int i = 0; i < maxNumAborts; i++) {
                    Integer abortSequence = mAbortQueue.poll(CAPTURE_RESULT_TIMEOUT_MS,
                            TimeUnit.MILLISECONDS);
                    if (abortSequence == null) {
                        break;
                    }
                    abortList.add(abortSequence);
                }
            }  catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }

            return abortList;
        }

        /**
         * Wait until the capture start of a request and expected timestamp arrives or it times
         * out after a number of capture starts.
         *
         * @param request The request for the capture start to wait for.
         * @param timestamp The timestamp for the capture start to wait for.
         * @param numCaptureStartsWait The number of capture start events to wait for before timing
         *                             out.
         */
        public void waitForCaptureStart(CaptureRequest request, Long timestamp,
                int numCaptureStartsWait) throws Exception {
            Pair<CaptureRequest, Long> expectedShutter = new Pair<>(request, timestamp);

            int i = 0;
            do {
                Pair<CaptureRequest, Long> shutter = mCaptureStartQueue.poll(
                        CAPTURE_RESULT_TIMEOUT_MS, TimeUnit.MILLISECONDS);

                if (shutter == null) {
                    throw new TimeoutRuntimeException(""Unable to get any more capture start "" +
                            ""event after waiting for "" + CAPTURE_RESULT_TIMEOUT_MS + "" ms."");
                } else if (expectedShutter.equals(shutter)) {
                    return;
                }

            } while (i++ < numCaptureStartsWait);

            throw new TimeoutRuntimeException(""Unable to get the expected capture start "" +
                    ""event after waiting for "" + numCaptureStartsWait + "" capture starts"");
        }

        /**
         * Wait until it receives capture sequence completed callback for a given squence ID.
         *
         * @param sequenceId The sequence ID of the capture sequence completed callback to wait for.
         * @param timeoutMs Time to wait for each capture sequence complete callback before
         *                  timing out.
         */
        public long getCaptureSequenceLastFrameNumber(int sequenceId, long timeoutMs) {
            try {
                while (true) {
                    Pair<Integer, Long> completedSequence =
                            mCaptureSequenceCompletedQueue.poll(timeoutMs, TimeUnit.MILLISECONDS);
                    assertNotNull(""Wait for a capture sequence completed timed out in "" +
                            timeoutMs + ""ms"", completedSequence);

                    if (completedSequence.first.equals(sequenceId)) {
                        return completedSequence.second.longValue();
                    }
                }
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }
        }

        public boolean hasMoreResults()
        {
            return !mQueue.isEmpty();
        }

        public boolean hasMoreFailures()
        {
            return !mFailureQueue.isEmpty();
        }

        public int getNumLostBuffers()
        {
            return mBufferLostQueue.size();
        }

        public boolean hasMoreAbortedSequences()
        {
            return !mAbortQueue.isEmpty();
        }

        public void drain() {
            mQueue.clear();
            mNumFramesArrived.getAndSet(0);
            mFailureQueue.clear();
            mBufferLostQueue.clear();
            mCaptureStartQueue.clear();
            mAbortQueue.clear();
        }
    }

    public static boolean hasCapability(CameraCharacteristics characteristics, int capability) {
        int [] capabilities =
                characteristics.get(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
        for (int c : capabilities) {
            if (c == capability) {
                return true;
            }
        }
        return false;
    }

    public static boolean isSystemCamera(CameraManager manager, String cameraId)
            throws CameraAccessException {
        CameraCharacteristics characteristics = manager.getCameraCharacteristics(cameraId);
        return hasCapability(characteristics,
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_SYSTEM_CAMERA);
    }

    public static String[] getCameraIdListForTesting(CameraManager manager,
            boolean getSystemCameras)
            throws CameraAccessException {
        String [] ids = manager.getCameraIdListNoLazy();
        List<String> idsForTesting = new ArrayList<String>();
        for (String id : ids) {
            boolean isSystemCamera = isSystemCamera(manager, id);
            if (getSystemCameras == isSystemCamera) {
                idsForTesting.add(id);
            }
        }
        return idsForTesting.toArray(new String[idsForTesting.size()]);
    }

    public static Set<Set<String>> getConcurrentCameraIds(CameraManager manager,
            boolean getSystemCameras)
            throws CameraAccessException {
        Set<String> cameraIds = new HashSet<String>(Arrays.asList(getCameraIdListForTesting(manager, getSystemCameras)));
        Set<Set<String>> combinations =  manager.getConcurrentCameraIds();
        Set<Set<String>> correctComb = new HashSet<Set<String>>();
        for (Set<String> comb : combinations) {
            Set<String> filteredIds = new HashSet<String>();
            for (String id : comb) {
                if (cameraIds.contains(id)) {
                    filteredIds.add(id);
                }
            }
            if (filteredIds.isEmpty()) {
                continue;
            }
            correctComb.add(filteredIds);
        }
        return correctComb;
    }

    /**
     * Block until the camera is opened.
     *
     * <p>Don't use this to test #onDisconnected/#onError since this will throw
     * an AssertionError if it fails to open the camera device.</p>
     *
     * @return CameraDevice opened camera device
     *
     * @throws IllegalArgumentException
     *            If the handler is null, or if the handler's looper is current.
     * @throws CameraAccessException
     *            If open fails immediately.
     * @throws BlockingOpenException
     *            If open fails after blocking for some amount of time.
     * @throws TimeoutRuntimeException
     *            If opening times out. Typically unrecoverable.
     */
    public static CameraDevice openCamera(CameraManager manager, String cameraId,
            CameraDevice.StateCallback listener, Handler handler) throws CameraAccessException,
            BlockingOpenException {

        /**
         * Although camera2 API allows 'null' Handler (it will just use the current
         * thread's Looper), this is not what we want for CTS.
         *
         * In CTS the default looper is used only to process events in between test runs,
         * so anything sent there would not be executed inside a test and the test would fail.
         *
         * In this case, BlockingCameraManager#openCamera performs the check for us.
         */
        return (new BlockingCameraManager(manager)).openCamera(cameraId, listener, handler);
    }


    /**
     * Block until the camera is opened.
     *
     * <p>Don't use this to test #onDisconnected/#onError since this will throw
     * an AssertionError if it fails to open the camera device.</p>
     *
     * @throws IllegalArgumentException
     *            If the handler is null, or if the handler's looper is current.
     * @throws CameraAccessException
     *            If open fails immediately.
     * @throws BlockingOpenException
     *            If open fails after blocking for some amount of time.
     * @throws TimeoutRuntimeException
     *            If opening times out. Typically unrecoverable.
     */
    public static CameraDevice openCamera(CameraManager manager, String cameraId, Handler handler)
            throws CameraAccessException,
            BlockingOpenException {
        return openCamera(manager, cameraId, /*listener*/null, handler);
    }

    /**
     * Configure a new camera session with output surfaces and type.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputSurfaces The surface list that used for camera output.
     * @param listener The callback CameraDevice will notify when capture results are available.
     */
    public static CameraCaptureSession configureCameraSession(CameraDevice camera,
            List<Surface> outputSurfaces, boolean isHighSpeed,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);
        if (isHighSpeed) {
            camera.createConstrainedHighSpeedCaptureSession(outputSurfaces,
                    sessionListener, handler);
        } else {
            camera.createCaptureSession(outputSurfaces, sessionListener, handler);
        }
        CameraCaptureSession session =
                sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertFalse(""Camera session should not be a reprocessable session"",
                session.isReprocessable());
        String sessionType = isHighSpeed ? ""High Speed"" : ""Normal"";
        assertTrue(""Capture session type must be "" + sessionType,
                isHighSpeed ==
                CameraConstrainedHighSpeedCaptureSession.class.isAssignableFrom(session.getClass()));

        return session;
    }

    /**
     * Build a new constrained camera session with output surfaces, type and recording session
     * parameters.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputSurfaces The surface list that used for camera output.
     * @param listener The callback CameraDevice will notify when capture results are available.
     * @param initialRequest Initial request settings to use as session parameters.
     */
    public static CameraCaptureSession buildConstrainedCameraSession(CameraDevice camera,
            List<Surface> outputSurfaces, CameraCaptureSession.StateCallback listener,
            Handler handler, CaptureRequest initialRequest) throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);

        List<OutputConfiguration> outConfigurations = new ArrayList<>(outputSurfaces.size());
        for (Surface surface : outputSurfaces) {
            outConfigurations.add(new OutputConfiguration(surface));
        }
        SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_HIGH_SPEED, outConfigurations,
                new HandlerExecutor(handler), sessionListener);
        sessionConfig.setSessionParameters(initialRequest);
        camera.createCaptureSession(sessionConfig);

        CameraCaptureSession session =
                sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertFalse(""Camera session should not be a reprocessable session"",
                session.isReprocessable());
        assertTrue(""Capture session type must be High Speed"",
                CameraConstrainedHighSpeedCaptureSession.class.isAssignableFrom(
                        session.getClass()));

        return session;
    }

    /**
     * Configure a new camera session with output configurations.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputs The OutputConfiguration list that is used for camera output.
     * @param listener The callback CameraDevice will notify when capture results are available.
     */
    public static CameraCaptureSession configureCameraSessionWithConfig(CameraDevice camera,
            List<OutputConfiguration> outputs,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);
        camera.createCaptureSessionByOutputConfigurations(outputs, sessionListener, handler);
        CameraCaptureSession session =
                sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertFalse(""Camera session should not be a reprocessable session"",
                session.isReprocessable());
        return session;
    }

    /**
     * Try configure a new camera session with output configurations.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputs The OutputConfiguration list that is used for camera output.
     * @param initialRequest The session parameters passed in during stream configuration
     * @param listener The callback CameraDevice will notify when capture results are available.
     */
    public static CameraCaptureSession tryConfigureCameraSessionWithConfig(CameraDevice camera,
            List<OutputConfiguration> outputs, CaptureRequest initialRequest,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);
        SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_REGULAR, outputs, new HandlerExecutor(handler),
                sessionListener);
        sessionConfig.setSessionParameters(initialRequest);
        camera.createCaptureSession(sessionConfig);

        Integer[] sessionStates = {BlockingSessionCallback.SESSION_READY,
                                   BlockingSessionCallback.SESSION_CONFIGURE_FAILED};
        int state = sessionListener.getStateWaiter().waitForAnyOfStates(
                Arrays.asList(sessionStates), SESSION_CONFIGURE_TIMEOUT_MS);

        CameraCaptureSession session = null;
        if (state == BlockingSessionCallback.SESSION_READY) {
            session = sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
            assertFalse(""Camera session should not be a reprocessable session"",
                    session.isReprocessable());
        }
        return session;
    }

    /**
     * Configure a new camera session with output surfaces and initial session parameters.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputSurfaces The surface list that used for camera output.
     * @param listener The callback CameraDevice will notify when session is available.
     * @param handler The handler used to notify callbacks.
     * @param initialRequest Initial request settings to use as session parameters.
     */
    public static CameraCaptureSession configureCameraSessionWithParameters(CameraDevice camera,
            List<Surface> outputSurfaces, BlockingSessionCallback listener,
            Handler handler, CaptureRequest initialRequest) throws CameraAccessException {
        List<OutputConfiguration> outConfigurations = new ArrayList<>(outputSurfaces.size());
        for (Surface surface : outputSurfaces) {
            outConfigurations.add(new OutputConfiguration(surface));
        }
        SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_REGULAR, outConfigurations,
                new HandlerExecutor(handler), listener);
        sessionConfig.setSessionParameters(initialRequest);
        camera.createCaptureSession(sessionConfig);

        CameraCaptureSession session = listener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertFalse(""Camera session should not be a reprocessable session"",
                session.isReprocessable());
        assertFalse(""Capture session type must be regular"",
                CameraConstrainedHighSpeedCaptureSession.class.isAssignableFrom(
                        session.getClass()));

        return session;
    }

    /**
     * Configure a new camera session with output surfaces.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputSurfaces The surface list that used for camera output.
     * @param listener The callback CameraDevice will notify when capture results are available.
     */
    public static CameraCaptureSession configureCameraSession(CameraDevice camera,
            List<Surface> outputSurfaces,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {

        return configureCameraSession(camera, outputSurfaces, /*isHighSpeed*/false,
                listener, handler);
    }

    public static CameraCaptureSession configureReprocessableCameraSession(CameraDevice camera,
            InputConfiguration inputConfiguration, List<Surface> outputSurfaces,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {
        List<OutputConfiguration> outputConfigs = new ArrayList<OutputConfiguration>();
        for (Surface surface : outputSurfaces) {
            outputConfigs.add(new OutputConfiguration(surface));
        }
        CameraCaptureSession session = configureReprocessableCameraSessionWithConfigurations(
                camera, inputConfiguration, outputConfigs, listener, handler);

        return session;
    }

    public static CameraCaptureSession configureReprocessableCameraSessionWithConfigurations(
            CameraDevice camera, InputConfiguration inputConfiguration,
            List<OutputConfiguration> outputConfigs, CameraCaptureSession.StateCallback listener,
            Handler handler) throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);
        SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_REGULAR, outputConfigs, new HandlerExecutor(handler),
                sessionListener);
        sessionConfig.setInputConfiguration(inputConfiguration);
        camera.createCaptureSession(sessionConfig);

        Integer[] sessionStates = {BlockingSessionCallback.SESSION_READY,
                                   BlockingSessionCallback.SESSION_CONFIGURE_FAILED};
        int state = sessionListener.getStateWaiter().waitForAnyOfStates(
                Arrays.asList(sessionStates), SESSION_CONFIGURE_TIMEOUT_MS);

        assertTrue(""Creating a reprocessable session failed."",
                state == BlockingSessionCallback.SESSION_READY);
        CameraCaptureSession session =
                sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertTrue(""Camera session should be a reprocessable session"", session.isReprocessable());

        return session;
    }

    /**
     * Create a reprocessable camera session with input and output configurations.
     *
     * @param camera The CameraDevice to be configured.
     * @param inputConfiguration The input configuration used to create this session.
     * @param outputs The output configurations used to create this session.
     * @param listener The callback CameraDevice will notify when capture results are available.
     * @param handler The handler used to notify callbacks.
     * @return The session ready to use.
     * @throws CameraAccessException
     */
    public static CameraCaptureSession configureReprocCameraSessionWithConfig(CameraDevice camera,
            InputConfiguration inputConfiguration, List<OutputConfiguration> outputs,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);
        camera.createReprocessableCaptureSessionByConfigurations(inputConfiguration, outputs,
                sessionListener, handler);

        Integer[] sessionStates = {BlockingSessionCallback.SESSION_READY,
                                   BlockingSessionCallback.SESSION_CONFIGURE_FAILED};
        int state = sessionListener.getStateWaiter().waitForAnyOfStates(
                Arrays.asList(sessionStates), SESSION_CONFIGURE_TIMEOUT_MS);

        assertTrue(""Creating a reprocessable session failed."",
                state == BlockingSessionCallback.SESSION_READY);

        CameraCaptureSession session =
                sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertTrue(""Camera session should be a reprocessable session"", session.isReprocessable());

        return session;
    }

    public static <T> void assertArrayNotEmpty(T arr, String message) {
        assertTrue(message, arr != null && Array.getLength(arr) > 0);
    }

    /**
     * Check if the format is a legal YUV format camera supported.
     */
    public static void checkYuvFormat(int format) {
        if ((format != ImageFormat.YUV_420_888) &&
                (format != ImageFormat.NV21) &&
                (format != ImageFormat.YV12)) {
            fail(""Wrong formats: "" + format);
        }
    }

    /**
     * Check if image size and format match given size and format.
     */
    public static void checkImage(Image image, int width, int height, int format) {
        // Image reader will wrap YV12/NV21 image by YUV_420_888
        if (format == ImageFormat.NV21 || format == ImageFormat.YV12) {
            format = ImageFormat.YUV_420_888;
        }
        assertNotNull(""Input image is invalid"", image);
        assertEquals(""Format doesn't match"", format, image.getFormat());
        assertEquals(""Width doesn't match"", width, image.getWidth());
        assertEquals(""Height doesn't match"", height, image.getHeight());
    }

    /**
     * <p>Read data from all planes of an Image into a contiguous unpadded, unpacked
     * 1-D linear byte array, such that it can be write into disk, or accessed by
     * software conveniently. It supports YUV_420_888/NV21/YV12 and JPEG input
     * Image format.</p>
     *
     * <p>For YUV_420_888/NV21/YV12/Y8/Y16, it returns a byte array that contains
     * the Y plane data first, followed by U(Cb), V(Cr) planes if there is any
     * (xstride = width, ystride = height for chroma and luma components).</p>
     *
     * <p>For JPEG, it returns a 1-D byte array contains a complete JPEG image.</p>
     *
     * <p>For YUV P010, it returns a byte array that contains Y plane first, followed
     * by the interleaved U(Cb)/V(Cr) plane.</p>
     */
    public static byte[] getDataFromImage(Image image) {
        assertNotNull(""Invalid image:"", image);
        int format = image.getFormat();
        int width = image.getWidth();
        int height = image.getHeight();
        int rowStride, pixelStride;
        byte[] data = null;

        // Read image data
        Plane[] planes = image.getPlanes();
        assertTrue(""Fail to get image planes"", planes != null && planes.length > 0);

        // Check image validity
        checkAndroidImageFormat(image);

        ByteBuffer buffer = null;
        // JPEG doesn't have pixelstride and rowstride, treat it as 1D buffer.
        // Same goes for DEPTH_POINT_CLOUD, RAW_PRIVATE, DEPTH_JPEG, and HEIC
        if (format == ImageFormat.JPEG || format == ImageFormat.DEPTH_POINT_CLOUD ||
                format == ImageFormat.RAW_PRIVATE || format == ImageFormat.DEPTH_JPEG ||
                format == ImageFormat.HEIC) {
            buffer = planes[0].getBuffer();
            assertNotNull(""Fail to get jpeg/depth/heic ByteBuffer"", buffer);
            data = new byte[buffer.remaining()];
            buffer.get(data);
            buffer.rewind();
            return data;
        } else if (format == ImageFormat.YCBCR_P010) {
            // P010 samples are stored within 16 bit values
            int offset = 0;
            int bytesPerPixelRounded = (ImageFormat.getBitsPerPixel(format) + 7) / 8;
            data = new byte[width * height * bytesPerPixelRounded];
            assertTrue(""Unexpected number of planes, expected "" + 3 + "" actual "" + planes.length,
                    planes.length == 3);
            for (int i = 0; i < 2; i++) {
                buffer = planes[i].getBuffer();
                assertNotNull(""Fail to get bytebuffer from plane"", buffer);
                buffer.rewind();
                rowStride = planes[i].getRowStride();
                if (VERBOSE) {
                    Log.v(TAG, ""rowStride "" + rowStride);
                    Log.v(TAG, ""width "" + width);
                    Log.v(TAG, ""height "" + height);
                }
                int h = (i == 0) ? height : height / 2;
                for (int row = 0; row < h; row++) {
                    int length = rowStride;
                    buffer.get(data, offset, length);
                    offset += length;
                }
                if (VERBOSE) Log.v(TAG, ""Finished reading data from plane "" + i);
                buffer.rewind();
            }
            return data;
        }

        int offset = 0;
        data = new byte[width * height * ImageFormat.getBitsPerPixel(format) / 8];
        int maxRowSize = planes[0].getRowStride();
        for (int i = 0; i < planes.length; i++) {
            if (maxRowSize < planes[i].getRowStride()) {
                maxRowSize = planes[i].getRowStride();
            }
        }
        byte[] rowData = new byte[maxRowSize];
        if(VERBOSE) Log.v(TAG, ""get data from "" + planes.length + "" planes"");
        for (int i = 0; i < planes.length; i++) {
            buffer = planes[i].getBuffer();
            assertNotNull(""Fail to get bytebuffer from plane"", buffer);
            buffer.rewind();
            rowStride = planes[i].getRowStride();
            pixelStride = planes[i].getPixelStride();
            assertTrue(""pixel stride "" + pixelStride + "" is invalid"", pixelStride > 0);
            if (VERBOSE) {
                Log.v(TAG, ""pixelStride "" + pixelStride);
                Log.v(TAG, ""rowStride "" + rowStride);
                Log.v(TAG, ""width "" + width);
                Log.v(TAG, ""height "" + height);
            }
            // For multi-planar yuv images, assuming yuv420 with 2x2 chroma subsampling.
            int w = (i == 0) ? width : width / 2;
            int h = (i == 0) ? height : height / 2;
            assertTrue(""rowStride "" + rowStride + "" should be >= width "" + w , rowStride >= w);
            for (int row = 0; row < h; row++) {
                int bytesPerPixel = ImageFormat.getBitsPerPixel(format) / 8;
                int length;
                if (pixelStride == bytesPerPixel) {
                    // Special case: optimized read of the entire row
                    length = w * bytesPerPixel;
                    buffer.get(data, offset, length);
                    offset += length;
                } else {
                    // Generic case: should work for any pixelStride but slower.
                    // Use intermediate buffer to avoid read byte-by-byte from
                    // DirectByteBuffer, which is very bad for performance
                    length = (w - 1) * pixelStride + bytesPerPixel;
                    buffer.get(rowData, 0, length);
                    for (int col = 0; col < w; col++) {
                        data[offset++] = rowData[col * pixelStride];
                    }
                }
                // Advance buffer the remainder of the row stride
                if (row < h - 1) {
                    buffer.position(buffer.position() + rowStride - length);
                }
            }
            if (VERBOSE) Log.v(TAG, ""Finished reading data from plane "" + i);
            buffer.rewind();
        }
        return data;
    }

    /**
     * <p>Check android image format validity for an image, only support below formats:</p>
     *
     * <p>YUV_420_888/NV21/YV12, can add more for future</p>
     */
    public static void checkAndroidImageFormat(Image image) {
        int format = image.getFormat();
        Plane[] planes = image.getPlanes();
        switch (format) {
            case ImageFormat.YUV_420_888:
            case ImageFormat.NV21:
            case ImageFormat.YV12:
            case ImageFormat.YCBCR_P010:
                assertEquals(""YUV420 format Images should have 3 planes"", 3, planes.length);
                break;
            case ImageFormat.JPEG:
            case ImageFormat.RAW_SENSOR:
            case ImageFormat.RAW_PRIVATE:
            case ImageFormat.DEPTH16:
            case ImageFormat.DEPTH_POINT_CLOUD:
            case ImageFormat.DEPTH_JPEG:
            case ImageFormat.Y8:
            case ImageFormat.HEIC:
                assertEquals(""JPEG/RAW/depth/Y8 Images should have one plane"", 1, planes.length);
                break;
            default:
                fail(""Unsupported Image Format: "" + format);
        }
    }

    public static void dumpFile(String fileName, Bitmap data) {
        FileOutputStream outStream;
        try {
            Log.v(TAG, ""output will be saved as "" + fileName);
            outStream = new FileOutputStream(fileName);
        } catch (IOException ioe) {
            throw new RuntimeException(""Unable to create debug output file "" + fileName, ioe);
        }

        try {
            data.compress(Bitmap.CompressFormat.JPEG, /*quality*/90, outStream);
            outStream.close();
        } catch (IOException ioe) {
            throw new RuntimeException(""failed writing data to file "" + fileName, ioe);
        }
    }

    public static void dumpFile(String fileName, byte[] data) {
        FileOutputStream outStream;
        try {
            Log.v(TAG, ""output will be saved as "" + fileName);
            outStream = new FileOutputStream(fileName);
        } catch (IOException ioe) {
            throw new RuntimeException(""Unable to create debug output file "" + fileName, ioe);
        }

        try {
            outStream.write(data);
            outStream.close();
        } catch (IOException ioe) {
            throw new RuntimeException(""failed writing data to file "" + fileName, ioe);
        }
    }

    /**
     * Get the available output sizes for the user-defined {@code format}.
     *
     * <p>Note that implementation-defined/hidden formats are not supported.</p>
     */
    public static Size[] getSupportedSizeForFormat(int format, String cameraId,
            CameraManager cameraManager) throws CameraAccessException {
        CameraCharacteristics properties = cameraManager.getCameraCharacteristics(cameraId);
        assertNotNull(""Can't get camera characteristics!"", properties);
        if (VERBOSE) {
            Log.v(TAG, ""get camera characteristics for camera: "" + cameraId);
        }
        StreamConfigurationMap configMap =
                properties.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        Size[] availableSizes = configMap.getOutputSizes(format);
        assertArrayNotEmpty(availableSizes, ""availableSizes should not be empty for format: ""
                + format);
        Size[] highResAvailableSizes = configMap.getHighResolutionOutputSizes(format);
        if (highResAvailableSizes != null && highResAvailableSizes.length > 0) {
            Size[] allSizes = new Size[availableSizes.length + highResAvailableSizes.length];
            System.arraycopy(availableSizes, 0, allSizes, 0,
                    availableSizes.length);
            System.arraycopy(highResAvailableSizes, 0, allSizes, availableSizes.length,
                    highResAvailableSizes.length);
            availableSizes = allSizes;
        }
        if (VERBOSE) Log.v(TAG, ""Supported sizes are: "" + Arrays.deepToString(availableSizes));
        return availableSizes;
    }

    /**
     * Get the available output sizes for the given class.
     *
     */
    public static Size[] getSupportedSizeForClass(Class klass, String cameraId,
            CameraManager cameraManager) throws CameraAccessException {
        CameraCharacteristics properties = cameraManager.getCameraCharacteristics(cameraId);
        assertNotNull(""Can't get camera characteristics!"", properties);
        if (VERBOSE) {
            Log.v(TAG, ""get camera characteristics for camera: "" + cameraId);
        }
        StreamConfigurationMap configMap =
                properties.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        Size[] availableSizes = configMap.getOutputSizes(klass);
        assertArrayNotEmpty(availableSizes, ""availableSizes should not be empty for class: ""
                + klass);
        Size[] highResAvailableSizes = configMap.getHighResolutionOutputSizes(ImageFormat.PRIVATE);
        if (highResAvailableSizes != null && highResAvailableSizes.length > 0) {
            Size[] allSizes = new Size[availableSizes.length + highResAvailableSizes.length];
            System.arraycopy(availableSizes, 0, allSizes, 0,
                    availableSizes.length);
            System.arraycopy(highResAvailableSizes, 0, allSizes, availableSizes.length,
                    highResAvailableSizes.length);
            availableSizes = allSizes;
        }
        if (VERBOSE) Log.v(TAG, ""Supported sizes are: "" + Arrays.deepToString(availableSizes));
        return availableSizes;
    }

    /**
     * Size comparator that compares the number of pixels it covers.
     *
     * <p>If two the areas of two sizes are same, compare the widths.</p>
     */
    public static class SizeComparator implements Comparator<Size> {
        @Override
        public int compare(Size lhs, Size rhs) {
            return CameraUtils
                    .compareSizes(lhs.getWidth(), lhs.getHeight(), rhs.getWidth(), rhs.getHeight());
        }
    }

    /**
     * Get sorted size list in descending order. Remove the sizes larger than
     * the bound. If the bound is null, don't do the size bound filtering.
     */
    static public List<Size> getSupportedPreviewSizes(String cameraId,
            CameraManager cameraManager, Size bound) throws CameraAccessException {

        Size[] rawSizes = getSupportedSizeForClass(android.view.SurfaceHolder.class, cameraId,
                cameraManager);
        assertArrayNotEmpty(rawSizes,
                ""Available sizes for SurfaceHolder class should not be empty"");
        if (VERBOSE) {
            Log.v(TAG, ""Supported sizes are: "" + Arrays.deepToString(rawSizes));
        }

        if (bound == null) {
            return getAscendingOrderSizes(Arrays.asList(rawSizes), /*ascending*/false);
        }

        List<Size> sizes = new ArrayList<Size>();
        for (Size sz: rawSizes) {
            if (sz.getWidth() <= bound.getWidth() && sz.getHeight() <= bound.getHeight()) {
                sizes.add(sz);
            }
        }
        return getAscendingOrderSizes(sizes, /*ascending*/false);
    }

    /**
     * Get a sorted list of sizes from a given size list.
     *
     * <p>
     * The size is compare by area it covers, if the areas are same, then
     * compare the widths.
     * </p>
     *
     * @param sizeList The input size list to be sorted
     * @param ascending True if the order is ascending, otherwise descending order
     * @return The ordered list of sizes
     */
    static public List<Size> getAscendingOrderSizes(final List<Size> sizeList, boolean ascending) {
        if (sizeList == null) {
            throw new IllegalArgumentException(""sizeList shouldn't be null"");
        }

        Comparator<Size> comparator = new SizeComparator();
        List<Size> sortedSizes = new ArrayList<Size>();
        sortedSizes.addAll(sizeList);
        Collections.sort(sortedSizes, comparator);
        if (!ascending) {
            Collections.reverse(sortedSizes);
        }

        return sortedSizes;
    }

    /**
     * Get sorted (descending order) size list for given format. Remove the sizes larger than
     * the bound. If the bound is null, don't do the size bound filtering.
     */
    static public List<Size> getSortedSizesForFormat(String cameraId,
            CameraManager cameraManager, int format, Size bound) throws CameraAccessException {
        Comparator<Size> comparator = new SizeComparator();
        Size[] sizes = getSupportedSizeForFormat(format, cameraId, cameraManager);
        List<Size> sortedSizes = null;
        if (bound != null) {
            sortedSizes = new ArrayList<Size>(/*capacity*/1);
            for (Size sz : sizes) {
                if (comparator.compare(sz, bound) <= 0) {
                    sortedSizes.add(sz);
                }
            }
        } else {
            sortedSizes = Arrays.asList(sizes);
        }
        assertTrue(""Supported size list should have at least one element"",
                sortedSizes.size() > 0);

        Collections.sort(sortedSizes, comparator);
        // Make it in descending order.
        Collections.reverse(sortedSizes);
        return sortedSizes;
    }

    /**
     * Get supported video size list for a given camera device.
     *
     * <p>
     * Filter out the sizes that are larger than the bound. If the bound is
     * null, don't do the size bound filtering.
     * </p>
     */
    static public List<Size> getSupportedVideoSizes(String cameraId,
            CameraManager cameraManager, Size bound) throws CameraAccessException {

        Size[] rawSizes = getSupportedSizeForClass(android.media.MediaRecorder.class,
                cameraId, cameraManager);
        assertArrayNotEmpty(rawSizes,
                ""Available sizes for MediaRecorder class should not be empty"");
        if (VERBOSE) {
            Log.v(TAG, ""Supported sizes are: "" + Arrays.deepToString(rawSizes));
        }

        if (bound == null) {
            return getAscendingOrderSizes(Arrays.asList(rawSizes), /*ascending*/false);
        }

        List<Size> sizes = new ArrayList<Size>();
        for (Size sz: rawSizes) {
            if (sz.getWidth() <= bound.getWidth() && sz.getHeight() <= bound.getHeight()) {
                sizes.add(sz);
            }
        }
        return getAscendingOrderSizes(sizes, /*ascending*/false);
    }

    /**
     * Get supported video size list (descending order) for a given camera device.
     *
     * <p>
     * Filter out the sizes that are larger than the bound. If the bound is
     * null, don't do the size bound filtering.
     * </p>
     */
    static public List<Size> getSupportedStillSizes(String cameraId,
            CameraManager cameraManager, Size bound) throws CameraAccessException {
        return getSortedSizesForFormat(cameraId, cameraManager, ImageFormat.JPEG, bound);
    }

    static public List<Size> getSupportedHeicSizes(String cameraId,
            CameraManager cameraManager, Size bound) throws CameraAccessException {
        return getSortedSizesForFormat(cameraId, cameraManager, ImageFormat.HEIC, bound);
    }

    static public Size getMinPreviewSize(String cameraId, CameraManager cameraManager)
            throws CameraAccessException {
        List<Size> sizes = getSupportedPreviewSizes(cameraId, cameraManager, null);
        return sizes.get(sizes.size() - 1);
    }

    /**
     * Get max supported preview size for a camera device.
     */
    static public Size getMaxPreviewSize(String cameraId, CameraManager cameraManager)
            throws CameraAccessException {
        return getMaxPreviewSize(cameraId, cameraManager, /*bound*/null);
    }

    /**
     * Get max preview size for a camera device in the supported sizes that are no larger
     * than the bound.
     */
    static public Size getMaxPreviewSize(String cameraId, CameraManager cameraManager, Size bound)
            throws CameraAccessException {
        List<Size> sizes = getSupportedPreviewSizes(cameraId, cameraManager, bound);
        return sizes.get(0);
    }

    /**
     * Get max depth size for a camera device.
     */
    static public Size getMaxDepthSize(String cameraId, CameraManager cameraManager)
            throws CameraAccessException {
        List<Size> sizes = getSortedSizesForFormat(cameraId, cameraManager, ImageFormat.DEPTH16,
                /*bound*/ null);
        return sizes.get(0);
    }

    /**
     * Get the largest size by area.
     *
     * @param sizes an array of sizes, must have at least 1 element
     *
     * @return Largest Size
     *
     * @throws IllegalArgumentException if sizes was null or had 0 elements
     */
    public static Size getMaxSize(Size... sizes) {
        if (sizes == null || sizes.length == 0) {
            throw new IllegalArgumentException(""sizes was empty"");
        }

        Size sz = sizes[0];
        for (Size size : sizes) {
            if (size.getWidth() * size.getHeight() > sz.getWidth() * sz.getHeight()) {
                sz = size;
            }
        }

        return sz;
    }

    /**
     * Get the largest size by area within (less than) bound
     *
     * @param sizes an array of sizes, must have at least 1 element
     *
     * @return Largest Size. Null if no such size exists within bound.
     *
     * @throws IllegalArgumentException if sizes was null or had 0 elements, or bound is invalid.
     */
    public static Size getMaxSizeWithBound(Size[] sizes, int bound) {
        if (sizes == null || sizes.length == 0) {
            throw new IllegalArgumentException(""sizes was empty"");
        }
        if (bound <= 0) {
            throw new IllegalArgumentException(""bound is invalid"");
        }

        Size sz = null;
        for (Size size : sizes) {
            if (size.getWidth() * size.getHeight() >= bound) {
                continue;
            }

            if (sz == null ||
                    size.getWidth() * size.getHeight() > sz.getWidth() * sz.getHeight()) {
                sz = size;
            }
        }

        return sz;
    }

    /**
     * Returns true if the given {@code array} contains the given element.
     *
     * @param array {@code array} to check for {@code elem}
     * @param elem {@code elem} to test for
     * @return {@code true} if the given element is contained
     */
    public static boolean contains(int[] array, int elem) {
        if (array == null) return false;
        for (int i = 0; i < array.length; i++) {
            if (elem == array[i]) return true;
        }
        return false;
    }

    /**
     * Get object array from byte array.
     *
     * @param array Input byte array to be converted
     * @return Byte object array converted from input byte array
     */
    public static Byte[] toObject(byte[] array) {
        return convertPrimitiveArrayToObjectArray(array, Byte.class);
    }

    /**
     * Get object array from int array.
     *
     * @param array Input int array to be converted
     * @return Integer object array converted from input int array
     */
    public static Integer[] toObject(int[] array) {
        return convertPrimitiveArrayToObjectArray(array, Integer.class);
    }

    /**
     * Get object array from float array.
     *
     * @param array Input float array to be converted
     * @return Float object array converted from input float array
     */
    public static Float[] toObject(float[] array) {
        return convertPrimitiveArrayToObjectArray(array, Float.class);
    }

    /**
     * Get object array from double array.
     *
     * @param array Input double array to be converted
     * @return Double object array converted from input double array
     */
    public static Double[] toObject(double[] array) {
        return convertPrimitiveArrayToObjectArray(array, Double.class);
    }

    /**
     * Convert a primitive input array into its object array version (e.g. from int[] to Integer[]).
     *
     * @param array Input array object
     * @param wrapperClass The boxed class it converts to
     * @return Boxed version of primitive array
     */
    private static <T> T[] convertPrimitiveArrayToObjectArray(final Object array,
            final Class<T> wrapperClass) {
        // getLength does the null check and isArray check already.
        int arrayLength = Array.getLength(array);
        if (arrayLength == 0) {
            throw new IllegalArgumentException(""Input array shouldn't be empty"");
        }

        @SuppressWarnings(""unchecked"")
        final T[] result = (T[]) Array.newInstance(wrapperClass, arrayLength);
        for (int i = 0; i < arrayLength; i++) {
            Array.set(result, i, Array.get(array, i));
        }
        return result;
    }

    /**
     * Validate image based on format and size.
     *
     * @param image The image to be validated.
     * @param width The image width.
     * @param height The image height.
     * @param format The image format.
     * @param filePath The debug dump file path, null if don't want to dump to
     *            file.
     * @throws UnsupportedOperationException if calling with an unknown format
     */
    public static void validateImage(Image image, int width, int height, int format,
            String filePath) {
        checkImage(image, width, height, format);

        /**
         * TODO: validate timestamp:
         * 1. capture result timestamp against the image timestamp (need
         * consider frame drops)
         * 2. timestamps should be monotonically increasing for different requests
         */
        if(VERBOSE) Log.v(TAG, ""validating Image"");
        byte[] data = getDataFromImage(image);
        assertTrue(""Invalid image data"", data != null && data.length > 0);

        switch (format) {
            // Clients must be able to process and handle depth jpeg images like any other
            // regular jpeg.
            case ImageFormat.DEPTH_JPEG:
            case ImageFormat.JPEG:
                validateJpegData(data, width, height, filePath);
                break;
            case ImageFormat.YCBCR_P010:
                validateP010Data(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.YUV_420_888:
            case ImageFormat.YV12:
                validateYuvData(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.RAW_SENSOR:
                validateRaw16Data(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.DEPTH16:
                validateDepth16Data(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.DEPTH_POINT_CLOUD:
                validateDepthPointCloudData(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.RAW_PRIVATE:
                validateRawPrivateData(data, width, height, image.getTimestamp(), filePath);
                break;
            case ImageFormat.Y8:
                validateY8Data(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.HEIC:
                validateHeicData(data, width, height, filePath);
                break;
            default:
                throw new UnsupportedOperationException(""Unsupported format for validation: ""
                        + format);
        }
    }

    public static class HandlerExecutor implements Executor {
        private final Handler mHandler;

        public HandlerExecutor(Handler handler) {
            assertNotNull(""handler must be valid"", handler);
            mHandler = handler;
        }

        @Override
        public void execute(Runnable runCmd) {
            mHandler.post(runCmd);
        }
    }

    /**
     * Provide a mock for {@link CameraDevice.StateCallback}.
     *
     * <p>Only useful because mockito can't mock {@link CameraDevice.StateCallback} which is an
     * abstract class.</p>
     *
     * <p>
     * Use this instead of other classes when needing to verify interactions, since
     * trying to spy on {@link BlockingStateCallback} (or others) will cause unnecessary extra
     * interactions which will cause false test failures.
     * </p>
     *
     */
    public static class MockStateCallback extends CameraDevice.StateCallback {

        @Override
        public void onOpened(CameraDevice camera) {
        }

        @Override
        public void onDisconnected(CameraDevice camera) {
        }

        @Override
        public void onError(CameraDevice camera, int error) {
        }

        private MockStateCallback() {}

        /**
         * Create a Mockito-ready mocked StateCallback.
         */
        public static MockStateCallback mock() {
            return Mockito.spy(new MockStateCallback());
        }
    }

    public static void validateJpegData(byte[] jpegData, int width, int height, String filePath) {
        BitmapFactory.Options bmpOptions = new BitmapFactory.Options();
        // DecodeBound mode: only parse the frame header to get width/height.
        // it doesn't decode the pixel.
        bmpOptions.inJustDecodeBounds = true;
        BitmapFactory.decodeByteArray(jpegData, 0, jpegData.length, bmpOptions);
        assertEquals(width, bmpOptions.outWidth);
        assertEquals(height, bmpOptions.outHeight);

        // Pixel decoding mode: decode whole image. check if the image data
        // is decodable here.
        assertNotNull(""Decoding jpeg failed"",
                BitmapFactory.decodeByteArray(jpegData, 0, jpegData.length));
        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + "".jpeg"";
            dumpFile(fileName, jpegData);
        }
    }

    private static void validateYuvData(byte[] yuvData, int width, int height, int format,
            long ts, String filePath) {
        checkYuvFormat(format);
        if (VERBOSE) Log.v(TAG, ""Validating YUV data"");
        int expectedSize = width * height * ImageFormat.getBitsPerPixel(format) / 8;
        assertEquals(""Yuv data doesn't match"", expectedSize, yuvData.length);

        // TODO: Can add data validation for test pattern.

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".yuv"";
            dumpFile(fileName, yuvData);
        }
    }

    private static void validateP010Data(byte[] p010Data, int width, int height, int format,
            long ts, String filePath) {
        if (VERBOSE) Log.v(TAG, ""Validating P010 data"");
        // The P010 10 bit samples are stored in two bytes so the size needs to be adjusted
        // accordingly.
        int bytesPerPixelRounded = (ImageFormat.getBitsPerPixel(format) + 7) / 8;
        int expectedSize = width * height * bytesPerPixelRounded;
        assertEquals(""P010 data doesn't match"", expectedSize, p010Data.length);

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".p010"";
            dumpFile(fileName, p010Data);
        }
    }
    private static void validateRaw16Data(byte[] rawData, int width, int height, int format,
            long ts, String filePath) {
        if (VERBOSE) Log.v(TAG, ""Validating raw data"");
        int expectedSize = width * height * ImageFormat.getBitsPerPixel(format) / 8;
        assertEquals(""Raw data doesn't match"", expectedSize, rawData.length);

        // TODO: Can add data validation for test pattern.

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".raw16"";
            dumpFile(fileName, rawData);
        }

        return;
    }

    private static void validateY8Data(byte[] rawData, int width, int height, int format,
            long ts, String filePath) {
        if (VERBOSE) Log.v(TAG, ""Validating Y8 data"");
        int expectedSize = width * height * ImageFormat.getBitsPerPixel(format) / 8;
        assertEquals(""Y8 data doesn't match"", expectedSize, rawData.length);

        // TODO: Can add data validation for test pattern.

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".y8"";
            dumpFile(fileName, rawData);
        }

        return;
    }

    private static void validateRawPrivateData(byte[] rawData, int width, int height,
            long ts, String filePath) {
        if (VERBOSE) Log.v(TAG, ""Validating private raw data"");
        // Expect each RAW pixel should occupy at least one byte and no more than 30 bytes
        int expectedSizeMin = width * height;
        int expectedSizeMax = width * height * 30;

        assertTrue(""Opaque RAW size "" + rawData.length + ""out of normal bound ["" +
                expectedSizeMin + "","" + expectedSizeMax + ""]"",
                expectedSizeMin <= rawData.length && rawData.length <= expectedSizeMax);

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".rawPriv"";
            dumpFile(fileName, rawData);
        }

        return;
    }

    private static void validateDepth16Data(byte[] depthData, int width, int height, int format,
            long ts, String filePath) {

        if (VERBOSE) Log.v(TAG, ""Validating depth16 data"");
        int expectedSize = width * height * ImageFormat.getBitsPerPixel(format) / 8;
        assertEquals(""Depth data doesn't match"", expectedSize, depthData.length);


        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".depth16"";
            dumpFile(fileName, depthData);
        }

        return;

    }

    private static void validateDepthPointCloudData(byte[] depthData, int width, int height, int format,
            long ts, String filePath) {

        if (VERBOSE) Log.v(TAG, ""Validating depth point cloud data"");

        // Can't validate size since it is variable

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".depth_point_cloud"";
            dumpFile(fileName, depthData);
        }

        return;

    }

    private static void validateHeicData(byte[] heicData, int width, int height, String filePath) {
        BitmapFactory.Options bmpOptions = new BitmapFactory.Options();
        // DecodeBound mode: only parse the frame header to get width/height.
        // it doesn't decode the pixel.
        bmpOptions.inJustDecodeBounds = true;
        BitmapFactory.decodeByteArray(heicData, 0, heicData.length, bmpOptions);
        assertEquals(width, bmpOptions.outWidth);
        assertEquals(height, bmpOptions.outHeight);

        // Pixel decoding mode: decode whole image. check if the image data
        // is decodable here.
        assertNotNull(""Decoding heic failed"",
                BitmapFactory.decodeByteArray(heicData, 0, heicData.length));
        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + "".heic"";
            dumpFile(fileName, heicData);
        }
    }

    public static <T> T getValueNotNull(CaptureResult result, CaptureResult.Key<T> key) {
        if (result == null) {
            throw new IllegalArgumentException(""Result must not be null"");
        }

        T value = result.get(key);
        assertNotNull(""Value of Key "" + key.getName() + ""shouldn't be null"", value);
        return value;
    }

    public static <T> T getValueNotNull(CameraCharacteristics characteristics,
            CameraCharacteristics.Key<T> key) {
        if (characteristics == null) {
            throw new IllegalArgumentException(""Camera characteristics must not be null"");
        }

        T value = characteristics.get(key);
        assertNotNull(""Value of Key "" + key.getName() + ""shouldn't be null"", value);
        return value;
    }

    /**
     * Get a crop region for a given zoom factor and center position.
     * <p>
     * The center position is normalized position in range of [0, 1.0], where
     * (0, 0) represents top left corner, (1.0. 1.0) represents bottom right
     * corner. The center position could limit the effective minimal zoom
     * factor, for example, if the center position is (0.75, 0.75), the
     * effective minimal zoom position becomes 2.0. If the requested zoom factor
     * is smaller than 2.0, a crop region with 2.0 zoom factor will be returned.
     * </p>
     * <p>
     * The aspect ratio of the crop region is maintained the same as the aspect
     * ratio of active array.
     * </p>
     *
     * @param zoomFactor The zoom factor to generate the crop region, it must be
     *            >= 1.0
     * @param center The normalized zoom center point that is in the range of [0, 1].
     * @param maxZoom The max zoom factor supported by this device.
     * @param activeArray The active array size of this device.
     * @return crop region for the given normalized center and zoom factor.
     */
    public static Rect getCropRegionForZoom(float zoomFactor, final PointF center,
            final float maxZoom, final Rect activeArray) {
        if (zoomFactor < 1.0) {
            throw new IllegalArgumentException(""zoom factor "" + zoomFactor + "" should be >= 1.0"");
        }
        if (center.x > 1.0 || center.x < 0) {
            throw new IllegalArgumentException(""center.x "" + center.x
                    + "" should be in range of [0, 1.0]"");
        }
        if (center.y > 1.0 || center.y < 0) {
            throw new IllegalArgumentException(""center.y "" + center.y
                    + "" should be in range of [0, 1.0]"");
        }
        if (maxZoom < 1.0) {
            throw new IllegalArgumentException(""max zoom factor "" + maxZoom + "" should be >= 1.0"");
        }
        if (activeArray == null) {
            throw new IllegalArgumentException(""activeArray must not be null"");
        }

        float minCenterLength = Math.min(Math.min(center.x, 1.0f - center.x),
                Math.min(center.y, 1.0f - center.y));
        float minEffectiveZoom =  0.5f / minCenterLength;
        if (minEffectiveZoom > maxZoom) {
            throw new IllegalArgumentException(""Requested center "" + center.toString() +
                    "" has minimal zoomable factor "" + minEffectiveZoom + "", which exceeds max""
                            + "" zoom factor "" + maxZoom);
        }

        if (zoomFactor < minEffectiveZoom) {
            Log.w(TAG, ""Requested zoomFactor "" + zoomFactor + "" < minimal zoomable factor ""
                    + minEffectiveZoom + "". It will be overwritten by "" + minEffectiveZoom);
            zoomFactor = minEffectiveZoom;
        }

        int cropCenterX = (int)(activeArray.width() * center.x);
        int cropCenterY = (int)(activeArray.height() * center.y);
        int cropWidth = (int) (activeArray.width() / zoomFactor);
        int cropHeight = (int) (activeArray.height() / zoomFactor);

        return new Rect(
                /*left*/cropCenterX - cropWidth / 2,
                /*top*/cropCenterY - cropHeight / 2,
                /*right*/ cropCenterX + cropWidth / 2,
                /*bottom*/cropCenterY + cropHeight / 2);
    }

    /**
     * Get AeAvailableTargetFpsRanges and sort them in descending order by max fps
     *
     * @param staticInfo camera static metadata
     * @return AeAvailableTargetFpsRanges in descending order by max fps
     */
    public static Range<Integer>[] getDescendingTargetFpsRanges(StaticMetadata staticInfo) {
        Range<Integer>[] fpsRanges = staticInfo.getAeAvailableTargetFpsRangesChecked();
        Arrays.sort(fpsRanges, new Comparator<Range<Integer>>() {
            public int compare(Range<Integer> r1, Range<Integer> r2) {
                return r2.getUpper() - r1.getUpper();
            }
        });
        return fpsRanges;
    }

    /**
     * Get AeAvailableTargetFpsRanges with max fps not exceeding 30
     *
     * @param staticInfo camera static metadata
     * @return AeAvailableTargetFpsRanges with max fps not exceeding 30
     */
    public static List<Range<Integer>> getTargetFpsRangesUpTo30(StaticMetadata staticInfo) {
        Range<Integer>[] fpsRanges = staticInfo.getAeAvailableTargetFpsRangesChecked();
        ArrayList<Range<Integer>> fpsRangesUpTo30 = new ArrayList<Range<Integer>>();
        for (Range<Integer> fpsRange : fpsRanges) {
            if (fpsRange.getUpper() <= 30) {
                fpsRangesUpTo30.add(fpsRange);
            }
        }
        return fpsRangesUpTo30;
    }

    /**
     * Get AeAvailableTargetFpsRanges with max fps greater than 30
     *
     * @param staticInfo camera static metadata
     * @return AeAvailableTargetFpsRanges with max fps greater than 30
     */
    public static List<Range<Integer>> getTargetFpsRangesGreaterThan30(StaticMetadata staticInfo) {
        Range<Integer>[] fpsRanges = staticInfo.getAeAvailableTargetFpsRangesChecked();
        ArrayList<Range<Integer>> fpsRangesGreaterThan30 = new ArrayList<Range<Integer>>();
        for (Range<Integer> fpsRange : fpsRanges) {
            if (fpsRange.getUpper() > 30) {
                fpsRangesGreaterThan30.add(fpsRange);
            }
        }
        return fpsRangesGreaterThan30;
    }

    /**
     * Calculate output 3A region from the intersection of input 3A region and cropped region.
     *
     * @param requestRegions The input 3A regions
     * @param cropRect The cropped region
     * @return expected 3A regions output in capture result
     */
    public static MeteringRectangle[] getExpectedOutputRegion(
            MeteringRectangle[] requestRegions, Rect cropRect){
        MeteringRectangle[] resultRegions = new MeteringRectangle[requestRegions.length];
        for (int i = 0; i < requestRegions.length; i++) {
            Rect requestRect = requestRegions[i].getRect();
            Rect resultRect = new Rect();
            boolean intersect = resultRect.setIntersect(requestRect, cropRect);
            resultRegions[i] = new MeteringRectangle(
                    resultRect,
                    intersect ? requestRegions[i].getMeteringWeight() : 0);
        }
        return resultRegions;
    }

    /**
     * Copy source image data to destination image.
     *
     * @param src The source image to be copied from.
     * @param dst The destination image to be copied to.
     * @throws IllegalArgumentException If the source and destination images have
     *             different format, size, or one of the images is not copyable.
     */
    public static void imageCopy(Image src, Image dst) {
        if (src == null || dst == null) {
            throw new IllegalArgumentException(""Images should be non-null"");
        }
        if (src.getFormat() != dst.getFormat()) {
            throw new IllegalArgumentException(""Src and dst images should have the same format"");
        }
        if (src.getFormat() == ImageFormat.PRIVATE ||
                dst.getFormat() == ImageFormat.PRIVATE) {
            throw new IllegalArgumentException(""PRIVATE format images are not copyable"");
        }

        Size srcSize = new Size(src.getWidth(), src.getHeight());
        Size dstSize = new Size(dst.getWidth(), dst.getHeight());
        if (!srcSize.equals(dstSize)) {
            throw new IllegalArgumentException(""source image size "" + srcSize + "" is different""
                    + "" with "" + ""destination image size "" + dstSize);
        }

        // TODO: check the owner of the dst image, it must be from ImageWriter, other source may
        // not be writable. Maybe we should add an isWritable() method in image class.

        Plane[] srcPlanes = src.getPlanes();
        Plane[] dstPlanes = dst.getPlanes();
        ByteBuffer srcBuffer = null;
        ByteBuffer dstBuffer = null;
        for (int i = 0; i < srcPlanes.length; i++) {
            srcBuffer = srcPlanes[i].getBuffer();
            dstBuffer = dstPlanes[i].getBuffer();
            int srcPos = srcBuffer.position();
            srcBuffer.rewind();
            dstBuffer.rewind();
            int srcRowStride = srcPlanes[i].getRowStride();
            int dstRowStride = dstPlanes[i].getRowStride();
            int srcPixStride = srcPlanes[i].getPixelStride();
            int dstPixStride = dstPlanes[i].getPixelStride();

            if (srcPixStride > 2 || dstPixStride > 2) {
                throw new IllegalArgumentException(""source pixel stride "" + srcPixStride +
                        "" with destination pixel stride "" + dstPixStride +
                        "" is not supported"");
            }

            if (srcRowStride == dstRowStride && srcPixStride == dstPixStride &&
                    srcPixStride == 1) {
                // Fast path, just copy the content in the byteBuffer all together.
                dstBuffer.put(srcBuffer);
            } else {
                Size effectivePlaneSize = getEffectivePlaneSizeForImage(src, i);
                int srcRowByteCount = srcRowStride;
                int dstRowByteCount = dstRowStride;
                byte[] srcDataRow = new byte[Math.max(srcRowStride, dstRowStride)];

                if (srcPixStride == dstPixStride && srcPixStride == 1) {
                    // Row by row copy case
                    for (int row = 0; row < effectivePlaneSize.getHeight(); row++) {
                        if (row == effectivePlaneSize.getHeight() - 1) {
                            // Special case for interleaved planes: need handle the last row
                            // carefully to avoid memory corruption. Check if we have enough bytes
                            // to copy.
                            srcRowByteCount = Math.min(srcRowByteCount, srcBuffer.remaining());
                            dstRowByteCount = Math.min(dstRowByteCount, dstBuffer.remaining());
                        }
                        srcBuffer.get(srcDataRow, /*offset*/0, srcRowByteCount);
                        dstBuffer.put(srcDataRow, /*offset*/0, dstRowByteCount);
                    }
                } else {
                    // Row by row per pixel copy case
                    byte[] dstDataRow = new byte[dstRowByteCount];
                    for (int row = 0; row < effectivePlaneSize.getHeight(); row++) {
                        if (row == effectivePlaneSize.getHeight() - 1) {
                            // Special case for interleaved planes: need handle the last row
                            // carefully to avoid memory corruption. Check if we have enough bytes
                            // to copy.
                            int remainingBytes = srcBuffer.remaining();
                            if (srcRowByteCount > remainingBytes) {
                                srcRowByteCount = remainingBytes;
                            }
                            remainingBytes = dstBuffer.remaining();
                            if (dstRowByteCount > remainingBytes) {
                                dstRowByteCount = remainingBytes;
                            }
                        }
                        srcBuffer.get(srcDataRow, /*offset*/0, srcRowByteCount);
                        int pos = dstBuffer.position();
                        dstBuffer.get(dstDataRow, /*offset*/0, dstRowByteCount);
                        dstBuffer.position(pos);
                        for (int x = 0; x < effectivePlaneSize.getWidth(); x++) {
                            dstDataRow[x * dstPixStride] = srcDataRow[x * srcPixStride];
                        }
                        dstBuffer.put(dstDataRow, /*offset*/0, dstRowByteCount);
                    }
                }
            }
            srcBuffer.position(srcPos);
            dstBuffer.rewind();
        }
    }

    private static Size getEffectivePlaneSizeForImage(Image image, int planeIdx) {
        switch (image.getFormat()) {
            case ImageFormat.YUV_420_888:
                if (planeIdx == 0) {
                    return new Size(image.getWidth(), image.getHeight());
                } else {
                    return new Size(image.getWidth() / 2, image.getHeight() / 2);
                }
            case ImageFormat.JPEG:
            case ImageFormat.RAW_SENSOR:
            case ImageFormat.RAW10:
            case ImageFormat.RAW12:
            case ImageFormat.DEPTH16:
                return new Size(image.getWidth(), image.getHeight());
            case ImageFormat.PRIVATE:
                return new Size(0, 0);
            default:
                throw new UnsupportedOperationException(
                        String.format(""Invalid image format %d"", image.getFormat()));
        }
    }

    /**
     * <p>
     * Checks whether the two images are strongly equal.
     * </p>
     * <p>
     * Two images are strongly equal if and only if the data, formats, sizes,
     * and timestamps are same. For {@link ImageFormat#PRIVATE PRIVATE} format
     * images, the image data is not not accessible thus the data comparison is
     * effectively skipped as the number of planes is zero.
     * </p>
     * <p>
     * Note that this method compares the pixel data even outside of the crop
     * region, which may not be necessary for general use case.
     * </p>
     *
     * @param lhsImg First image to be compared with.
     * @param rhsImg Second image to be compared with.
     * @return true if the two images are equal, false otherwise.
     * @throws IllegalArgumentException If either of image is null.
     */
    public static boolean isImageStronglyEqual(Image lhsImg, Image rhsImg) {
        if (lhsImg == null || rhsImg == null) {
            throw new IllegalArgumentException(""Images should be non-null"");
        }

        if (lhsImg.getFormat() != rhsImg.getFormat()) {
            Log.i(TAG, ""lhsImg format "" + lhsImg.getFormat() + "" is different with rhsImg format ""
                    + rhsImg.getFormat());
            return false;
        }

        if (lhsImg.getWidth() != rhsImg.getWidth()) {
            Log.i(TAG, ""lhsImg width "" + lhsImg.getWidth() + "" is different with rhsImg width ""
                    + rhsImg.getWidth());
            return false;
        }

        if (lhsImg.getHeight() != rhsImg.getHeight()) {
            Log.i(TAG, ""lhsImg height "" + lhsImg.getHeight() + "" is different with rhsImg height ""
                    + rhsImg.getHeight());
            return false;
        }

        if (lhsImg.getTimestamp() != rhsImg.getTimestamp()) {
            Log.i(TAG, ""lhsImg timestamp "" + lhsImg.getTimestamp()
                    + "" is different with rhsImg timestamp "" + rhsImg.getTimestamp());
            return false;
        }

        if (!lhsImg.getCropRect().equals(rhsImg.getCropRect())) {
            Log.i(TAG, ""lhsImg crop rect "" + lhsImg.getCropRect()
                    + "" is different with rhsImg crop rect "" + rhsImg.getCropRect());
            return false;
        }

        // Compare data inside of the image.
        Plane[] lhsPlanes = lhsImg.getPlanes();
        Plane[] rhsPlanes = rhsImg.getPlanes();
        ByteBuffer lhsBuffer = null;
        ByteBuffer rhsBuffer = null;
        for (int i = 0; i < lhsPlanes.length; i++) {
            lhsBuffer = lhsPlanes[i].getBuffer();
            rhsBuffer = rhsPlanes[i].getBuffer();
            lhsBuffer.rewind();
            rhsBuffer.rewind();
            // Special case for YUV420_888 buffer with different layout or
            // potentially differently interleaved U/V planes.
            if (lhsImg.getFormat() == ImageFormat.YUV_420_888 &&
                    (lhsPlanes[i].getPixelStride() != rhsPlanes[i].getPixelStride() ||
                     lhsPlanes[i].getRowStride() != rhsPlanes[i].getRowStride() ||
                     (lhsPlanes[i].getPixelStride() != 1))) {
                int width = getEffectivePlaneSizeForImage(lhsImg, i).getWidth();
                int height = getEffectivePlaneSizeForImage(lhsImg, i).getHeight();
                int rowSizeL = lhsPlanes[i].getRowStride();
                int rowSizeR = rhsPlanes[i].getRowStride();
                byte[] lhsRow = new byte[rowSizeL];
                byte[] rhsRow = new byte[rowSizeR];
                int pixStrideL = lhsPlanes[i].getPixelStride();
                int pixStrideR = rhsPlanes[i].getPixelStride();
                for (int r = 0; r < height; r++) {
                    if (r == height -1) {
                        rowSizeL = lhsBuffer.remaining();
                        rowSizeR = rhsBuffer.remaining();
                    }
                    lhsBuffer.get(lhsRow, /*offset*/0, rowSizeL);
                    rhsBuffer.get(rhsRow, /*offset*/0, rowSizeR);
                    for (int c = 0; c < width; c++) {
                        if (lhsRow[c * pixStrideL] != rhsRow[c * pixStrideR]) {
                            Log.i(TAG, String.format(
                                    ""byte buffers for plane %d row %d col %d don't match."",
                                    i, r, c));
                            return false;
                        }
                    }
                }
            } else {
                // Compare entire buffer directly
                if (!lhsBuffer.equals(rhsBuffer)) {
                    Log.i(TAG, ""byte buffers for plane "" +  i + "" don't match."");
                    return false;
                }
            }
        }

        return true;
    }

    /**
     * Set jpeg related keys in a capture request builder.
     *
     * @param builder The capture request builder to set the keys inl
     * @param exifData The exif data to set.
     * @param thumbnailSize The thumbnail size to set.
     * @param collector The camera error collector to collect errors.
     */
    public static void setJpegKeys(CaptureRequest.Builder builder, ExifTestData exifData,
            Size thumbnailSize, CameraErrorCollector collector) {
        builder.set(CaptureRequest.JPEG_THUMBNAIL_SIZE, thumbnailSize);
        builder.set(CaptureRequest.JPEG_GPS_LOCATION, exifData.gpsLocation);
        builder.set(CaptureRequest.JPEG_ORIENTATION, exifData.jpegOrientation);
        builder.set(CaptureRequest.JPEG_QUALITY, exifData.jpegQuality);
        builder.set(CaptureRequest.JPEG_THUMBNAIL_QUALITY,
                exifData.thumbnailQuality);

        // Validate request set and get.
        collector.expectEquals(""JPEG thumbnail size request set and get should match"",
                thumbnailSize, builder.get(CaptureRequest.JPEG_THUMBNAIL_SIZE));
        collector.expectTrue(""GPS locations request set and get should match."",
                areGpsFieldsEqual(exifData.gpsLocation,
                builder.get(CaptureRequest.JPEG_GPS_LOCATION)));
        collector.expectEquals(""JPEG orientation request set and get should match"",
                exifData.jpegOrientation,
                builder.get(CaptureRequest.JPEG_ORIENTATION));
        collector.expectEquals(""JPEG quality request set and get should match"",
                exifData.jpegQuality, builder.get(CaptureRequest.JPEG_QUALITY));
        collector.expectEquals(""JPEG thumbnail quality request set and get should match"",
                exifData.thumbnailQuality,
                builder.get(CaptureRequest.JPEG_THUMBNAIL_QUALITY));
    }

    /**
     * Simple validation of JPEG"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.usespermissiondiffcertapp.UriGrantsTest"	"testGrantPersistableUriPermission"	""	"/home/gpoor/cts-12-source/cts/hostsidetests/appsecurity/test-apps/UsePermissionDiffCert/src/com/android/cts/usespermissiondiffcertapp/UriGrantsTest.java"	""	"public void testGrantPersistableUriPermission() {
        final ContentResolver resolver = getContext().getContentResolver();

        final Uri target = Uri.withAppendedPath(PERM_URI_GRANTING, ""foo"");
        final ClipData clip = makeSingleClipData(target);

        // Make sure we can't see the target
        assertReadingClipNotAllowed(clip, ""reading should have failed"");
        assertWritingClipNotAllowed(clip, ""writing should have failed"");

        // Make sure we can't take a grant we don't have
        try {
            resolver.takePersistableUriPermission(target, Intent.FLAG_GRANT_READ_URI_PERMISSION);
            fail(""taking read should have failed"");
        } catch (SecurityException expected) {
        }

        // And since we were just installed, no persisted grants yet
        assertNoPersistedUriPermission();

        // Now, let's grant ourselves some access
        ReceiveUriActivity.clearStarted();
        grantClipUriPermissionViaActivity(clip, Intent.FLAG_GRANT_READ_URI_PERMISSION
                | Intent.FLAG_GRANT_PERSISTABLE_URI_PERMISSION);
        ReceiveUriActivity.waitForStart();

        // We should now have reading access, even before taking the persistable
        // grant. Persisted grants should still be empty.
        assertReadingClipAllowed(clip);
        assertWritingClipNotAllowed(clip, ""writing should have failed"");
        assertNoPersistedUriPermission();

        // Take the read grant and verify we have it!
        long before = System.currentTimeMillis();
        resolver.takePersistableUriPermission(target, Intent.FLAG_GRANT_READ_URI_PERMISSION);
        long after = System.currentTimeMillis();
        assertPersistedUriPermission(target, Intent.FLAG_GRANT_READ_URI_PERMISSION, before, after);

        // Make sure we can't take a grant we don't have
        try {
            resolver.takePersistableUriPermission(target, Intent.FLAG_GRANT_WRITE_URI_PERMISSION);
            fail(""taking write should have failed"");
        } catch (SecurityException expected) {
        }

        // Launch again giving ourselves persistable read and write access
        ReceiveUriActivity.clearNewIntent();
        grantClipUriPermissionViaActivity(clip, Intent.FLAG_GRANT_READ_URI_PERMISSION
                | Intent.FLAG_GRANT_WRITE_URI_PERMISSION
                | Intent.FLAG_GRANT_PERSISTABLE_URI_PERMISSION);
        ReceiveUriActivity.waitForNewIntent();

        // Previous persisted grant should be unchanged
        assertPersistedUriPermission(target, Intent.FLAG_GRANT_READ_URI_PERMISSION, before, after);

        // We should have both read and write; read is persisted, and write
        // isn't persisted yet.
        assertReadingClipAllowed(clip);
        assertWritingClipAllowed(clip);

        // Take again, but still only read; should just update timestamp
        before = System.currentTimeMillis();
        resolver.takePersistableUriPermission(target, Intent.FLAG_GRANT_READ_URI_PERMISSION);
        after = System.currentTimeMillis();
        assertPersistedUriPermission(target, Intent.FLAG_GRANT_READ_URI_PERMISSION, before, after);

        // And take yet again, both read and write
        before = System.currentTimeMillis();
        resolver.takePersistableUriPermission(target,
                Intent.FLAG_GRANT_READ_URI_PERMISSION | Intent.FLAG_GRANT_WRITE_URI_PERMISSION);
        after = System.currentTimeMillis();
        assertPersistedUriPermission(target,
                Intent.FLAG_GRANT_READ_URI_PERMISSION | Intent.FLAG_GRANT_WRITE_URI_PERMISSION,
                before, after);

        // Now drop the persisted grant; write first, then read
        resolver.releasePersistableUriPermission(target, Intent.FLAG_GRANT_READ_URI_PERMISSION);
        assertPersistedUriPermission(target, Intent.FLAG_GRANT_WRITE_URI_PERMISSION, before, after);
        resolver.releasePersistableUriPermission(target, Intent.FLAG_GRANT_WRITE_URI_PERMISSION);
        assertNoPersistedUriPermission();

        // And even though we dropped the persistable grants, our activity is
        // still running with the global grants (until reboot).
        assertReadingClipAllowed(clip);
        assertWritingClipAllowed(clip);

        ReceiveUriActivity.finishCurInstanceSync();
    }

    /**
     * Validate behavior of prefix permission grants.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.usespermissiondiffcertapp.UriGrantsTest"	"testGrantPrefixUriPermission"	""	"/home/gpoor/cts-12-source/cts/hostsidetests/appsecurity/test-apps/UsePermissionDiffCert/src/com/android/cts/usespermissiondiffcertapp/UriGrantsTest.java"	""	"public void testGrantPrefixUriPermission() throws Exception {
        final Uri target = Uri.withAppendedPath(PERM_URI_GRANTING, ""foo1"");
        final Uri targetMeow = Uri.withAppendedPath(target, ""meow"");
        final Uri targetMeowCat = Uri.withAppendedPath(targetMeow, ""cat"");

        final ClipData clip = makeSingleClipData(target);
        final ClipData clipMeow = makeSingleClipData(targetMeow);
        final ClipData clipMeowCat = makeSingleClipData(targetMeowCat);

        // Make sure we can't see the target
        assertReadingClipNotAllowed(clip, ""reading should have failed"");
        assertWritingClipNotAllowed(clip, ""writing should have failed"");

        // Give ourselves prefix read access
        ReceiveUriActivity.clearStarted();
        grantClipUriPermissionViaActivity(clipMeow, Intent.FLAG_GRANT_READ_URI_PERMISSION
                | Intent.FLAG_GRANT_PREFIX_URI_PERMISSION);
        ReceiveUriActivity.waitForStart();

        // Verify prefix read access
        assertReadingClipNotAllowed(clip, ""reading should have failed"");
        assertReadingClipAllowed(clipMeow);
        assertReadingClipAllowed(clipMeowCat);
        assertWritingClipNotAllowed(clip, ""writing should have failed"");
        assertWritingClipNotAllowed(clipMeow, ""writing should have failed"");
        assertWritingClipNotAllowed(clipMeowCat, ""writing should have failed"");

        // Now give ourselves exact write access
        ReceiveUriActivity.clearNewIntent();
        grantClipUriPermissionViaActivity(clip, Intent.FLAG_GRANT_WRITE_URI_PERMISSION);
        ReceiveUriActivity.waitForNewIntent();

        // Verify we have exact write access, but not prefix write
        assertReadingClipNotAllowed(clip, ""reading should have failed"");
        assertReadingClipAllowed(clipMeow);
        assertReadingClipAllowed(clipMeowCat);
        assertWritingClipAllowed(clip);
        assertWritingClipNotAllowed(clipMeow, ""writing should have failed"");
        assertWritingClipNotAllowed(clipMeowCat, ""writing should have failed"");

        ReceiveUriActivity.finishCurInstanceSync();
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.usespermissiondiffcertapp.UriGrantsTest"	"testGrantPersistablePrefixUriPermission"	""	"/home/gpoor/cts-12-source/cts/hostsidetests/appsecurity/test-apps/UsePermissionDiffCert/src/com/android/cts/usespermissiondiffcertapp/UriGrantsTest.java"	""	"public void testGrantPersistablePrefixUriPermission() {
        final ContentResolver resolver = getContext().getContentResolver();

        final Uri target = Uri.withAppendedPath(PERM_URI_GRANTING, ""foo2"");
        final Uri targetMeow = Uri.withAppendedPath(target, ""meow"");

        final ClipData clip = makeSingleClipData(target);
        final ClipData clipMeow = makeSingleClipData(targetMeow);

        // Make sure we can't see the target
        assertReadingClipNotAllowed(clip, ""reading should have failed"");

        // Give ourselves prefix read access
        ReceiveUriActivity.clearStarted();
        grantClipUriPermissionViaActivity(clip, Intent.FLAG_GRANT_READ_URI_PERMISSION
                | Intent.FLAG_GRANT_PERSISTABLE_URI_PERMISSION
                | Intent.FLAG_GRANT_PREFIX_URI_PERMISSION);
        ReceiveUriActivity.waitForStart();

        // Verify prefix read access
        assertReadingClipAllowed(clip);
        assertReadingClipAllowed(clipMeow);

        // Verify we can persist direct grant
        long before = System.currentTimeMillis();
        resolver.takePersistableUriPermission(target, Intent.FLAG_GRANT_READ_URI_PERMISSION);
        long after = System.currentTimeMillis();
        assertPersistedUriPermission(target, Intent.FLAG_GRANT_READ_URI_PERMISSION, before, after);

        // But we can't take anywhere under the prefix
        try {
            resolver.takePersistableUriPermission(targetMeow,
                    Intent.FLAG_GRANT_READ_URI_PERMISSION);
            fail(""taking under prefix should have failed"");
        } catch (SecurityException expected) {
        }

        // Should still have access regardless of taking
        assertReadingClipAllowed(clip);
        assertReadingClipAllowed(clipMeow);

        // And clean up our grants
        resolver.releasePersistableUriPermission(target, Intent.FLAG_GRANT_READ_URI_PERMISSION);
        assertNoPersistedUriPermission();

        ReceiveUriActivity.finishCurInstanceSync();
    }

    /**
     * Validate behavior of directly granting/revoking permission grants.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.CodecDecoderTest"	"isFormatSupported"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/CodecDecoderTest.java"	""	"/*
 *.
 */

package android.mediav2.cts;

import android.media.MediaCodec;
import android.media.MediaCodecInfo;
import android.media.MediaExtractor;
import android.media.MediaFormat;
import android.util.Log;
import android.view.Surface;

import androidx.test.filters.LargeTest;
import androidx.test.filters.SmallTest;

import org.junit.Assume;
import org.junit.Ignore;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.junit.runners.Parameterized;

import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.nio.channels.FileChannel;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.List;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;

/**
 * Validate decode functionality of listed decoder components
 *
 * The test aims to test all decoders advertised in MediaCodecList. Hence we are not using
 * MediaCodecList#findDecoderForFormat to create codec. Further, it can so happen that the
 * test clip chosen is not supported by component (codecCapabilities.isFormatSupported()
 * fails), then it is better to replace the clip but not skip testing the component. The idea
 * of these tests are not to cover CDD requirements but to test components and their plugins
 */
@RunWith(Parameterized.class)
public class CodecDecoderTest extends CodecDecoderTestBase {
    private static final String LOG_TAG = CodecDecoderTest.class.getSimpleName();
    private static final float RMS_ERROR_TOLERANCE = 1.05f;        // 5%

    private final String mRefFile;
    private final String mReconfigFile;
    private final float mRmsError;
    private final long mRefCRC;

    public CodecDecoderTest(String decoder, String mime, String testFile, String refFile,
            String reconfigFile, float rmsError, long refCRC) {
        super(decoder, mime, testFile);
        mRefFile = refFile;
        mReconfigFile = reconfigFile;
        mRmsError = rmsError;
        mRefCRC = refCRC;
    }

    static short[] setUpAudioReference(String file) throws IOException {
        File refFile = new File(file);
        short[] refData;
        try (FileInputStream refStream = new FileInputStream(refFile)) {
            FileChannel fileChannel = refStream.getChannel();
            int length = (int) refFile.length();
            ByteBuffer refBuffer = ByteBuffer.allocate(length);
            refBuffer.order(ByteOrder.LITTLE_ENDIAN);
            fileChannel.read(refBuffer);
            refData = new short[length / 2];
            refBuffer.position(0);
            for (int i = 0; i < length / 2; i++) {
                refData[i] = refBuffer.getShort();
            }
        }
        return refData;
    }

    private ArrayList<MediaCodec.BufferInfo> createSubFrames(ByteBuffer buffer, int sfCount) {
        int size = (int) mExtractor.getSampleSize();
        if (size < 0) return null;
        mExtractor.readSampleData(buffer, 0);
        long pts = mExtractor.getSampleTime();
        int flags = mExtractor.getSampleFlags();
        if (size < sfCount) sfCount = size;
        ArrayList<MediaCodec.BufferInfo> list = new ArrayList<>();
        int offset = 0;
        for (int i = 0; i < sfCount; i++) {
            MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
            info.offset = offset;
            info.presentationTimeUs = pts;
            info.flags = 0;
            if ((flags & MediaExtractor.SAMPLE_FLAG_SYNC) != 0) {
                info.flags |= MediaCodec.BUFFER_FLAG_KEY_FRAME;
            }
            if ((flags & MediaExtractor.SAMPLE_FLAG_PARTIAL_FRAME) != 0) {
                info.flags |= MediaCodec.BUFFER_FLAG_PARTIAL_FRAME;
            }
            if (i != sfCount - 1) {
                info.size = size / sfCount;
                info.flags |= MediaExtractor.SAMPLE_FLAG_PARTIAL_FRAME;
            } else {
                info.size = size - offset;
            }
            list.add(info);
            offset += info.size;
        }
        return list;
    }

    @Parameterized.Parameters(name = ""{index}({0}_{1})"")
    public static Collection<Object[]> input() {
        final boolean isEncoder = false;
        final boolean needAudio = true;
        final boolean needVideo = true;
        // mime, testClip, referenceClip, reconfigureTestClip, refRmsError, refCRC32
        final List<Object[]> exhaustiveArgsList = Arrays.asList(new Object[][]{
                {MediaFormat.MIMETYPE_AUDIO_MPEG, ""bbb_1ch_8kHz_lame_cbr.mp3"",
                        ""bbb_1ch_8kHz_s16le.raw"", ""bbb_2ch_44kHz_lame_vbr.mp3"", 91.022f, -1L},
                {MediaFormat.MIMETYPE_AUDIO_MPEG, ""bbb_2ch_44kHz_lame_cbr.mp3"",
                        ""bbb_2ch_44kHz_s16le.raw"", ""bbb_1ch_16kHz_lame_vbr.mp3"", 103.60f, -1L},
                {MediaFormat.MIMETYPE_AUDIO_AMR_WB, ""bbb_1ch_16kHz_16kbps_amrwb.3gp"",
                        ""bbb_1ch_16kHz_s16le.raw"", ""bbb_1ch_16kHz_23kbps_amrwb.3gp"", 2393.598f,
                        -1L},
                {MediaFormat.MIMETYPE_AUDIO_AMR_NB, ""bbb_1ch_8kHz_10kbps_amrnb.3gp"",
                        ""bbb_1ch_8kHz_s16le.raw"", ""bbb_1ch_8kHz_8kbps_amrnb.3gp"", -1.0f, -1L},
                {MediaFormat.MIMETYPE_AUDIO_FLAC, ""bbb_1ch_16kHz_flac.mka"",
                        ""bbb_1ch_16kHz_s16le.raw"", ""bbb_2ch_44kHz_flac.mka"", 0.0f, -1L},
                {MediaFormat.MIMETYPE_AUDIO_FLAC, ""bbb_2ch_44kHz_flac.mka"",
                        ""bbb_2ch_44kHz_s16le.raw"", ""bbb_1ch_16kHz_flac.mka"", 0.0f, -1L},
                {MediaFormat.MIMETYPE_AUDIO_RAW, ""bbb_1ch_16kHz.wav"", ""bbb_1ch_16kHz_s16le.raw"",
                        ""bbb_2ch_44kHz.wav"", 0.0f, -1L},
                {MediaFormat.MIMETYPE_AUDIO_RAW, ""bbb_2ch_44kHz.wav"", ""bbb_2ch_44kHz_s16le.raw"",
                        ""bbb_1ch_16kHz.wav"", 0.0f, -1L},
                {MediaFormat.MIMETYPE_AUDIO_G711_ALAW, ""bbb_1ch_8kHz_alaw.wav"",
                        ""bbb_1ch_8kHz_s16le.raw"", ""bbb_2ch_8kHz_alaw.wav"", 23.08678f, -1L},
                {MediaFormat.MIMETYPE_AUDIO_G711_MLAW, ""bbb_1ch_8kHz_mulaw.wav"",
                        ""bbb_1ch_8kHz_s16le.raw"", ""bbb_2ch_8kHz_mulaw.wav"", 24.4131f, -1L},
                {MediaFormat.MIMETYPE_AUDIO_MSGSM, ""bbb_1ch_8kHz_gsm.wav"",
                        ""bbb_1ch_8kHz_s16le.raw"", ""bbb_1ch_8kHz_gsm.wav"", 946.02698f, -1L},
                {MediaFormat.MIMETYPE_AUDIO_VORBIS, ""bbb_1ch_16kHz_vorbis.mka"",
                        ""bbb_1ch_8kHz_s16le.raw"", ""bbb_2ch_44kHz_vorbis.mka"", -1.0f, -1L},
                {MediaFormat.MIMETYPE_AUDIO_OPUS, ""bbb_2ch_48kHz_opus.mka"",
                        ""bbb_2ch_48kHz_s16le.raw"", ""bbb_1ch_48kHz_opus.mka"", -1.0f, -1L},
                {MediaFormat.MIMETYPE_AUDIO_AAC, ""bbb_1ch_16kHz_aac.mp4"",
                        ""bbb_1ch_16kHz_s16le.raw"", ""bbb_2ch_44kHz_aac.mp4"", -1.0f, -1L},
                {MediaFormat.MIMETYPE_VIDEO_MPEG2, ""bbb_340x280_768kbps_30fps_mpeg2.mp4"", null,
                        ""bbb_520x390_1mbps_30fps_mpeg2.mp4"", -1.0f, -1L},
                {MediaFormat.MIMETYPE_VIDEO_AVC, ""bbb_340x280_768kbps_30fps_avc.mp4"", null,
                        ""bbb_520x390_1mbps_30fps_avc.mp4"", -1.0f, 1746312400L},
                {MediaFormat.MIMETYPE_VIDEO_HEVC, ""bbb_520x390_1mbps_30fps_hevc.mp4"", null,
                        ""bbb_340x280_768kbps_30fps_hevc.mp4"", -1.0f, 3061322606L},
                {MediaFormat.MIMETYPE_VIDEO_MPEG4, ""bbb_128x96_64kbps_12fps_mpeg4.mp4"",
                        null, ""bbb_176x144_192kbps_15fps_mpeg4.mp4"", -1.0f, -1L},
                {MediaFormat.MIMETYPE_VIDEO_H263, ""bbb_176x144_128kbps_15fps_h263.3gp"",
                        null, ""bbb_176x144_192kbps_10fps_h263.3gp"", -1.0f, -1L},
                {MediaFormat.MIMETYPE_VIDEO_VP8, ""bbb_340x280_768kbps_30fps_vp8.webm"", null,
                        ""bbb_520x390_1mbps_30fps_vp8.webm"", -1.0f, 2030620796L},
                {MediaFormat.MIMETYPE_VIDEO_VP9, ""bbb_340x280_768kbps_30fps_vp9.webm"", null,
                        ""bbb_520x390_1mbps_30fps_vp9.webm"", -1.0f, 4122701060L},
                {MediaFormat.MIMETYPE_VIDEO_AV1, ""bbb_340x280_768kbps_30fps_av1.mp4"", null,
                        ""bbb_520x390_1mbps_30fps_av1.mp4"", -1.0f, 400672933L},
        });
        return prepareParamList(exhaustiveArgsList, isEncoder, needAudio, needVideo, true);
    }

    private native boolean nativeTestSimpleDecode(String decoder, Surface surface, String mime,
            String testFile, String refFile, float rmsError, long checksum);

    static void verify(OutputManager outBuff, String refFile, float rmsError, long refCRC)
            throws IOException {
        if (rmsError >= 0) {
            short[] refData = setUpAudioReference(mInpPrefix + refFile);
            float currError = outBuff.getRmsError(refData);
            float errMargin = rmsError * RMS_ERROR_TOLERANCE;
            assertTrue(String.format(""%s rms error too high ref/exp/got %f/%f/%f"", refFile,
                    rmsError, errMargin, currError), currError <= errMargin);
        } else if (refCRC >= 0) {
            assertEquals(""checksum mismatch"", refCRC, outBuff.getCheckSumImage());
        }
    }

    /**
     * Tests decoder for combinations:
     * 1. Codec Sync Mode, Signal Eos with Last frame
     * 2. Codec Sync Mode, Signal Eos Separately
     * 3. Codec Async Mode, Signal Eos with Last frame
     * 4. Codec Async Mode, Signal Eos Separately
     * In all these scenarios, Timestamp ordering is verified, For audio the Rms of output has to be
     * within the allowed tolerance. The output has to be consistent (not flaky) in all runs.
     */
    @LargeTest"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaCodecPlayerTest"	"MediaCodecPlayerTest"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaCodecPlayerTest.java"	""	"public void test/*
 *.
 */

package android.media.cts;

import android.media.MediaFormat;
import android.net.Uri;
import android.view.Surface;
import java.util.Arrays;
import java.util.List;

public class MediaCodecPlayerTest extends MediaCodecPlayerTestBase<MediaStubActivity2> {

    private static final String MIME_VIDEO_AVC = MediaFormat.MIMETYPE_VIDEO_AVC;

    private static final String CLEAR_AUDIO_PATH = ""/clear/h264/llama/llama_aac_audio.mp4"";
    private static final String CLEAR_VIDEO_PATH = ""/clear/h264/llama/llama_h264_main_720p_8000.mp4"";

    private static final int VIDEO_WIDTH_CLEAR = 1280;
    private static final int VIDEO_HEIGHT_CLEAR = 674;

    public MediaCodecPlayerTest() {
        super(MediaStubActivity2.class);
    }

    private void playOnSurfaces(List<Surface> surfaces) throws Exception {
        testPlayback(
            MIME_VIDEO_AVC, new String[0],
            Uri.parse(Utils.getMediaPath() + CLEAR_AUDIO_PATH),
            Uri.parse(Utils.getMediaPath() + CLEAR_VIDEO_PATH),
            VIDEO_WIDTH_CLEAR, VIDEO_HEIGHT_CLEAR, surfaces);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.CodecUnitTest"	"TestApi"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/CodecUnitTest.java"	""	"/*
 *.
 */

package android.mediav2.cts;

import android.media.Image;
import android.media.MediaCodec;
import android.media.MediaCodecInfo;
import android.media.MediaFormat;
import android.os.Bundle;
import android.util.Pair;

import androidx.test.filters.SmallTest;

import org.junit.After;
import org.junit.Ignore;
import org.junit.Rule;
import org.junit.Test;
import org.junit.experimental.runners.Enclosed;
import org.junit.rules.Timeout;
import org.junit.runner.RunWith;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.concurrent.TimeUnit;

import static org.junit.Assert.assertFalse;
import static org.junit.Assert.assertNotNull;
import static org.junit.Assert.assertNull;
import static org.junit.Assert.assertTrue;
import static org.junit.Assert.fail;

@RunWith(Enclosed.class)
public class CodecUnitTest {
    static final int PER_TEST_TIMEOUT_MS = 10000;
    static final long STALL_TIME_MS = 1000;

    @SmallTest
    public static class TestApi extends CodecTestBase {
        @Rule
        public Timeout timeout = new Timeout(PER_TEST_TIMEOUT_MS, TimeUnit.MILLISECONDS);

        @After
        public void hasSeenError() {
            assertFalse(mAsyncHandle.hasSeenError());
        }

        public TestApi() {
            mAsyncHandle = new CodecAsyncHandler();
        }

        void enqueueInput(int bufferIndex) {
            fail(""something went wrong, shouldn't have reached here"");
        }

        void dequeueOutput(int bufferIndex, MediaCodec.BufferInfo info) {
            if ((info.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                mSawOutputEOS = true;
            }
            mCodec.releaseOutputBuffer(bufferIndex, false);
        }

        private MediaFormat getSampleAudioFormat() {
            MediaFormat format = new MediaFormat();
            String mime = MediaFormat.MIMETYPE_AUDIO_AAC;
            format.setString(MediaFormat.KEY_MIME, mime);
            format.setInteger(MediaFormat.KEY_BIT_RATE, 64000);
            format.setInteger(MediaFormat.KEY_SAMPLE_RATE, 16000);
            format.setInteger(MediaFormat.KEY_CHANNEL_COUNT, 1);
            return format;
        }

        private MediaFormat getSampleVideoFormat() {
            MediaFormat format = new MediaFormat();
            String mime = MediaFormat.MIMETYPE_VIDEO_AVC;
            format.setString(MediaFormat.KEY_MIME, mime);
            format.setInteger(MediaFormat.KEY_BIT_RATE, 256000);
            format.setInteger(MediaFormat.KEY_WIDTH, 352);
            format.setInteger(MediaFormat.KEY_HEIGHT, 288);
            format.setInteger(MediaFormat.KEY_FRAME_RATE, 30);
            format.setFloat(MediaFormat.KEY_I_FRAME_INTERVAL, 1.0f);
            format.setInteger(MediaFormat.KEY_COLOR_FORMAT,
                    MediaCodecInfo.CodecCapabilities.COLOR_FormatYUV420Flexible);
            return format;
        }

        private Bundle updateBitrate(int bitrate) {
            final Bundle bitrateUpdate = new Bundle();
            bitrateUpdate.putInt(MediaCodec.PARAMETER_KEY_VIDEO_BITRATE, bitrate);
            return bitrateUpdate;
        }

        void testConfigureCodecForIncompleteFormat(MediaFormat format, String[] keys,
                boolean isEncoder) throws IOException {
            String mime = format.getString(MediaFormat.KEY_MIME);
            if (isEncoder) {
                mCodec = MediaCodec.createEncoderByType(mime);
            } else {
                mCodec = MediaCodec.createDecoderByType(mime);
            }
            for (String key : keys) {
                MediaFormat formatClone = new MediaFormat(format);
                formatClone.removeKey(key);
                try {
                    mCodec.configure(formatClone, null, null,
                            isEncoder ? MediaCodec.CONFIGURE_FLAG_ENCODE : 0);
                    fail(""codec configure succeeds with missing mandatory keys :: "" + key);
                } catch (Exception e) {
                    if (!(e instanceof IllegalArgumentException)) {
                        fail(""codec configure rec/exp :: "" + e.toString() +
                                "" / IllegalArgumentException"");
                    }
                }
            }
            try {
                mCodec.configure(format, null, null,
                        isEncoder ? MediaCodec.CONFIGURE_FLAG_ENCODE : 0);
            } catch (Exception e) {
                fail(""configure failed unexpectedly"");
            } finally {
                mCodec.release();
            }
        }

        void testConfigureCodecForBadFlags(boolean isEncoder) throws IOException {
            MediaFormat format = getSampleAudioFormat();
            String mime = format.getString(MediaFormat.KEY_MIME);
            if (isEncoder) {
                mCodec = MediaCodec.createEncoderByType(mime);
            } else {
                mCodec = MediaCodec.createDecoderByType(mime);
            }
            try {
                mCodec.configure(format, null, null,
                        isEncoder ? 0 : MediaCodec.CONFIGURE_FLAG_ENCODE);
                fail(""codec configure succeeds with bad configure flag"");
            } catch (Exception e) {
                if (!(e instanceof IllegalArgumentException)) {
                    fail(""codec configure rec/exp :: "" + e.toString() +
                            "" / IllegalArgumentException"");
                }
            } finally {
                mCodec.release();
            }
        }

        void tryConfigureCodecInInvalidState(MediaFormat format, boolean isAsync, String msg) {
            try {
                configureCodec(format, isAsync, false, true);
                fail(msg);
            } catch (IllegalStateException e) {
                // expected
            }
        }

        void tryDequeueInputBufferInInvalidState(String msg) {
            try {
                mCodec.dequeueInputBuffer(Q_DEQ_TIMEOUT_US);
                fail(msg);
            } catch (IllegalStateException e) {
                // expected
            }
        }

        void tryDequeueOutputBufferInInvalidState(String msg) {
            try {
                MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
                mCodec.dequeueOutputBuffer(info, Q_DEQ_TIMEOUT_US);
                fail(msg);
            } catch (IllegalStateException e) {
                // expected
            }
        }

        void tryFlushInInvalidState(String msg) {
            try {
                flushCodec();
                fail(msg);
            } catch (IllegalStateException e) {
                // expected
            }
        }

        void tryGetMetaData(String msg) {
            try {
                mCodec.getName();
            } catch (IllegalStateException e) {
                fail(""get name resulted in"" + e.getMessage());
            }

            try {
                mCodec.getCanonicalName();
            } catch (IllegalStateException e) {
                fail(""get canonical name resulted in"" + e.getMessage());
            }

            try {
                mCodec.getCodecInfo();
            } catch (IllegalStateException e) {
                fail(""get codec info resulted in"" + e.getMessage());
            }

            try {
                mCodec.getMetrics();
            } catch (IllegalStateException e) {
                fail(""get metrics resulted in"" + e.getMessage());
            }
        }

        void tryGetInputBufferInInvalidState(String msg) {
            try {
                mCodec.getInputBuffer(0);
                fail(msg);
            } catch (IllegalStateException e) {
                // expected
            }
        }

        void tryGetInputFormatInInvalidState(String msg) {
            try {
                mCodec.getInputFormat();
                fail(msg);
            } catch (IllegalStateException e) {
                // expected
            }
        }

        void tryGetOutputBufferInInvalidState(String msg) {
            try {
                mCodec.getOutputBuffer(0);
                fail(msg);
            } catch (IllegalStateException e) {
                // expected
            }
        }

        void tryGetOutputFormatInInvalidState(String msg) {
            try {
                mCodec.getOutputFormat();
                fail(msg);
            } catch (IllegalStateException e) {
                // expected
            }

            try {
                mCodec.getOutputFormat(0);
                fail(msg);
            } catch (IllegalStateException e) {
                // expected
            }
        }

        void tryStartInInvalidState(String msg) {
            try {
                mCodec.start();
                fail(msg);
            } catch (IllegalStateException e) {
                // expected
            }
        }

        void tryGetInputImageInInvalidState(String msg) {
            try {
                mCodec.getInputImage(0);
                fail(msg);
            } catch (IllegalStateException e) {
                // expected
            }
        }

        void tryGetOutputImageInInvalidState(String msg) {
            try {
                mCodec.getOutputImage(0);
                fail(msg);
            } catch (IllegalStateException e) {
                // expected
            }
        }

        void tryQueueInputBufferInInvalidState(String msg) {
            try {
                mCodec.queueInputBuffer(0, 0, 0, 0, MediaCodec.BUFFER_FLAG_END_OF_STREAM);
                fail(msg);
            } catch (IllegalStateException e) {
                // expected
            }
        }

        void tryReleaseOutputBufferInInvalidState(String msg) {
            try {
                mCodec.releaseOutputBuffer(0, false);
                fail(msg);
            } catch (IllegalStateException e) {
                // expected
            }
        }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.CodecUnitTest"	"testConfigureForNullFormat"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/CodecUnitTest.java"	""	"@Ignore(""TODO(b/151302868)"")
        public void testConfigureForNullFormat() throws IOException {
            mCodec = MediaCodec.createEncoderByType(MediaFormat.MIMETYPE_AUDIO_AAC);
            mCodec.configure(null, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE);
            mCodec.release();
        }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.CodecUnitTest"	"testConfigureForEmptyFormat"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/CodecUnitTest.java"	""	"@Ignore(""TODO(b/151302868)"")
        public void testConfigureForEmptyFormat() throws IOException {
            mCodec = MediaCodec.createEncoderByType(MediaFormat.MIMETYPE_AUDIO_AAC);
            mCodec.configure(new MediaFormat(), null, null, MediaCodec.CONFIGURE_FLAG_ENCODE);
            mCodec.release();
        }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaRecorderTest"	"testRecorderMPEG2TS"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaRecorderTest.java"	""	"public void testRecorderMPEG2TS() throws Exception {
        int width;
        int height;
        Camera camera = null;
        if (!hasCamera()) {
            MediaUtils.skipTest(""no camera"");
            return;
        }
        if (!hasMicrophone() || !hasAac()) {
            MediaUtils.skipTest(""no audio codecs or microphone"");
            return;
        }
        // Try to get camera profile for QUALITY_LOW; if unavailable,
        // set the video size to default value.
        CamcorderProfile profile = CamcorderProfile.get(
                0 /* cameraId */, CamcorderProfile.QUALITY_LOW);
        if (profile != null) {
            width = profile.videoFrameWidth;
            height = profile.videoFrameHeight;
        } else {
            width = VIDEO_WIDTH;
            height = VIDEO_HEIGHT;
        }
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.MIC);
        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);
        mMediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.MPEG_2_TS);
        mMediaRecorder.setVideoEncoder(MediaRecorder.VideoEncoder.H264);
        mMediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.AAC);
        mMediaRecorder.setVideoSize(width, height);
        mMediaRecorder.setVideoEncodingBitRate(VIDEO_BIT_RATE_IN_BPS);
        mMediaRecorder.setPreviewDisplay(mActivity.getSurfaceHolder().getSurface());
        mMediaRecorder.prepare();
        mMediaRecorder.start();
        Thread.sleep(RECORD_TIME_MS);

        // verify some getMetrics() behaviors while we're here.
        PersistableBundle metrics = mMediaRecorder.getMetrics();
        if (metrics == null) {
            fail(""MediaRecorder.getMetrics() returned null metrics"");
        } else if (metrics.isEmpty()) {
            fail(""MediaRecorder.getMetrics() returned empty metrics"");
        } else {
            int size = metrics.size();
            Set<String> keys = metrics.keySet();

            if (size == 0) {
                fail(""MediaRecorder.getMetrics().size() reports empty record"");
            }

            if (keys == null) {
                fail(""MediaMetricsSet returned no keys"");
            } else if (keys.size() != size) {
                fail(""MediaMetricsSet.keys().size() mismatch MediaMetricsSet.size()"");
            }

            // ensure existence of some known fields
            int videoBitRate = metrics.getInt(MediaRecorder.MetricsConstants.VIDEO_BITRATE, -1);
            if (videoBitRate != VIDEO_BIT_RATE_IN_BPS) {
                fail(""getMetrics() videoEncodeBitrate set "" +
                     VIDEO_BIT_RATE_IN_BPS + "" got "" + videoBitRate);
            }

            // valid values are -1.0 and >= 0;
            // tolerate some floating point rounding variability
            double captureFrameRate = metrics.getDouble(MediaRecorder.MetricsConstants.CAPTURE_FPS, -2);
            if (captureFrameRate < 0.) {
                assertEquals(""getMetrics() capture framerate="" + captureFrameRate, -1.0, captureFrameRate, 0.001);
            }
        }

        mMediaRecorder.stop();
        checkOutputExist();
    }

    @UiThreadTest"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaRecorderTest"	"testRecordingAudioInRawFormats"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaRecorderTest.java"	""	"public void testRecordingAudioInRawFormats() throws Exception {
        int testsRun = 0;
        if (hasAmrNb()) {
            testsRun += testRecordAudioInRawFormat(
                    MediaRecorder.OutputFormat.AMR_NB,
                    MediaRecorder.AudioEncoder.AMR_NB);
        }

        if (hasAmrWb()) {
            testsRun += testRecordAudioInRawFormat(
                    MediaRecorder.OutputFormat.AMR_WB,
                    MediaRecorder.AudioEncoder.AMR_WB);
        }

        if (hasAac()) {
            testsRun += testRecordAudioInRawFormat(
                    MediaRecorder.OutputFormat.AAC_ADTS,
                    MediaRecorder.AudioEncoder.AAC);
        }
        if (testsRun == 0) {
            MediaUtils.skipTest(""no audio codecs or microphone"");
        }
    }

    private int testRecordAudioInRawFormat(
            int fileFormat, int codec) throws Exception {
        if (!hasMicrophone()) {
            return 0; // skip
        }
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.MIC);
        mMediaRecorder.setOutputFormat(fileFormat);
        mMediaRecorder.setOutputFile(OUTPUT_PATH);
        mMediaRecorder.setAudioEncoder(codec);
        recordMedia(MAX_FILE_SIZE, mOutFile);
        return 1;
    }

    private void configureDefaultMediaRecorder() {
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.MIC);
        mMediaRecorder.setAudioChannels(AUDIO_NUM_CHANNELS);
        mMediaRecorder.setAudioSamplingRate(AUDIO_SAMPLE_RATE_HZ);
        mMediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.THREE_GPP);
        mMediaRecorder.setOutputFile(OUTPUT_PATH);
        mMediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.AAC);
        mMediaRecorder.setMaxFileSize(MAX_FILE_SIZE * 10);
    }

    @CddTest(requirement=""5.4.1/C-1-4"")"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaRecorderTest"	"testGetActiveMicrophones"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaRecorderTest.java"	""	"public void testGetActiveMicrophones() throws Exception {
        if (!hasMicrophone() || !hasAac()) {
            MediaUtils.skipTest(""no audio codecs or microphone"");
            return;
        }
        configureDefaultMediaRecorder();
        mMediaRecorder.prepare();
        mMediaRecorder.start();
        Thread.sleep(1000);
        List<MicrophoneInfo> activeMicrophones = mMediaRecorder.getActiveMicrophones();
        assertTrue(activeMicrophones.size() > 0);
        for (MicrophoneInfo activeMicrophone : activeMicrophones) {
            printMicrophoneInfo(activeMicrophone);
        }
        mMediaRecorder.stop();
    }

    private void printMicrophoneInfo(MicrophoneInfo microphone) {
        Log.i(TAG, ""deviceId:"" + microphone.getDescription());
        Log.i(TAG, ""portId:"" + microphone.getId());
        Log.i(TAG, ""type:"" + microphone.getType());
        Log.i(TAG, ""address:"" + microphone.getAddress());
        Log.i(TAG, ""deviceLocation:"" + microphone.getLocation());
        Log.i(TAG, ""deviceGroup:"" + microphone.getGroup()
            + "" index:"" + microphone.getIndexInTheGroup());
        MicrophoneInfo.Coordinate3F position = microphone.getPosition();
        Log.i(TAG, ""position:"" + position.x + "","" + position.y + "","" + position.z);
        MicrophoneInfo.Coordinate3F orientation = microphone.getOrientation();
        Log.i(TAG, ""orientation:"" + orientation.x + "","" + orientation.y + "","" + orientation.z);
        Log.i(TAG, ""frequencyResponse:"" + microphone.getFrequencyResponse());
        Log.i(TAG, ""channelMapping:"" + microphone.getChannelMapping());
        Log.i(TAG, ""sensitivity:"" + microphone.getSensitivity());
        Log.i(TAG, ""max spl:"" + microphone.getMaxSpl());
        Log.i(TAG, ""min spl:"" + microphone.getMinSpl());
        Log.i(TAG, ""directionality:"" + microphone.getDirectionality());
        Log.i(TAG, ""******"");
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaRecorderTest"	"testRecorderAudio"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaRecorderTest.java"	""	"public void testRecorderAudio() throws Exception {
        if (!hasMicrophone() || !hasAac()) {
            MediaUtils.skipTest(""no audio codecs or microphone"");
            return;
        }
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.MIC);
        assertEquals(0, mMediaRecorder.getMaxAmplitude());
        mMediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.THREE_GPP);
        mMediaRecorder.setOutputFile(OUTPUT_PATH);
        mMediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.AAC);
        mMediaRecorder.setAudioChannels(AUDIO_NUM_CHANNELS);
        mMediaRecorder.setAudioSamplingRate(AUDIO_SAMPLE_RATE_HZ);
        mMediaRecorder.setAudioEncodingBitRate(AUDIO_BIT_RATE_IN_BPS);
        recordMedia(MAX_FILE_SIZE, mOutFile);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaRecorderTest"	"testOnInfoListener"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaRecorderTest.java"	""	"public void testOnInfoListener() throws Exception {
        if (!hasMicrophone() || !hasAac()) {
            MediaUtils.skipTest(""no audio codecs or microphone"");
            return;
        }
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.MIC);
        mMediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.THREE_GPP);
        mMediaRecorder.setMaxDuration(MAX_DURATION_MSEC);
        mMediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.AAC);
        mMediaRecorder.prepare();
        mMediaRecorder.start();
        Thread.sleep(RECORD_TIME_MS);
        assertTrue(mOnInfoCalled);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaRecorderTest"	"testSetMaxDuration"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaRecorderTest.java"	""	"public void testSetMaxDuration() throws Exception {
        if (!hasMicrophone() || !hasAac()) {
            MediaUtils.skipTest(""no audio codecs or microphone"");
            return;
        }
        testSetMaxDuration(RECORD_TIME_LONG_MS, RECORDED_DUR_TOLERANCE_MS);
    }

    private void testSetMaxDuration(long durationMs, long toleranceMs) throws Exception {
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.MIC);
        mMediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.THREE_GPP);
        mMediaRecorder.setMaxDuration((int)durationMs);
        mMediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.AAC);
        mMediaRecorder.prepare();
        mMediaRecorder.start();
        long startTimeMs = System.currentTimeMillis();
        if (!mMaxDurationCond.block(durationMs + toleranceMs)) {
            fail(""timed out waiting for MEDIA_RECORDER_INFO_MAX_DURATION_REACHED"");
        }
        long endTimeMs = System.currentTimeMillis();
        long actualDurationMs = endTimeMs - startTimeMs;
        mMediaRecorder.stop();
        checkRecordedTime(durationMs, actualDurationMs, toleranceMs);
    }

    private void checkRecordedTime(long expectedMs, long actualMs, long tolerance) {
        assertEquals(expectedMs, actualMs, tolerance);
        long actualFileDurationMs = getRecordedFileDurationMs(OUTPUT_PATH);
        assertEquals(actualFileDurationMs, actualMs, tolerance);
    }

    private int getRecordedFileDurationMs(final String fileName) {
        MediaMetadataRetriever retriever = new MediaMetadataRetriever();
        retriever.setDataSource(fileName);
        String durationStr = retriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_DURATION);
        assertNotNull(durationStr);
        return Integer.parseInt(durationStr);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaRecorderTest"	"testRecordExceedFileSizeLimit"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaRecorderTest.java"	""	"public void testRecordExceedFileSizeLimit() throws Exception {
        if (!hasMicrophone() || !hasCamera() || !hasAmrNb() || !hasH264()) {
            MediaUtils.skipTest(""no microphone, camera, or codecs"");
            return;
        }
        long fileSize = 128 * 1024;
        long tolerance = 50 * 1024;
        int width;
        int height;
        List<String> recordFileList = new ArrayList<String>();
        mFileIndex = 0;

        // Override the handler in setup for test.
        mMediaRecorder.setOnInfoListener(new OnInfoListener() {
            public void onInfo(MediaRecorder mr, int what, int extra) {
                mOnInfoCalled = true;
                if (what ==
                    MediaRecorder.MEDIA_RECORDER_INFO_MAX_DURATION_REACHED) {
                    Log.v(TAG, ""max duration reached"");
                    mMaxDurationCond.open();
                } else if (what ==
                    MediaRecorder.MEDIA_RECORDER_INFO_MAX_FILESIZE_REACHED) {
                    if (!mExpectMaxFileCond) {
                        fail(""Do not expect MEDIA_RECORDER_INFO_MAX_FILESIZE_REACHED"");
                    } else {
                        Log.v(TAG, ""max file size reached"");
                        mMaxFileSizeCond.open();
                    }
                } else if (what ==
                    MediaRecorder.MEDIA_RECORDER_INFO_MAX_FILESIZE_APPROACHING) {
                    Log.v(TAG, ""max file size is approaching"");
                    mMaxFileSizeApproachingCond.open();

                    // Test passing a read-only FileDescriptor and expect IOException.
                    if (mFileIndex == 1) {
                        RandomAccessFile file = null;
                        try {
                            String path = OUTPUT_PATH + '0';
                            file = new RandomAccessFile(path, ""r"");
                            mMediaRecorder.setNextOutputFile(file.getFD());
                            fail(""Expect IOException."");
                        } catch (IOException e) {
                            // Expected.
                        } finally {
                            try {
                                file.close();
                            } catch (IOException e) {
                                fail(""Fail to close file"");
                            }
                        }
                    }

                    // Test passing a read-only FileDescriptor and expect IOException.
                    if (mFileIndex == 2) {
                        ParcelFileDescriptor out = null;
                        String path = null;
                        try {
                            path = OUTPUT_PATH + '0';
                            out = ParcelFileDescriptor.open(new File(path),
                                    ParcelFileDescriptor.MODE_READ_ONLY | ParcelFileDescriptor.MODE_CREATE);
                            mMediaRecorder.setNextOutputFile(out.getFileDescriptor());
                            fail(""Expect IOException."");
                        } catch (IOException e) {
                            // Expected.
                        } finally {
                            try {
                                out.close();
                            } catch (IOException e) {
                                fail(""Fail to close file"");
                            }
                        }
                    }

                    // Test passing a write-only FileDescriptor and expect NO IOException.
                    if (mFileIndex == 3) {
                        try {
                            ParcelFileDescriptor out = null;
                            String path = OUTPUT_PATH + mFileIndex;
                            out = ParcelFileDescriptor.open(new File(path),
                                    ParcelFileDescriptor.MODE_WRITE_ONLY | ParcelFileDescriptor.MODE_CREATE);
                            mMediaRecorder.setNextOutputFile(out.getFileDescriptor());
                            out.close();
                            recordFileList.add(path);
                            mFileIndex++;
                        } catch (IOException e) {
                            fail(""Fail to set next output file error: "" + e);
                        }
                    } else if (mFileIndex < 6) {
                        try {
                            String path = OUTPUT_PATH + mFileIndex;
                            File nextFile = new File(path);
                            mMediaRecorder.setNextOutputFile(nextFile);
                            recordFileList.add(path);
                            mFileIndex++;
                        } catch (IOException e) {
                            fail(""Fail to set next output file error: "" + e);
                        }
                    }
                } else if (what ==
                    MediaRecorder.MEDIA_RECORDER_INFO_NEXT_OUTPUT_FILE_STARTED) {
                    Log.v(TAG, ""Next output file started"");
                    mNextOutputFileStartedCond.open();
                }
            }
        });
        mExpectMaxFileCond = false;
        mMediaRecorder.setOutputFile(OUTPUT_PATH + mFileIndex);
        recordFileList.add(OUTPUT_PATH + mFileIndex);
        mFileIndex++;
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.MIC);
        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);
        mMediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.MPEG_4);
        mMediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.AAC);
        mMediaRecorder.setVideoEncoder(MediaRecorder.VideoEncoder.H264);
        // Try to get camera profile for QUALITY_LOW; if unavailable,
        // set the video size to default value.
        CamcorderProfile profile = CamcorderProfile.get(
                0 /* cameraId */, CamcorderProfile.QUALITY_LOW);
        if (profile != null) {
            width = profile.videoFrameWidth;
            height = profile.videoFrameHeight;
        } else {
            width = VIDEO_WIDTH;
            height = VIDEO_HEIGHT;
        }
        mMediaRecorder.setVideoSize(width, height);
        mMediaRecorder.setVideoEncodingBitRate(256000);
        mMediaRecorder.setPreviewDisplay(mActivity.getSurfaceHolder().getSurface());
        mMediaRecorder.setMaxFileSize(fileSize);
        mMediaRecorder.prepare();
        mMediaRecorder.start();

        // Record total 5 files including previous one.
        int fileCount = 0;
        while (fileCount < 5) {
            if (!mMaxFileSizeApproachingCond.block(MAX_FILE_SIZE_TIMEOUT_MS)) {
                fail(""timed out waiting for MEDIA_RECORDER_INFO_MAX_FILESIZE_APPROACHING"");
            }
            if (!mNextOutputFileStartedCond.block(MAX_FILE_SIZE_TIMEOUT_MS)) {
                fail(""timed out waiting for MEDIA_RECORDER_INFO_NEXT_OUTPUT_FILE_STARTED"");
            }
            fileCount++;
            mMaxFileSizeApproachingCond.close();
            mNextOutputFileStartedCond.close();
        }

        mExpectMaxFileCond = true;
        if (!mMaxFileSizeCond.block(MAX_FILE_SIZE_TIMEOUT_MS)) {
            fail(""timed out waiting for MEDIA_RECORDER_INFO_MAX_FILESIZE_REACHED"");
        }
        mMediaRecorder.stop();

        for (String filePath : recordFileList) {
            checkOutputFileSize(filePath, fileSize, tolerance);
        }
    }

    private void checkOutputFileSize(final String fileName, long fileSize, long tolerance) {
        File file = new File(fileName);
        assertTrue(file.exists());
        assertEquals(fileSize, file.length(), tolerance);
        assertTrue(file.delete());
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaRecorderTest"	"testOnErrorListener"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaRecorderTest.java"	""	"public void testOnErrorListener() throws Exception {
        if (!hasMicrophone() || !hasAac()) {
            MediaUtils.skipTest(""no audio codecs or microphone"");
            return;
        }
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.DEFAULT);
        mMediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.THREE_GPP);
        mMediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.AAC);

        recordMedia(MAX_FILE_SIZE, mOutFile);
        // TODO: how can we trigger a recording error?
        assertFalse(mOnErrorCalled);
    }

    private void setupRecorder(String filename, boolean useSurface, boolean hasAudio)
            throws Exception {
        int codec = MediaRecorder.VideoEncoder.H264;
        int frameRate = getMaxFrameRateForCodec(codec);
        if (mMediaRecorder == null) {
            mMediaRecorder = new MediaRecorder();
        }

        if (!useSurface) {
            mCamera = Camera.open(0);
            Camera.Parameters params = mCamera.getParameters();
            frameRate = params.getPreviewFrameRate();
            setSupportedResolution(mCamera);
            mCamera.unlock();
            mMediaRecorder.setCamera(mCamera);
            mMediaRecorder.setPreviewDisplay(mActivity.getSurfaceHolder().getSurface());
        }

        mMediaRecorder.setVideoSource(useSurface ?
                MediaRecorder.VideoSource.SURFACE : MediaRecorder.VideoSource.CAMERA);

        if (hasAudio) {
            mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.MIC);
        }

        mMediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.THREE_GPP);
        mMediaRecorder.setOutputFile(filename);

        mMediaRecorder.setVideoEncoder(MediaRecorder.VideoEncoder.H264);
        mMediaRecorder.setVideoFrameRate(frameRate);
        mMediaRecorder.setVideoSize(mVideoWidth, mVideoHeight);

        if (hasAudio) {
            mMediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.AMR_NB);
        }
    }

    private Surface tryGetSurface(boolean shouldThrow) throws Exception {
        Surface surface = null;
        try {
            surface = mMediaRecorder.getSurface();
            assertFalse(""failed to throw IllegalStateException"", shouldThrow);
        } catch (IllegalStateException e) {
            assertTrue(""threw unexpected exception: "" + e, shouldThrow);
        }
        return surface;
    }

    private boolean validateGetSurface(boolean useSurface) {
        Log.v(TAG,""validateGetSurface, useSurface="" + useSurface);
        if (!useSurface && !hasCamera()) {
            // pass if testing camera source but no hardware
            return true;
        }
        Surface surface = null;
        boolean success = true;
        try {
            setupRecorder(OUTPUT_PATH, useSurface, false /* hasAudio */);

            /* Test: getSurface() before prepare()
             * should throw IllegalStateException
             */
            surface = tryGetSurface(true /* shouldThow */);

            mMediaRecorder.prepare();

            /* Test: getSurface() after prepare()
             * should succeed for surface source
             * should fail for camera source
             */
            surface = tryGetSurface(!useSurface);

            mMediaRecorder.start();

            /* Test: getSurface() after start()
             * should succeed for surface source
             * should fail for camera source
             */
            surface = tryGetSurface(!useSurface);

            try {
                mMediaRecorder.stop();
            } catch (Exception e) {
                // stop() could fail if the recording is empty, as we didn't render anything.
                // ignore any failure in stop, we just want it stopped.
            }

            /* Test: getSurface() after stop()
             * should throw IllegalStateException
             */
            surface = tryGetSurface(true /* shouldThow */);
        } catch (Exception e) {
            Log.d(TAG, e.toString());
            success = false;
        } finally {
            // reset to clear states, as stop() might have failed
            mMediaRecorder.reset();

            if (mCamera != null) {
                mCamera.release();
                mCamera = null;
            }
            if (surface != null) {
                surface.release();
                surface = null;
            }
        }

        return success;
    }

    private void trySetInputSurface(Surface surface) throws Exception {
        boolean testBadArgument = (surface == null);
        try {
            mMediaRecorder.setInputSurface(testBadArgument ? new Surface() : surface);
            fail(""failed to throw exception"");
        } catch (IllegalArgumentException e) {
            // OK only if testing bad arg
            assertTrue(""threw unexpected exception: "" + e, testBadArgument);
        } catch (IllegalStateException e) {
            // OK only if testing error case other than bad arg
            assertFalse(""threw unexpected exception: "" + e, testBadArgument);
        }
    }

    private boolean validatePersistentSurface(boolean errorCase) {
        Log.v(TAG, ""validatePersistentSurface, errorCase="" + errorCase);

        Surface surface = MediaCodec.createPersistentInputSurface();
        if (surface == null) {
            return false;
        }
        Surface placeholder = null;

        boolean success = true;
        try {
            setupRecorder(OUTPUT_PATH, true /* useSurface */, false /* hasAudio */);

            if (errorCase) {
                /*
                 * Test: should throw if called with non-persistent surface
                 */
                trySetInputSurface(null);
            } else {
                /*
                 * Test: should succeed if called with a persistent surface before prepare()
                 */
                mMediaRecorder.setInputSurface(surface);
            }

            /*
             * Test: getSurface() should fail before prepare
             */
            placeholder = tryGetSurface(true /* shouldThow */);

            mMediaRecorder.prepare();

            /*
             * Test: setInputSurface() should fail after prepare
             */
            trySetInputSurface(surface);

            /*
             * Test: getSurface() should fail if setInputSurface() succeeded
             */
            placeholder = tryGetSurface(!errorCase /* shouldThow */);

            mMediaRecorder.start();

            /*
             * Test: setInputSurface() should fail after start
             */
            trySetInputSurface(surface);

            /*
             * Test: getSurface() should fail if setInputSurface() succeeded
             */
            placeholder = tryGetSurface(!errorCase /* shouldThow */);

            try {
                mMediaRecorder.stop();
            } catch (Exception e) {
                // stop() could fail if the recording is empty, as we didn't render anything.
                // ignore any failure in stop, we just want it stopped.
            }

            /*
             * Test: getSurface() should fail after stop
             */
            placeholder = tryGetSurface(true /* shouldThow */);
        } catch (Exception e) {
            Log.d(TAG, e.toString());
            success = false;
        } finally {
            // reset to clear states, as stop() might have failed
            mMediaRecorder.reset();

            if (mCamera != null) {
                mCamera.release();
                mCamera = null;
            }
            if (surface != null) {
                surface.release();
                surface = null;
            }
            if (placeholder != null) {
                placeholder.release();
                placeholder = null;
            }
        }

        return success;
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaRecorderTest"	"testPersistentSurfaceRecordingTimeLapse"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaRecorderTest.java"	""	"public void testPersistentSurfaceRecordingTimeLapse() {
        assertTrue(testRecordFromSurface(true /* persistent */, true /* timelapse */));
    }

    private void recordMedia(long maxFileSize, File outFile) throws Exception {
        mMediaRecorder.setMaxFileSize(maxFileSize);
        mMediaRecorder.prepare();
        mMediaRecorder.start();
        Thread.sleep(RECORD_TIME_MS);
        mMediaRecorder.stop();

        assertTrue(outFile.exists());

        // The max file size is always guaranteed.
        // We just make sure that the margin is not too big
        assertTrue(outFile.length() < 1.1 * maxFileSize);
        assertTrue(outFile.length() > 0);
    }

    private boolean hasCamera() {
        return mActivity.getPackageManager().hasSystemFeature(PackageManager.FEATURE_CAMERA);
    }

    private boolean hasMicrophone() {
        return mActivity.getPackageManager().hasSystemFeature(
                PackageManager.FEATURE_MICROPHONE);
    }

    private static boolean hasAmrNb() {
        return MediaUtils.hasEncoder(MediaFormat.MIMETYPE_AUDIO_AMR_NB);
    }

    private static boolean hasAmrWb() {
        return MediaUtils.hasEncoder(MediaFormat.MIMETYPE_AUDIO_AMR_WB);
    }

    private static boolean hasAac() {
        return MediaUtils.hasEncoder(MediaFormat.MIMETYPE_AUDIO_AAC);
    }

    private static boolean hasH264() {
        return MediaUtils.hasEncoder(MediaFormat.MIMETYPE_VIDEO_AVC);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaRecorderTest"	"testAudioRecordInfoCallback"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaRecorderTest.java"	""	"public void testAudioRecordInfoCallback() throws Exception {
        if (!hasMicrophone() || !hasAac()) {
            MediaUtils.skipTest(""no audio codecs or microphone"");
            return;
        }
        AudioRecordingConfigurationTest.MyAudioRecordingCallback callback =
                new AudioRecordingConfigurationTest.MyAudioRecordingCallback(
                        0 /*unused*/, MediaRecorder.AudioSource.DEFAULT);
        mMediaRecorder.registerAudioRecordingCallback(mExec, callback);
        configureDefaultMediaRecorder();
        mMediaRecorder.prepare();
        mMediaRecorder.start();
        callback.await(TEST_TIMING_TOLERANCE_MS);
        assertTrue(callback.mCalled);
        assertTrue(callback.mConfigs.size() <= 1);
        if (callback.mConfigs.size() == 1) {
            checkRecordingConfig(callback.mConfigs.get(0));
        }
        Thread.sleep(RECORD_TIME_MS);
        mMediaRecorder.stop();
        mMediaRecorder.unregisterAudioRecordingCallback(callback);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaRecorderTest"	"testGetActiveRecordingConfiguration"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaRecorderTest.java"	""	"public void testGetActiveRecordingConfiguration() throws Exception {
        if (!hasMicrophone() || !hasAac()) {
            MediaUtils.skipTest(""no audio codecs or microphone"");
            return;
        }
        configureDefaultMediaRecorder();
        mMediaRecorder.prepare();
        mMediaRecorder.start();
        Thread.sleep(1000);
        AudioRecordingConfiguration config = mMediaRecorder.getActiveRecordingConfiguration();
        checkRecordingConfig(config);
        mMediaRecorder.stop();
    }

    private Executor mExec = new Executor() {
        @Override
        public void execute(Runnable command) {
            command.run();
        }
    };

    private static void checkRecordingConfig(AudioRecordingConfiguration config) {
        assertNotNull(config);
        AudioFormat format = config.getClientFormat();
        assertEquals(AUDIO_NUM_CHANNELS, format.getChannelCount());
        assertEquals(AUDIO_SAMPLE_RATE_HZ, format.getSampleRate());
        assertEquals(MediaRecorder.AudioSource.MIC, config.getAudioSource());
        assertNotNull(config.getAudioDevice());
        assertNotNull(config.getClientEffects());
        assertNotNull(config.getEffects());
        // no requirement here, just testing the API
        config.isClientSilenced();
    }

    /*
     * Microphone Direction API tests
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaRecorderTest"	"testPrivacySensitive"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaRecorderTest.java"	""	"public void testPrivacySensitive() throws Exception {
        if (!MediaUtils.check(mIsAtLeastR, ""test needs Android 11"")) return;
        if (!hasMicrophone() || !hasAac()) {
            MediaUtils.skipTest(""no audio codecs or microphone"");
            return;
        }
        for (final boolean privacyOn : new boolean[] { false, true} ) {
            mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.MIC);
            mMediaRecorder.setPrivacySensitive(privacyOn);
            assertEquals(privacyOn, mMediaRecorder.isPrivacySensitive());
            mMediaRecorder.reset();
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaRecorderTest"	"testPrivacySensitiveDefaults"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaRecorderTest.java"	""	"public void testPrivacySensitiveDefaults() throws Exception {
        if (!MediaUtils.check(mIsAtLeastR, ""test needs Android 11"")) return;
        if (!hasMicrophone() || !hasAac()) {
            MediaUtils.skipTest(""no audio codecs or microphone"");
            return;
        }
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.MIC);
        assertFalse(mMediaRecorder.isPrivacySensitive());
        mMediaRecorder.reset();

        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.VOICE_COMMUNICATION);
        assertTrue(mMediaRecorder.isPrivacySensitive());
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.appdataisolation.appa.AppATests"	"testAppACurProfileDataAccessible"	""	"/home/gpoor/cts-12-source/cts/hostsidetests/appsecurity/test-apps/AppDataIsolationTestApp/AppA/src/com/android/cts/appdataisolation/appa/AppATests.java"	""	"public void testAppACurProfileDataAccessible() {
        assertDirIsAccessible(""/data/misc/profiles/cur/""+ getCurrentUserId() + ""/""
                + mContext.getPackageName());
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.appdataisolation.appa.AppATests"	"testAppARefProfileDataAccessible"	""	"/home/gpoor/cts-12-source/cts/hostsidetests/appsecurity/test-apps/AppDataIsolationTestApp/AppA/src/com/android/cts/appdataisolation/appa/AppATests.java"	""	"public void testAppARefProfileDataAccessible() {
        assertDirIsAccessible(""/data/misc/profiles/ref/""
                + mContext.getPackageName());
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.appdataisolation.appa.AppATests"	"testAppAUnlockDeviceAndVerifyCeDeExternalDataExist"	""	"/home/gpoor/cts-12-source/cts/hostsidetests/appsecurity/test-apps/AppDataIsolationTestApp/AppA/src/com/android/cts/appdataisolation/appa/AppATests.java"	""	"public void testAppAUnlockDeviceAndVerifyCeDeExternalDataExist() throws Exception {

        final CountDownLatch unlocked = new CountDownLatch(1);
        final CountDownLatch bootCompleted = new CountDownLatch(1);
        final BroadcastReceiver receiver = new BroadcastReceiver() {
            @Override
            public void onReceive(Context context, Intent intent) {
                switch (intent.getAction()) {
                    case Intent.ACTION_USER_UNLOCKED:
                        unlocked.countDown();
                        break;
                    case Intent.ACTION_BOOT_COMPLETED:
                        bootCompleted.countDown();
                        break;
                }
            }
        };
        mContext.registerReceiver(receiver, new IntentFilter(Intent.ACTION_USER_UNLOCKED));
        mContext.registerReceiver(receiver, new IntentFilter(Intent.ACTION_BOOT_COMPLETED));

        testUnlockDevice();

        assertTrue(""User not unlocked"", unlocked.await(1, TimeUnit.MINUTES));
        assertTrue(""No locked boot complete"", bootCompleted.await(1, TimeUnit.MINUTES));

        setUpExternalStoragePaths();

        // The test app process should be still running, make sure CE DE now is available
        testAppACeDataExists();
        testAppADeDataExists();
        testAppAExternalDirsDoExist();
        testAppACurProfileDataAccessible();
        testAppARefProfileDataAccessible();

        // Verify after unlocking device, app a has still no access to app b dir.
        testCannotAccessAppBDataDir();
        testCanNotAccessAppBExternalDirs();
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.taskswitching.appa.AppAActivity"	"getListView"	""	"/home/gpoor/cts-12-source/cts/hostsidetests/ui/appA/src/android/taskswitching/appa/AppAActivity.java"	""	"public void test/*
 *.
 */

package android.taskswitching.appa;

import android.app.ListActivity;
import android.os.Bundle;
import android.os.Handler;
import android.os.RemoteCallback;
import android.view.WindowManager;
import android.widget.ArrayAdapter;
import android.widget.ListView;

/**
 * Simple activity to notify completion via broadcast after onResume.
 * This is for measuring taskswitching time between two apps.
 */
public class AppAActivity extends ListActivity {
    private static final int NUMBER_ELEMENTS = 1000;
    private Handler mHandler;

    private String[] mItems = new String[NUMBER_ELEMENTS];

    @Override
    public void onCreate(Bundle icicle) {
        super.onCreate(icicle);
        getWindow().addFlags(WindowManager.LayoutParams.FLAG_SHOW_WHEN_LOCKED);
        for (int i = 0; i < NUMBER_ELEMENTS; i++) {
            mItems[i] = ""A"" + Integer.toString(i);
        }
        setListAdapter(new ArrayAdapter<String>(this,
                android.R.layout.simple_list_item_1, mItems));
        ListView view = getListView();
        mHandler = new Handler();
    }

    @Override
    public void onResume() {
        super.onResume();
        mHandler.post(() -> {
            getIntent().<RemoteCallback>getParcelableExtra(""callback"").sendResult(null);
        });
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.audio.USBRestrictRecordAActivity"	"LocalClickListener"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/audio/USBRestrictRecordAActivity.java"	""	"public void test/*
 *.
 */

package com.android.cts.verifier.audio;

import android.app.Activity;
import android.app.PendingIntent;
import android.content.BroadcastReceiver;
import android.content.Context;
import android.content.Intent;
import android.content.IntentFilter;
import android.content.pm.PackageInfo;
import android.content.pm.PackageManager;
import android.hardware.usb.UsbDevice;
import android.hardware.usb.UsbManager;
import android.Manifest;
import android.os.Bundle;
import android.util.Log;
import android.view.View;
import android.widget.TextView;
import android.widget.Toast;

import java.util.Collection;
import java.util.HashMap;

import com.android.cts.verifier.PassFailButtons;
import com.android.cts.verifier.R;  // needed to access resource in CTSVerifier project namespace.

/*
 * This tests the USB Restrict Record functionality for the explicit USB device open case
 *   (case ""A"").
 * The other 2 cases are:
 *   A SINGLE activity is invoked when a USB device is plugged in. (Case B)
 *   ONE OF A MULTIPLE activities is iUSBRestrictedRecordAActivity. (Case C)
 *
 * We are using simple single-character distiguishes to avoid really long class names.
 */
public class USBRestrictRecordAActivity extends PassFailButtons.Activity {
    private static final String TAG = ""USBRestrictRecordAActivity"";
    private static final boolean DEBUG = false;

    private LocalClickListener mButtonClickListener = new LocalClickListener();

    private Context mContext;

    // Test MUST be run WITHOUT record pemission
    private boolean mHasRecordPermission;

    // System USB stuff
    private UsbManager mUsbManager;


    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        setContentView(R.layout.usb_restrictrecord);

        findViewById(R.id.test_button).setOnClickListener(mButtonClickListener);

        mContext = this;

        mUsbManager = (UsbManager)getSystemService(Context.USB_SERVICE);

        setPassFailButtonClickListeners();
        getPassButton().setEnabled(false);
        setInfoResources(R.string.audio_usb_restrict_record_test,
                R.string.audio_usb_restrict_record_entry, -1);

        mHasRecordPermission = hasRecordPermission();

        if (mHasRecordPermission) {
            TextView tx = findViewById(R.id.usb_restrictrecord_instructions);
            tx.setText(getResources().getString(R.string.audio_usb_restrict_permission_info));
        }
        findViewById(R.id.test_button).setEnabled(!mHasRecordPermission);
    }

    private boolean hasRecordPermission() {
        try {
            PackageManager pm = getPackageManager();
            PackageInfo packageInfo = pm.getPackageInfo(
                    getApplicationInfo().packageName, PackageManager.GET_PERMISSIONS);

            if (packageInfo.requestedPermissions != null) {
                for (String permission : packageInfo.requestedPermissions) {
                    if (permission.equals(Manifest.permission.RECORD_AUDIO)) {
                        return checkSelfPermission(permission) == PackageManager.PERMISSION_GRANTED;
                    }
                }
            }
        } catch (PackageManager.NameNotFoundException e) {
            Log.e(TAG, ""Unable to load package's permissions"", e);
            Toast.makeText(this, R.string.runtime_permissions_error, Toast.LENGTH_SHORT).show();
        }
        return false;
    }

    public class LocalClickListener implements View.OnClickListener {
        @Override
        public void onClick(View view) {
            int id = view.getId();
            switch (id) {
                case R.id.test_button:
                    connectUSB(mContext);
                    break;
            }
        }
    }

    private class ConnectDeviceBroadcastReceiver extends BroadcastReceiver {
        private final String TAG = ""ConnectDeviceBroadcastReceiver"";
        @Override
        public void onReceive(Context context, Intent intent) {
            String action = intent.getAction();
            if (ACTION_USB_PERMISSION.equals(action)) {
                synchronized (this) {
                    getPassButton().setEnabled(true);

                    // These messages don't really matter
                    if (intent.getBooleanExtra(UsbManager.EXTRA_PERMISSION_GRANTED, false)) {
                        Toast.makeText(mContext, ""Permission Granted."", Toast.LENGTH_SHORT).show();
                    }
                    else {
                        Toast.makeText(mContext, ""Permission Denied."", Toast.LENGTH_SHORT).show();
                    }
                }
            }
        }
    }

    private static final String ACTION_USB_PERMISSION = ""com.android.usbdescriptors.USB_PERMISSION"";

    public void connectUSB(Context context) {
        HashMap<String, UsbDevice> deviceList = mUsbManager.getDeviceList();
        Collection<UsbDevice> deviceCollection = deviceList.values();
        Object[] devices = deviceCollection.toArray();
        if (devices.length > 0) {
            UsbDevice theDevice = (UsbDevice) devices[0];

            PendingIntent permissionIntent =
                    PendingIntent.getBroadcast(context, 0, new Intent(ACTION_USB_PERMISSION), PendingIntent.FLAG_MUTABLE_UNAUDITED);

            IntentFilter filter = new IntentFilter(ACTION_USB_PERMISSION);
            ConnectDeviceBroadcastReceiver usbReceiver =
                    new ConnectDeviceBroadcastReceiver();
            context.registerReceiver(usbReceiver, filter);

            mUsbManager.requestPermission(theDevice, permissionIntent);
        }
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.multiprocess.camera.cts.Camera2Activity"	"getCameraIdListNoLazy"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/multiprocess/camera/cts/Camera2Activity.java"	""	"public void test/*
 *.
 */

package android.hardware.multiprocess.camera.cts;

import android.app.Activity;
import android.content.Context;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.os.Bundle;
import android.os.Handler;
import android.util.Log;

/**
 * Activity implementing basic access of the Camera2 API.
 *
 * <p />
 * This will log all errors to {@link android.hardware.multiprocess.camera.cts.ErrorLoggingService}.
 */
public class Camera2Activity extends Activity {
    private static final String TAG = ""Camera2Activity"";

    ErrorLoggingService.ErrorServiceConnection mErrorServiceConnection;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        Log.i(TAG, ""onCreate called."");
        super.onCreate(savedInstanceState);
        mErrorServiceConnection = new ErrorLoggingService.ErrorServiceConnection(this);
        mErrorServiceConnection.start();
    }

    @Override
    protected void onPause() {
        Log.i(TAG, ""onPause called."");
        super.onPause();
    }

    @Override
    protected void onResume() {
        Log.i(TAG, ""onResume called."");
        super.onResume();

        try {
            CameraManager manager = (CameraManager) getSystemService(Context.CAMERA_SERVICE);

            if (manager == null) {
                mErrorServiceConnection.logAsync(TestConstants.EVENT_CAMERA_ERROR, TAG +
                        "" could not connect camera service"");
                return;
            }
            // TODO: http://b/145308043 move this back to getCameraIdListNoLazy()
            String[] cameraIds = manager.getCameraIdList();

            if (cameraIds == null || cameraIds.length == 0) {
                mErrorServiceConnection.logAsync(TestConstants.EVENT_CAMERA_ERROR, TAG +
                        "" device reported having no cameras"");
                return;
            }

            manager.registerAvailabilityCallback(new CameraManager.AvailabilityCallback() {
                @Override
                public void onCameraAvailable(String cameraId) {
                    super.onCameraAvailable(cameraId);
                    mErrorServiceConnection.logAsync(TestConstants.EVENT_CAMERA_AVAILABLE,
                            cameraId);
                    Log.i(TAG, ""Camera "" + cameraId + "" is available"");
                }

                @Override
                public void onCameraUnavailable(String cameraId) {
                    super.onCameraUnavailable(cameraId);
                    mErrorServiceConnection.logAsync(TestConstants.EVENT_CAMERA_UNAVAILABLE,
                            cameraId);
                    Log.i(TAG, ""Camera "" + cameraId + "" is unavailable"");
                }

                @Override
                public void onPhysicalCameraAvailable(String cameraId, String physicalCameraId) {
                    super.onPhysicalCameraAvailable(cameraId, physicalCameraId);
                    mErrorServiceConnection.logAsync(TestConstants.EVENT_CAMERA_AVAILABLE,
                            cameraId + "" : "" + physicalCameraId);
                    Log.i(TAG, ""Camera "" + cameraId + "" : "" + physicalCameraId + "" is available"");
                }

                @Override
                public void onPhysicalCameraUnavailable(String cameraId, String physicalCameraId) {
                    super.onPhysicalCameraUnavailable(cameraId, physicalCameraId);
                    mErrorServiceConnection.logAsync(TestConstants.EVENT_CAMERA_UNAVAILABLE,
                            cameraId + "" : "" + physicalCameraId);
                    Log.i(TAG, ""Camera "" + cameraId + "" : "" + physicalCameraId + "" is unavailable"");
                }
            }, null);

            final String chosen = cameraIds[0];

            manager.openCamera(chosen, new CameraDevice.StateCallback() {
                @Override
                public void onOpened(CameraDevice cameraDevice) {
                    mErrorServiceConnection.logAsync(TestConstants.EVENT_CAMERA_CONNECT,
                            chosen);
                    Log.i(TAG, ""Camera "" + chosen + "" is opened"");
                }

                @Override
                public void onDisconnected(CameraDevice cameraDevice) {
                    mErrorServiceConnection.logAsync(TestConstants.EVENT_CAMERA_EVICTED,
                            chosen);
                    Log.i(TAG, ""Camera "" + chosen + "" is disconnected"");
                }

                @Override
                public void onError(CameraDevice cameraDevice, int i) {
                    mErrorServiceConnection.logAsync(TestConstants.EVENT_CAMERA_ERROR, TAG +
                            "" Camera "" + chosen + "" experienced error "" + i);
                    Log.e(TAG, ""Camera "" + chosen + "" onError called with error "" + i);
                }
            }, null);
        } catch (CameraAccessException e) {
            mErrorServiceConnection.logAsync(TestConstants.EVENT_CAMERA_ERROR, TAG +
                    "" camera exception during connection: "" + e);
            Log.e(TAG, ""Access exception: "" + e);
        }
    }

    @Override
    protected void onDestroy() {
        Log.i(TAG, ""onDestroy called."");
        super.onDestroy();
        if (mErrorServiceConnection != null) {
            mErrorServiceConnection.stop();
            mErrorServiceConnection = null;
        }
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.telephony.cts.TelephonyCallbackTest"	"testOnDataConnectionStateChangedByRegisterTelephonyCallback"	"CtsTelephonyTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/telephony/current/src/android/telephony/cts/TelephonyCallbackTest.java"	""	"public void testOnDataConnectionStateChangedByRegisterTelephonyCallback() throws Throwable {
        if (mCm.getNetworkInfo(ConnectivityManager.TYPE_MOBILE) == null) {
            Log.d(TAG, ""Skipping test that requires ConnectivityManager.TYPE_MOBILE"");
            return;
        }
        assertFalse(mOnDataConnectionStateChangedCalled);
        assertFalse(mOnDataConnectionStateChangedWithNetworkTypeCalled);

        mHandler.post(() -> {
            mDataConnectionStateCallback = new DataConnectionStateListener();
            registerTelephonyCallback(mDataConnectionStateCallback);

        });
        synchronized (mLock) {
            if (!mOnDataConnectionStateChangedCalled ||
                    !mOnDataConnectionStateChangedWithNetworkTypeCalled) {
                mLock.wait(WAIT_TIME);
            }
        }

        assertTrue(mOnDataConnectionStateChangedCalled);
        assertTrue(mOnDataConnectionStateChangedWithNetworkTypeCalled);

        // Test unregister
        unRegisterTelephonyCallback(mOnDataConnectionStateChangedCalled,
                mDataConnectionStateCallback);
    }

    private DataActivityListener mDataActivityCallback;

    private class DataActivityListener extends TelephonyCallback
            implements TelephonyCallback.DataActivityListener {
        @Override
        public void onDataActivity(int direction) {
            synchronized (mLock) {
                mOnDataActivityCalled = true;
                mLock.notify();
            }
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.telephony.cts.TelephonyCallbackTest"	"testOnDataActivityByRegisterTelephonyCallback"	"CtsTelephonyTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/telephony/current/src/android/telephony/cts/TelephonyCallbackTest.java"	""	"public void testOnDataActivityByRegisterTelephonyCallback() throws Throwable {
        if (mCm.getNetworkInfo(ConnectivityManager.TYPE_MOBILE) == null) {
            Log.d(TAG, ""Skipping test that requires ConnectivityManager.TYPE_MOBILE"");
            return;
        }
        assertFalse(mOnDataActivityCalled);

        mHandler.post(() -> {
            mDataActivityCallback = new DataActivityListener();
            registerTelephonyCallback(mDataActivityCallback);

        });
        synchronized (mLock) {
            if (!mOnDataActivityCalled) {
                mLock.wait(WAIT_TIME);
            }
        }

        assertTrue(mOnDataActivityCalled);

        // Test unregister
        unRegisterTelephonyCallback(mOnDataActivityCalled, mDataActivityCallback);
    }

    private CellInfoListener mCellInfoCallback;

    private class CellInfoListener extends TelephonyCallback
            implements TelephonyCallback.CellInfoListener {
        @Override
        public void onCellInfoChanged(List<CellInfo> cellInfo) {
            synchronized (mLock) {
                mOnCellInfoChangedCalled = true;
                mLock.notify();
            }
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.telephony.cts.TelephonyCallbackTest"	"testOnCellInfoChangedByRegisterTelephonyCallback"	"CtsTelephonyTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/telephony/current/src/android/telephony/cts/TelephonyCallbackTest.java"	""	"public void testOnCellInfoChangedByRegisterTelephonyCallback() throws Throwable {
        if (mCm.getNetworkInfo(ConnectivityManager.TYPE_MOBILE) == null) {
            Log.d(TAG, ""Skipping test that requires ConnectivityManager.TYPE_MOBILE"");
            return;
        }
        assertFalse(mOnDataActivityCalled);

        TelephonyManagerTest.grantLocationPermissions();
        mHandler.post(() -> {
            mCellInfoCallback = new CellInfoListener();
            registerTelephonyCallback(mCellInfoCallback);
        });
        synchronized (mLock) {
            if (!mOnCellInfoChangedCalled) {
                mLock.wait(WAIT_TIME);
            }
        }

        assertTrue(mOnCellInfoChangedCalled);

        // Test unregister
        unRegisterTelephonyCallback(mOnCellInfoChangedCalled, mCellInfoCallback);
    }

    private UserMobileDataStateListener mUserMobileDataStateCallback;

    private class UserMobileDataStateListener extends TelephonyCallback
            implements TelephonyCallback.UserMobileDataStateListener {
        @Override
        public void onUserMobileDataStateChanged(boolean state) {
            synchronized (mLock) {
                mOnUserMobileDataStateChanged = true;
                mLock.notify();
            }
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraDeviceTest"	"testCreateSessionWithParameters"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CameraDeviceTest.java"	""	"public void testCreateSessionWithParameters() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                openDevice(mCameraIdsUnderTest[i], mCameraMockListener);
                waitForDeviceState(STATE_OPENED, CAMERA_OPEN_TIMEOUT_MS);

                testCreateSessionWithParametersByCamera(mCameraIdsUnderTest[i], /*reprocessable*/false);
                testCreateSessionWithParametersByCamera(mCameraIdsUnderTest[i], /*reprocessable*/true);
            }
            finally {
                closeDevice(mCameraIdsUnderTest[i], mCameraMockListener);
            }
        }
    }

    /**
     * Verify creating a session with additional parameters works
     */
    private void testCreateSessionWithParametersByCamera(String cameraId, boolean reprocessable)
            throws Exception {
        final int SESSION_TIMEOUT_MS = 1000;
        final int CAPTURE_TIMEOUT_MS = 3000;
        int inputFormat = ImageFormat.YUV_420_888;
        int outputFormat = inputFormat;
        Size outputSize = mOrderedPreviewSizes.get(0);
        Size inputSize = outputSize;
        InputConfiguration inputConfig = null;

        if (VERBOSE) {
            Log.v(TAG, ""Testing creating session with parameters for camera "" + cameraId);
        }

        CameraCharacteristics characteristics = mCameraManager.getCameraCharacteristics(cameraId);
        StreamConfigurationMap config = characteristics.get(
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);

        if (reprocessable) {
            //Pick a supported i/o format and size combination.
            //Ideally the input format should match the output.
            boolean found = false;
            int inputFormats [] = config.getInputFormats();
            if (inputFormats.length == 0) {
                return;
            }

            for (int inFormat : inputFormats) {
                int outputFormats [] = config.getValidOutputFormatsForInput(inFormat);
                for (int outFormat : outputFormats) {
                    if (inFormat == outFormat) {
                        inputFormat = inFormat;
                        outputFormat = outFormat;
                        found = true;
                        break;
                    }
                }
                if (found) {
                    break;
                }
            }

            //In case the above combination doesn't exist, pick the first first supported
            //pair.
            if (!found) {
                inputFormat = inputFormats[0];
                int outputFormats [] = config.getValidOutputFormatsForInput(inputFormat);
                assertTrue(""No output formats supported for input format: "" + inputFormat,
                        (outputFormats.length > 0));
                outputFormat = outputFormats[0];
            }

            Size inputSizes[] = config.getInputSizes(inputFormat);
            Size outputSizes[] = config.getOutputSizes(outputFormat);
            assertTrue(""No valid sizes supported for input format: "" + inputFormat,
                    (inputSizes.length > 0));
            assertTrue(""No valid sizes supported for output format: "" + outputFormat,
                    (outputSizes.length > 0));

            inputSize = inputSizes[0];
            outputSize = outputSizes[0];
            inputConfig = new InputConfiguration(inputSize.getWidth(),
                    inputSize.getHeight(), inputFormat);
        } else {
            if (config.isOutputSupportedFor(outputFormat)) {
                outputSize = config.getOutputSizes(outputFormat)[0];
            } else {
                return;
            }
        }

        ImageReader imageReader = ImageReader.newInstance(outputSize.getWidth(),
                outputSize.getHeight(), outputFormat, /*maxImages*/1);

        try {
            mSessionMockListener = spy(new BlockingSessionCallback());
            mSessionWaiter = mSessionMockListener.getStateWaiter();
            List<OutputConfiguration> outputs = new ArrayList<>();
            outputs.add(new OutputConfiguration(imageReader.getSurface()));
            SessionConfiguration sessionConfig = new SessionConfiguration(
                    SessionConfiguration.SESSION_REGULAR, outputs,
                    new HandlerExecutor(mHandler), mSessionMockListener);

            CaptureRequest.Builder builder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
            builder.addTarget(imageReader.getSurface());
            CaptureRequest request = builder.build();

            sessionConfig.setInputConfiguration(inputConfig);
            sessionConfig.setSessionParameters(request);
            mCamera.createCaptureSession(sessionConfig);

            mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);

            // Verify we can capture a frame with the session.
            SimpleCaptureCallback captureListener = new SimpleCaptureCallback();
            SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
            imageReader.setOnImageAvailableListener(imageListener, mHandler);

            mSession.capture(request, captureListener, mHandler);
            captureListener.getCaptureResultForRequest(request, CAPTURE_TIMEOUT_MS);
            imageListener.getImage(CAPTURE_TIMEOUT_MS).close();
        } finally {
            imageReader.close();
            mSession.close();
        }
    }

    /**
     * Verify creating sessions back to back and only the last one is valid for
     * submitting requests.
     */
    private void testCreateSessionsByCamera(String cameraId) throws Exception {
        final int NUM_SESSIONS = 3;
        final int SESSION_TIMEOUT_MS = 1000;
        final int CAPTURE_TIMEOUT_MS = 3000;

        if (VERBOSE) {
            Log.v(TAG, ""Testing creating sessions for camera "" + cameraId);
        }

        Size yuvSize = getSortedSizesForFormat(cameraId, mCameraManager, ImageFormat.YUV_420_888,
                /*bound*/null).get(0);
        Size jpegSize = getSortedSizesForFormat(cameraId, mCameraManager, ImageFormat.JPEG,
                /*bound*/null).get(0);

        // Create a list of image readers. JPEG for last one and YUV for the rest.
        List<ImageReader> imageReaders = new ArrayList<>();
        List<CameraCaptureSession> allSessions = new ArrayList<>();

        try {
            for (int i = 0; i < NUM_SESSIONS - 1; i++) {
                imageReaders.add(ImageReader.newInstance(yuvSize.getWidth(), yuvSize.getHeight(),
                        ImageFormat.YUV_420_888, /*maxImages*/1));
            }
            imageReaders.add(ImageReader.newInstance(jpegSize.getWidth(), jpegSize.getHeight(),
                    ImageFormat.JPEG, /*maxImages*/1));

            // Create multiple sessions back to back.
            MultipleSessionCallback sessionListener =
                    new MultipleSessionCallback(/*failOnConfigureFailed*/true);
            for (int i = 0; i < NUM_SESSIONS; i++) {
                List<Surface> outputs = new ArrayList<>();
                outputs.add(imageReaders.get(i).getSurface());
                mCamera.createCaptureSession(outputs, sessionListener, mHandler);
            }

            // Verify we get onConfigured() for all sessions.
            allSessions = sessionListener.getAllSessions(NUM_SESSIONS,
                    SESSION_TIMEOUT_MS * NUM_SESSIONS);
            assertEquals(String.format(""Got %d sessions but configured %d sessions"",
                    allSessions.size(), NUM_SESSIONS), allSessions.size(), NUM_SESSIONS);

            // Verify all sessions except the last one are closed.
            for (int i = 0; i < NUM_SESSIONS - 1; i++) {
                sessionListener.waitForSessionClose(allSessions.get(i), SESSION_TIMEOUT_MS);
            }

            // Verify we can capture a frame with the last session.
            CameraCaptureSession session = allSessions.get(allSessions.size() - 1);
            SimpleCaptureCallback captureListener = new SimpleCaptureCallback();
            ImageReader reader = imageReaders.get(imageReaders.size() - 1);
            SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
            reader.setOnImageAvailableListener(imageListener, mHandler);

            CaptureRequest.Builder builder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
            builder.addTarget(reader.getSurface());
            CaptureRequest request = builder.build();

            session.capture(request, captureListener, mHandler);
            captureListener.getCaptureResultForRequest(request, CAPTURE_TIMEOUT_MS);
            imageListener.getImage(CAPTURE_TIMEOUT_MS).close();
        } finally {
            for (ImageReader reader : imageReaders) {
                reader.close();
            }
            for (CameraCaptureSession session : allSessions) {
                session.close();
            }
        }
    }

    private void prepareTestByCamera() throws Exception {
        final int PREPARE_TIMEOUT_MS = 10000;

        mSessionMockListener = spy(new BlockingSessionCallback());

        SurfaceTexture output1 = new SurfaceTexture(1);
        Surface output1Surface = new Surface(output1);
        SurfaceTexture output2 = new SurfaceTexture(2);
        Surface output2Surface = new Surface(output2);

        ArrayList<OutputConfiguration> outConfigs = new ArrayList<OutputConfiguration> ();
        outConfigs.add(new OutputConfiguration(output1Surface));
        outConfigs.add(new OutputConfiguration(output2Surface));
        SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_REGULAR, outConfigs,
                new HandlerExecutor(mHandler), mSessionMockListener);
        CaptureRequest.Builder r = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        sessionConfig.setSessionParameters(r.build());
        mCamera.createCaptureSession(sessionConfig);

        mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);

        // Try basic prepare

        mSession.prepare(output1Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(1))
                .onSurfacePrepared(eq(mSession), eq(output1Surface));

        // Should not complain if preparing already prepared stream

        mSession.prepare(output1Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(2))
                .onSurfacePrepared(eq(mSession), eq(output1Surface));

        // Check surface not included in session

        SurfaceTexture output3 = new SurfaceTexture(3);
        Surface output3Surface = new Surface(output3);
        try {
            mSession.prepare(output3Surface);
            // Legacy camera prepare always succeed
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                fail(""Preparing surface not part of session must throw IllegalArgumentException"");
            }
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Ensure second prepare also works

        mSession.prepare(output2Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(1))
                .onSurfacePrepared(eq(mSession), eq(output2Surface));

        // Use output1

        r = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        r.addTarget(output1Surface);

        mSession.capture(r.build(), null, null);

        try {
            mSession.prepare(output1Surface);
            // Legacy camera prepare always succeed
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                fail(""Preparing already-used surface must throw IllegalArgumentException"");
            }
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Create new session with outputs 1 and 3, ensure output1Surface still can't be prepared
        // again

        mSessionMockListener = spy(new BlockingSessionCallback());

        ArrayList<Surface> outputSurfaces = new ArrayList<Surface>(
            Arrays.asList(output1Surface, output3Surface));
        mCamera.createCaptureSession(outputSurfaces, mSessionMockListener, mHandler);

        mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);

        try {
            mSession.prepare(output1Surface);
            // Legacy camera prepare always succeed
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                fail(""Preparing surface used in previous session must throw "" +
                        ""IllegalArgumentException"");
            }
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Use output3, wait for result, then make sure prepare still doesn't work

        r = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        r.addTarget(output3Surface);

        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        mSession.capture(r.build(), resultListener, mHandler);

        resultListener.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);

        try {
            mSession.prepare(output3Surface);
            // Legacy camera prepare always succeed
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                fail(""Preparing already-used surface must throw IllegalArgumentException"");
            }
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Create new session with outputs 1 and 2, ensure output2Surface can be prepared again

        mSessionMockListener = spy(new BlockingSessionCallback());

        outputSurfaces = new ArrayList<>(
            Arrays.asList(output1Surface, output2Surface));
        mCamera.createCaptureSession(outputSurfaces, mSessionMockListener, mHandler);

        mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);

        mSession.prepare(output2Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(1))
                .onSurfacePrepared(eq(mSession), eq(output2Surface));

        try {
            mSession.prepare(output1Surface);
            // Legacy camera prepare always succeed
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                fail(""Preparing surface used in previous session must throw "" +
                        ""IllegalArgumentException"");
            }
        } catch (IllegalArgumentException e) {
            // expected
        }

        output1.release();
        output2.release();
        output3.release();
    }

    private void prepareTestForSharedSurfacesByCamera() throws Exception {
        final int PREPARE_TIMEOUT_MS = 10000;

        mSessionMockListener = spy(new BlockingSessionCallback());

        SurfaceTexture output1 = new SurfaceTexture(1);
        Surface output1Surface = new Surface(output1);
        SurfaceTexture output2 = new SurfaceTexture(2);
        Surface output2Surface = new Surface(output2);

        List<Surface> outputSurfaces = new ArrayList<>(
            Arrays.asList(output1Surface, output2Surface));
        OutputConfiguration surfaceSharedConfig = new OutputConfiguration(
            OutputConfiguration.SURFACE_GROUP_ID_NONE, output1Surface);
        surfaceSharedConfig.enableSurfaceSharing();
        surfaceSharedConfig.addSurface(output2Surface);

        List<OutputConfiguration> outputConfigurations = new ArrayList<>();
        outputConfigurations.add(surfaceSharedConfig);
        mCamera.createCaptureSessionByOutputConfigurations(
                outputConfigurations, mSessionMockListener, mHandler);

        mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);

        // Try prepare on output1Surface
        mSession.prepare(output1Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(1))
                .onSurfacePrepared(eq(mSession), eq(output1Surface));
        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(1))
                .onSurfacePrepared(eq(mSession), eq(output2Surface));

        // Try prepare on output2Surface
        mSession.prepare(output2Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(2))
                .onSurfacePrepared(eq(mSession), eq(output1Surface));
        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(2))
                .onSurfacePrepared(eq(mSession), eq(output2Surface));

        // Try prepare on output1Surface again
        mSession.prepare(output1Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(3))
                .onSurfacePrepared(eq(mSession), eq(output1Surface));
        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(3))
                .onSurfacePrepared(eq(mSession), eq(output2Surface));
    }

    private void invalidRequestCaptureTestByCamera() throws Exception {
        if (VERBOSE) Log.v(TAG, ""invalidRequestCaptureTestByCamera"");

        List<CaptureRequest> emptyRequests = new ArrayList<CaptureRequest>();
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest unConfiguredRequest = requestBuilder.build();
        List<CaptureRequest> unConfiguredRequests = new ArrayList<CaptureRequest>();
        unConfiguredRequests.add(unConfiguredRequest);

        try {
            // Test: CameraCaptureSession capture should throw IAE for null request.
            mSession.capture(/*request*/null, /*listener*/null, mHandler);
            mCollector.addMessage(
                    ""Session capture should throw IllegalArgumentException for null request"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession capture should throw IAE for request
            // without surface configured.
            mSession.capture(unConfiguredRequest, /*listener*/null, mHandler);
            mCollector.addMessage(""Session capture should throw "" +
                    ""IllegalArgumentException for request without surface configured"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession setRepeatingRequest should throw IAE for null request.
            mSession.setRepeatingRequest(/*request*/null, /*listener*/null, mHandler);
            mCollector.addMessage(""Session setRepeatingRequest should throw "" +
                    ""IllegalArgumentException for null request"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession setRepeatingRequest should throw IAE for for request
            // without surface configured.
            mSession.setRepeatingRequest(unConfiguredRequest, /*listener*/null, mHandler);
            mCollector.addMessage(""Capture zero burst should throw IllegalArgumentException "" +
                    ""for request without surface configured"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession captureBurst should throw IAE for null request list.
            mSession.captureBurst(/*requests*/null, /*listener*/null, mHandler);
            mCollector.addMessage(""Session captureBurst should throw "" +
                    ""IllegalArgumentException for null request list"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession captureBurst should throw IAE for empty request list.
            mSession.captureBurst(emptyRequests, /*listener*/null, mHandler);
            mCollector.addMessage(""Session captureBurst should throw "" +
                    "" IllegalArgumentException for empty request list"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession captureBurst should throw IAE for request
            // without surface configured.
            mSession.captureBurst(unConfiguredRequests, /*listener*/null, mHandler);
            fail(""Session captureBurst should throw IllegalArgumentException "" +
                    ""for null request list"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession setRepeatingBurst should throw IAE for null request list.
            mSession.setRepeatingBurst(/*requests*/null, /*listener*/null, mHandler);
            mCollector.addMessage(""Session setRepeatingBurst should throw "" +
                    ""IllegalArgumentException for null request list"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession setRepeatingBurst should throw IAE for empty request list.
            mSession.setRepeatingBurst(emptyRequests, /*listener*/null, mHandler);
            mCollector.addMessage(""Session setRepeatingBurst should throw "" +
                    ""IllegalArgumentException for empty request list"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession setRepeatingBurst should throw IAE for request
            // without surface configured.
            mSession.setRepeatingBurst(unConfiguredRequests, /*listener*/null, mHandler);
            mCollector.addMessage(""Session setRepeatingBurst should throw "" +
                    ""IllegalArgumentException for request without surface configured"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }
    }

    private class IsCaptureResultNotEmpty
            implements ArgumentMatcher<TotalCaptureResult> {
        @Override
        public boolean matches(TotalCaptureResult result) {
            /**
             * Do the simple verification here. Only verify the timestamp for now.
             * TODO: verify more required capture result metadata fields.
             */
            Long timeStamp = result.get(CaptureResult.SENSOR_TIMESTAMP);
            if (timeStamp != null && timeStamp.longValue() > 0L) {
                return true;
            }
            return false;
        }
    }

    /**
     * Run capture test with different test configurations.
     *
     * @param burst If the test uses {@link CameraCaptureSession#captureBurst} or
     * {@link CameraCaptureSession#setRepeatingBurst} to capture the burst.
     * @param repeating If the test uses {@link CameraCaptureSession#setRepeatingBurst} or
     * {@link CameraCaptureSession#setRepeatingRequest} for repeating capture.
     * @param abort If the test uses {@link CameraCaptureSession#abortCaptures} to stop the
     * repeating capture.  It has no effect if repeating is false.
     * @param useExecutor If the test uses {@link java.util.concurrent.Executor} instead of
     * {@link android.os.Handler} for callback invocation.
     */
    private void runCaptureTest(boolean burst, boolean repeating, boolean abort,
            boolean useExecutor) throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                openDevice(mCameraIdsUnderTest[i], mCameraMockListener);
                waitForDeviceState(STATE_OPENED, CAMERA_OPEN_TIMEOUT_MS);

                prepareCapture();

                if (!burst) {
                    // Test: that a single capture of each template type succeeds.
                    for (int j = 0; j < sTemplates.length; j++) {
                        // Skip video snapshots for LEGACY mode
                        if (mStaticInfo.isHardwareLevelLegacy() &&
                                sTemplates[j] == CameraDevice.TEMPLATE_VIDEO_SNAPSHOT) {
                            continue;
                        }
                        // Skip non-PREVIEW templates for non-color output
                        if (!mStaticInfo.isColorOutputSupported() &&
                                sTemplates[j] != CameraDevice.TEMPLATE_PREVIEW) {
                            continue;
                        }

                        captureSingleShot(mCameraIdsUnderTest[i], sTemplates[j], repeating, abort,
                                useExecutor);
                    }
                }
                else {
                    // Test: burst of one shot
                    captureBurstShot(mCameraIdsUnderTest[i], sTemplates, 1, repeating, abort, useExecutor);

                    int template = mStaticInfo.isColorOutputSupported() ?
                        CameraDevice.TEMPLATE_STILL_CAPTURE :
                        CameraDevice.TEMPLATE_PREVIEW;
                    int[] templates = new int[] {
                        template,
                        template,
                        template,
                        template,
                        template
                    };

                    // Test: burst of 5 shots of the same template type
                    captureBurstShot(mCameraIdsUnderTest[i], templates, templates.length, repeating, abort,
                            useExecutor);

                    if (mStaticInfo.isColorOutputSupported()) {
                        // Test: burst of 6 shots of different template types
                        captureBurstShot(mCameraIdsUnderTest[i], sTemplates, sTemplates.length, repeating,
                                abort, useExecutor);
                    }
                }
                verify(mCameraMockListener, never())
                        .onError(
                                any(CameraDevice.class),
                                anyInt());
            } catch (Exception e) {
                mCollector.addError(e);
            } finally {
                try {
                    closeSession();
                } catch (Exception e) {
                    mCollector.addError(e);
                }finally {
                    closeDevice(mCameraIdsUnderTest[i], mCameraMockListener);
                }
            }
        }
    }

    private void captureSingleShot(
            String id,
            int template,
            boolean repeating, boolean abort, boolean useExecutor) throws Exception {

        assertEquals(""Bad initial state for preparing to capture"",
                mLatestSessionState, SESSION_READY);

        final Executor executor = useExecutor ? new HandlerExecutor(mHandler) : null;
        CaptureRequest.Builder requestBuilder = mCamera.createCaptureRequest(template);
        assertNotNull(""Failed to create capture request"", requestBuilder);
        requestBuilder.addTarget(mReaderSurface);
        CameraCaptureSession.CaptureCallback mockCaptureCallback =
                mock(CameraCaptureSession.CaptureCallback.class);

        if (VERBOSE) {
            Log.v(TAG, String.format(""Capturing shot for device %s, template %d"",
                    id, template));
        }

        if (executor != null) {
            startCapture(requestBuilder.build(), repeating, mockCaptureCallback, executor);
        } else {
            startCapture(requestBuilder.build(), repeating, mockCaptureCallback, mHandler);
        }
        waitForSessionState(SESSION_ACTIVE, SESSION_ACTIVE_TIMEOUT_MS);

        int expectedCaptureResultCount = repeating ? REPEATING_CAPTURE_EXPECTED_RESULT_COUNT : 1;
        verifyCaptureResults(mockCaptureCallback, expectedCaptureResultCount);

        if (repeating) {
            if (abort) {
                mSession.abortCaptures();
                // Have to make sure abort and new requests aren't interleave together.
                waitForSessionState(SESSION_READY, SESSION_READY_TIMEOUT_MS);

                // Capture a single capture, and verify the result.
                SimpleCaptureCallback resultCallback = new SimpleCaptureCallback();
                CaptureRequest singleRequest = requestBuilder.build();
                if (executor != null) {
                    mSession.captureSingleRequest(singleRequest, executor, resultCallback);
                } else {
                    mSession.capture(singleRequest, resultCallback, mHandler);
                }
                resultCallback.getCaptureResultForRequest(singleRequest, CAPTURE_RESULT_TIMEOUT_MS);

                // Resume the repeating, and verify that results are returned.
                if (executor != null) {
                    mSession.setSingleRepeatingRequest(singleRequest, executor, resultCallback);
                } else {
                    mSession.setRepeatingRequest(singleRequest, resultCallback, mHandler);
                }
                for (int i = 0; i < REPEATING_CAPTURE_EXPECTED_RESULT_COUNT; i++) {
                    resultCallback.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);
                }
            }
            mSession.stopRepeating();
        }
        waitForSessionState(SESSION_READY, SESSION_READY_TIMEOUT_MS);
    }

    private void captureBurstShot(
            String id,
            int[] templates,
            int len,
            boolean repeating,
            boolean abort, boolean useExecutor) throws Exception {

        assertEquals(""Bad initial state for preparing to capture"",
                mLatestSessionState, SESSION_READY);

        assertTrue(""Invalid args to capture function"", len <= templates.length);
        List<CaptureRequest> requests = new ArrayList<CaptureRequest>();
        List<CaptureRequest> postAbortRequests = new ArrayList<CaptureRequest>();
        final Executor executor = useExecutor ? new HandlerExecutor(mHandler) : null;
        for (int i = 0; i < len; i++) {
            // Skip video snapshots for LEGACY mode
            if (mStaticInfo.isHardwareLevelLegacy() &&
                    templates[i] == CameraDevice.TEMPLATE_VIDEO_SNAPSHOT) {
                continue;
            }
            // Skip non-PREVIEW templates for non-color outpu
            if (!mStaticInfo.isColorOutputSupported() &&
                    templates[i] != CameraDevice.TEMPLATE_PREVIEW) {
                continue;
            }

            CaptureRequest.Builder requestBuilder = mCamera.createCaptureRequest(templates[i]);
            assertNotNull(""Failed to create capture request"", requestBuilder);
            requestBuilder.addTarget(mReaderSurface);
            requests.add(requestBuilder.build());
            if (abort) {
                postAbortRequests.add(requestBuilder.build());
            }
        }
        CameraCaptureSession.CaptureCallback mockCaptureCallback =
                mock(CameraCaptureSession.CaptureCallback.class);

        if (VERBOSE) {
            Log.v(TAG, String.format(""Capturing burst shot for device %s"", id));
        }

        if (!repeating) {
            if (executor != null) {
                mSession.captureBurstRequests(requests, executor, mockCaptureCallback);
            } else {
                mSession.captureBurst(requests, mockCaptureCallback, mHandler);
            }
        }
        else {
            if (executor != null) {
                mSession.setRepeatingBurstRequests(requests, executor, mockCaptureCallback);
            } else {
                mSession.setRepeatingBurst(requests, mockCaptureCallback, mHandler);
            }
        }
        waitForSessionState(SESSION_ACTIVE, SESSION_READY_TIMEOUT_MS);

        int expectedResultCount = requests.size();
        if (repeating) {
            expectedResultCount *= REPEATING_CAPTURE_EXPECTED_RESULT_COUNT;
        }

        verifyCaptureResults(mockCaptureCallback, expectedResultCount);

        if (repeating) {
            if (abort) {
                mSession.abortCaptures();
                // Have to make sure abort and new requests aren't interleave together.
                waitForSessionState(SESSION_READY, SESSION_READY_TIMEOUT_MS);

                // Capture a burst of captures, and verify the results.
                SimpleCaptureCallback resultCallback = new SimpleCaptureCallback();
                if (executor != null) {
                    mSession.captureBurstRequests(postAbortRequests, executor, resultCallback);
                } else {
                    mSession.captureBurst(postAbortRequests, resultCallback, mHandler);
                }
                // Verify that the results are returned.
                for (int i = 0; i < postAbortRequests.size(); i++) {
                    resultCallback.getCaptureResultForRequest(
                            postAbortRequests.get(i), CAPTURE_RESULT_TIMEOUT_MS);
                }

                // Resume the repeating, and verify that results are returned.
                if (executor != null) {
                    mSession.setRepeatingBurstRequests(requests, executor, resultCallback);
                } else {
                    mSession.setRepeatingBurst(requests, resultCallback, mHandler);
                }
                for (int i = 0; i < REPEATING_CAPTURE_EXPECTED_RESULT_COUNT; i++) {
                    resultCallback.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);
                }
            }
            mSession.stopRepeating();
        }
        waitForSessionState(SESSION_READY, SESSION_READY_TIMEOUT_MS);
    }

    /**
     * Precondition: Device must be in known OPENED state (has been waited for).
     *
     * <p>Creates a new capture session and waits until it is in the {@code SESSION_READY} state.
     * </p>
     *
     * <p>Any existing capture session will be closed as a result of calling this.</p>
     * */
    private void prepareCapture() throws Exception {
        if (VERBOSE) Log.v(TAG, ""prepareCapture"");

        assertTrue(""Bad initial state for preparing to capture"",
                mLatestDeviceState == STATE_OPENED);

        if (mSession != null) {
            if (VERBOSE) Log.v(TAG, ""prepareCapture - closing existing session"");
            closeSession();
        }

        // Create a new session listener each time, it's not reusable across cameras
        mSessionMockListener = spy(new BlockingSessionCallback());
        mSessionWaiter = mSessionMockListener.getStateWaiter();

        if (!mStaticInfo.isColorOutputSupported()) {
            createDefaultImageReader(getMaxDepthSize(mCamera.getId(), mCameraManager),
                    ImageFormat.DEPTH16, MAX_NUM_IMAGES, new ImageDropperListener());
        } else {
            createDefaultImageReader(DEFAULT_CAPTURE_SIZE, ImageFormat.YUV_420_888, MAX_NUM_IMAGES,
                    new ImageDropperListener());
        }

        List<Surface> outputSurfaces = new ArrayList<>(Arrays.asList(mReaderSurface));
        mCamera.createCaptureSession(outputSurfaces, mSessionMockListener, mHandler);

        mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        waitForSessionState(SESSION_CONFIGURED, SESSION_CONFIGURE_TIMEOUT_MS);
        waitForSessionState(SESSION_READY, SESSION_READY_TIMEOUT_MS);
    }

    private void waitForDeviceState(int state, long timeoutMs) {
        mCameraMockListener.waitForState(state, timeoutMs);
        mLatestDeviceState = state;
    }

    private void waitForSessionState(int state, long timeoutMs) {
        mSessionWaiter.waitForState(state, timeoutMs);
        mLatestSessionState = state;
    }

    private void verifyCaptureResults(
            CameraCaptureSession.CaptureCallback mockListener,
            int expectResultCount) {
        final int TIMEOUT_PER_RESULT_MS = 2000;
        // Should receive expected number of capture results.
        verify(mockListener,
                timeout(TIMEOUT_PER_RESULT_MS * expectResultCount).atLeast(expectResultCount))
                        .onCaptureCompleted(
                                eq(mSession),
                                isA(CaptureRequest.class),
                                argThat(new IsCaptureResultNotEmpty()));
        // Should not receive any capture failed callbacks.
        verify(mockListener, never())
                        .onCaptureFailed(
                                eq(mSession),
                                isA(CaptureRequest.class),
                                isA(CaptureFailure.class));
        // Should receive expected number of capture shutter calls
        verify(mockListener,
                atLeast(expectResultCount))
                        .onCaptureStarted(
                               eq(mSession),
                               isA(CaptureRequest.class),
                               anyLong(),
                               anyLong());
    }

    private void checkFpsRange(CaptureRequest.Builder request, int template,
            CameraCharacteristics props) {
        CaptureRequest.Key<Range<Integer>> fpsRangeKey = CONTROL_AE_TARGET_FPS_RANGE;
        Range<Integer> fpsRange;
        if ((fpsRange = mCollector.expectKeyValueNotNull(request, fpsRangeKey)) == null) {
            return;
        }

        int minFps = fpsRange.getLower();
        int maxFps = fpsRange.getUpper();
        Range<Integer>[] availableFpsRange = props
                .get(CameraCharacteristics.CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES);
        boolean foundRange = false;
        for (int i = 0; i < availableFpsRange.length; i += 1) {
            if (minFps == availableFpsRange[i].getLower()
                    && maxFps == availableFpsRange[i].getUpper()) {
                foundRange = true;
                break;
            }
        }
        if (!foundRange) {
            mCollector.addMessage(String.format(""Unable to find the fps range (%d, %d)"",
                    minFps, maxFps));
            return;
        }


        if (template != CameraDevice.TEMPLATE_MANUAL &&
                template != CameraDevice.TEMPLATE_STILL_CAPTURE) {
            if (maxFps < MIN_FPS_REQUIRED_FOR_STREAMING) {
                mCollector.addMessage(""Max fps should be at least ""
                        + MIN_FPS_REQUIRED_FOR_STREAMING);
                return;
            }

            // Relax framerate constraints on legacy mode
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                // Need give fixed frame rate for video recording template.
                if (template == CameraDevice.TEMPLATE_RECORD) {
                    if (maxFps != minFps) {
                        mCollector.addMessage(""Video recording frame rate should be fixed"");
                    }
                }
            }
        }
    }

    private void checkAfMode(CaptureRequest.Builder request, int template,
            CameraCharacteristics props) {
        boolean hasFocuser = props.getKeys().contains(CameraCharacteristics.
                LENS_INFO_MINIMUM_FOCUS_DISTANCE) &&
                (props.get(CameraCharacteristics.LENS_INFO_MINIMUM_FOCUS_DISTANCE) > 0f);

        if (!hasFocuser) {
            return;
        }

        int targetAfMode = CaptureRequest.CONTROL_AF_MODE_AUTO;
        int[] availableAfMode = props.get(CameraCharacteristics.CONTROL_AF_AVAILABLE_MODES);
        if (template == CameraDevice.TEMPLATE_PREVIEW ||
                template == CameraDevice.TEMPLATE_STILL_CAPTURE ||
                template == CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG) {
            // Default to CONTINUOUS_PICTURE if it is available, otherwise AUTO.
            for (int i = 0; i < availableAfMode.length; i++) {
                if (availableAfMode[i] == CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_PICTURE) {
                    targetAfMode = CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_PICTURE;
                    break;
                }
            }
        } else if (template == CameraDevice.TEMPLATE_RECORD ||
                template == CameraDevice.TEMPLATE_VIDEO_SNAPSHOT) {
            // Default to CONTINUOUS_VIDEO if it is available, otherwise AUTO.
            for (int i = 0; i < availableAfMode.length; i++) {
                if (availableAfMode[i] == CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_VIDEO) {
                    targetAfMode = CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_VIDEO;
                    break;
                }
            }
        } else if (template == CameraDevice.TEMPLATE_MANUAL) {
            targetAfMode = CaptureRequest.CONTROL_AF_MODE_OFF;
        }

        mCollector.expectKeyValueEquals(request, CONTROL_AF_MODE, targetAfMode);
        if (mStaticInfo.areKeysAvailable(CaptureRequest.LENS_FOCUS_DISTANCE)) {
            mCollector.expectKeyValueNotNull(request, LENS_FOCUS_DISTANCE);
        }
    }

    private void checkAntiBandingMode(CaptureRequest.Builder request, int template) {
        if (template == CameraDevice.TEMPLATE_MANUAL) {
            return;
        }

        if (!mStaticInfo.isColorOutputSupported()) return;

        List<Integer> availableAntiBandingModes =
                Arrays.asList(toObject(mStaticInfo.getAeAvailableAntiBandingModesChecked()));

        if (availableAntiBandingModes.contains(CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_AUTO)) {
            mCollector.expectKeyValueEquals(request, CONTROL_AE_ANTIBANDING_MODE,
                    CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_AUTO);
        } else {
            mCollector.expectKeyValueIsIn(request, CONTROL_AE_ANTIBANDING_MODE,
                    CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_50HZ,
                    CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_60HZ);
        }
    }

    /**
     * <p>Check if 3A metering settings are ""up to HAL"" in request template</p>
     *
     * <p>This function doesn't fail the test immediately, it updates the
     * test pass/fail status and appends the failure message to the error collector each key.</p>
     *
     * @param regions The metering rectangles to be checked
     */
    private void checkMeteringRect(MeteringRectangle[] regions) {
        if (regions == null) {
            return;
        }
        mCollector.expectNotEquals(""Number of metering region should not be 0"", 0, regions.length);
        for (int i = 0; i < regions.length; i++) {
            mCollector.expectEquals(""Default metering regions should have all zero weight"",
                    0, regions[i].getMeteringWeight());
        }
    }

    /**
     * <p>Check if the request settings are suitable for a given request template.</p>
     *
     * <p>This function doesn't fail the test immediately, it updates the
     * test pass/fail status and appends the failure message to the error collector each key.</p>
     *
     * @param request The request to be checked.
     * @param template The capture template targeted by this request.
     * @param props The CameraCharacteristics this request is checked against with.
     */
    private void checkRequestForTemplate(CaptureRequest.Builder request, int template,
            CameraCharacteristics props) {
        Integer hwLevel = props.get(CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL);
        boolean isExternalCamera = (hwLevel ==
                CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL);

        // 3A settings--AE/AWB/AF.
        Integer maxRegionsAeVal = props.get(CameraCharacteristics.CONTROL_MAX_REGIONS_AE);
        int maxRegionsAe = maxRegionsAeVal != null ? maxRegionsAeVal : 0;
        Integer maxRegionsAwbVal = props.get(CameraCharacteristics.CONTROL_MAX_REGIONS_AWB);
        int maxRegionsAwb = maxRegionsAwbVal != null ? maxRegionsAwbVal : 0;
        Integer maxRegionsAfVal = props.get(CameraCharacteristics.CONTROL_MAX_REGIONS_AF);
        int maxRegionsAf = maxRegionsAfVal != null ? maxRegionsAfVal : 0;

        checkFpsRange(request, template, props);

        checkAfMode(request, template, props);
        checkAntiBandingMode(request, template);

        if (template == CameraDevice.TEMPLATE_MANUAL) {
            mCollector.expectKeyValueEquals(request, CONTROL_MODE, CaptureRequest.CONTROL_MODE_OFF);
            mCollector.expectKeyValueEquals(request, CONTROL_AE_MODE,
                    CaptureRequest.CONTROL_AE_MODE_OFF);
            mCollector.expectKeyValueEquals(request, CONTROL_AWB_MODE,
                    CaptureRequest.CONTROL_AWB_MODE_OFF);
        } else {
            mCollector.expectKeyValueEquals(request, CONTROL_MODE,
                    CaptureRequest.CONTROL_MODE_AUTO);
            if (mStaticInfo.isColorOutputSupported()) {
                mCollector.expectKeyValueEquals(request, CONTROL_AE_MODE,
                        CaptureRequest.CONTROL_AE_MODE_ON);
                mCollector.expectKeyValueEquals(request, CONTROL_AE_EXPOSURE_COMPENSATION, 0);
                mCollector.expectKeyValueEquals(request, CONTROL_AE_PRECAPTURE_TRIGGER,
                        CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_IDLE);
                // if AE lock is not supported, expect the control key to be non-exist or false
                if (mStaticInfo.isAeLockSupported() || request.get(CONTROL_AE_LOCK) != null) {
                    mCollector.expectKeyValueEquals(request, CONTROL_AE_LOCK, false);
                }

                mCollector.expectKeyValueEquals(request, CONTROL_AF_TRIGGER,
                        CaptureRequest.CONTROL_AF_TRIGGER_IDLE);

                mCollector.expectKeyValueEquals(request, CONTROL_AWB_MODE,
                        CaptureRequest.CONTROL_AWB_MODE_AUTO);
                // if AWB lock is not supported, expect the control key to be non-exist or false
                if (mStaticInfo.isAwbLockSupported() || request.get(CONTROL_AWB_LOCK) != null) {
                    mCollector.expectKeyValueEquals(request, CONTROL_AWB_LOCK, false);
                }

                // Check 3A regions.
                if (VERBOSE) {
                    Log.v(TAG, String.format(""maxRegions is: {AE: %s, AWB: %s, AF: %s}"",
                                    maxRegionsAe, maxRegionsAwb, maxRegionsAf));
                }
                if (maxRegionsAe > 0) {
                    mCollector.expectKeyValueNotNull(request, CONTROL_AE_REGIONS);
                    MeteringRectangle[] aeRegions = request.get(CONTROL_AE_REGIONS);
                    checkMeteringRect(aeRegions);
                }
                if (maxRegionsAwb > 0) {
                    mCollector.expectKeyValueNotNull(request, CONTROL_AWB_REGIONS);
                    MeteringRectangle[] awbRegions = request.get(CONTROL_AWB_REGIONS);
                    checkMeteringRect(awbRegions);
                }
                if (maxRegionsAf > 0) {
                    mCollector.expectKeyValueNotNull(request, CONTROL_AF_REGIONS);
                    MeteringRectangle[] afRegions = request.get(CONTROL_AF_REGIONS);
                    checkMeteringRect(afRegions);
                }
            }
        }

        // Sensor settings.

        mCollector.expectEquals(""Lens aperture must be present in request if available apertures "" +
                        ""are present in metadata, and vice-versa."",
                mStaticInfo.areKeysAvailable(CameraCharacteristics.LENS_INFO_AVAILABLE_APERTURES),
                mStaticInfo.areKeysAvailable(CaptureRequest.LENS_APERTURE));
        if (mStaticInfo.areKeysAvailable(CameraCharacteristics.LENS_INFO_AVAILABLE_APERTURES)) {
            float[] availableApertures =
                    props.get(CameraCharacteristics.LENS_INFO_AVAILABLE_APERTURES);
            if (availableApertures.length > 1) {
                mCollector.expectKeyValueNotNull(request, LENS_APERTURE);
            }
        }

        mCollector.expectEquals(""Lens filter density must be present in request if available "" +
                        ""filter densities are present in metadata, and vice-versa."",
                mStaticInfo.areKeysAvailable(CameraCharacteristics.
                        LENS_INFO_AVAILABLE_FILTER_DENSITIES),
                mStaticInfo.areKeysAvailable(CaptureRequest.LENS_FILTER_DENSITY));
        if (mStaticInfo.areKeysAvailable(CameraCharacteristics.
                LENS_INFO_AVAILABLE_FILTER_DENSITIES)) {
            float[] availableFilters =
                    props.get(CameraCharacteristics.LENS_INFO_AVAILABLE_FILTER_DENSITIES);
            if (availableFilters.length > 1) {
                mCollector.expectKeyValueNotNull(request, LENS_FILTER_DENSITY);
            }
        }


        if (!isExternalCamera) {
            float[] availableFocalLen =
                    props.get(CameraCharacteristics.LENS_INFO_AVAILABLE_FOCAL_LENGTHS);
            if (availableFocalLen.length > 1) {
                mCollector.expectKeyValueNotNull(request, LENS_FOCAL_LENGTH);
            }
        }


        mCollector.expectEquals(""Lens optical stabilization must be present in request if "" +
                        ""available optical stabilizations are present in metadata, and vice-versa."",
                mStaticInfo.areKeysAvailable(CameraCharacteristics.
                        LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION),
                mStaticInfo.areKeysAvailable(CaptureRequest.LENS_OPTICAL_STABILIZATION_MODE));
        if (mStaticInfo.areKeysAvailable(CameraCharacteristics.
                LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION)) {
            int[] availableOIS =
                    props.get(CameraCharacteristics.LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION);
            if (availableOIS.length > 1) {
                mCollector.expectKeyValueNotNull(request, LENS_OPTICAL_STABILIZATION_MODE);
            }
        }

        if (mStaticInfo.areKeysAvailable(SENSOR_TEST_PATTERN_MODE)) {
            mCollector.expectKeyValueEquals(request, SENSOR_TEST_PATTERN_MODE,
                    CaptureRequest.SENSOR_TEST_PATTERN_MODE_OFF);
        }

        if (mStaticInfo.areKeysAvailable(BLACK_LEVEL_LOCK)) {
            mCollector.expectKeyValueEquals(request, BLACK_LEVEL_LOCK, false);
        }

        if (mStaticInfo.areKeysAvailable(SENSOR_FRAME_DURATION)) {
            mCollector.expectKeyValueNotNull(request, SENSOR_FRAME_DURATION);
        }

        if (mStaticInfo.areKeysAvailable(SENSOR_EXPOSURE_TIME)) {
            mCollector.expectKeyValueNotNull(request, SENSOR_EXPOSURE_TIME);
        }

        if (mStaticInfo.areKeysAvailable(SENSOR_SENSITIVITY)) {
            mCollector.expectKeyValueNotNull(request, SENSOR_SENSITIVITY);
        }

        // ISP-processing settings.
        if (mStaticInfo.isColorOutputSupported()) {
            mCollector.expectKeyValueEquals(
                    request, STATISTICS_FACE_DETECT_MODE,
                    CaptureRequest.STATISTICS_FACE_DETECT_MODE_OFF);
            mCollector.expectKeyValueEquals(request, FLASH_MODE, CaptureRequest.FLASH_MODE_OFF);
        }

        List<Integer> availableCaps = mStaticInfo.getAvailableCapabilitiesChecked();
        if (mStaticInfo.areKeysAvailable(STATISTICS_LENS_SHADING_MAP_MODE)) {
            // If the device doesn't support RAW, all template should have OFF as default.
            if (!availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                mCollector.expectKeyValueEquals(
                        request, STATISTICS_LENS_SHADING_MAP_MODE,
                        CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE_OFF);
            }
        }

        boolean supportReprocessing =
                availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING) ||
                availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING);


        if (template == CameraDevice.TEMPLATE_STILL_CAPTURE) {

            // Ok with either FAST or HIGH_QUALITY
            if (mStaticInfo.areKeysAvailable(COLOR_CORRECTION_MODE)) {
                mCollector.expectKeyValueNotEquals(
                        request, COLOR_CORRECTION_MODE,
                        CaptureRequest.COLOR_CORRECTION_MODE_TRANSFORM_MATRIX);
            }

            // Edge enhancement, noise reduction and aberration correction modes.
            mCollector.expectEquals(""Edge mode must be present in request if "" +
                            ""available edge modes are present in metadata, and vice-versa."",
                    mStaticInfo.areKeysAvailable(CameraCharacteristics.
                            EDGE_AVAILABLE_EDGE_MODES),
                    mStaticInfo.areKeysAvailable(CaptureRequest.EDGE_MODE));
            if (mStaticInfo.areKeysAvailable(EDGE_MODE)) {
                List<Integer> availableEdgeModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableEdgeModesChecked()));
                // Don't need check fast as fast or high quality must be both present or both not.
                if (availableEdgeModes.contains(CaptureRequest.EDGE_MODE_HIGH_QUALITY)) {
                    mCollector.expectKeyValueEquals(request, EDGE_MODE,
                            CaptureRequest.EDGE_MODE_HIGH_QUALITY);
                } else {
                    mCollector.expectKeyValueEquals(request, EDGE_MODE,
                            CaptureRequest.EDGE_MODE_OFF);
                }
            }
            if (mStaticInfo.areKeysAvailable(SHADING_MODE)) {
                List<Integer> availableShadingModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableShadingModesChecked()));
                mCollector.expectKeyValueEquals(request, SHADING_MODE,
                        CaptureRequest.SHADING_MODE_HIGH_QUALITY);
            }

            mCollector.expectEquals(""Noise reduction mode must be present in request if "" +
                            ""available noise reductions are present in metadata, and vice-versa."",
                    mStaticInfo.areKeysAvailable(CameraCharacteristics.
                            NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES),
                    mStaticInfo.areKeysAvailable(CaptureRequest.NOISE_REDUCTION_MODE));
            if (mStaticInfo.areKeysAvailable(
                    CameraCharacteristics.NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES)) {
                List<Integer> availableNoiseReductionModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableNoiseReductionModesChecked()));
                // Don't need check fast as fast or high quality must be both present or both not.
                if (availableNoiseReductionModes
                        .contains(CaptureRequest.NOISE_REDUCTION_MODE_HIGH_QUALITY)) {
                    mCollector.expectKeyValueEquals(
                            request, NOISE_REDUCTION_MODE,
                            CaptureRequest.NOISE_REDUCTION_MODE_HIGH_QUALITY);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, NOISE_REDUCTION_MODE, CaptureRequest.NOISE_REDUCTION_MODE_OFF);
                }
            }

            mCollector.expectEquals(""Hot pixel mode must be present in request if "" +
                            ""available hot pixel modes are present in metadata, and vice-versa."",
                    mStaticInfo.areKeysAvailable(CameraCharacteristics.
                            HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES),
                    mStaticInfo.areKeysAvailable(CaptureRequest.HOT_PIXEL_MODE));

            if (mStaticInfo.areKeysAvailable(HOT_PIXEL_MODE)) {
                List<Integer> availableHotPixelModes =
                        Arrays.asList(toObject(
                                mStaticInfo.getAvailableHotPixelModesChecked()));
                if (availableHotPixelModes
                        .contains(CaptureRequest.HOT_PIXEL_MODE_HIGH_QUALITY)) {
                    mCollector.expectKeyValueEquals(
                            request, HOT_PIXEL_MODE,
                            CaptureRequest.HOT_PIXEL_MODE_HIGH_QUALITY);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, HOT_PIXEL_MODE, CaptureRequest.HOT_PIXEL_MODE_OFF);
                }
            }

            boolean supportAvailableAberrationModes = mStaticInfo.areKeysAvailable(
                    CameraCharacteristics.COLOR_CORRECTION_AVAILABLE_ABERRATION_MODES);
            boolean supportAberrationRequestKey = mStaticInfo.areKeysAvailable(
                    CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE);
            mCollector.expectEquals(""Aberration correction mode must be present in request if "" +
                    ""available aberration correction reductions are present in metadata, and ""
                    + ""vice-versa."", supportAvailableAberrationModes, supportAberrationRequestKey);
            if (supportAberrationRequestKey) {
                List<Integer> availableAberrationModes = Arrays.asList(
                        toObject(mStaticInfo.getAvailableColorAberrationModesChecked()));
                // Don't need check fast as fast or high quality must be both present or both not.
                if (availableAberrationModes
                        .contains(CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_HIGH_QUALITY)) {
                    mCollector.expectKeyValueEquals(
                            request, COLOR_CORRECTION_ABERRATION_MODE,
                            CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_HIGH_QUALITY);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, COLOR_CORRECTION_ABERRATION_MODE,
                            CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_OFF);
                }
            }
        } else if (template == CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG && supportReprocessing) {
            mCollector.expectKeyValueEquals(request, EDGE_MODE,
                    CaptureRequest.EDGE_MODE_ZERO_SHUTTER_LAG);
            mCollector.expectKeyValueEquals(request, NOISE_REDUCTION_MODE,
                    CaptureRequest.NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG);
        } else if (template == CameraDevice.TEMPLATE_PREVIEW ||
                template == CameraDevice.TEMPLATE_RECORD) {

            // Ok with either FAST or HIGH_QUALITY
            if (mStaticInfo.areKeysAvailable(COLOR_CORRECTION_MODE)) {
                mCollector.expectKeyValueNotEquals(
                        request, COLOR_CORRECTION_MODE,
                        CaptureRequest.COLOR_CORRECTION_MODE_TRANSFORM_MATRIX);
            }

            if (mStaticInfo.areKeysAvailable(EDGE_MODE)) {
                List<Integer> availableEdgeModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableEdgeModesChecked()));
                if (availableEdgeModes.contains(CaptureRequest.EDGE_MODE_FAST)) {
                    mCollector.expectKeyValueEquals(request, EDGE_MODE,
                            CaptureRequest.EDGE_MODE_FAST);
                } else {
                    mCollector.expectKeyValueEquals(request, EDGE_MODE,
                            CaptureRequest.EDGE_MODE_OFF);
                }
            }

            if (mStaticInfo.areKeysAvailable(SHADING_MODE)) {
                List<Integer> availableShadingModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableShadingModesChecked()));
                mCollector.expectKeyValueEquals(request, SHADING_MODE,
                        CaptureRequest.SHADING_MODE_FAST);
            }

            if (mStaticInfo.areKeysAvailable(NOISE_REDUCTION_MODE)) {
                List<Integer> availableNoiseReductionModes =
                        Arrays.asList(toObject(
                                mStaticInfo.getAvailableNoiseReductionModesChecked()));
                if (availableNoiseReductionModes
                        .contains(CaptureRequest.NOISE_REDUCTION_MODE_FAST)) {
                    mCollector.expectKeyValueEquals(
                            request, NOISE_REDUCTION_MODE,
                            CaptureRequest.NOISE_REDUCTION_MODE_FAST);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, NOISE_REDUCTION_MODE, CaptureRequest.NOISE_REDUCTION_MODE_OFF);
                }
            }

            if (mStaticInfo.areKeysAvailable(HOT_PIXEL_MODE)) {
                List<Integer> availableHotPixelModes =
                        Arrays.asList(toObject(
                                mStaticInfo.getAvailableHotPixelModesChecked()));
                if (availableHotPixelModes
                        .contains(CaptureRequest.HOT_PIXEL_MODE_FAST)) {
                    mCollector.expectKeyValueEquals(
                            request, HOT_PIXEL_MODE,
                            CaptureRequest.HOT_PIXEL_MODE_FAST);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, HOT_PIXEL_MODE, CaptureRequest.HOT_PIXEL_MODE_OFF);
                }
            }

            if (mStaticInfo.areKeysAvailable(COLOR_CORRECTION_ABERRATION_MODE)) {
                List<Integer> availableAberrationModes = Arrays.asList(
                        toObject(mStaticInfo.getAvailableColorAberrationModesChecked()));
                if (availableAberrationModes
                        .contains(CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_FAST)) {
                    mCollector.expectKeyValueEquals(
                            request, COLOR_CORRECTION_ABERRATION_MODE,
                            CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_FAST);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, COLOR_CORRECTION_ABERRATION_MODE,
                            CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_OFF);
                }
            }
        } else {
            if (mStaticInfo.areKeysAvailable(EDGE_MODE)) {
                mCollector.expectKeyValueNotNull(request, EDGE_MODE);
            }

            if (mStaticInfo.areKeysAvailable(NOISE_REDUCTION_MODE)) {
                mCollector.expectKeyValueNotNull(request, NOISE_REDUCTION_MODE);
            }

            if (mStaticInfo.areKeysAvailable(COLOR_CORRECTION_ABERRATION_MODE)) {
                mCollector.expectKeyValueNotNull(request, COLOR_CORRECTION_ABERRATION_MODE);
            }
        }

        // Tone map and lens shading modes.
        if (template == CameraDevice.TEMPLATE_STILL_CAPTURE) {
            mCollector.expectEquals(""Tonemap mode must be present in request if "" +
                            ""available tonemap modes are present in metadata, and vice-versa."",
                    mStaticInfo.areKeysAvailable(CameraCharacteristics.
                            TONEMAP_AVAILABLE_TONE_MAP_MODES),
                    mStaticInfo.areKeysAvailable(CaptureRequest.TONEMAP_MODE));
            if (mStaticInfo.areKeysAvailable(
                    CameraCharacteristics.TONEMAP_AVAILABLE_TONE_MAP_MODES)) {
                List<Integer> availableToneMapModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableToneMapModesChecked()));
                if (availableToneMapModes.contains(CaptureRequest.TONEMAP_MODE_HIGH_QUALITY)) {
                    mCollector.expectKeyValueEquals(request, TONEMAP_MODE,
                            CaptureRequest.TONEMAP_MODE_HIGH_QUALITY);
                } else {
                    mCollector.expectKeyValueEquals(request, TONEMAP_MODE,
                            CaptureRequest.TONEMAP_MODE_FAST);
                }
            }

            // Still capture template should have android.statistics.lensShadingMapMode ON when
            // RAW capability is supported.
            if (mStaticInfo.areKeysAvailable(STATISTICS_LENS_SHADING_MAP_MODE) &&
                    availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                    mCollector.expectKeyValueEquals(request, STATISTICS_LENS_SHADING_MAP_MODE,
                            STATISTICS_LENS_SHADING_MAP_MODE_ON);
            }
        } else {
            if (mStaticInfo.areKeysAvailable(TONEMAP_MODE)) {
                mCollector.expectKeyValueNotEquals(request, TONEMAP_MODE,
                        CaptureRequest.TONEMAP_MODE_CONTRAST_CURVE);
                mCollector.expectKeyValueNotEquals(request, TONEMAP_MODE,
                        CaptureRequest.TONEMAP_MODE_GAMMA_VALUE);
                mCollector.expectKeyValueNotEquals(request, TONEMAP_MODE,
                        CaptureRequest.TONEMAP_MODE_PRESET_CURVE);
            }
            if (mStaticInfo.areKeysAvailable(STATISTICS_LENS_SHADING_MAP_MODE)) {
                mCollector.expectKeyValueEquals(request, STATISTICS_LENS_SHADING_MAP_MODE,
                        CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE_OFF);
            }
            if (mStaticInfo.areKeysAvailable(STATISTICS_HOT_PIXEL_MAP_MODE)) {
                mCollector.expectKeyValueEquals(request, STATISTICS_HOT_PIXEL_MAP_MODE,
                        false);
            }
        }

        // Enable ZSL
        if (template != CameraDevice.TEMPLATE_STILL_CAPTURE) {
            if (mStaticInfo.areKeysAvailable(CONTROL_ENABLE_ZSL)) {
                    mCollector.expectKeyValueEquals(request, CONTROL_ENABLE_ZSL, false);
            }
        }

        int[] outputFormats = mStaticInfo.getAvailableFormats(
                StaticMetadata.StreamDirection.Output);
        boolean supportRaw = false;
        for (int format : outputFormats) {
            if (format == ImageFormat.RAW_SENSOR || format == ImageFormat.RAW10 ||
                    format == ImageFormat.RAW12 || format == ImageFormat.RAW_PRIVATE) {
                supportRaw = true;
                break;
            }
        }
        if (supportRaw) {
            mCollector.expectKeyValueEquals(request,
                    CONTROL_POST_RAW_SENSITIVITY_BOOST,
                    DEFAULT_POST_RAW_SENSITIVITY_BOOST);
        }

        switch(template) {
            case CameraDevice.TEMPLATE_PREVIEW:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_PREVIEW);
                break;
            case CameraDevice.TEMPLATE_STILL_CAPTURE:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_STILL_CAPTURE);
                break;
            case CameraDevice.TEMPLATE_RECORD:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_VIDEO_RECORD);
                break;
            case CameraDevice.TEMPLATE_VIDEO_SNAPSHOT:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_VIDEO_SNAPSHOT);
                break;
            case CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_ZERO_SHUTTER_LAG);
                break;
            case CameraDevice.TEMPLATE_MANUAL:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_MANUAL);
                break;
            default:
                // Skip unknown templates here
        }

        // Check distortion correction mode
        if (mStaticInfo.isDistortionCorrectionSupported()) {
            mCollector.expectKeyValueNotEquals(request, DISTORTION_CORRECTION_MODE,
                    CaptureRequest.DISTORTION_CORRECTION_MODE_OFF);
        }

        // Scaler settings
        if (mStaticInfo.areKeysAvailable(
                CameraCharacteristics.SCALER_AVAILABLE_ROTATE_AND_CROP_MODES)) {
            List<Integer> rotateAndCropModes = Arrays.asList(toObject(
                props.get(CameraCharacteristics.SCALER_AVAILABLE_ROTATE_AND_CROP_MODES)));
            if (rotateAndCropModes.contains(SCALER_ROTATE_AND_CROP_AUTO)) {
                mCollector.expectKeyValueEquals(request, SCALER_ROTATE_AND_CROP,
                        CaptureRequest.SCALER_ROTATE_AND_CROP_AUTO);
            }
        }

        // Check JPEG quality
        if (mStaticInfo.isColorOutputSupported()) {
            mCollector.expectKeyValueNotNull(request, JPEG_QUALITY);
        }

        // TODO: use the list of keys from CameraCharacteristics to avoid expecting
        //       keys which are not available by this CameraDevice.
    }

    private void captureTemplateTestByCamera(String cameraId, int template) throws Exception {
        try {
            openDevice(cameraId, mCameraMockListener);

            assertTrue(""Camera template "" + template + "" is out of range!"",
                    template >= CameraDevice.TEMPLATE_PREVIEW
                            && template <= CameraDevice.TEMPLATE_MANUAL);

            mCollector.setCameraId(cameraId);

            try {
                CaptureRequest.Builder request = mCamera.createCaptureRequest(template);
                assertNotNull(""Failed to create capture request for template "" + template, request);

                CameraCharacteristics props = mStaticInfo.getCharacteristics();
                checkRequestForTemplate(request, template, props);
            } catch (IllegalArgumentException e) {
                if (template == CameraDevice.TEMPLATE_MANUAL &&
                        !mStaticInfo.isCapabilitySupported(CameraCharacteristics.
                                REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
                    // OK
                } else if (template == CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG &&
                        !mStaticInfo.isCapabilitySupported(CameraCharacteristics.
                                REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING)) {
                    // OK.
                } else if (sLegacySkipTemplates.contains(template) &&
                        mStaticInfo.isHardwareLevelLegacy()) {
                    // OK
                } else if (template != CameraDevice.TEMPLATE_PREVIEW &&
                        mStaticInfo.isDepthOutputSupported() &&
                        !mStaticInfo.isColorOutputSupported()) {
                    // OK, depth-only devices need only support PREVIEW template
                } else {
                    throw e; // rethrow
                }
            }
        }
        finally {
            try {
                closeSession();
            } finally {
                closeDevice(cameraId, mCameraMockListener);
            }
        }
    }

    /**
     * Start capture with given {@link #CaptureRequest}.
     *
     * @param request The {@link #CaptureRequest} to be captured.
     * @param repeating If the capture is single capture or repeating.
     * @param listener The {@link #CaptureCallback} camera device used to notify callbacks.
     * @param handler The handler camera device used to post callbacks.
     */
    @Override
    protected void startCapture(CaptureRequest request, boolean repeating,
            CameraCaptureSession.CaptureCallback listener, Handler handler)
                    throws CameraAccessException {
        if (VERBOSE) Log.v(TAG, ""Starting capture from session"");

        if (repeating) {
            mSession.setRepeatingRequest(request, listener, handler);
        } else {
            mSession.capture(request, listener, handler);
        }
    }

    /**
     * Start capture with given {@link #CaptureRequest}.
     *
     * @param request The {@link #CaptureRequest} to be captured.
     * @param repeating If the capture is single capture or repeating.
     * @param listener The {@link #CaptureCallback} camera device used to notify callbacks.
     * @param executor The executor used to invoke callbacks.
     */
    protected void startCapture(CaptureRequest request, boolean repeating,
            CameraCaptureSession.CaptureCallback listener, Executor executor)
                    throws CameraAccessException {
        if (VERBOSE) Log.v(TAG, ""Starting capture from session"");

        if (repeating) {
            mSession.setSingleRepeatingRequest(request, executor, listener);
        } else {
            mSession.captureSingleRequest(request, executor, listener);
        }
    }

    /**
     * Close a {@link #CameraCaptureSession capture session}; blocking until
     * the close finishes with a transition to {@link CameraCaptureSession.StateCallback#onClosed}.
     */
    protected void closeSession() {
        if (mSession == null) {
            return;
        }

        mSession.close();
        waitForSessionState(SESSION_CLOSED, SESSION_CLOSE_TIMEOUT_MS);
        mSession = null;

        mSessionMockListener = null;
        mSessionWaiter = null;
    }

    /**
     * A camera capture session listener that keeps all the configured and closed sessions.
     */
    private class MultipleSessionCallback extends CameraCaptureSession.StateCallback {
        public static final int SESSION_CONFIGURED = 0;
        public static final int SESSION_CLOSED = 1;

        final List<CameraCaptureSession> mSessions = new ArrayList<>();
        final Map<CameraCaptureSession, Integer> mSessionStates = new HashMap<>();
        CameraCaptureSession mCurrentConfiguredSession = null;

        final ReentrantLock mLock = new ReentrantLock();
        final Condition mNewStateCond = mLock.newCondition();

        final boolean mFailOnConfigureFailed;

        /**
         * If failOnConfigureFailed is true, it calls fail() when onConfigureFailed() is invoked
         * for any session.
         */
        public MultipleSessionCallback(boolean failOnConfigureFailed) {
            mFailOnConfigureFailed = failOnConfigureFailed;
        }

        @Override
        public void onClosed(CameraCaptureSession session) {
            mLock.lock();
            mSessionStates.put(session, SESSION_CLOSED);
            mNewStateCond.signal();
            mLock.unlock();
        }

        @Override
        public void onConfigured(CameraCaptureSession session) {
            mLock.lock();
            mSessions.add(session);
            mSessionStates.put(session, SESSION_CONFIGURED);
            mNewStateCond.signal();
            mLock.unlock();
        }

        @Override
        public void onConfigureFailed(CameraCaptureSession session) {
            if (mFailOnConfigureFailed) {
                fail(""Configuring a session failed"");
            }
        }

        /**
         * Get a number of sessions that have been configured.
         */
        public List<CameraCaptureSession> getAllSessions(int numSessions, int timeoutMs)
                throws Exception {
            long remainingTime = timeoutMs;
            mLock.lock();
            try {
                while (mSessions.size() < numSessions) {
                    long startTime = SystemClock.elapsedRealtime();
                    boolean ret = mNewStateCond.await(remainingTime, TimeUnit.MILLISECONDS);
                    remainingTime -= (SystemClock.elapsedRealtime() - startTime);
                    ret &= remainingTime > 0;

                    assertTrue(""Get "" + numSessions + "" sessions timed out after "" + timeoutMs +
                            ""ms"", ret);
                }

                return mSessions;
            } finally {
                mLock.unlock();
            }
        }

        /**
         * Wait until a previously-configured sessoin is closed or it times out.
         */
        public void waitForSessionClose(CameraCaptureSession session, int timeoutMs) throws Exception {
            long remainingTime = timeoutMs;
            mLock.lock();
            try {
                while (mSessionStates.get(session).equals(SESSION_CLOSED) == false) {
                    long startTime = SystemClock.elapsedRealtime();
                    boolean ret = mNewStateCond.await(remainingTime, TimeUnit.MILLISECONDS);
                    remainingTime -= (SystemClock.elapsedRealtime() - startTime);
                    ret &= remainingTime > 0;

                    assertTrue(""Wait for session close timed out after "" + timeoutMs + ""ms"", ret);
                }
            } finally {
                mLock.unlock();
            }
        }
    }

    /**
     * Verify audio restrictions are set properly for single CameraDevice usage
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraDeviceTest"	"testAudioRestrictionSingleDevice"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CameraDeviceTest.java"	""	"public void testAudioRestrictionSingleDevice() throws Exception {
        int[] testModes = {
            CameraDevice.AUDIO_RESTRICTION_VIBRATION_SOUND,
            CameraDevice.AUDIO_RESTRICTION_NONE,
            CameraDevice.AUDIO_RESTRICTION_VIBRATION,
        };
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                openDevice(mCameraIdsUnderTest[i], mCameraMockListener);
                waitForDeviceState(STATE_OPENED, CAMERA_OPEN_TIMEOUT_MS);

                for (int mode : testModes) {
                    mCamera.setCameraAudioRestriction(mode);
                    int retMode = mCamera.getCameraAudioRestriction();
                    assertTrue(""Audio restriction mode mismatch: input: "" + mode +
                            "", output:"" + retMode, mode == retMode);
                }

                try {
                    // Test invalid mode
                    mCamera.setCameraAudioRestriction(42);
                    fail(""Should get IllegalArgumentException for invalid mode"");
                } catch (IllegalArgumentException e) {
                    // expected
                }
            }
            finally {
                closeDevice(mCameraIdsUnderTest[i], mCameraMockListener);
            }
        }
    }

    private void testTwoCameraDevicesAudioRestriction(String id0, String id1) throws Exception {
        BlockingStateCallback cam0Cb = new BlockingStateCallback();
        BlockingStateCallback cam1Cb = new BlockingStateCallback();
        CameraDevice cam0 = null;
        CameraDevice cam1 = null;
        try {
            cam0 = CameraTestUtils.openCamera(mCameraManager, id0, cam0Cb, mHandler);
            cam0Cb.waitForState(STATE_OPENED, CAMERA_OPEN_TIMEOUT_MS);

            int mode0 = CameraDevice.AUDIO_RESTRICTION_VIBRATION_SOUND;
            cam0.setCameraAudioRestriction(mode0);
            int retMode = cam0.getCameraAudioRestriction();
            assertTrue(""Audio restriction mode mismatch: input: "" + mode0 + "", output:"" + retMode,
                    retMode == mode0);

            try {
                cam1 = CameraTestUtils.openCamera(mCameraManager, id1, cam1Cb, mHandler);
            } catch (CameraAccessException | BlockingOpenException e) {
                Log.i(TAG, ""Camera "" + id1 + ""cannot be opened along with camera "" + id0 +
                        "", skipping the test"");
                return;
            }
            cam1Cb.waitForState(STATE_OPENED, CAMERA_OPEN_TIMEOUT_MS);

            // See if cam0 is evicted.
            try {
                final int cameraEvictedTimeoutMs = 1000;
                cam0Cb.waitForState(STATE_DISCONNECTED, cameraEvictedTimeoutMs);
                fail(""Opened camera "" + id0 + "" is evicted by a later open call for camera "" +
                        id1 + "" from the same process"");
            } catch (TimeoutRuntimeException e) {
                // camera 0 is not evicted
            }

            // The output mode should be union of all CameraDevices
            int mode1 = CameraDevice.AUDIO_RESTRICTION_VIBRATION;
            int expectMode = mode0 | mode1;
            cam1.setCameraAudioRestriction(mode1);
            retMode = cam1.getCameraAudioRestriction();
            assertTrue(""Audio restriction mode mismatch: expect: "" + expectMode +
                    "", output:"" + retMode, retMode == expectMode);

            // test turning off mute settings also
            mode0 = CameraDevice.AUDIO_RESTRICTION_NONE;
            expectMode = mode0 | mode1;
            cam0.setCameraAudioRestriction(mode0);
            retMode = cam0.getCameraAudioRestriction();
            assertTrue(""Audio restriction mode mismatch: expect: "" + expectMode +
                    "", output:"" + retMode, retMode == expectMode);

            // mode should be NONE when both device set to NONE
            mode1 = CameraDevice.AUDIO_RESTRICTION_NONE;
            expectMode = mode0 | mode1;
            cam1.setCameraAudioRestriction(mode1);
            retMode = cam1.getCameraAudioRestriction();
            assertTrue(""Audio restriction mode mismatch: expect: "" + expectMode +
                    "", output:"" + retMode, retMode == expectMode);

            // test removal of VIBRATE won't affect existing VIBRATE_SOUND state
            mode0 = CameraDevice.AUDIO_RESTRICTION_VIBRATION_SOUND;
            expectMode = mode0 | mode1;
            cam0.setCameraAudioRestriction(mode0);
            retMode = cam0.getCameraAudioRestriction();
            assertTrue(""Audio restriction mode mismatch: expect: "" + expectMode +
                    "", output:"" + retMode, retMode == expectMode);

            mode1 = CameraDevice.AUDIO_RESTRICTION_VIBRATION;
            expectMode = mode0 | mode1;
            cam1.setCameraAudioRestriction(mode1);
            retMode = cam1.getCameraAudioRestriction();
            assertTrue(""Audio restriction mode mismatch: expect: "" + expectMode +
                    "", output:"" + retMode, retMode == expectMode);

            mode1 = CameraDevice.AUDIO_RESTRICTION_NONE;
            expectMode = mode0 | mode1;
            cam1.setCameraAudioRestriction(mode1);
            retMode = cam1.getCameraAudioRestriction();
            assertTrue(""Audio restriction mode mismatch: expect: "" + expectMode +
                    "", output:"" + retMode, retMode == expectMode);

            // Now test CameraDevice.close will remove setting and exception is thrown for closed
            // camera.
            cam0.close();
            cam0Cb.waitForState(STATE_CLOSED, CAMERA_CLOSE_TIMEOUT_MS);
            try {
                cam0.setCameraAudioRestriction(mode0);
                fail(""Should get IllegalStateException for closed camera."");
            } catch (IllegalStateException e) {
                // expected;
            }

            cam0 = null;
            cam0Cb = null;
            expectMode = mode1;
            cam1.setCameraAudioRestriction(mode1);
            retMode = cam1.getCameraAudioRestriction();
            assertTrue(""Audio restriction mode mismatch: expect: "" + expectMode +
                    "", output:"" + retMode, retMode == expectMode);
        } finally {
            if (cam0 != null) {
                cam0.close();
                cam0Cb.waitForState(STATE_CLOSED, CAMERA_CLOSE_TIMEOUT_MS);
                cam0Cb = null;
            }
            if (cam1 != null) {
                cam1.close();
                cam1Cb.waitForState(STATE_CLOSED, CAMERA_CLOSE_TIMEOUT_MS);
                cam1Cb = null;
            }
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaCodecListTest"	"testRequiredMediaCodecList"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaCodecListTest.java"	""	"public void testRequiredMediaCodecList() {
        List<CodecType> requiredList = getRequiredCodecTypes();
        List<CodecType> supportedList = getSupportedCodecTypes();
        assertTrue(areRequiredCodecTypesSupported(requiredList, supportedList));
        for (CodecType type : requiredList) {
            assertTrue(""cannot find "" + type, type.canBeFound());
        }
    }

    private boolean hasCamera() {
        PackageManager pm = getContext().getPackageManager();
        return pm.hasSystemFeature(pm.FEATURE_CAMERA_FRONT) ||
                pm.hasSystemFeature(pm.FEATURE_CAMERA);
    }

    private boolean hasMicrophone() {
        PackageManager pm = getContext().getPackageManager();
        return pm.hasSystemFeature(pm.FEATURE_MICROPHONE);
    }

    private boolean isWatch() {
        PackageManager pm = getContext().getPackageManager();
        return pm.hasSystemFeature(pm.FEATURE_WATCH);
    }

    private boolean isHandheld() {
        // handheld nature is not exposed to package manager, for now
        // we check for touchscreen and NOT watch and NOT tv
        PackageManager pm = getContext().getPackageManager();
        return pm.hasSystemFeature(pm.FEATURE_TOUCHSCREEN)
                && !pm.hasSystemFeature(pm.FEATURE_WATCH)
                && !pm.hasSystemFeature(pm.FEATURE_TELEVISION)
                && !pm.hasSystemFeature(pm.FEATURE_AUTOMOTIVE);
    }

    private boolean isAutomotive() {
        PackageManager pm = getContext().getPackageManager();
        return pm.hasSystemFeature(pm.FEATURE_AUTOMOTIVE);
    }

    private boolean isPC() {
        PackageManager pm = getContext().getPackageManager();
        return pm.hasSystemFeature(pm.FEATURE_PC);
    }

    // Find whether the given codec can be found using MediaCodecList.find methods.
    private boolean codecCanBeFound(boolean isEncoder, MediaFormat format) {
        String codecName = isEncoder
                ? mRegularCodecs.findEncoderForFormat(format)
                : mRegularCodecs.findDecoderForFormat(format);
        return codecName != null;
    }

    /*
     * Find whether all required media codec types are supported
     */
    private boolean areRequiredCodecTypesSupported(
        List<CodecType> requiredList, List<CodecType> supportedList) {
        for (CodecType requiredCodec: requiredList) {
            boolean isSupported = false;
            for (CodecType supportedCodec: supportedList) {
                if (requiredCodec.equals(supportedCodec)) {
                    isSupported = true;
                }
            }
            if (!isSupported) {
                String codec = requiredCodec.mMimeTypeName
                                + "", "" + (requiredCodec.mIsEncoder? ""encoder"": ""decoder"");
                Log.e(TAG, ""Media codec ("" + codec + "") is not supported"");
                return false;
            }
        }
        return true;
    }

    /*
     * Find all the media codec types are supported.
     */
    private List<CodecType> getSupportedCodecTypes() {
        List<CodecType> supportedList = new ArrayList<CodecType>();
        for (MediaCodecInfo info : mRegularInfos) {
            String[] types = info.getSupportedTypes();
            assertTrue(""Unexpected number of supported types"", types.length > 0);
            boolean isEncoder = info.isEncoder();
            for (int j = 0; j < types.length; ++j) {
                supportedList.add(new CodecType(types[j], isEncoder, null /* sampleFormat */));
            }
        }
        return supportedList;
    }

    /*
     * This list should be kept in sync with the CCD document
     * See http://developer.android.com/guide/appendix/media-formats.html
     */
    private List<CodecType> getRequiredCodecTypes() {
        List<CodecType> list = new ArrayList<CodecType>(16);

        // Mandatory audio decoders

        list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_FLAC, false, 48000));
        list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_MPEG, false, 8000));  // mp3
        list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_MPEG, false, 48000)); // mp3
        list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_VORBIS, false, 8000));
        list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_VORBIS, false, 48000));
        list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_AAC, false, 8000));
        list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_AAC, false, 48000));
        list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_RAW, false, 8000));
        list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_RAW, false, 44100));
        list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_OPUS, false, 48000));

        // Mandatory audio encoders (for non-watch devices with camera)

        if (hasMicrophone() && !isWatch()) {
            list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_AAC, true, 8000));
            list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_AAC, true, 48000));
            // flac encoder is not required
            // list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_FLAC, true));  // encoder
        }

        // Mandatory audio encoders for handheld devices
        if (isHandheld()) {
            list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_AMR_NB, false, 8000));  // decoder
            list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_AMR_NB, true,  8000));  // encoder
            list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_AMR_WB, false, 16000)); // decoder
            list.add(new AudioCodec(MediaFormat.MIMETYPE_AUDIO_AMR_WB, true,  16000)); // encoder
        }

        // Mandatory video codecs (for non-watch devices)

        if (!isWatch()) {
            list.add(new VideoCodec(MediaFormat.MIMETYPE_VIDEO_AVC, false));   // avc decoder
            list.add(new VideoCodec(MediaFormat.MIMETYPE_VIDEO_AVC, true));    // avc encoder
            list.add(new VideoCodec(MediaFormat.MIMETYPE_VIDEO_VP8, false));   // vp8 decoder
            list.add(new VideoCodec(MediaFormat.MIMETYPE_VIDEO_VP8, true));    // vp8 encoder
            list.add(new VideoCodec(MediaFormat.MIMETYPE_VIDEO_VP9, false));   // vp9 decoder

            // According to CDD, hevc decoding is not mandatory for automotive and PC devices.
            if (!isAutomotive() && !isPC()) {
                list.add(new VideoCodec(MediaFormat.MIMETYPE_VIDEO_HEVC, false));  // hevc decoder
            }
            list.add(new VideoCodec(MediaFormat.MIMETYPE_VIDEO_MPEG4, false)); // m4v decoder
            list.add(new VideoCodec(MediaFormat.MIMETYPE_VIDEO_H263, false));  // h263 decoder
            if (hasCamera()) {
                list.add(new VideoCodec(MediaFormat.MIMETYPE_VIDEO_H263, true)); // h263 encoder
            }
        }

        return list;
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaCodecListTest"	"testFindDecoderWithAacProfile"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaCodecListTest.java"	""	"public void testFindDecoderWithAacProfile() throws Exception {
        Log.d(TAG, ""testFindDecoderWithAacProfile"");
        MediaFormat format = MediaFormat.createAudioFormat(
                MediaFormat.MIMETYPE_AUDIO_AAC, 8000, 1);
        List<Integer> profiles = new ArrayList<>();
        profiles.add(MediaCodecInfo.CodecProfileLevel.AACObjectLC);
        profiles.add(MediaCodecInfo.CodecProfileLevel.AACObjectHE);
        profiles.add(MediaCodecInfo.CodecProfileLevel.AACObjectHE_PS);
        // The API is added at 5.0, so the profile below must be supported.
        profiles.add(MediaCodecInfo.CodecProfileLevel.AACObjectELD);
        for (int profile : profiles) {
            format.setInteger(MediaFormat.KEY_AAC_PROFILE, profile);
            String codecName = mRegularCodecs.findDecoderForFormat(format);
            assertNotNull(""Profile "" + profile + "" must be supported."", codecName);
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaCodecListTest"	"testFindEncoderWithAacProfile"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaCodecListTest.java"	""	"public void testFindEncoderWithAacProfile() throws Exception {
        Log.d(TAG, ""testFindEncoderWithAacProfile"");
        MediaFormat format = MediaFormat.createAudioFormat(
                MediaFormat.MIMETYPE_AUDIO_AAC, 8000, 1);
        List<Integer> profiles = new ArrayList<>();
        if (hasMicrophone() && !isWatch()) {
            profiles.add(MediaCodecInfo.CodecProfileLevel.AACObjectLC);
            // The API is added at 5.0, so the profiles below must be supported.
            profiles.add(MediaCodecInfo.CodecProfileLevel.AACObjectHE);
            profiles.add(MediaCodecInfo.CodecProfileLevel.AACObjectELD);
        }
        for (int profile : profiles) {
            format.setInteger(MediaFormat.KEY_AAC_PROFILE, profile);
            String codecName = mRegularCodecs.findEncoderForFormat(format);
            assertNotNull(""Profile "" + profile + "" must be supported."", codecName);
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaCodecListTest"	"testInputChannelLimits"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaCodecListTest.java"	""	"public void testInputChannelLimits() throws IOException {
        if (!MediaUtils.check(sIsAtLeastS, ""testInputChannelLimits invalid before Android 12"")) {
            return;
        }
        for (MediaCodecInfo info : mAllInfos) {
            boolean isEncoder = info.isEncoder();
            if (!isEncoder) {
                continue;
            }
            for (String mime: info.getSupportedTypes()) {
                CodecCapabilities caps = info.getCapabilitiesForType(mime);
                boolean isVideo = (caps.getVideoCapabilities() != null);

                if (isVideo) {
                    continue;
                }
                AudioCapabilities acaps = caps.getAudioCapabilities();

                int countMin = acaps.getMinInputChannelCount();
                int countMax = acaps.getMaxInputChannelCount();
                Range<Integer>[] countRanges = acaps.getInputChannelCountRanges();

                assertNotNull(""getInputChannelCountRanges() null, codec="" + info.getName(),
                                countRanges);
                assertTrue(""getInputChannelCountRanges() empty range codec="" + info.getName(),
                                countRanges.length > 0);

                assertEquals(""first range lower != min mismatch codec="" + info.getName(),
                                countMin, countRanges[0].getLower().intValue());
                assertEquals(""last range upper != max mismatch codec="" + info.getName(),
                                countMax, countRanges[countRanges.length-1].getUpper().intValue());

                int foundLow = Integer.MAX_VALUE;
                int foundHigh = Integer.MIN_VALUE;
                for (Range<Integer> oneRange: countRanges) {
                    int upper = oneRange.getUpper().intValue();
                    if (foundHigh < upper) {
                        foundHigh = upper;
                    }
                    int lower = oneRange.getLower().intValue();
                    if (foundLow > lower) {
                        foundLow = lower;
                    }
                    assertTrue(lower <= upper);
                }
                assertEquals(""minimum count mismatch codec="" + info.getName(),
                                countMin, foundLow);
                assertEquals(""maximum count mismatch codec="" + info.getName(),
                                countMax, foundHigh);
            }
        }
    }



    private void testCanonicalCodecIsNotAnAlias(String canonicalName) {
        // canonical name must point to a non-alias
        for (MediaCodecInfo canonical : mAllInfos) {
            if (canonical.getName().equals(canonicalName)) {
                assertFalse(canonical.isAlias());
                return;
            }
        }
        fail(""could not find info to canonical name '"" + canonicalName + ""'"");
    }

    private String getCustomPartOfComponentName(MediaCodecInfo info) {
        String name = info.getName();
        if (name.startsWith(""OMX."") || name.startsWith(""c2."")) {
            // strip off OMX.<vendor_name>.
            return name.replaceFirst(""^OMX\\.([^.]+)\\."", """");
        }
        return name;
    }

    private void testKindInCodecNamesIsMeaningful(MediaCodecInfo info) {
        String name = getCustomPartOfComponentName(info);
        // codec names containing 'encoder' or 'enc' must be encoders, 'decoder' or 'dec' must
        // be decoders
        if (name.matches(""(?i)\\b(encoder|enc)\\b"")) {
            assertTrue(info.isEncoder());
        }
        if (name.matches(""(?i)\\b(decoder|dec)\\b"")) {
            assertFalse(info.isEncoder());
        }
    }

    private void testMediaTypeInCodecNamesIsMeaningful(MediaCodecInfo info) {
        // Codec names containing media type names must support that media type
        String name = getCustomPartOfComponentName(info);

        Set<String> supportedTypes = new HashSet<String>(Arrays.asList(info.getSupportedTypes()));

        // video types
        if (name.matches(""(?i)\\b(mp(eg)?2)\\b"")) {
            // this may refer to audio mpeg1-layer2 or video mpeg2
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_VIDEO_MPEG2)
                        || supportedTypes.contains(MediaFormat.MIMETYPE_AUDIO_MPEG + ""-L2""));
        }
        if (name.matches(""(?i)\\b(h\\.?263)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_VIDEO_H263));
        }
        if (name.matches(""(?i)\\b(mp(eg)?4)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_VIDEO_MPEG4));
        }
        if (name.matches(""(?i)\\b(h\\.?264|avc)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_VIDEO_AVC));
        }
        if (name.matches(""(?i)\\b(vp8)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_VIDEO_VP8));
        }
        if (name.matches(""(?i)\\b(h\\.?265|hevc)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_VIDEO_HEVC));
        }
        if (name.matches(""(?i)\\b(vp9)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_VIDEO_VP9));
        }
        if (name.matches(""(?i)\\b(av0?1)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_VIDEO_AV1));
        }

        // audio types
        if (name.matches(""(?i)\\b(mp(eg)?3)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_AUDIO_MPEG));
        }
        if (name.matches(""(?i)\\b(x?aac)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_AUDIO_AAC));
        }
        if (name.matches(""(?i)\\b(pcm)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_AUDIO_RAW));
        }
        if (name.matches(""(?i)\\b(raw)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_AUDIO_RAW)
                        || supportedTypes.contains(MediaFormat.MIMETYPE_VIDEO_RAW));
        }
        if (name.matches(""(?i)\\b(amr)\\b"")) {
            if (name.matches(""(?i)\\b(nb)\\b"")) {
                assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_AUDIO_AMR_NB));
            } else if (name.matches(""(?i)\\b(wb)\\b"")) {
                assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_AUDIO_AMR_WB));
            } else {
                assertTrue(
                    supportedTypes.contains(MediaFormat.MIMETYPE_AUDIO_AMR_NB)
                            || supportedTypes.contains(MediaFormat.MIMETYPE_AUDIO_AMR_WB));
            }
        }
        if (name.matches(""(?i)\\b(opus)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_AUDIO_OPUS));
        }
        if (name.matches(""(?i)\\b(vorbis)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_AUDIO_VORBIS));
        }
        if (name.matches(""(?i)\\b(flac)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_AUDIO_FLAC));
        }
        if (name.matches(""(?i)\\b(ac3)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_AUDIO_AC3));
        }
        if (name.matches(""(?i)\\b(ac4)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_AUDIO_AC4));
        }
        if (name.matches(""(?i)\\b(eac3)\\b"")) {
            assertTrue(supportedTypes.contains(MediaFormat.MIMETYPE_AUDIO_EAC3)
                        || supportedTypes.contains(MediaFormat.MIMETYPE_AUDIO_EAC3_JOC));
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestXheAac"	"isAlias"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestXheAac.java"	""	"/*
 *.
 */

package android.media.cts;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertNotNull;
import static org.junit.Assert.assertTrue;
import static org.junit.Assert.fail;

import android.app.Instrumentation;
import android.content.res.AssetFileDescriptor;
import android.content.res.Resources;
import android.media.MediaCodec;
import android.media.MediaCodecInfo;
import android.media.MediaCodecList;
import android.media.MediaExtractor;
import android.media.MediaFormat;
import android.media.cts.DecoderTest.AudioParameter;
import android.media.cts.DecoderTestAacDrc.DrcParams;
import android.os.Build;
import android.os.Bundle;
import android.platform.test.annotations.AppModeFull;
import android.util.Log;

import androidx.test.InstrumentationRegistry;

import com.android.compatibility.common.util.ApiLevelUtil;
import com.android.compatibility.common.util.MediaUtils;

import org.junit.Before;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.junit.runners.JUnit4;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

@AppModeFull(reason = ""DecoderTest is non-instant"")
@RunWith(JUnit4.class)
public class DecoderTestXheAac {
    private static final String TAG = ""DecoderTestXheAac"";

    private static final boolean sIsAndroidRAndAbove =
            ApiLevelUtil.isAtLeast(Build.VERSION_CODES.R);

    private Resources mResources;

    // list of all AAC decoders as enumerated through the MediaCodecList
    // lazy initialization in setUp()
    private static ArrayList<String> sAacDecoderNames;
    private static String defaultAacDecoder = null;
    @Before
    public void setUp() throws Exception {
        final Instrumentation inst = InstrumentationRegistry.getInstrumentation();
        assertNotNull(inst);
        mResources = inst.getContext().getResources();
        // build a list of all AAC decoders on which to run the test
        if (sAacDecoderNames == null) {
            sAacDecoderNames = initAacDecoderNames();
        }
    }

    protected static ArrayList<String> initAacDecoderNames() throws IOException {
        ArrayList<String> aacDecoderNames = new ArrayList<String>(1);
        // Default aac decoder (the one that gets created when createDecoderByType with AAC mime
        // is called) is expected to pass all DRC tests
        if (defaultAacDecoder != null) {
            aacDecoderNames.add(defaultAacDecoder);
        } else {
            MediaCodec decoder = MediaCodec.createDecoderByType(MediaFormat.MIMETYPE_AUDIO_AAC);
            aacDecoderNames.add(decoder.getName());
            defaultAacDecoder = decoder.getName();
            decoder.release();
        }
        // Add all decoders that advertise support for AACObjectXHE profile as decoders that
        // support xHE-AAC profile are expected to support DRC
        MediaFormat format = MediaFormat.createAudioFormat(MediaFormat.MIMETYPE_AUDIO_AAC, 48000,
                2);
        // Set both KEY_AAC_PROFILE and KEY_PROFILE as some codecs may only recognize one of
        // these two keys
        format.setInteger(MediaFormat.KEY_AAC_PROFILE,
                MediaCodecInfo.CodecProfileLevel.AACObjectXHE);
        format.setInteger(MediaFormat.KEY_PROFILE, MediaCodecInfo.CodecProfileLevel.AACObjectXHE);

        final MediaCodecList mediaCodecList = new MediaCodecList(MediaCodecList.REGULAR_CODECS);
        final MediaCodecInfo[] mediaCodecInfos = mediaCodecList.getCodecInfos();
        for (MediaCodecInfo mediaCodecInfo : mediaCodecInfos) {
            if (mediaCodecInfo.isAlias()) {
                continue;
            }
            if (mediaCodecInfo.isEncoder()) {
                continue;
            }
            final String codecName = mediaCodecInfo.getName();
            final String[] mimeTypes = mediaCodecInfo.getSupportedTypes();
            for (String mimeType : mimeTypes) {
                if (MediaFormat.MIMETYPE_AUDIO_AAC.equalsIgnoreCase(mimeType)) {
                    MediaCodecInfo.CodecCapabilities caps = mediaCodecInfo.getCapabilitiesForType(
                            mimeType);
                    if (caps.isFormatSupported(format)) {
                        if (!aacDecoderNames.contains(codecName)) {
                            aacDecoderNames.add(codecName);
                        }
                    }
                    break;
                }
            }
        }
        return aacDecoderNames;
    }

    /**
     * Verify the correct decoding of USAC bitstreams with different MPEG-D DRC effect types.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestXheAac"	"testDecodeUsacDrcEffectTypeM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestXheAac.java"	""	"public void testDecodeUsacDrcEffectTypeM4a() throws Exception {
        Log.v(TAG, ""START testDecodeUsacDrcEffectTypeM4a"");

        assertTrue(""No AAC decoder found"", sAacDecoderNames.size() > 0);

        for (String aacDecName : sAacDecoderNames) {
            try {
                runDecodeUsacDrcEffectTypeM4a(aacDecName);
            } catch (Error err) {
                throw new Error(err.getMessage() + "" [dec="" + aacDecName + ""]"" , err);
            }
        }
    }

    private void runDecodeUsacDrcEffectTypeM4a(String aacDecName) throws Exception {
        Log.v(TAG, ""testDecodeUsacDrcEffectTypeM4a running for dec="" + aacDecName);
        // test DRC effectTypeID 1 ""NIGHT""
        // L -3dB -> normalization factor = 1/(10^(-3/10)) = 0.5011f
        // R +3dB -> normalization factor = 1/(10^( 3/10)) = 1.9952f
        try {
            checkUsacDrcEffectType(1, 0.5011f, 1.9952f, ""Night"", 2, 0, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcEffectTypeM4a Night/2/0 failed for dec="" + aacDecName);
            throw new RuntimeException(e);
        }

        // test DRC effectTypeID 2 ""NOISY""
        // L +3dB -> normalization factor = 1/(10^( 3/10)) = 1.9952f
        // R -6dB -> normalization factor = 1/(10^(-6/10)) = 0.2511f
        try {
            checkUsacDrcEffectType(2, 1.9952f, 0.2511f, ""Noisy"", 2, 0, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcEffectTypeM4a Noisy/2/0 failed for dec="" + aacDecName);
            throw new RuntimeException(e);
        }

        // test DRC effectTypeID 3 ""LIMITED""
        // L -6dB -> normalization factor = 1/(10^(-6/10)) = 0.2511f
        // R +6dB -> normalization factor = 1/(10^( 6/10)) = 3.9810f
        try {
            checkUsacDrcEffectType(3, 0.2511f, 3.9810f, ""Limited"", 2, 0, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcEffectTypeM4a Limited/2/0 failed for dec=""
                    + aacDecName);
            throw new RuntimeException(e);
        }

        // test DRC effectTypeID 6 ""GENERAL""
        // L +6dB -> normalization factor = 1/(10^( 6/10)) = 3.9810f
        // R -3dB -> normalization factor = 1/(10^(-3/10)) = 0.5011f
        try {
            checkUsacDrcEffectType(6, 3.9810f, 0.5011f, ""General"", 2, 0, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcEffectTypeM4a General/2/0 failed for dec=""
                    + aacDecName);
            throw new RuntimeException(e);
        }

        // test DRC effectTypeID 1 ""NIGHT""
        // L    -6dB -> normalization factor = 1/(10^(-6/10)) = 0.2511f
        // R    +6dB -> normalization factor = 1/(10^( 6/10)) = 3.9810f
        // mono -6dB -> normalization factor = 1/(10^(-6/10)) = 0.2511f
        try {
            checkUsacDrcEffectType(1, 0.2511f, 3.9810f, ""Night"", 2, 1, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcEffectTypeM4a Night/2/1 for dec="" + aacDecName);
            throw new RuntimeException(e);
        }
        try {
            checkUsacDrcEffectType(1, 0.2511f, 0.0f, ""Night"", 1, 1, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcEffectTypeM4a Night/1/1 for dec="" + aacDecName);
            throw new RuntimeException(e);
        }

        // test DRC effectTypeID 2 ""NOISY""
        // L    +6dB -> normalization factor = 1/(10^( 6/10))   = 3.9810f
        // R    -9dB -> normalization factor = 1/(10^(-9/10))  = 0.1258f
        // mono +6dB -> normalization factor = 1/(10^( 6/10))   = 3.9810f
        try {
            checkUsacDrcEffectType(2, 3.9810f, 0.1258f, ""Noisy"", 2, 1, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcEffectTypeM4a Noisy/2/1 for dec="" + aacDecName);
            throw new RuntimeException(e);
        }
        try {
            checkUsacDrcEffectType(2, 3.9810f, 0.0f, ""Noisy"", 1, 1, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcEffectTypeM4a Night/2/1 for dec="" + aacDecName);
            throw new RuntimeException(e);
        }

        // test DRC effectTypeID 3 ""LIMITED""
        // L    -9dB -> normalization factor = 1/(10^(-9/10)) = 0.1258f
        // R    +9dB -> normalization factor = 1/(10^( 9/10)) = 7.9432f
        // mono -9dB -> normalization factor = 1/(10^(-9/10)) = 0.1258f
        try {
            checkUsacDrcEffectType(3, 0.1258f, 7.9432f, ""Limited"", 2, 1, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcEffectTypeM4a Limited/2/1 for dec="" + aacDecName);
            throw new RuntimeException(e);
        }
        try {
            checkUsacDrcEffectType(3, 0.1258f, 0.0f, ""Limited"", 1, 1, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcEffectTypeM4a Limited/1/1 for dec="" + aacDecName);
            throw new RuntimeException(e);
        }

        // test DRC effectTypeID 6 ""GENERAL""
        // L    +9dB -> normalization factor = 1/(10^( 9/10)) = 7.9432f
        // R    -6dB -> normalization factor = 1/(10^(-6/10))  = 0.2511f
        // mono +9dB -> normalization factor = 1/(10^( 9/10)) = 7.9432f
        try {
            checkUsacDrcEffectType(6, 7.9432f, 0.2511f, ""General"", 2, 1, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcEffectTypeM4a General/2/1 for dec="" + aacDecName);
            throw new RuntimeException(e);
        }
        try {
            checkUsacDrcEffectType(6, 7.9432f, 0.0f, ""General"", 1, 1, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcEffectTypeM4a General/1/1 for dec="" + aacDecName);
            throw new RuntimeException(e);
        }
    }

    /**
     * Verify the correct decoding of USAC bitstreams with album mode.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestXheAac"	"testDecodeUsacDrcAlbumModeM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestXheAac.java"	""	"public void testDecodeUsacDrcAlbumModeM4a() throws Exception {
        Log.v(TAG, ""START testDecodeUsacDrcAlbumModeM4a"");

        // Album mode is R feature
        if (!MediaUtils.check(sIsAndroidRAndAbove, ""Album mode support requires Android R""))
                return;

        assertTrue(""No AAC decoder found"", sAacDecoderNames.size() > 0);

        for (String aacDecName : sAacDecoderNames) {
            try {
                runDecodeUsacDrcAlbumModeM4a(aacDecName);
            } catch (Error err) {
                throw new Error(err.getMessage() + "" [dec="" + aacDecName + ""]"" , err);
            }
        }
    }

    private void runDecodeUsacDrcAlbumModeM4a(String aacDecName) throws Exception {
        // test DRC Album Mode
        // Track loudness = -19dB
        // Album Loudness = -21 dB
        // Fading Gains = -6 dB
        // Album Mode ON : Gains = -24 - (-21) = -3dB
        // Album Mode OFF : Gains = (-24 -(-19)) + (-6) = -11 dB
        try {
            checkUsacDrcAlbumMode(R.raw.noise_2ch_48khz_tlou_19lufs_alou_21lufs_mp4, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcAlbumModeM4a for decoder"" + aacDecName);
            throw new RuntimeException(e);
        }
    }

    /**
     * Verify the correct decoding of USAC bitstreams with config changes.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestXheAac"	"testDecodeUsacStreamSwitchingM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestXheAac.java"	""	"public void testDecodeUsacStreamSwitchingM4a() throws Exception {
        Log.v(TAG, ""START testDecodeUsacStreamSwitchingM4a"");

        assertTrue(""No AAC decoder found"", sAacDecoderNames.size() > 0);

        for (String aacDecName : sAacDecoderNames) {
            try {
                runDecodeUsacStreamSwitchingM4a(aacDecName);
            } catch (Error err) {
                throw new Error(err.getMessage() + "" [dec="" + aacDecName + ""]"" , err);
            }
        }
    }

    private void runDecodeUsacStreamSwitchingM4a(String aacDecName) throws Exception {
        // Stereo
        // switch between SBR ratios and stereo modes
        try {
            checkUsacStreamSwitching(2.5459829E12f, 2,
                    R.raw.noise_2ch_44_1khz_aot42_19_lufs_config_change_mp4, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacStreamSwitchingM4a failed 2ch sbr/stereo switch for ""
                    + aacDecName);
            throw new RuntimeException(e);
        }

        // Mono
        // switch between SBR ratios and stereo modes
        try {
            checkUsacStreamSwitching(2.24669126E12f, 1,
                    R.raw.noise_1ch_38_4khz_aot42_19_lufs_config_change_mp4, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacStreamSwitchingM4a failed 1ch sbr/stereo switch for ""
                    + aacDecName);
            throw new RuntimeException(e);
        }

        // Stereo
        // switch between USAC modes
        try {
            checkUsacStreamSwitching(2.1E12f, 2,
                    R.raw.noise_2ch_35_28khz_aot42_19_lufs_drc_config_change_mp4, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacStreamSwitchingM4a failed 2ch USAC mode switch for ""
                    + aacDecName);
            throw new RuntimeException(e);
        }

        // Mono
        // switch between USAC modes
        try {
            checkUsacStreamSwitching(1.7E12f, 1,
                    R.raw.noise_1ch_29_4khz_aot42_19_lufs_drc_config_change_mp4, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacStreamSwitchingM4a failed 1ch USAC mode switch for ""
                    + aacDecName);
            throw new RuntimeException(e);
        }

    }

    /**
     * Verify the correct decoding of USAC bitstreams with various sampling rates.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestXheAac"	"testDecodeUsacSamplingRatesM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestXheAac.java"	""	"public void testDecodeUsacSamplingRatesM4a() throws Exception {
        Log.v(TAG, ""START testDecodeUsacSamplingRatesM4a"");

        assertTrue(""No AAC decoder found"", sAacDecoderNames.size() > 0);

        for (String aacDecName : sAacDecoderNames) {
            try {
                runDecodeUsacSamplingRatesM4a(aacDecName);
            } catch (Error err) {
                throw new Error(err.getMessage() + "" [dec="" + aacDecName + ""]"" , err);
            }
        }
    }

    private void runDecodeUsacSamplingRatesM4a(String aacDecName) throws Exception {
        try {
            checkUsacSamplingRate(R.raw.noise_2ch_08khz_aot42_19_lufs_mp4, aacDecName);
            checkUsacSamplingRate(R.raw.noise_2ch_12khz_aot42_19_lufs_mp4, aacDecName);
            checkUsacSamplingRate(R.raw.noise_2ch_22_05khz_aot42_19_lufs_mp4, aacDecName);
            checkUsacSamplingRate(R.raw.noise_2ch_64khz_aot42_19_lufs_mp4, aacDecName);
            checkUsacSamplingRate(R.raw.noise_2ch_88_2khz_aot42_19_lufs_mp4, aacDecName);
            checkUsacSamplingRateWoLoudness(R.raw.noise_2ch_19_2khz_aot42_no_ludt_mp4,
                    aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacSamplingRatesM4a for decoder"" + aacDecName);
            throw new RuntimeException(e);
        }
    }

    /**
     * Verify the correct decoding of USAC bitstreams with different boost and attenuation settings
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestXheAac"	"testDecodeUsacDrcBoostAndAttenuationM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestXheAac.java"	""	"public void testDecodeUsacDrcBoostAndAttenuationM4a() throws Exception {
        Log.v(TAG, ""START testDecodeUsacDrcBoostAndAttenuationM4a"");

        if (!MediaUtils.check(sIsAndroidRAndAbove, ""Att/Boost corrected in Android R""))
            return;

        assertTrue(""No AAC decoder found"", sAacDecoderNames.size() > 0);

        for (String aacDecName : sAacDecoderNames) {
            try {
                runDecodeUsacDrcBoostAndAttenuationM4a(aacDecName);
            } catch (Error err) {
                throw new Error(err.getMessage() + "" [dec="" + aacDecName + ""]"" , err);
            }
        }
    }

    private void runDecodeUsacDrcBoostAndAttenuationM4a(String aacDecName) throws Exception {
        Log.v(TAG, ""testDecodeUsacDrcBoostAndAttenuationM4a running for dec="" + aacDecName);
        // test drcBoost and drcAttenuation parameters
        // DRC effectTypeID 6 ""GENERAL""
        // L +6dB -> normalization factor = 10^(6/10 * (1 - boostFactor:64/127)) = 1.9844f
        // R -3dB -> normalization factor = 10^(-3/10 * (1 - attenuationFactor:127/127)) = 1.0f
        try {
            checkUsacDrcBoostAndAttenuation(1.9844f, 1.0f, 64, 127, 2, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcBoostAndAttenuationM4a failed for dec="" + aacDecName);
            throw new RuntimeException(e);
        }

        // test drcBoost and drcAttenuation parameters
        // DRC effectTypeID 6 ""GENERAL""
        // L +6dB -> normalization factor = 10^(6/10 * (1 - boostFactor:127/127)) = 1.0f
        // R -3dB -> normalization factor = 10^(-3/10 * (1 - attenuationFactor:64/127)) = 0.7099f
        try {
            checkUsacDrcBoostAndAttenuation(1.0f, 0.7099f, 127, 64, 2, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcBoostAndAttenuationM4a failed for dec="" + aacDecName);
            throw new RuntimeException(e);
        }

        // test drcBoost and drcAttenuation parameters
        // DRC effectTypeID 6 ""GENERAL""
        // L +6dB -> normalization factor = 10^(6/10 * (1 - boostFactor:0/127)) = 3.9811f
        // R -3dB -> normalization factor = 10^(-3/10 * (1 - attenuationFactor:127/127)) = 1.0f
        try {
            checkUsacDrcBoostAndAttenuation(3.9811f, 1.0f, 0, 127, 2, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcBoostAndAttenuationM4a failed for dec="" + aacDecName);
            throw new RuntimeException(e);
        }

        // test drcBoost and drcAttenuation parameters
        // DRC effectTypeID 6 ""GENERAL""
        // L +6dB -> normalization factor = 10^(6/10 * (1 - boostFactor:127/127)) = 1.0f
        // R -3dB -> normalization factor = 10^(-3/10 * (1 - attenuationFactor:0/127)) = 0.5012f
        try {
            checkUsacDrcBoostAndAttenuation(1.0f, 0.5012f, 127, 0, 2, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcBoostAndAttenuationM4a failed for dec="" + aacDecName);
            throw new RuntimeException(e);
        }
    }

    /**
     * verify the correct decoding of USAC bitstreams when different kinds of loudness values
     * are present
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestXheAac"	"testDecodeUsacDrcLoudnessPreferenceM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestXheAac.java"	""	"public void testDecodeUsacDrcLoudnessPreferenceM4a() throws Exception {
        Log.v(TAG, ""START testDecodeUsacDrcLoudnessPreferenceM4a"");

        if (!MediaUtils.check(sIsAndroidRAndAbove, ""Loudness preference in Android R""))
            return;

        assertTrue(""No AAC decoder found"", sAacDecoderNames.size() > 0);

        for (String aacDecName : sAacDecoderNames) {
            try {
                runDecodeUsacDrcLoudnessPreferenceM4a(aacDecName);
            } catch (Error err) {
                throw new Error(err.getMessage() + "" [dec="" + aacDecName + ""]"" , err);
            }
        }
    }

    private void runDecodeUsacDrcLoudnessPreferenceM4a(String aacDecName) throws Exception {
        Log.v(TAG, ""testDecodeUsacDrcLoudnessPreferenceM4a running for dec="" + aacDecName);
        // test drc loudness preference
        // anchor loudness (-17 LUFS) and program loudness (-19 LUFS) are present in one stream
        // -> anchor loudness should be selected
        // the bitstream is decoded with targetLoudnessLevel = -16 LUFS and
        // checked against the energy of the decoded signal without loudness normalization
        // normfactor = loudness of waveform - targetLoudnessLevel = -1dB = 0.7943
        try {
            checkUsacDrcLoudnessPreference(
                    R.raw.noise_2ch_48khz_tlou_19lufs_anchor_17lufs_mp4, 0.7943f, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcLoudnessPreferenceM4a failed for dec="" + aacDecName);
            throw new RuntimeException(e);
        }

        // test drc loudness preference
        // expert loudness (-23 LUFS) and program loudness (-19 LUFS) are present in one stream
        // -> expert loudness should be selected
        // the bitstream is decoded with targetLoudnessLevel = -16 LUFS and
        // checked against the energy of the decoded signal without loudness normalization
        // normfactor = loudness of waveform - targetLoudnessLevel = -7dB = 0.1995
        try {
            checkUsacDrcLoudnessPreference(
                    R.raw.noise_2ch_48khz_tlou_19lufs_expert_23lufs_mp4, 0.1995f, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcLoudnessPreferenceM4a failed for dec="" + aacDecName);
            throw new RuntimeException(e);
        }
    }

    /**
     * Verify that the correct output loudness values are returned when decoding USAC bitstreams
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestXheAac"	"testDecodeUsacDrcOutputLoudnessM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestXheAac.java"	""	"public void testDecodeUsacDrcOutputLoudnessM4a() throws Exception {
        Log.v(TAG, ""START testDecodeUsacDrcOutputLoudnessM4a"");

        assertTrue(""No AAC decoder found"", sAacDecoderNames.size() > 0);

        for (String aacDecName : sAacDecoderNames) {
            try {
                runDecodeUsacDrcOutputLoudnessM4a(aacDecName);
            } catch (Error err) {
                throw new Error(err.getMessage() + "" [dec="" + aacDecName + ""]"" , err);
            }
        }
    }

    private void runDecodeUsacDrcOutputLoudnessM4a(String aacDecName) throws Exception {
        Log.v(TAG, ""testDecodeUsacDrcOutputLoudnessM4a running for dec="" + aacDecName);
        // test drc output loudness
        // testfile without loudness metadata and loudness normalization off -> expected value: -1
        try {
            checkUsacDrcOutputLoudness(
                    R.raw.noise_2ch_19_2khz_aot42_no_ludt_mp4, -1, -1, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcOutputLoudnessM4a failed for dec="" + aacDecName);
            throw new RuntimeException(e);
        }

        Log.v(TAG, ""testDecodeUsacDrcOutputLoudnessM4a running for dec="" + aacDecName);
        // test drc output loudness
        // testfile without loudness metadata and loudness normalization on
        // -> expected value: -1
        try {
            checkUsacDrcOutputLoudness(
                    R.raw.noise_2ch_19_2khz_aot42_no_ludt_mp4, 64, -1, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcOutputLoudnessM4a failed for dec="" + aacDecName);
            throw new RuntimeException(e);
        }

        // test drc output loudness
        // testfile with MPEG-D DRC loudness metadata and loudness normalization off
        // -> expected value: loudness metadata in bitstream (-19*-4 = 76)
        try {
            checkUsacDrcOutputLoudness(
                    R.raw.noise_2ch_08khz_aot42_19_lufs_mp4, -1, 76, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcOutputLoudnessM4a failed for dec="" + aacDecName);
            throw new RuntimeException(e);
        }

        // test drc output loudness
        // testfile with MPEG-D DRC loudness metadata and loudness normalization off
        // -> expected value: loudness metadata in bitstream (-22*-4 = 88)
        try {
            checkUsacDrcOutputLoudness(
                    R.raw.noise_1ch_38_4khz_aot42_19_lufs_config_change_mp4, -1, 88, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcOutputLoudnessM4a failed for dec="" + aacDecName);
            throw new RuntimeException(e);
        }

        // test drc output loudness
        // testfile with MPEG-D DRC loudness metadata and loudness normalization on
        // -> expected value: target loudness value (92)
        try {
            checkUsacDrcOutputLoudness(
                    R.raw.noise_2ch_08khz_aot42_19_lufs_mp4, 92, 92, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacDrcOutputLoudnessM4a failed for dec="" + aacDecName);
            throw new RuntimeException(e);
        }
    }


    /**
     * Verify that seeking works correctly for USAC.
     * Sync samples have to be taken into consideration.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestXheAac"	"testDecodeUsacSyncSampleSeekingM4a"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestXheAac.java"	""	"public void testDecodeUsacSyncSampleSeekingM4a() throws Exception {
        Log.v(TAG, ""START testDecodeUsacSyncSampleSeekingM4a"");
        if(!sIsAndroidRAndAbove) {
            // The fix for b/158471477 was released in mainline release 300802800
            // See https://android-build.googleplex.com/builds/treetop/googleplex-android-review/11990700
            final int MIN_VERSION = 300802800;
            TestUtils.assumeMainlineModuleAtLeast(""com.google.android.media.swcodec"", MIN_VERSION);
            TestUtils.assumeMainlineModuleAtLeast(""com.google.android.media"", MIN_VERSION);
        }

        assertTrue(""No AAC decoder found"", sAacDecoderNames.size() > 0);

        for (String aacDecName : sAacDecoderNames) {
            try {
                runDecodeUsacSyncSampleSeekingM4a(aacDecName);
            } catch (Error err) {
                throw new Error(err.getMessage() + "" [dec="" + aacDecName + ""]"" , err);
            }
        }
    }

    private void runDecodeUsacSyncSampleSeekingM4a(String aacDecName) throws Exception {
        Log.v(TAG, ""testDecodeUsacSyncSampleSeekingM4a running for dec="" + aacDecName);
        // test usac seeking
        try {
            checkUsacSyncSampleSeeking(R.raw.sine_2ch_48khz_aot42_seek_mp4, aacDecName);
        } catch (Exception e) {
            Log.v(TAG, ""testDecodeUsacSyncSampleSeekingM4a failed for dec="" + aacDecName);
            throw new RuntimeException(e);
        }
        Log.v(TAG, ""testDecodeUsacSyncSampleSeekingM4a running for dec="" + aacDecName);
    }

    /**
     *  Internal utilities
     */

    /**
     * USAC test DRC Effect Type
     */
    private void checkUsacDrcEffectType(int effectTypeID, float normFactor_L, float normFactor_R,
                 String effectTypeName, int nCh, int aggressiveDrc, String decoderName)
                         throws Exception {
        for (boolean runtimeChange : new boolean[] {false, true}) {
            if (runtimeChange && !sIsAndroidRAndAbove) {
                // changing decoder configuration after it has been initialized requires R and above
                continue;
            }
            int testinput = -1;
            AudioParameter decParams = new AudioParameter();
            DrcParams drcParams_def  = new DrcParams(127, 127, 96, 0, -1);
            DrcParams drcParams_test = new DrcParams(127, 127, 96, 0, effectTypeID);

            if (aggressiveDrc == 0) {
                testinput = R.raw.noise_2ch_32khz_aot42_19_lufs_drc_mp4;
            } else {
                if (nCh == 2) {
                    testinput = R.raw.noise_2ch_35_28khz_aot42_19_lufs_drc_config_change_mp4;
                } else if (nCh == 1){
                    testinput = R.raw.noise_1ch_29_4khz_aot42_19_lufs_drc_config_change_mp4;
                }
            }

            short[] decSamples_def  = decodeToMemory(decParams, testinput,
                    -1, null, drcParams_def, decoderName);
            short[] decSamples_test = decodeToMemory(decParams, testinput,
                    -1, null, drcParams_test, decoderName, runtimeChange);

            float[] nrg_def  = checkEnergyUSAC(decSamples_def, decParams, nCh, 1, 0);
            float[] nrg_test = checkEnergyUSAC(decSamples_test, decParams, nCh, 1, 1);

            if (nCh == 2) {
                float nrgRatio_L = (nrg_test[1]/nrg_def[1])/normFactor_L;
                float nrgRatio_R = (nrg_test[2]/nrg_def[2])/normFactor_R;
                if ((nrgRatio_R > 1.05f || nrgRatio_R < 0.95f)
                        || (nrgRatio_L > 1.05f || nrgRatio_L < 0.95f) ){
                    throw new Exception(""DRC Effect Type '"" + effectTypeName + ""' not as expected"");
                }
            } else if (nCh == 1){
                float nrgRatio_L = (nrg_test[0]/nrg_def[0])/normFactor_L;
                if (nrgRatio_L > 1.05f || nrgRatio_L < 0.95f){
                    throw new Exception(""DRC Effect Type '"" + effectTypeName + ""' not as expected"");
                }
            }
        }
    }

    /**
     * USAC test stream switching
     */
    private void checkUsacStreamSwitching(float nrg_ref, int encNch, int testinput,
            String decoderName) throws Exception
    {
        AudioParameter decParams = new AudioParameter();
        DrcParams drcParams      = new DrcParams(127, 127, 64, 0, -1);

        // Check stereo stream switching
        short[] decSamples = decodeToMemory(decParams, testinput,
                -1, null, drcParams, decoderName);
        float[] nrg = checkEnergyUSAC(decSamples, decParams, encNch, 1);

        float nrgRatio = nrg[0] / nrg_ref;

        // Check if energy levels are within 15% of the reference
        // Energy drops within the decoded stream are checked by checkEnergyUSAC() within every
        // 250ms interval
        if (nrgRatio > 1.15f || nrgRatio < 0.85f ) {
            throw new Exception(""Config switching not as expected"");
        }
    }

    /**
     * USAC test sampling rate
     */
    private void checkUsacSamplingRate(int testinput, String decoderName) throws Exception {
        AudioParameter decParams  = new AudioParameter();
        DrcParams drcParams_def   = new DrcParams(127, 127, 64, 0, -1);
        DrcParams drcParams_test  = new DrcParams(127, 127, 96, 0, -1);

        short[] decSamples_def  = decodeToMemory(decParams, testinput,
                -1, null, drcParams_def, decoderName);
        short[] decSamples_test = decodeToMemory(decParams, testinput,
                -1, null, drcParams_test, decoderName);

        float[] nrg_def  = checkEnergyUSAC(decSamples_def, decParams, 2, 1);
        float[] nrg_test = checkEnergyUSAC(decSamples_test, decParams, 2, 1);

        float nrgRatio = nrg_def[0]/nrg_test[0];

        // normFactor = 1/(10^(-8/10)) = 6.3f
        nrgRatio = nrgRatio / 6.3f;

        // Check whether behavior is as expected
        if (nrgRatio > 1.05f || nrgRatio < 0.95f ){
            throw new Exception(""Sampling rate not supported"");
        }
    }

    /**
     * USAC test sampling rate for streams without loudness application
     */
    private void checkUsacSamplingRateWoLoudness(int testinput, String decoderName) throws Exception
    {
        AudioParameter decParams  = new AudioParameter();
        DrcParams drcParams       = new DrcParams();

        short[] decSamples = decodeToMemory(decParams, testinput, -1, null, drcParams, decoderName);

        float[] nrg = checkEnergyUSAC(decSamples, decParams, 2, 1);

        float nrg_ref  = 3.15766394E12f;
        float nrgRatio = nrg_ref/nrg[0];

        // Check whether behavior is as expected
        if (nrgRatio > 1.05f || nrgRatio < 0.95f ){
            throw new Exception(""Sampling rate not supported"");
        }
    }

    /**
     * USAC test DRC Album Mode
     */
    private void checkUsacDrcAlbumMode(int testinput, String decoderName) throws Exception {
        for (boolean runtimeChange : new boolean[] {false, true}) {
            AudioParameter decParams = new AudioParameter();
            DrcParams drcParams_album_off = new DrcParams(127, 127, 64, 0, 0, 0);
            DrcParams drcParams_album_on  = new DrcParams(127, 127, 64, 0, 0, 1);

            short[] decSamples_album_off = decodeToMemory(
                    decParams, testinput, -1, null, drcParams_album_off, decoderName);
            short[] decSamples_album_on = decodeToMemory(
                    decParams, testinput, -1, null, drcParams_album_on, decoderName, runtimeChange);

            float[] nrg_album_off  = checkEnergyUSAC(decSamples_album_off, decParams, 2, 1);
            float[] nrg_album_on = checkEnergyUSAC(decSamples_album_on, decParams, 2, 1);

            float normFactor = 6.3095f;

            float nrgRatio = (nrg_album_on[0]/nrg_album_off[0])/normFactor;
            float nrgRatio_L = (nrg_album_on[1]/nrg_album_off[1])/normFactor;
            float nrgRatio_R = (nrg_album_on[2]/nrg_album_off[2])/normFactor;

            if (nrgRatio > 1.05f || nrgRatio < 0.95f ){
                throw new Exception(""DRC Album Mode not supported, energy ratio "" + nrgRatio);
            }
        }
    }

    /**
     * USAC test DRC Boost and Attenuation
     */
    private void checkUsacDrcBoostAndAttenuation(float normFactor_L, float normFactor_R,
                                                 int boostFactor, int attenuationFactor,
                                                 int nCh, String decoderName) throws Exception {
        for (boolean runtimeChange : new boolean[] {false, true}) {
            if (runtimeChange && !sIsAndroidRAndAbove) {
                // changing decoder configuration after it has been initialized requires R and above
                continue;
            }

            int testinput = R.raw.noise_2ch_32khz_aot42_19_lufs_drc_mp4;

            AudioParameter decParams = new AudioParameter();
            DrcParams drcParams_def = new DrcParams(127, 127, 64, 0, 6);
            DrcParams drcParams_test = new DrcParams(boostFactor, attenuationFactor, 64, 0, 6);

            short[] decSamples_def = decodeToMemory(decParams, testinput, -1, null,
                    drcParams_def, decoderName);
            short[] decSamples_test = decodeToMemory(decParams, testinput, -1, null,
                    drcParams_test, decoderName, runtimeChange);

            float[] nrg_def = checkEnergyUSAC(decSamples_def, decParams, 2, 1);
            float[] nrg_test = checkEnergyUSAC(decSamples_test, decParams, 2, 1);

            float nrgRatioLeft = nrg_test[1] / nrg_def[1];
            float nrgRatioRight = nrg_test[2] / nrg_def[2];

            float testValueLeft = normFactor_L * nrgRatioLeft;
            float testValueRight = normFactor_R * nrgRatioRight;

            // Check whether loudness behavior is as expected
            if (testValueLeft > 1.05f || testValueLeft < 0.95f) {
                throw new Exception(""DRC boost/attenuation behavior not as expected"");
            }
            if (testValueRight > 1.05f || testValueRight < 0.95f) {
                throw new Exception(""DRC boost/attenuation behavior not as expected"");
            }
        }
    }

    /**
    * USAC test Loudness Preference
    */
    private void checkUsacDrcLoudnessPreference(int testInput, float normFactor, String decoderName) throws Exception {

        AudioParameter decParams = new AudioParameter();
        DrcParams drcParams_def = new DrcParams(127, 127, -1, 0, 6);
        DrcParams drcParams_test = new DrcParams(127, 127, 64, 0, 6);

        // Check drc loudness preference
        short[] decSamples_def = decodeToMemory(decParams, testInput, -1, null, drcParams_def, decoderName);
        short[] decSamples_test = decodeToMemory(decParams, testInput, -1, null, drcParams_test, decoderName);

        float[] nrg_def  = checkEnergyUSAC(decSamples_def, decParams, 2, 1);
        float[] nrg_test = checkEnergyUSAC(decSamples_test, decParams, 2, 1);

        float nrgRatio = (nrg_test[0]/nrg_def[0]);
        nrgRatio = nrgRatio * normFactor;

        if (nrgRatio > 1.05f || nrgRatio < 0.95f ){
            throw new Exception(""DRC Loudness preference not as expected"");
        }
    }

    /**
    * USAC test Output Loudness
    */
    private void checkUsacDrcOutputLoudness(int testInput, int decoderTargetLevel,
            int expectedOutputLoudness, String decoderName) throws Exception {
        for (boolean runtimeChange : new boolean[] {false, true}) {
            AudioParameter decParams = new AudioParameter();
            DrcParams drcParams_test = new DrcParams(127, 127, decoderTargetLevel, 0, 6);

            // Check drc loudness preference
            short[] decSamples_test = decodeToMemory(
                    decParams, testInput, -1, null, drcParams_test,
                    decoderName, runtimeChange, expectedOutputLoudness);
        }
    }

    private void checkUsacSyncSampleSeeking(int testInput, String decoderName) throws Exception {

        AudioParameter decParams = new AudioParameter();
        DrcParams drcParams_def = new DrcParams();

        short[] decSamples_seek_next_sync = decodeToMemory(decParams, testInput, -1, null,
                drcParams_def, decoderName, false, -2, true, 1100000,
                MediaExtractor.SEEK_TO_NEXT_SYNC);
        float[] nrg_seek_next_sync = checkEnergyUSAC(decSamples_seek_next_sync, decParams, 2, 1);
    }

    /**
     * Perform a segmented energy analysis on given audio signal samples and run several tests on
     * the energy values.
     *
     * The main purpose is to verify whether a USAC decoder implementation applies Spectral Band
     * Replication (SBR), Parametric Stereo (PS) and Dynamic Range Control (DRC) correctly. All
     * tools are inherent parts to either the MPEG-D USAC audio codec or the MPEG-D DRC tool.
     *
     * In addition, this test can verify the correct decoding of multi-channel (e.g. 5.1 channel)
     * streams or the creation of a downmixed signal.
     *
     * Note: This test procedure is not an MPEG Conformance Test and can not serve as a replacement.
     *
     * @param decSamples the decoded audio samples to be tested
     * @param decParams the audio parameters of the given audio samples (decSamples)
     * @param encNch the encoded number of audio channels (number of channels of the original
     *               input)
     * @param drcContext indicate to use test criteria applicable for DRC testing
     * @return array of energies, at index 0: accumulated energy of all channels, and
     *     index 1 and over contain the individual channel energies
     * @throws RuntimeException
     */
    protected float[] checkEnergyUSAC(short[] decSamples, AudioParameter decParams,
                                      int encNch, int drcContext)
    {
        final float[] nrg = checkEnergyUSAC(decSamples, decParams, encNch, drcContext,  0);
        return nrg;
    }

    /**
     * Same as {@link #checkEnergyUSAC(short[], AudioParameter, int, int)} but with DRC effect type
     * @param decSamples
     * @param decParams
     * @param encNch
     * @param drcContext
     * @param drcApplied indicate if MPEG-D DRC Effect Type has been applied
     * @return
     * @throws RuntimeException
     */
    private float[] checkEnergyUSAC(short[] decSamples, AudioParameter decParams,
                                    int encNch, int drcContext,  int drcApplied)
            throws RuntimeException
    {
        String localTag = TAG + ""#checkEnergyUSAC"";

        // the number of segments per block
        final int nSegPerBlk = 4;
        // the number of input channels
        final int nCh = encNch;
        // length of one (LB/HB) block [samples]
        final int nBlkSmp = decParams.getSamplingRate();
        // length of one segment [samples]
        final int nSegSmp = nBlkSmp / nSegPerBlk;
        // actual # samples per channel (total)
        final int smplPerChan = decSamples.length / nCh;
        // actual # samples per segment (all ch)
        final int nSegSmpTot = nSegSmp * nCh;
        // signal offset between chans [segments]
        final int nSegChOffst = 2 * nSegPerBlk;
        // // the number of channels to be analyzed
        final int procNch = Math.min(nCh, encNch);
        // all original configs with more than five channel have an LFE
        final int encEffNch = (encNch > 5) ? encNch-1 : encNch;
        // expected number of decoded audio samples
        final int expSmplPerChan = Math.max(encEffNch, 2) * nSegChOffst * nSegSmp;
        // flag telling that input is dmx signal
        final boolean isDmx = nCh < encNch;
        final float nrgRatioThresh = 0.0f;
        // the num analyzed channels with signal
        int effProcNch = procNch;

        // get the signal offset by counting zero samples at the very beginning (over all channels)
        // sample value threshold 4 signal search
        final int zeroSigThresh = 1;
        // receives the number of samples that are in front of the actual signal
        int signalStart = smplPerChan;
        // receives the number of null samples (per chan) at the very beginning
        int noiseStart = signalStart;

        for (int smpl = 0; smpl < decSamples.length; smpl++) {
            int value = Math.abs(decSamples[smpl]);

            if (value > 0 && noiseStart == signalStart) {
                // store start of prepended noise (can be same as signalStart)
                noiseStart = smpl / nCh;
            }

            if (value > zeroSigThresh) {
                // store signal start offset [samples]
                signalStart = smpl / nCh;
                break;
            }
        }

        signalStart = (signalStart > noiseStart+1) ? signalStart : noiseStart;

        // check if the decoder reproduced a waveform or kept silent
        assertTrue (""no signal found in any channel!"", signalStart < smplPerChan);

        // max num seg that fit into signal
        final int totSeg = (smplPerChan - signalStart) / nSegSmp;
        // max num relevant samples (per channel)
        final int totSmp = nSegSmp * totSeg;

        // check if the number of reproduced segments in the audio file is valid
        assertTrue(""no segments left to test after signal search"", totSeg > 0);

        // get the energies and the channel offsets by searching for the first segment above the
        // energy threshold:
        // ratio of zeroNrgThresh to the max nrg
        final double zeroMaxNrgRatio = 0.001f;
        // threshold to classify segment energies
        double zeroNrgThresh = nSegSmp * nSegSmp;
        // will store the max seg nrg over all ch
        double totMaxNrg = 0.0f;
        // array receiving the segment energies
        double[][] nrg = new double[procNch][totSeg];
        // array for channel offsets
        int[] offset = new int[procNch];
        // array receiving the segment energy status over all channels
        boolean[] sigSeg = new boolean[totSeg];
        // energy per channel
        double[] nrgPerChannel = new double[procNch];
        // return value: [0]: total energy of all channels
        //               [1 ... procNch + 1]: energy of the individual channels
        float[] nrgTotal = new float[procNch + 1];
        // mapping array to sort channels
        int[] chMap = new int[nCh];

        // calculate the segmental energy for each channel
        for (int ch = 0; ch < procNch; ch++) {
            offset[ch] = -1;

            for (int seg = 0; seg < totSeg; seg++) {
                final int smpStart = (signalStart * nCh) + (seg * nSegSmpTot) + ch;
                final int smpStop = smpStart + nSegSmpTot;

                for (int smpl = smpStart; smpl < smpStop; smpl += nCh) {
                    // accumulate total energy per channel
                    nrgPerChannel[ch] += decSamples[smpl] * decSamples[smpl];
                    // accumulate segment energy
                    nrg[ch][seg] += nrgPerChannel[ch];
                }

                // store 1st segment (index) per channel which has energy above the threshold to get
                // the ch offsets
                if (nrg[ch][seg] > zeroNrgThresh && offset[ch] < 0) {
                    offset[ch] = seg / nSegChOffst;
                }

                // store the max segment nrg over all ch
                if (nrg[ch][seg] > totMaxNrg) {
                    totMaxNrg = nrg[ch][seg];
                }

                // store whether the channel has energy in this segment
                sigSeg[seg] |= nrg[ch][seg] > zeroNrgThresh;
            }

            // if one channel has no signal it is most probably the LFE the LFE is no
            // effective channel
            if (offset[ch] < 0) {
                effProcNch -= 1;
                offset[ch] = effProcNch;
            }

            // recalculate the zero signal threshold based on the 1st channels max energy for
            // all subsequent checks
            if (ch == 0) {
                zeroNrgThresh = zeroMaxNrgRatio * totMaxNrg;
            }
        }

        // check if the LFE is decoded properly
        assertTrue(""more than one LFE detected"", effProcNch >= procNch - 1);

        // check if the amount of samples is valid
        assertTrue(String.format(""less samples decoded than expected: %d < %d"",
                                 decSamples.length - (signalStart * nCh),
                                 totSmp * effProcNch),
                                 decSamples.length - (signalStart * nCh) >= totSmp * effProcNch);

        // for multi-channel signals the only valid front channel orders
        // are L, R, C or C, L, R (L=left, R=right, C=center)
        if (procNch >= 5) {
            final int[] frontChMap1 = {2, 0, 1};
            final int[] frontChMap2 = {0, 1, 2};

            // check if the channel mapping is valid
            if (!(Arrays.equals(Arrays.copyOfRange(offset, 0, 3), frontChMap1)
                    || Arrays.equals(Arrays.copyOfRange(offset, 0, 3), frontChMap2))) {
                fail(""wrong front channel mapping"");
            }
        }

        // create mapping table to address channels from front to back the LFE must be last
        if (drcContext == 0) {
            // check whether every channel occurs exactly once
            for (int ch = 0; ch < effProcNch; ch++) {
                int occurred = 0;

                for (int idx = 0; idx < procNch; idx++) {
                    if (offset[idx] == ch) {
                        occurred += 1;
                        chMap[ch] = idx;
                    }
                }

                // check if one channel is mapped more than one time
                assertTrue(String.format(""channel %d occurs %d times in the mapping"", ch, occurred),
                        occurred == 1);
            }
        } else {
            for (int ch = 0; ch < procNch; ch++) {
                chMap[ch] = ch;
            }
        }

        // reference min energy for the 1st ch; others will be compared against 1st
        double refMinNrg = zeroNrgThresh;

        // calculate total energy, min and max energy
        for (int ch = 0; ch < procNch; ch++) {
            // resolve channel mapping
            int idx = chMap[ch];
            // signal offset [segments]
            final int ofst = offset[idx] * nSegChOffst;

            if (ch <= effProcNch && ofst < totSeg) {
                // the last segment that has energy
                int nrgSegEnd;
                // the number of segments with energy
                int nrgSeg;

                if (drcContext == 0) {

                    // the first channel of a mono or stereo signal has full signal all others have
                    // one LB + one HB block
                    if ((encNch <= 2) && (ch == 0)) {
                        nrgSeg = totSeg;
                    } else {
                        nrgSeg = Math.min(totSeg, (2 * nSegPerBlk) + ofst) - ofst;
                    }
                } else {
                    nrgSeg = totSeg;
                }

                nrgSegEnd = ofst + nrgSeg;

                // find min and max energy of all segments that should have signal
                double minNrg = nrg[idx][ofst]; // channels minimum segment energy
                double maxNrg = nrg[idx][ofst]; // channels maximum segment energy

                // values of 1st segment already assigned
                for (int seg = ofst + 1; seg < nrgSegEnd; seg++) {
                    if (nrg[idx][seg] < minNrg) {
                        minNrg = nrg[idx][seg];
                    }

                    if (nrg[idx][seg] > maxNrg) {
                        maxNrg = nrg[idx][seg];
                    }
                }

                // check if the energy of this channel is > 0
                assertTrue(String.format(""max energy of channel %d is zero"", ch),maxNrg > 0.0f);

                if (drcContext == 0) {
                    // check the channels minimum energy >= refMinNrg
                    assertTrue(String.format(""channel %d has not enough energy"", ch),
                            minNrg >= refMinNrg);

                    if (ch == 0) {
                        // use 85% of 1st channels min energy as reference the other chs must meet
                        refMinNrg = minNrg * 0.85f;
                    } else if (isDmx && (ch == 1)) {
                        // in case of downmixed signal the energy can be lower depending on the
                        refMinNrg *= 0.50f;
                    }

                    // calculate and check the energy ratio
                    final double nrgRatio = minNrg / maxNrg;

                    // check if the threshold is exceeded
                    assertTrue(String.format(""energy ratio of channel %d below threshold"", ch),
                            nrgRatio >= nrgRatioThresh);

                    if (!isDmx) {
                        if (nrgSegEnd < totSeg) {
                            // consider that some noise can extend into the subsequent segment
                            // allow this to be at max 20% of the channels minimum energy
                            assertTrue(
                                    String.format(""min energy after noise above threshold (%.2f)"",
                                    nrg[idx][nrgSegEnd]),
                                    nrg[idx][nrgSegEnd] < minNrg * 0.20f);
                            nrgSegEnd += 1;
                        }
                    } else {
                        // ignore all subsequent segments in case of a downmixed signal
                        nrgSegEnd = totSeg;
                    }

                    // zero-out the verified energies to simplify the subsequent check
                    for (int seg = ofst; seg < nrgSegEnd; seg++) {
                        nrg[idx][seg] = 0.0f;
                    }

                    // check zero signal parts
                    for (int seg = 0; seg < totSeg; seg++) {
                        assertTrue(String.format(""segment %d in channel %d has signal where should ""
                                + ""be none (%.2f)"", seg, ch, nrg[idx][seg]),
                                nrg[idx][seg] < zeroNrgThresh);
                    }
                }
            }

            // test whether each segment has energy in at least one channel
            for (int seg = 0; seg < totSeg; seg++) {
                assertTrue(String.format(""no channel has energy in segment %d"", seg), sigSeg[seg]);
            }

            nrgTotal[0]     += (float)nrgPerChannel[ch];
            nrgTotal[ch + 1] = (float)nrgPerChannel[ch];
        }

        float errorMargin = 0.0f;
        if (drcApplied == 0) {
            errorMargin = 0.25f;
        } else if (drcApplied == 1) {
            errorMargin = 0.40f;
        }

        float totSegEnergy = 0.0f;
        float[] segEnergy = new float[totSeg];
        for (int seg = totSeg - 1; seg >= 0; seg--) {
            if (seg != 0) {
                for (int ch = 0; ch < nCh; ch++) {
                    segEnergy[seg] += nrg[ch][seg] - nrg[ch][seg - 1];
                }
                totSegEnergy += segEnergy[seg];
            } else {
                for (int ch = 0; ch < nCh; ch++) {
                    segEnergy[seg] += nrg[ch][seg];
                }
            }
        }
        float avgSegEnergy = totSegEnergy / (totSeg - 1);
        for (int seg = 1; seg < totSeg; seg++) {
            float energyRatio = segEnergy[seg] / avgSegEnergy;
            if ((energyRatio > (1.0f + errorMargin) ) || (energyRatio < (1.0f - errorMargin) )) {
                fail(""Energy drop out"");
            }
        }

        // return nrgTotal: [0]: accumulated energy of all channels, [1 ... ] channel energies
        return nrgTotal;
    }

    /**
     * Decodes a compressed bitstream in the ISOBMFF into the RAM of the device.
     *
     * The decoder decodes compressed audio data as stored in the ISO Base Media File Format
     * (ISOBMFF) aka .mp4/.m4a. The decoder is not reproducing the waveform but stores the decoded
     * samples in the RAM of the device under test.
     *
     * @param audioParams the decoder parameter configuration
     * @param testinput the compressed audio stream
     * @param eossample the End-Of-Stream indicator
     * @param timestamps the time stamps to decode
     * @param drcParams the MPEG-D DRC decoder parameter configuration
     * @param decoderName if non null, the name of the decoder to use for the decoding, otherwise
     *     the default decoder for the format will be used
     * @param runtimeChange defines whether the decoder is configured at runtime or configured
     *                      before starting to decode
     * @param expectedOutputLoudness value to check if the correct output loudness is returned
     *     by the decoder
     * @param seek_enable defines whether there will be an initial seek
     * @param seek_duration seeking duration in microseconds
     * @param seek_mode seeking mode 
     *
     * @throws RuntimeException
     */
    public short[] decodeToMemory(AudioParameter audioParams, int testinput, int eossample,
            List<Long> timestamps, DrcParams drcParams, String decoderName, boolean runtimeChange,
            int expectedOutputLoudness,
            boolean seek_enable, int seek_duration, int seek_mode) throws IOException {
        // TODO: code is the same as in DecoderTest, differences are:
        //          - addition of application of DRC parameters
        //          - no need/use of resetMode, configMode
        //       Split method so code can be shared

        final String localTag = TAG + ""#decodeToMemory"";
        short [] decoded = new short[0];
        int decodedIdx = 0;

        AssetFileDescriptor testFd = mResources.openRawResourceFd(testinput);

        MediaExtractor extractor;
        MediaCodec codec;
        ByteBuffer[] codecInputBuffers;
        ByteBuffer[] codecOutputBuffers;

        extractor = new MediaExtractor();
        extractor.setDataSource(testFd.getFileDescriptor(), testFd.getStartOffset(),
                testFd.getLength());
        testFd.close();

        assertEquals(""wrong number of tracks"", 1, extractor.getTrackCount());
        MediaFormat format = extractor.getTrackFormat(0);
        String mime = format.getString(MediaFormat.KEY_MIME);
        assertTrue(""not an audio file"", mime.startsWith(""audio/""));

        MediaFormat configFormat = format;
        if (decoderName == null) {
            codec = MediaCodec.createDecoderByType(mime);
        } else {
            codec = MediaCodec.createByCodecName(decoderName);
        }

        // set DRC parameters
        if (drcParams != null) {
            configFormat.setInteger(MediaFormat.KEY_AAC_DRC_HEAVY_COMPRESSION, drcParams.mHeavy);
            if (!runtimeChange) {
                configFormat.setInteger(MediaFormat.KEY_AAC_DRC_BOOST_FACTOR, drcParams.mBoost);
                configFormat.setInteger(MediaFormat.KEY_AAC_DRC_ATTENUATION_FACTOR, drcParams.mCut);
                if (drcParams.mDecoderTargetLevel != 0) {
                    configFormat.setInteger(MediaFormat.KEY_AAC_DRC_TARGET_REFERENCE_LEVEL,
                            drcParams.mDecoderTargetLevel);
                }
                if (drcParams.mEffectType != 0){
                    configFormat.setInteger(MediaFormat.KEY_AAC_DRC_EFFECT_TYPE,
                            drcParams.mEffectType);
                }
                if (drcParams.mAlbumMode != 0) {
                    configFormat.setInteger(MediaFormat.KEY_AAC_DRC_ALBUM_MODE,
                            drcParams.mAlbumMode);
                }
            }
        }

        Log.v(localTag, ""configuring with "" + configFormat);
        codec.configure(configFormat, null /* surface */, null /* crypto */, 0 /* flags */);

        if (drcParams != null && sIsAndroidRAndAbove) { // querying output format requires R
            if(!runtimeChange) {
                if (drcParams.mAlbumMode != 0) {
                    int albumModeFromCodec = DecoderTest.getOutputFormatInteger(codec,
                            MediaFormat.KEY_AAC_DRC_ALBUM_MODE);
                    if (albumModeFromCodec != drcParams.mAlbumMode) {
                        fail(""Drc AlbumMode received from MediaCodec is not the Album Mode set"");
                    }
                }
                if (drcParams.mEffectType != 0) {
                    final int effectTypeFromCodec = DecoderTest.getOutputFormatInteger(codec,
                            MediaFormat.KEY_AAC_DRC_EFFECT_TYPE);
                    if (effectTypeFromCodec != drcParams.mEffectType) {
                        fail(""Drc Effect Type received from MediaCodec is not the Effect Type set"");
                    }
                }
                if (drcParams.mDecoderTargetLevel != 0) {
                    final int targetLevelFromCodec = DecoderTest.getOutputFormatInteger(codec,
                            MediaFormat.KEY_AAC_DRC_TARGET_REFERENCE_LEVEL);
                    if (targetLevelFromCodec != drcParams.mDecoderTargetLevel) {
                        fail(""Drc Target Reference Level received from MediaCodec is not the Target Reference Level set"");
                    }
                }
            }
        }

        codec.start();
        codecInputBuffers = codec.getInputBuffers();
        codecOutputBuffers = codec.getOutputBuffers();

        if (drcParams != null) {
            if (runtimeChange) {
                Bundle b = new Bundle();
                b.putInt(MediaFormat.KEY_AAC_DRC_BOOST_FACTOR, drcParams.mBoost);
                b.putInt(MediaFormat.KEY_AAC_DRC_ATTENUATION_FACTOR, drcParams.mCut);
                if (drcParams.mEffectType != 0) {
                    b.putInt(MediaFormat.KEY_AAC_DRC_EFFECT_TYPE, drcParams.mEffectType);
                }
                if (drcParams.mDecoderTargetLevel != 0) {
                    b.putInt(MediaFormat.KEY_AAC_DRC_TARGET_REFERENCE_LEVEL,
                            drcParams.mDecoderTargetLevel);
                }
                if (drcParams.mAlbumMode != 0) {
                    b.putInt(MediaFormat.KEY_AAC_DRC_ALBUM_MODE, drcParams.mAlbumMode);
                }
                codec.setParameters(b);
            }
        }

        extractor.selectTrack(0);

        // execute initial seeking if specified
        if (seek_enable) {
            codec.flush();
            extractor.seekTo(seek_duration, seek_mode);
        }

        // start decoding
        final long kTimeOutUs = 5000;
        MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
        boolean sawInputEOS = false;
        boolean sawOutputEOS = false;
        int noOutputCounter = 0;
        int samplecounter = 0;

        // main decoding loop
        while (!sawOutputEOS && noOutputCounter < 50) {
            noOutputCounter++;
            if (!sawInputEOS) {
                int inputBufIndex = codec.dequeueInputBuffer(kTimeOutUs);

                if (inputBufIndex >= 0) {
                    ByteBuffer dstBuf = codecInputBuffers[inputBufIndex];

                    int sampleSize =
                        extractor.readSampleData(dstBuf, 0 /* offset */);

                    long presentationTimeUs = 0;

                    if (sampleSize < 0 && eossample > 0) {
                        fail(""test is broken: never reached eos sample"");
                    }

                    if (sampleSize < 0) {
                        Log.d(TAG, ""saw input EOS."");
                        sawInputEOS = true;
                        sampleSize = 0;
                    } else {
                        if (samplecounter == eossample) {
                            sawInputEOS = true;
                        }
                        samplecounter++;
                        presentationTimeUs = extractor.getSampleTime();
                    }

                    codec.queueInputBuffer(
                            inputBufIndex,
                            0 /* offset */,
                            sampleSize,
                            presentationTimeUs,
                            sawInputEOS ? MediaCodec.BUFFER_FLAG_END_OF_STREAM : 0);

                    if (!sawInputEOS) {
                        extractor.advance();
                    }
                }
            }

            int res = codec.dequeueOutputBuffer(info, kTimeOutUs);

            if (res >= 0) {
                //Log.d(TAG, ""got frame, size "" + info.size + ""/"" + info.presentationTimeUs);

                if (info.size > 0) {
                    noOutputCounter = 0;
                    if (timestamps != null) {
                        timestamps.add(info.presentationTimeUs);
                    }
                }

                int outputBufIndex = res;
                ByteBuffer buf = codecOutputBuffers[outputBufIndex];

                if (decodedIdx + (info.size / 2) >= decoded.length) {
                    decoded = Arrays.copyOf(decoded, decodedIdx + (info.size / 2));
                }

                buf.position(info.offset);
                for (int i = 0; i < info.size; i += 2) {
                    decoded[decodedIdx++] = buf.getShort();
                }

                codec.releaseOutputBuffer(outputBufIndex, false /* render */);

                if ((info.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                    Log.d(TAG, ""saw output EOS."");
                    sawOutputEOS = true;
                }
            } else if (res == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                codecOutputBuffers = codec.getOutputBuffers();

                Log.d(TAG, ""output buffers have changed."");
            } else if (res == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                MediaFormat oformat = codec.getOutputFormat();
                audioParams.setNumChannels(oformat.getInteger(MediaFormat.KEY_CHANNEL_COUNT));
                audioParams.setSamplingRate(oformat.getInteger(MediaFormat.KEY_SAMPLE_RATE));
                Log.d(TAG, ""output format has changed to "" + oformat);
            } else {
                Log.d(TAG, ""dequeueOutputBuffer returned "" + res);
            }
        }

        if (noOutputCounter >= 50) {
            fail(""decoder stopped outputting data"");
        }

        // check if MediaCodec gives back correct drc parameters
        if (drcParams != null && sIsAndroidRAndAbove) {
            if (drcParams.mAlbumMode != 0) {
                final int albumModeFromCodec = DecoderTest.getOutputFormatInteger(codec,
                        MediaFormat.KEY_AAC_DRC_ALBUM_MODE);
                assertEquals(""DRC AlbumMode received from MediaCodec is not the Album Mode set""
                        + "" runtime:"" + runtimeChange, drcParams.mAlbumMode, albumModeFromCodec);
            }
            if (drcParams.mEffectType != 0) {
                final int effectTypeFromCodec = DecoderTest.getOutputFormatInteger(codec,
                        MediaFormat.KEY_AAC_DRC_EFFECT_TYPE);
                assertEquals(""DRC Effect Type received from MediaCodec is not the Effect Type set""
                        + "" runtime:"" + runtimeChange, drcParams.mEffectType, effectTypeFromCodec);
            }
            if (drcParams.mDecoderTargetLevel != 0) {
                final int targetLevelFromCodec = DecoderTest.getOutputFormatInteger(codec,
                        MediaFormat.KEY_AAC_DRC_TARGET_REFERENCE_LEVEL);
                assertEquals(""DRC Target Ref Level received from MediaCodec is not the level set""
                        + "" runtime:"" + runtimeChange,
                        drcParams.mDecoderTargetLevel, targetLevelFromCodec);
            }

            final int cutFromCodec = DecoderTest.getOutputFormatInteger(codec,
                    MediaFormat.KEY_AAC_DRC_ATTENUATION_FACTOR);
            assertEquals(""Attenuation factor received from MediaCodec differs from set:"",
                    drcParams.mCut, cutFromCodec);
            final int boostFromCodec = DecoderTest.getOutputFormatInteger(codec,
                    MediaFormat.KEY_AAC_DRC_BOOST_FACTOR);
            assertEquals(""Boost factor received from MediaCodec differs from set:"",
                    drcParams.mBoost, boostFromCodec);
        }

        // expectedOutputLoudness == -2 indicates that output loudness is not tested
        if (expectedOutputLoudness != -2 && sIsAndroidRAndAbove) {
            final int outputLoudnessFromCodec = DecoderTest.getOutputFormatInteger(codec,
                    MediaFormat.KEY_AAC_DRC_OUTPUT_LOUDNESS);
            if (outputLoudnessFromCodec != expectedOutputLoudness) {
                fail(""Received decoder output loudness is not the expected value"");
            }
        }

        codec.stop();
        codec.release();
        return decoded;
    }

    private short[] decodeToMemory(AudioParameter audioParams, int testinput,
            int eossample, List<Long> timestamps, DrcParams drcParams, String decoderName)
            throws IOException
    {
        final short[] decoded = decodeToMemory(audioParams, testinput, eossample, timestamps,
                drcParams, decoderName, false, -2, false, 0, 0);
        return decoded;
    }

    private short[] decodeToMemory(AudioParameter audioParams, int testinput,
            int eossample, List<Long> timestamps, DrcParams drcParams, String decoderName,
            boolean runtimeChange)
        throws IOException
    {
        final short[] decoded = decodeToMemory(audioParams, testinput, eossample, timestamps,
                drcParams, decoderName, runtimeChange, -2, false, 0, 0);
        return decoded;
    }

    private short[] decodeToMemory(AudioParameter audioParams, int testinput,
        int eossample, List<Long> timestamps, DrcParams drcParams, String decoderName,
        boolean runtimeChange, int expectedOutputLoudness)
        throws IOException
    {
        short [] decoded = decodeToMemory(audioParams, testinput, eossample, timestamps, drcParams,
                decoderName, runtimeChange, expectedOutputLoudness, false, 0, 0);
        return decoded;
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.MuxerTest"	"testGetTrackFormatNative"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/MuxerTest.java"	""	"public void testGetTrackFormatNative() {
            Assume.assumeTrue(Build.VERSION.SDK_INT > Build.VERSION_CODES.R);
            assertTrue(nativeTestGetTrackFormat(mInpPath, mOutPath, mOutFormat));
        }
    }

    /**
     * Tests muxing multiple Video/Audio Tracks
     */
    @NonMediaMainlineTest
    @LargeTest
    @RunWith(Parameterized.class)
    public static class TestMultiTrack {
        private int mOutFormat;
        private String mSrcFileA;
        private String mSrcFileB;
        private String mInpPathA;
        private String mInpPathB;
        private String mRefPath;
        private String mOutPath;

        static {
            System.loadLibrary(""ctsmediav2muxer_jni"");
        }

        @Before
        public void prologue() throws IOException {
            mInpPathA = WorkDir.getMediaDirString() + mSrcFileA;
            mInpPathB = WorkDir.getMediaDirString() + mSrcFileB;
            mRefPath = File.createTempFile(""ref"", "".out"").getAbsolutePath();
            mOutPath = File.createTempFile(""tmp"", "".out"").getAbsolutePath();
        }

        @After
        public void epilogue() {
            new File(mRefPath).delete();
            new File(mOutPath).delete();
        }

        @Parameterized.Parameters(name = ""{index}({3})"")
        public static Collection<Object[]> input() {
            return Arrays.asList(new Object[][]{
                    // audio, video are 3 sec
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4, ""bbb_cif_768kbps_30fps_h263"" +
                            "".mp4"", ""bbb_stereo_48kHz_192kbps_aac.mp4"", ""mp4""},
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_WEBM, ""bbb_cif_768kbps_30fps_vp9.mkv""
                            , ""bbb_stereo_48kHz_192kbps_vorbis.ogg"", ""webm""},
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_3GPP, ""bbb_cif_768kbps_30fps_h263.mp4""
                            , ""bbb_mono_16kHz_20kbps_amrwb.amr"", ""3gpp""},

                    // audio 3 sec, video 10 sec
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4, ""bbb_qcif_512kbps_30fps_avc"" +
                            "".mp4"", ""bbb_stereo_48kHz_192kbps_aac.mp4"", ""mp4""},
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_WEBM, ""bbb_qcif_512kbps_30fps_vp9.webm""
                            , ""bbb_stereo_48kHz_192kbps_vorbis.ogg"", ""webm""},
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_3GPP, ""bbb_qcif_512kbps_30fps_h263.3gp""
                            , ""bbb_mono_16kHz_20kbps_amrwb.amr"", ""3gpp""},

                    // audio 10 sec, video 3 sec
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4, ""bbb_cif_768kbps_30fps_h263"" +
                            "".mp4"", ""bbb_stereo_48kHz_128kbps_aac.mp4"", ""mp4""},
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_WEBM, ""bbb_cif_768kbps_30fps_vp9.mkv""
                            , ""bbb_stereo_48kHz_128kbps_vorbis.ogg"", ""webm""},
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_3GPP, ""bbb_cif_768kbps_30fps_h263.mp4""
                            , ""bbb_mono_8kHz_8kbps_amrnb.3gp"", ""3gpp""},
            });
        }

        public TestMultiTrack(int outFormat, String srcFileA, String srcFileB, String testName) {
            mOutFormat = outFormat;
            mSrcFileA = srcFileA;
            mSrcFileB = srcFileB;
        }

        private native boolean nativeTestMultiTrack(int format, String fileA, String fileB,
                String fileR, String fileO);"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.MuxerTest"	"testMultiTrackNative"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/MuxerTest.java"	""	"public void testMultiTrackNative() {
            Assume.assumeTrue(shouldRunTest(mOutFormat));
            Assume.assumeTrue(""TODO(b/146423022)"",
                    mOutFormat != MediaMuxer.OutputFormat.MUXER_OUTPUT_WEBM);
            assertTrue(nativeTestMultiTrack(mOutFormat, mInpPathA, mInpPathB, mRefPath, mOutPath));
        }
    }

    /**
     * Add an offset to the presentation time of samples of a track. Mux with the added offset,
     * validate by re-extracting the muxer output file and compare with original.
     */
    @NonMediaMainlineTest
    @LargeTest
    @RunWith(Parameterized.class)
    public static class TestOffsetPts {
        private String mSrcFile;
        private int mOutFormat;
        private int[] mOffsetIndices;
        private String mInpPath;
        private String mOutPath;

        static {
            System.loadLibrary(""ctsmediav2muxer_jni"");
        }

        @Before
        public void prologue() throws IOException {
            mInpPath = WorkDir.getMediaDirString() + mSrcFile;
            mOutPath = File.createTempFile(""tmp"", "".out"").getAbsolutePath();
        }

        @After
        public void epilogue() {
            new File(mOutPath).delete();
        }

        @Parameterized.Parameters(name = ""{index}({3})"")
        public static Collection<Object[]> input() {
            return Arrays.asList(new Object[][]{
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4,
                            ""bbb_cif_768kbps_30fps_hevc_stereo_48kHz_192kbps_aac.mp4"",
                            new int[]{0}, ""mp4""},
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_WEBM,
                            ""bbb_cif_768kbps_30fps_vp8_stereo_48kHz_192kbps_vorbis.webm"",
                            new int[]{0}, ""webm""},
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_3GPP,
                            ""bbb_cif_768kbps_30fps_mpeg4_mono_16kHz_20kbps_amrwb.3gp"",
                            new int[]{0}, ""3gpp""},
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_OGG, ""bbb_stereo_48kHz_192kbps_opus.ogg"",
                            new int[]{10}, ""ogg""},
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4, ""bbb_cif_768kbps_30fps_avc.mp4"",
                            new int[]{6, 50, 77}, ""mp4""},
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_WEBM, ""bbb_cif_768kbps_30fps_vp9.mkv"",
                            new int[]{8, 44}, ""webm""},
            });
        }

        public TestOffsetPts(int outFormat, String file, int[] offsetIndices, String testName) {
            mOutFormat = outFormat;
            mSrcFile = file;
            mOffsetIndices = offsetIndices;
        }

        private native boolean nativeTestOffsetPts(int format, String srcFile, String dstFile,
                int[] offsetIndices);"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.MuxerTest"	"testOffsetPresentationTimeNative"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/MuxerTest.java"	""	"public void testOffsetPresentationTimeNative() {
            Assume.assumeTrue(shouldRunTest(mOutFormat));
            Assume.assumeTrue(""TODO(b/148978457)"",
                    mOutFormat != MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
            Assume.assumeTrue(""TODO(b/148978457)"",
                    mOutFormat != MediaMuxer.OutputFormat.MUXER_OUTPUT_3GPP);
            Assume.assumeTrue(""TODO(b/146423022)"",
                    mOutFormat != MediaMuxer.OutputFormat.MUXER_OUTPUT_WEBM);
            Assume.assumeTrue(""TODO(b/146421018)"",
                    mOutFormat != MediaMuxer.OutputFormat.MUXER_OUTPUT_OGG);
            assertTrue(nativeTestOffsetPts(mOutFormat, mInpPath, mOutPath, mOffsetIndices));
        }
    }

    /**
     * Tests whether appending audio and/or video data to an existing media file works in all
     * supported append modes.
     */
    @LargeTest
    @RunWith(Parameterized.class)
    public static class TestSimpleAppend {
        private static final String LOG_TAG = MuxerTestHelper.class.getSimpleName();
        private String mSrcFile;
        private String mInpPath;
        private String mOutPath;
        private int mOutFormat;
        private int mTrackCount;

        static {
            System.loadLibrary(""ctsmediav2muxer_jni"");
        }

        @Before
        public void prologue() throws IOException {
            mInpPath = WorkDir.getMediaDirString() + mSrcFile;
            mOutPath = File.createTempFile(""tmp"", "".out"").getAbsolutePath();
        }

        @After
        public void epilogue() {
            new File(mOutPath).delete();
        }

        @Parameterized.Parameters(name = ""{index}({3})"")
        public static Collection<Object[]> input() {
            return Arrays.asList(new Object[][]{
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4,
                            ""bbb_stereo_48kHz_128kbps_aac.mp4"", 1, ""mp4""},
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4,
                            ""bbb_1920x1080_avc_high_l42.mp4"", 1, ""mp4""},
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4,
                            ""bbb_cif_768kbps_30fps_h263_mono_8kHz_12kbps_amrnb.3gp"", 2, ""mp4""},
                    {MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4,
                            ""bbb_cif_768kbps_30fps_mpeg4_mono_16kHz_20kbps_amrwb.3gp"", 2, ""mp4""},
            });
        }

        public TestSimpleAppend(int outFormat, String srcFile, int trackCount, String testName) {
            mOutFormat = outFormat;
            mSrcFile = srcFile;
            mTrackCount = trackCount;
        }

        private native boolean nativeTestSimpleAppend(int outFormat, String srcPath,
                String outPath);

        private native boolean nativeTestAppendGetTrackCount(String srcPath, int trackCount);

        private native boolean nativeTestNoSamples(int outFormat, String srcPath, String outPath);

        private native boolean nativeTestIgnoreLastGOPAppend(int outFormat, String srcPath,
                String outPath);

        private native boolean nativeTestAppendGetTrackFormat(String srcPath);"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.MuxerTest"	"testAppendGetTrackFormatNative"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/MuxerTest.java"	""	"public void testAppendGetTrackFormatNative() {
            Assume.assumeTrue(Build.VERSION.SDK_INT > Build.VERSION_CODES.R);
            assertTrue(nativeTestAppendGetTrackFormat(mInpPath));
        }
    }


    /**
     * Audio, Video Codecs support a variety of file-types/container formats. For example,
     * AAC-LC supports MPEG4, 3GPP. Vorbis supports OGG and WEBM. H.263 supports 3GPP and WEBM.
     * This test takes the output of a codec and muxes it in to all possible container formats.
     * The results are checked for inconsistencies with the requirements of CDD.
     */
    @NonMediaMainlineTest
    @LargeTest
    @RunWith(Parameterized.class)
    public static class TestSimpleMux {
        private String mMime;
        private String mSrcFile;
        private String mInpPath;
        private String mOutPath;

        static {
            System.loadLibrary(""ctsmediav2muxer_jni"");
        }

        public TestSimpleMux(String mime, String srcFile, String testName) {
            mMime = mime;
            mSrcFile = srcFile;
        }

        @Before
        public void prologue() throws IOException {
            mInpPath = WorkDir.getMediaDirString() + mSrcFile;
            mOutPath = File.createTempFile(""tmp"", "".out"").getAbsolutePath();
        }

        @After
        public void epilogue() {
            new File(mOutPath).delete();
        }

        private boolean doesCodecRequireCSD(String aMime) {
            return (aMime == MediaFormat.MIMETYPE_VIDEO_AVC ||
                    aMime == MediaFormat.MIMETYPE_VIDEO_HEVC ||
                    aMime == MediaFormat.MIMETYPE_VIDEO_MPEG4 ||
                    aMime == MediaFormat.MIMETYPE_AUDIO_AAC);

        }

        private native boolean nativeTestSimpleMux(String srcPath, String outPath, String mime,
                String selector);

        private native boolean nativeTestSimpleAppend(String srcPath, String outPath, String mime,
                                                      String selector);

        @Parameterized.Parameters(name = ""{index}({2})"")
        public static Collection<Object[]> input() {
            return Arrays.asList(new Object[][]{
                    // Video Codecs
                    {MediaFormat.MIMETYPE_VIDEO_H263,
                            ""bbb_cif_768kbps_30fps_h263_mono_8kHz_12kbps_amrnb.3gp"", ""h263""},
                    {MediaFormat.MIMETYPE_VIDEO_AVC,
                            ""bbb_cif_768kbps_30fps_avc_stereo_48kHz_192kbps_vorbis.mp4"", ""avc""},
                    {MediaFormat.MIMETYPE_VIDEO_HEVC,
                            ""bbb_cif_768kbps_30fps_hevc_stereo_48kHz_192kbps_opus.mp4"", ""hevc""},
                    {MediaFormat.MIMETYPE_VIDEO_MPEG4,
                            ""bbb_cif_768kbps_30fps_mpeg4_mono_16kHz_20kbps_amrwb.3gp"", ""mpeg4""},
                    {MediaFormat.MIMETYPE_VIDEO_VP8,
                            ""bbb_cif_768kbps_30fps_vp8_stereo_48kHz_192kbps_vorbis.webm"", ""vp8""},
                    {MediaFormat.MIMETYPE_VIDEO_VP9,
                            ""bbb_cif_768kbps_30fps_vp9_stereo_48kHz_192kbps_opus.webm"", ""vp9""},
                    // Audio Codecs
                    {MediaFormat.MIMETYPE_AUDIO_AAC,
                            ""bbb_stereo_48kHz_128kbps_aac.mp4"", ""aac""},
                    {MediaFormat.MIMETYPE_AUDIO_AMR_NB,
                            ""bbb_cif_768kbps_30fps_h263_mono_8kHz_12kbps_amrnb.3gp"", ""amrnb""},
                    {MediaFormat.MIMETYPE_AUDIO_AMR_WB,
                            ""bbb_cif_768kbps_30fps_mpeg4_mono_16kHz_20kbps_amrwb.3gp"", ""amrwb""},
                    {MediaFormat.MIMETYPE_AUDIO_OPUS,
                            ""bbb_cif_768kbps_30fps_vp9_stereo_48kHz_192kbps_opus.webm"", ""opus""},
                    {MediaFormat.MIMETYPE_AUDIO_VORBIS,
                            ""bbb_cif_768kbps_30fps_vp8_stereo_48kHz_192kbps_vorbis.webm"", ""vorbis""},
                    // Metadata
                    {""application/gyro"",
                            ""video_176x144_3gp_h263_300kbps_25fps_aac_stereo_128kbps_11025hz_metadata_gyro_non_compliant.3gp"",
                            ""gyro-non-compliant""},
                    {""application/gyro"",
                            ""video_176x144_3gp_h263_300kbps_25fps_aac_stereo_128kbps_11025hz_metadata_gyro_compliant.3gp"",
                            ""gyro-compliant""},
            });
        }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.MuxerTest"	"testSimpleMuxNative"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/MuxerTest.java"	""	"public void testSimpleMuxNative() {
            Assume.assumeTrue(""TODO(b/146421018)"",
                    !mMime.equals(MediaFormat.MIMETYPE_AUDIO_OPUS));
            Assume.assumeTrue(""TODO(b/146923287)"",
                    !mMime.equals(MediaFormat.MIMETYPE_AUDIO_VORBIS));
            assertTrue(nativeTestSimpleMux(mInpPath, mOutPath, mMime, selector));
        }

        /* Does MediaMuxer throw IllegalStateException on missing codec specific data when required.
         * Check if relevant exception is thrown for AAC, AVC, HEVC, and MPEG4
         * codecs that require CSD in MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4.
         * TODO(b/156767190): Need to evaluate what all codecs need CSD and also what all formats
         * can contain these codecs, and add test cases accordingly.
         * TODO(b/156767190): Add similar tests in the native side/NDK as well.
         * TODO(b/156767190): Make a separate class, like TestNoCSDMux, instead of being part of
         * TestSimpleMux?
         */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.MuxerTest"	"testNoCSDMux"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/MuxerTest.java"	""	"public void testNoCSDMux() throws IOException {
            Assume.assumeTrue(doesCodecRequireCSD(mMime));
            MuxerTestHelper mediaInfo = new MuxerTestHelper(mInpPath, true);
            for (int format = MUXER_OUTPUT_FIRST; format <= MUXER_OUTPUT_LAST; format++) {
                // TODO(b/156767190)
                if(format != MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4) continue;
                MediaMuxer muxer = new MediaMuxer(mOutPath, format);
                Exception expected = null;
                String msg = String.format(""testNoCSDMux: inp: %s, mime %s, fmt: %s"", mSrcFile,
                                            mMime, formatStringPair.get(format));
                try {
                    mediaInfo.muxMedia(muxer);
                } catch (IllegalStateException e) {
                    expected = e;
                } catch (Exception e) {
                    fail(msg + "", unexpected exception:"" + e.getMessage());
                } finally {
                    assertNotNull(msg, expected);
                    muxer.release();
                }
            }
        }
    }

    @NonMediaMainlineTest
    @LargeTest
    @RunWith(Parameterized.class)
    public static class TestAddEmptyTracks {
        private final List<String> mimeListforTypeMp4 =
                Arrays.asList(MediaFormat.MIMETYPE_VIDEO_MPEG4, MediaFormat.MIMETYPE_VIDEO_H263,
                        MediaFormat.MIMETYPE_VIDEO_AVC, MediaFormat.MIMETYPE_VIDEO_HEVC,
                        MediaFormat.MIMETYPE_AUDIO_AAC, MediaFormat.MIMETYPE_IMAGE_ANDROID_HEIC,
                        MediaFormat.MIMETYPE_TEXT_SUBRIP);
        private final List<String> mimeListforTypeWebm =
                Arrays.asList(MediaFormat.MIMETYPE_VIDEO_VP8, MediaFormat.MIMETYPE_VIDEO_VP9,
                        MediaFormat.MIMETYPE_AUDIO_VORBIS, MediaFormat.MIMETYPE_AUDIO_OPUS);
        private final List<String> mimeListforType3gp =
                Arrays.asList(MediaFormat.MIMETYPE_VIDEO_MPEG4, MediaFormat.MIMETYPE_VIDEO_H263,
                        MediaFormat.MIMETYPE_VIDEO_AVC, MediaFormat.MIMETYPE_AUDIO_AAC,
                        MediaFormat.MIMETYPE_AUDIO_AMR_NB, MediaFormat.MIMETYPE_AUDIO_AMR_WB);
        private final List<String> mimeListforTypeOgg =
                Arrays.asList(MediaFormat.MIMETYPE_AUDIO_OPUS);
        private String mMime;
        private String mOutPath;

        public TestAddEmptyTracks(String mime) {
            mMime = mime;
        }

        @Before
        public void prologue() throws IOException {
            mOutPath = File.createTempFile(""tmp"", "".out"").getAbsolutePath();
        }

        @After
        public void epilogue() {
            new File(mOutPath).delete();
        }

        private boolean isMimeContainerPairValid(int format) {
            boolean result = false;
            if (format == MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4)
                result = mimeListforTypeMp4.contains(mMime);
            else if (format == MediaMuxer.OutputFormat.MUXER_OUTPUT_WEBM) {
                return mimeListforTypeWebm.contains(mMime);
            } else if (format == MediaMuxer.OutputFormat.MUXER_OUTPUT_3GPP) {
                result = mimeListforType3gp.contains(mMime);
            } else if (format == MediaMuxer.OutputFormat.MUXER_OUTPUT_OGG) {
                result = mimeListforTypeOgg.contains(mMime);
            }
            return result;
        }

        @Parameterized.Parameters(name = ""{index}({0})"")
        public static Collection<Object[]> input() {
            return Arrays.asList(new Object[][]{
                    // Video
                    {MediaFormat.MIMETYPE_VIDEO_H263},
                    {MediaFormat.MIMETYPE_VIDEO_AVC},
                    {MediaFormat.MIMETYPE_VIDEO_HEVC},
                    {MediaFormat.MIMETYPE_VIDEO_MPEG4},
                    {MediaFormat.MIMETYPE_VIDEO_VP8},
                    {MediaFormat.MIMETYPE_VIDEO_VP9},
                    // Audio
                    {MediaFormat.MIMETYPE_AUDIO_AAC},
                    {MediaFormat.MIMETYPE_AUDIO_AMR_NB},
                    {MediaFormat.MIMETYPE_AUDIO_AMR_WB},
                    {MediaFormat.MIMETYPE_AUDIO_OPUS},
                    {MediaFormat.MIMETYPE_AUDIO_VORBIS},
                    // Metadata
                    {MediaFormat.MIMETYPE_TEXT_SUBRIP},
                    // Image
                    {MediaFormat.MIMETYPE_IMAGE_ANDROID_HEIC}
            });
        }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.OfflineSessionTest"	"testUnsupportedOfflineSessionOutputs"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/OfflineSessionTest.java"	""	"public void testUnsupportedOfflineSessionOutputs() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing camera2 API for camera device "" + mCameraIdsUnderTest[i]);

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isOfflineProcessingSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support offline processing, skipping"");
                    continue;
                }

                openDevice(mCameraIdsUnderTest[i]);
                camera2UnsupportedOfflineOutputTest(true /*useSurfaceGroup*/);
                camera2UnsupportedOfflineOutputTest(false /*useSurfaceGroup*/);
            } finally {
                closeDevice();
            }
        }
    }

    private void checkForSequenceAbort(SimpleCaptureCallback resultListener, int sequenceId) {
        ArrayList<Integer> abortedSeq = resultListener.geAbortedSequences(
                1 /*maxNumbAborts*/);
        assertNotNull(""No aborted capture sequence ids present"", abortedSeq);
        assertTrue(""Unexpected number of aborted capture sequence ids : "" +
                abortedSeq.size() + "" expected 1"", abortedSeq.size() == 1);
        assertTrue(""Unexpected abort capture sequence id: "" +
                abortedSeq.get(0).intValue() + "" expected capture sequence id: "" +
                sequenceId, abortedSeq.get(0).intValue() == sequenceId);
    }

    private void verifyCaptureResults(SimpleCaptureCallback resultListener,
            SimpleImageReaderListener imageListener, int sequenceId, boolean offlineResults)
            throws Exception {
        long sequenceLastFrameNumber = resultListener.getCaptureSequenceLastFrameNumber(
                sequenceId, 0 /*timeoutMs*/);

        long lastFrameNumberReceived = -1;
        while (resultListener.hasMoreResults()) {
            TotalCaptureResult result = resultListener.getTotalCaptureResult(0 /*timeout*/);
            if (lastFrameNumberReceived < result.getFrameNumber()) {
                lastFrameNumberReceived = result.getFrameNumber();
            }

            if (imageListener != null) {
                long resultTimestamp = result.get(CaptureResult.SENSOR_TIMESTAMP);
                Image offlineImage = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                assertEquals(""Offline image timestamp: "" + offlineImage.getTimestamp() +
                        "" doesn't match with the result timestamp: "" + resultTimestamp,
                        offlineImage.getTimestamp(), resultTimestamp);
            }
        }

        while (resultListener.hasMoreFailures()) {
            ArrayList<CaptureFailure> failures = resultListener.getCaptureFailures(
                    /*maxNumFailures*/ 1);
            for (CaptureFailure failure : failures) {
                if (lastFrameNumberReceived < failure.getFrameNumber()) {
                    lastFrameNumberReceived = failure.getFrameNumber();
                }
            }
        }

        String assertString = offlineResults ?
                ""Last offline frame number from "" +
                ""onCaptureSequenceCompleted (%d) doesn't match the last frame number "" +
                ""received from results/failures (%d)"" :
                ""Last frame number from onCaptureSequenceCompleted "" +
                ""(%d) doesn't match the last frame number received from "" +
                ""results/failures (%d)"";
        assertEquals(String.format(assertString, sequenceLastFrameNumber, lastFrameNumberReceived),
                sequenceLastFrameNumber, lastFrameNumberReceived);
    }

    /**
     * Verify offline session behavior during common use cases
     *
     * @param cameraId      Id of the camera device under test
     * @param offlineSize   The offline surface size
     * @param offlineFormat The offline surface pixel format
     * @param testSequence  Specific scenario to be verified
     * @return true if the offline session switch is successful, false if there is any failure.
     */
    private boolean camera2OfflineSessionTest(String cameraId, Size offlineSize, int offlineFormat,
            OfflineTestSequence testSequence) throws Exception {
        boolean ret = false;
        int remoteOfflinePID = -1;
        Size previewSize = mOrderedPreviewSizes.get(0);
        for (Size sz : mOrderedPreviewSizes) {
            if (sz.getWidth() <= MANDATORY_STREAM_BOUND.getWidth() && sz.getHeight() <=
                    MANDATORY_STREAM_BOUND.getHeight()) {
                previewSize = sz;
                break;
            }
        }
        Size privateSize = previewSize;
        if (mAllStaticInfo.get(cameraId).isPrivateReprocessingSupported()) {
            privateSize = mAllStaticInfo.get(cameraId).getSortedSizesForInputFormat(
                    ImageFormat.PRIVATE, MANDATORY_STREAM_BOUND).get(0);
        }

        CaptureRequest.Builder previewRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder stillCaptureRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleCaptureCallback regularResultListener = new SimpleCaptureCallback();
        SimpleCaptureCallback offlineResultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
        ImageReader privateReader = null;
        ImageReader yuvCallbackReader = null;
        ImageReader jpegReader = null;
        int repeatingSeqId = -1;

        // Update preview size.
        updatePreviewSurface(previewSize);

        // Create ImageReader.
        createImageReader(offlineSize, offlineFormat, MAX_READER_IMAGES, imageListener);

        // Configure output streams with preview and offline streams.
        ArrayList<Surface> outputSurfaces = new ArrayList<Surface>();
        outputSurfaces.add(mPreviewSurface);
        outputSurfaces.add(mReaderSurface);
        final CameraCaptureSession.StateCallback sessionCb = mock(
                CameraCaptureSession.StateCallback.class);
        mSessionListener = new BlockingSessionCallback(sessionCb);
        mSession = configureCameraSession(mCamera, outputSurfaces, mSessionListener, mHandler);

        if (!mSession.supportsOfflineProcessing(mReaderSurface)) {
            Log.i(TAG, ""Camera does not support offline processing for still capture output"");
            return false;
        }

        // Configure the requests.
        previewRequest.addTarget(mPreviewSurface);
        stillCaptureRequest.addTarget(mReaderSurface);


        ArrayList<Integer> allowedOfflineStates = new ArrayList<Integer>();
        allowedOfflineStates.add(BlockingOfflineSessionCallback.STATE_READY);
        allowedOfflineStates.add(BlockingOfflineSessionCallback.STATE_SWITCH_FAILED);
        ArrayList<Surface> offlineSurfaces = new ArrayList<Surface>();
        offlineSurfaces.add(mReaderSurface);
        final CameraOfflineSessionCallback mockOfflineCb = mock(CameraOfflineSessionCallback.class);
        BlockingOfflineSessionCallback offlineCb = new BlockingOfflineSessionCallback(
                mockOfflineCb);
        ArrayList<CaptureRequest> offlineRequestList = new ArrayList<CaptureRequest>();
        for (int i = 0; i < MAX_READER_IMAGES; i++) {
            offlineRequestList.add(stillCaptureRequest.build());
        }

        if (testSequence != OfflineTestSequence.RepeatingSequenceAbort) {
            repeatingSeqId = mSession.setRepeatingRequest(previewRequest.build(), resultListener,
                    mHandler);
            checkInitialResults(resultListener);
        }

        int offlineSeqId = mSession.captureBurst(offlineRequestList, offlineResultListener,
                mHandler);

        if (testSequence == OfflineTestSequence.RepeatingSequenceAbort) {
            // Submit the preview repeating request after the offline burst so it can be delayed
            // long enough and fail to reach the camera processing pipeline.
            repeatingSeqId = mSession.setRepeatingRequest(previewRequest.build(), resultListener,
                    mHandler);
        }

        CameraOfflineSession offlineSession = mSession.switchToOffline(offlineSurfaces,
                new HandlerExecutor(mHandler), offlineCb);
        assertNotNull(""Invalid offline session"", offlineSession);

        // The regular capture session must be closed as well
        verify(sessionCb, times(1)).onClosed(mSession);

        int offlineState = offlineCb.waitForAnyOfStates(allowedOfflineStates,
                WAIT_FOR_STATE_TIMEOUT_MS);
        if (offlineState == BlockingOfflineSessionCallback.STATE_SWITCH_FAILED) {
            // A failure during offline mode switch is only allowed in case the switch gets
            // triggered too late without pending offline requests.
            verify(mockOfflineCb, times(1)).onSwitchFailed(offlineSession);
            verify(mockOfflineCb, times(0)).onReady(offlineSession);
            verify(mockOfflineCb, times(0)).onIdle(offlineSession);
            verify(mockOfflineCb, times(0)).onError(offlineSession,
                    CameraOfflineSessionCallback.STATUS_INTERNAL_ERROR);

            try {
                verifyCaptureResults(resultListener, null /*imageListener*/, repeatingSeqId,
                        false /*offlineResults*/);
            } catch (AssertionFailedError e) {
                if (testSequence == OfflineTestSequence.RepeatingSequenceAbort) {
                    checkForSequenceAbort(resultListener, repeatingSeqId);
                } else {
                    throw e;
                }
            }
            verifyCaptureResults(offlineResultListener, null /*imageListener*/, offlineSeqId,
                    true /*offlineResults*/);
        } else {
            verify(mockOfflineCb, times(1)).onReady(offlineSession);
            verify(mockOfflineCb, times(0)).onSwitchFailed(offlineSession);

            switch (testSequence) {
                case RepeatingSequenceAbort:
                    checkForSequenceAbort(resultListener, repeatingSeqId);
                    break;
                case CloseDeviceAndOpenRemote:
                    // According to the documentation, closing the initial camera device and
                    // re-opening the same device from a different client after successful
                    // offline session switch must not have any noticeable impact on the
                    // offline processing.
                    closeDevice();
                    remoteOfflinePID = startRemoteOfflineTestProcess(cameraId);

                    break;
                case CloseOfflineSession:
                    offlineSession.close();

                    break;
                case InitializeRegularSession:
                    // According to the documentation, initializing a regular capture session
                    // along with the offline session should not have any side effects.
                    // We also don't want to re-use the same offline output surface as part
                    // of the new  regular capture session.
                    outputSurfaces.remove(mReaderSurface);

                    // According to the specification, an active offline session must allow
                    // camera devices to support at least one preview stream, one yuv stream
                    // of size up-to 1080p, one jpeg stream with any supported size and
                    // an extra input/output private pair in case reprocessing is also available.
                    yuvCallbackReader = makeImageReader(previewSize, ImageFormat.YUV_420_888,
                            1 /*maxNumImages*/, new SimpleImageReaderListener(), mHandler);
                    outputSurfaces.add(yuvCallbackReader.getSurface());

                    jpegReader = makeImageReader(offlineSize, ImageFormat.JPEG,
                            1 /*maxNumImages*/, new SimpleImageReaderListener(), mHandler);
                    outputSurfaces.add(jpegReader.getSurface());

                    if (mAllStaticInfo.get(cameraId).isPrivateReprocessingSupported()) {
                        privateReader = makeImageReader(privateSize, ImageFormat.PRIVATE,
                                1 /*maxNumImages*/, new SimpleImageReaderListener(), mHandler);
                        outputSurfaces.add(privateReader.getSurface());

                        InputConfiguration inputConfig = new InputConfiguration(
                                privateSize.getWidth(), privateSize.getHeight(),
                                ImageFormat.PRIVATE);
                        mSession = CameraTestUtils.configureReprocessableCameraSession(mCamera,
                                inputConfig, outputSurfaces, mSessionListener, mHandler);

                    } else {
                        mSession = configureCameraSession(mCamera, outputSurfaces, mSessionListener,
                                mHandler);
                    }

                    mSession.setRepeatingRequest(previewRequest.build(), regularResultListener,
                            mHandler);

                    break;
                case NoExtraSteps:
                default:
            }

            if (testSequence != OfflineTestSequence.RepeatingSequenceAbort) {
                // The repeating non-offline request should be done after the switch returns.
                verifyCaptureResults(resultListener, null /*imageListener*/, repeatingSeqId,
                        false /*offlineResults*/);
            }

            if (testSequence != OfflineTestSequence.CloseOfflineSession) {
                offlineCb.waitForState(BlockingOfflineSessionCallback.STATE_IDLE,
                        WAIT_FOR_STATE_TIMEOUT_MS);
                verify(mockOfflineCb, times(1)).onIdle(offlineSession);
                verify(mockOfflineCb, times(0)).onError(offlineSession,
                        CameraOfflineSessionCallback.STATUS_INTERNAL_ERROR);

                // The offline requests should be done after we reach idle state.
                verifyCaptureResults(offlineResultListener, imageListener, offlineSeqId,
                        true /*offlineResults*/);

                offlineSession.close();
            }

            if (testSequence == OfflineTestSequence.InitializeRegularSession) {
                checkInitialResults(regularResultListener);
                stopPreview();

                if (privateReader != null) {
                    privateReader.close();
                }

                if (yuvCallbackReader != null) {
                    yuvCallbackReader.close();
                }

                if (jpegReader != null) {
                    jpegReader.close();
                }
            }

            offlineCb.waitForState(BlockingOfflineSessionCallback.STATE_CLOSED,
                  WAIT_FOR_STATE_TIMEOUT_MS);
            verify(mockOfflineCb, times(1)).onClosed(offlineSession);

            ret = true;
        }

        closeImageReader();

        stopRemoteOfflineTestProcess(remoteOfflinePID);

        return ret;
    }

    private void checkInitialResults(SimpleCaptureCallback resultListener) {
        CaptureResult result = resultListener.getCaptureResult(WAIT_FOR_FRAMES_TIMEOUT_MS);

        Long timestamp = result.get(CaptureResult.SENSOR_TIMESTAMP);
        assertNotNull(""Can't read a capture result timestamp"", timestamp);

        CaptureResult result2 = resultListener.getCaptureResult(WAIT_FOR_FRAMES_TIMEOUT_MS);

        Long timestamp2 = result2.get(CaptureResult.SENSOR_TIMESTAMP);
        assertNotNull(""Can't read a capture result 2 timestamp"", timestamp2);

        assertTrue(""Bad timestamps"", timestamp2 > timestamp);
    }

    private void camera2UnsupportedOfflineOutputTest(boolean useSurfaceGroup) throws Exception {
        CaptureRequest.Builder previewRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        Size previewSize = mOrderedPreviewSizes.get(0);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        updatePreviewSurface(previewSize);

        OutputConfiguration outConfig;
        if (useSurfaceGroup) {
            outConfig = new OutputConfiguration(1 /*surfaceGroupId*/, mPreviewSurface);
        } else {
            outConfig = new OutputConfiguration(mPreviewSurface);
            outConfig.enableSurfaceSharing();
        }

        ArrayList<OutputConfiguration> outputList = new ArrayList<OutputConfiguration>();
        outputList.add(outConfig);
        BlockingSessionCallback sessionListener = new BlockingSessionCallback();
        mCamera.createCaptureSessionByOutputConfigurations(outputList, sessionListener, mHandler);
        CameraCaptureSession session = sessionListener.waitAndGetSession(
                SESSION_CONFIGURE_TIMEOUT_MS);

        assertFalse(useSurfaceGroup ? ""Group surface outputs cannot support offline mode"" :
                ""Shared surface outputs cannot support offline mode"",
                session.supportsOfflineProcessing(mPreviewSurface));

        ArrayList<CaptureRequest> offlineRequestList = new ArrayList<CaptureRequest>();
        previewRequest.addTarget(mPreviewSurface);
        for (int i = 0; i < MAX_READER_IMAGES; i++) {
            offlineRequestList.add(previewRequest.build());
        }

        final CameraOfflineSessionCallback offlineCb = mock(CameraOfflineSessionCallback.class);
        ArrayList<Surface> offlineSurfaces = new ArrayList<Surface>();
        offlineSurfaces.add(mPreviewSurface);
        session.captureBurst(offlineRequestList, resultListener, mHandler);
        try {
            session.switchToOffline(offlineSurfaces, new HandlerExecutor(mHandler), offlineCb);
            fail(useSurfaceGroup ? ""Group surface outputs cannot be switched to offline mode"" :
                ""Shared surface outputs cannot be switched to offline mode"");
        } catch (IllegalArgumentException e) {
            // Expected
        }

        session.close();
    }

    private int startRemoteOfflineTestProcess(String cameraId) throws InterruptedException {
        // Ensure no running activity process with same name
        String cameraActivityName = mContext.getPackageName() + "":"" + REMOTE_PROCESS_NAME;
        ActivityManager activityManager = (ActivityManager) mContext.getSystemService(
                Context.ACTIVITY_SERVICE);
        List<ActivityManager.RunningAppProcessInfo> list = activityManager.getRunningAppProcesses();
        for (ActivityManager.RunningAppProcessInfo rai : list) {
            if (cameraActivityName.equals(rai.processName)) {
                fail(""Remote offline session test activity already running"");
                return -1;
            }
        }

        Activity activity = mActivityRule.getActivity();
        Intent activityIntent = new Intent(activity, REMOTE_PROCESS_CLASS);
        Bundle b = new Bundle();
        b.putString(CameraTestUtils.OFFLINE_CAMERA_ID, cameraId);
        activityIntent.putExtras(b);
        activity.startActivity(activityIntent);
        Thread.sleep(WAIT_FOR_REMOTE_ACTIVITY_LAUNCH_MS);

        // Fail if activity isn't running
        list = activityManager.getRunningAppProcesses();
        for (ActivityManager.RunningAppProcessInfo rai : list) {
            if (cameraActivityName.equals(rai.processName))
                return rai.pid;
        }

        fail(""Remote offline session test activity failed to start"");

        return -1;
    }

    private void stopRemoteOfflineTestProcess(int remotePID) throws InterruptedException {
        if (remotePID < 0) {
            return;
        }

        android.os.Process.killProcess(remotePID);
        Thread.sleep(WAIT_FOR_REMOTE_ACTIVITY_DESTROY_MS);

        ActivityManager activityManager = (ActivityManager) mContext.getSystemService(
                Context.ACTIVITY_SERVICE);
        String cameraActivityName = mContext.getPackageName() + "":"" + REMOTE_PROCESS_NAME;
        List<ActivityManager.RunningAppProcessInfo> list = activityManager.getRunningAppProcesses();
        for (ActivityManager.RunningAppProcessInfo rai : list) {
            if (cameraActivityName.equals(rai.processName))
                fail(""Remote offline session test activity is still running"");
        }

    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.isolatedsplitapp.SplitAppTest"	"AppContextTestRule"	""	"/home/gpoor/cts-12-source/cts/hostsidetests/appsecurity/test-apps/IsolatedSplitApp/src/com/android/cts/isolatedsplitapp/SplitAppTest.java"	""	"/*
 *.
 */

package com.android.cts.isolatedsplitapp;

import static org.hamcrest.CoreMatchers.*;
import static org.junit.Assert.*;

import android.app.Activity;
import android.app.Instrumentation;
import android.content.BroadcastReceiver;
import android.content.ComponentName;
import android.content.Context;
import android.content.Intent;
import android.content.res.Configuration;
import android.content.res.Resources;
import android.graphics.drawable.ColorDrawable;
import android.graphics.drawable.Drawable;
import android.os.Bundle;
import android.view.View;
import android.widget.LinearLayout;

import androidx.test.InstrumentationRegistry;
import androidx.test.rule.ActivityTestRule;
import androidx.test.runner.AndroidJUnit4;

import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.TestRule;
import org.junit.runner.Description;
import org.junit.runner.RunWith;
import org.junit.runners.model.Statement;

import java.util.Locale;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.TimeUnit;

@RunWith(AndroidJUnit4.class)
public class SplitAppTest {
    private static final String PACKAGE = ""com.android.cts.isolatedsplitapp"";

    private static final ComponentName BASE_ACTIVITY =
            ComponentName.createRelative(PACKAGE, "".BaseActivity"");
    private static final ComponentName FEATURE_A_ACTIVITY =
            ComponentName.createRelative(PACKAGE, "".feature_a.FeatureAActivity"");
    private static final ComponentName FEATURE_B_ACTIVITY =
            ComponentName.createRelative(PACKAGE, "".feature_b.FeatureBActivity"");
    private static final ComponentName FEATURE_C_ACTIVITY =
            ComponentName.createRelative(PACKAGE, "".feature_c.FeatureCActivity"");

    private static final String FEATURE_A_STRING =
            ""com.android.cts.isolatedsplitapp.feature_a:string/feature_a_string"";
    private static final String FEATURE_B_STRING =
            ""com.android.cts.isolatedsplitapp.feature_b:string/feature_b_string"";
    private static final String FEATURE_C_STRING =
            ""com.android.cts.isolatedsplitapp.feature_c:string/feature_c_string"";
    private static final String FEATURE_A_TEXTVIEW_ID =
            ""com.android.cts.isolatedsplitapp.feature_a:id/feature_a_text"";
    private static final String FEATURE_B_TEXTVIEW_ID =
            ""com.android.cts.isolatedsplitapp.feature_b:id/feature_b_text"";
    private static final String FEATURE_C_TEXTVIEW_ID =
            ""com.android.cts.isolatedsplitapp.feature_c:id/feature_c_text"";

    private static final Configuration PL = new Configuration();
    static {
        PL.setLocale(Locale.forLanguageTag(""pl""));
    }

    // Do not launch this activity lazily. We use this rule to launch all Activities,
    // so we use #launchActivity() with the correct Intent.
    @Rule
    public ActivityTestRule<Activity> mActivityRule =
            new ActivityTestRule<>(Activity.class, true /*initialTouchMode*/,
                    false /*launchActivity*/);

    @Rule
    public AppContextTestRule mAppContextTestRule = new AppContextTestRule();

    @Before
    public void setUp() {
        BaseActivity.setOverrideConfiguration(null);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.AdaptivePlaybackTest"	"EarlyEosTest"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/AdaptivePlaybackTest.java"	""	"public void test/*
 *.
 */

package android.media.cts;

import android.content.pm.PackageManager;
import android.media.MediaCodec;
import android.media.MediaCodecInfo;
import android.media.MediaCodecInfo.CodecCapabilities;
import android.media.MediaCodecInfo.CodecProfileLevel;
import android.media.MediaCodecList;
import android.media.MediaExtractor;
import android.media.MediaFormat;
import android.os.Build;
import android.platform.test.annotations.AppModeFull;
import android.util.Log;
import android.view.Surface;

import com.android.compatibility.common.util.ApiLevelUtil;
import com.android.compatibility.common.util.MediaUtils;

import android.opengl.GLES20;
import javax.microedition.khronos.opengles.GL10;

import java.io.IOException;
import java.lang.System;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Locale;
import java.util.Vector;
import java.util.concurrent.CopyOnWriteArrayList;
import java.util.zip.CRC32;

@MediaHeavyPresubmitTest
@AppModeFull
public class AdaptivePlaybackTest extends MediaPlayerTestBase {
    private static final String TAG = ""AdaptivePlaybackTest"";
    private boolean verify = false;
    private static final int MIN_FRAMES_BEFORE_DRC = 2;

    private static boolean sIsAtLeastS = ApiLevelUtil.isAtLeast(Build.VERSION_CODES.S);

    public Iterable<Codec> H264(CodecFactory factory) {
        return factory.createCodecList(
                MediaFormat.MIMETYPE_VIDEO_AVC,
                ""video_480x360_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                ""video_1280x720_mp4_h264_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                ""bbb_s1_720x480_mp4_h264_mp3_2mbps_30fps_aac_lc_5ch_320kbps_48000hz.mp4"");
    }

    public Iterable<Codec> HEVC(CodecFactory factory) {
        return factory.createCodecList(
                MediaFormat.MIMETYPE_VIDEO_HEVC,
                ""bbb_s1_720x480_mp4_hevc_mp3_1600kbps_30fps_aac_he_6ch_240kbps_48000hz.mp4"",
                ""bbb_s4_1280x720_mp4_hevc_mp31_4mbps_30fps_aac_he_stereo_80kbps_32000hz.mp4"",
                ""bbb_s1_352x288_mp4_hevc_mp2_600kbps_30fps_aac_he_stereo_96kbps_48000hz.mp4"");
    }

    public Iterable<Codec> Mpeg2(CodecFactory factory) {
        return factory.createCodecList(
                MediaFormat.MIMETYPE_VIDEO_MPEG2,
                ""video_640x360_mp4_mpeg2_2000kbps_30fps_aac_stereo_128kbps_48000hz.mp4"",
                ""video_1280x720_mp4_mpeg2_3000kbps_30fps_aac_stereo_128kbps_48000hz.mp4"");
    }

    public Iterable<Codec> H263(CodecFactory factory) {
        return factory.createCodecList(
                MediaFormat.MIMETYPE_VIDEO_H263,
                ""video_176x144_3gp_h263_300kbps_12fps_aac_stereo_128kbps_22050hz.3gp"",
                ""video_352x288_3gp_h263_300kbps_12fps_aac_stereo_128kbps_22050hz.3gp"");
    }

    public Iterable<Codec> Mpeg4(CodecFactory factory) {
        return factory.createCodecList(
                MediaFormat.MIMETYPE_VIDEO_MPEG4,
                ""video_1280x720_mp4_mpeg4_1000kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                ""video_480x360_mp4_mpeg4_860kbps_25fps_aac_stereo_128kbps_44100hz.mp4"",
                ""video_176x144_mp4_mpeg4_300kbps_25fps_aac_stereo_128kbps_44100hz.mp4"");
    }

    public Iterable<Codec> VP8(CodecFactory factory) {
        return factory.createCodecList(
                MediaFormat.MIMETYPE_VIDEO_VP8,
                ""video_480x360_webm_vp8_333kbps_25fps_vorbis_stereo_128kbps_48000hz.webm"",
                ""bbb_s3_1280x720_webm_vp8_8mbps_60fps_opus_6ch_384kbps_48000hz.webm"",
                ""bbb_s1_320x180_webm_vp8_800kbps_30fps_opus_5ch_320kbps_48000hz.webm"");
    }

    public Iterable<Codec> VP9(CodecFactory factory) {
        return factory.createCodecList(
                MediaFormat.MIMETYPE_VIDEO_VP9,
                ""video_480x360_webm_vp9_333kbps_25fps_vorbis_stereo_128kbps_48000hz.webm"",
                ""bbb_s4_1280x720_webm_vp9_0p31_4mbps_30fps_opus_stereo_128kbps_48000hz.webm"",
                ""bbb_s1_320x180_webm_vp9_0p11_600kbps_30fps_vorbis_mono_64kbps_48000hz.webm"");
    }

    public Iterable<Codec> AV1(CodecFactory factory) {
        return factory.createCodecList(
                MediaFormat.MIMETYPE_VIDEO_AV1,
                ""video_480x360_webm_av1_400kbps_30fps_vorbis_stereo_128kbps_48000hz.webm"",
                ""video_1280x720_webm_av1_2000kbps_30fps_vorbis_stereo_128kbps_48000hz.webm"",
                ""video_320x180_webm_av1_200kbps_30fps_vorbis_stereo_128kbps_48000hz.webm"");
    }

    CodecFactory ALL = new CodecFactory();
    CodecFactory SW  = new SWCodecFactory();
    CodecFactory HW  = new HWCodecFactory();

    public Iterable<Codec> H264()  { return H264(ALL);  }
    public Iterable<Codec> HEVC()  { return HEVC(ALL);  }
    public Iterable<Codec> VP8()   { return VP8(ALL);   }
    public Iterable<Codec> VP9()   { return VP9(ALL);   }
    public Iterable<Codec> AV1()   { return AV1(ALL);   }
    public Iterable<Codec> Mpeg2() { return Mpeg2(ALL); }
    public Iterable<Codec> Mpeg4() { return Mpeg4(ALL); }
    public Iterable<Codec> H263()  { return H263(ALL);  }

    public Iterable<Codec> AllCodecs() {
        return chain(H264(ALL), HEVC(ALL), VP8(ALL), VP9(ALL), AV1(ALL), Mpeg2(ALL), Mpeg4(ALL), H263(ALL));
    }

    public Iterable<Codec> SWCodecs() {
        return chain(H264(SW), HEVC(SW), VP8(SW), VP9(SW), AV1(SW), Mpeg2(SW), Mpeg4(SW), H263(SW));
    }

    public Iterable<Codec> HWCodecs() {
        return chain(H264(HW), HEVC(HW), VP8(HW), VP9(HW), AV1(HW), Mpeg2(HW), Mpeg4(HW), H263(HW));
    }

    /* tests for adaptive codecs */
    Test adaptiveEarlyEos     = new EarlyEosTest().adaptive();
    Test adaptiveEosFlushSeek = new EosFlushSeekTest().adaptive();
    Test adaptiveSkipAhead    = new AdaptiveSkipTest(true /* forward */);
    Test adaptiveSkipBack     = new AdaptiveSkipTest(false /* forward */);

    /* DRC tests for adaptive codecs */
    Test adaptiveReconfigDrc      = new ReconfigDrcTest().adaptive();
    Test adaptiveSmallReconfigDrc = new ReconfigDrcTest().adaptiveSmall();
    Test adaptiveDrc      = new AdaptiveDrcTest(); /* adaptive */
    Test adaptiveSmallDrc = new AdaptiveDrcTest().adaptiveSmall();

    /* tests for regular codecs */
    Test earlyEos          = new EarlyEosTest();
    Test eosFlushSeek      = new EosFlushSeekTest();
    Test flushConfigureDrc = new ReconfigDrcTest();

    Test[] allTests = {
        adaptiveEarlyEos,
        adaptiveEosFlushSeek,
        adaptiveSkipAhead,
        adaptiveSkipBack,
        adaptiveReconfigDrc,
        adaptiveSmallReconfigDrc,
        adaptiveDrc,
        adaptiveSmallDrc,
        earlyEos,
        eosFlushSeek,
        flushConfigureDrc,
    };

    /* helpers to run sets of tests */
    public void runEOS() { ex(AllCodecs(), new Test[] {
        adaptiveEarlyEos,
        adaptiveEosFlushSeek,
        adaptiveReconfigDrc,
        adaptiveSmallReconfigDrc,
        earlyEos,
        eosFlushSeek,
        flushConfigureDrc,
    }); }

    public void runAll() { ex(AllCodecs(), allTests); }
    public void runSW()  { ex(SWCodecs(),  allTests); }
    public void runHW()  { ex(HWCodecs(),  allTests); }

    public void verifyAll() { verify = true; try { runAll(); } finally { verify = false; } }
    public void verifySW()  { verify = true; try { runSW();  } finally { verify = false; } }
    public void verifyHW()  { verify = true; try { runHW();  } finally { verify = false; } }

    public void runH264()  { ex(H264(),  allTests); }
    public void runHEVC()  { ex(HEVC(),  allTests); }
    public void runVP8()   { ex(VP8(),   allTests); }
    public void runVP9()   { ex(VP9(),   allTests); }
    public void runAV1()   { ex(AV1(),   allTests); }
    public void runMpeg2() { ex(Mpeg2(), allTests); }
    public void runMpeg4() { ex(Mpeg4(), allTests); }
    public void runH263()  { ex(H263(),  allTests); }

    public void onlyH264HW()  { ex(H264(HW),  allTests); }
    public void onlyHEVCHW()  { ex(HEVC(HW),  allTests); }
    public void onlyVP8HW()   { ex(VP8(HW),   allTests); }
    public void onlyVP9HW()   { ex(VP9(HW),   allTests); }
    public void onlyAV1HW()   { ex(AV1(HW),   allTests); }
    public void onlyMpeg2HW() { ex(Mpeg2(HW), allTests); }
    public void onlyMpeg4HW() { ex(Mpeg4(HW), allTests); }
    public void onlyH263HW()  { ex(H263(HW),  allTests); }

    public void onlyH264SW()  { ex(H264(SW),  allTests); }
    public void onlyHEVCSW()  { ex(HEVC(SW),  allTests); }
    public void onlyVP8SW()   { ex(VP8(SW),   allTests); }
    public void onlyVP9SW()   { ex(VP9(SW),   allTests); }
    public void onlyAV1SW()   { ex(AV1(SW),   allTests); }
    public void onlyMpeg2SW() { ex(Mpeg2(SW), allTests); }
    public void onlyMpeg4SW() { ex(Mpeg4(SW), allTests); }
    public void onlyH263SW()  { ex(H263(SW),  allTests); }

    public void bytebuffer() { ex(H264(SW), new EarlyEosTest().byteBuffer()); }
    public void onlyTexture() { ex(H264(HW), new EarlyEosTest().texture()); }

    /* inidividual tests */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.AudioFormatTest"	"testFrameSize"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/AudioFormatTest.java"	""	"public void testFrameSize() throws Exception {
        int[] encodings = {
            AudioFormat.ENCODING_MP3,
            AudioFormat.ENCODING_AAC_LC,
            AudioFormat.ENCODING_AAC_HE_V1,
            AudioFormat.ENCODING_AAC_HE_V2,
            AudioFormat.ENCODING_OPUS,
            AudioFormat.ENCODING_MPEGH_BL_L3,
            AudioFormat.ENCODING_MPEGH_BL_L4,
            AudioFormat.ENCODING_MPEGH_LC_L3,
            AudioFormat.ENCODING_MPEGH_LC_L4,
            AudioFormat.ENCODING_DTS_UHD,
            AudioFormat.ENCODING_DRA,
        };
        for (int encoding : encodings) {
            final AudioFormat format = new AudioFormat.Builder()
                .setEncoding(encoding)
                .setSampleRate(44100)
                .setChannelMask(AudioFormat.CHANNEL_OUT_STEREO)
                .build();

            assertEquals(""AudioFormat with encoding "" + encoding + "" has the wrong frame size"",
                    1, format.getFrameSizeInBytes());
        }

        final AudioFormat formatPcmFloat = new AudioFormat.Builder()
            .setEncoding(AudioFormat.ENCODING_PCM_FLOAT)
            .setSampleRate(192000)
            .setChannelMask(AudioFormat.CHANNEL_OUT_STEREO)
            .build();

        assertEquals(""Float AudioFormat has the wrong frame size"",
            2 /* channels */ * 4 /* bytes per sample */, formatPcmFloat.getFrameSizeInBytes());
    }

    /**
     * Check whether the bits in a are all present in b.
     *
     * Used for channel position mask verification.
     */
    private boolean subsetOf(int a, int b) {
        return Integer.bitCount(a ^ b) == Integer.bitCount(b) - Integer.bitCount(a);
    }

    /**
     * Test case 8: Check validity of channel masks
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.security.cts.StagefrightTest"	"testStagefright_cve_2018_9531"	"CtsSecurityTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/security/src/android/security/cts/StagefrightTest.java"	""	"@AsbSecurityTest(cveBugId = 112661641)
    public void testStagefright_cve_2018_9531() throws Exception {
        assumeFalse(ModuleDetector.moduleIsPlayManaged(
                getInstrumentation().getContext().getPackageManager(),
                MainlineModule.MEDIA_SOFTWARE_CODEC));
        int[] frameSizes = getFrameSizes(R.raw.cve_2018_9531_framelen);
        CodecConfig codecConfig = new CodecConfig().setAudioParams(48000, 8);
        doStagefrightTestRawBlob(R.raw.cve_2018_9531_aac, ""audio/mp4a-latm"", codecConfig,
                frameSizes, new CrashUtils.Config().setSignals(CrashUtils.SIGSEGV,
                        CrashUtils.SIGBUS, CrashUtils.SIGABRT));
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.ExtractDecodeEditEncodeMuxTest"	"ExtractDecodeEditEncodeMuxTest"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/ExtractDecodeEditEncodeMuxTest.java"	""	"public void test/*
 *.
 */

package android.media.cts;

import android.annotation.TargetApi;
import android.content.res.AssetFileDescriptor;
import android.media.MediaCodec;
import android.media.MediaCodecInfo;
import android.media.MediaCodecInfo.CodecCapabilities;
import android.media.MediaCodecInfo.CodecProfileLevel;
import android.media.MediaCodecList;
import android.media.MediaExtractor;
import android.media.MediaFormat;
import android.media.MediaMuxer;
import android.media.MediaPlayer;
import android.os.Environment;
import android.os.ParcelFileDescriptor;
import android.platform.test.annotations.AppModeFull;
import android.test.ActivityInstrumentationTestCase2;
import android.util.Log;
import android.view.Surface;

import android.media.MediaCodecInfo;
import android.media.MediaCodecInfo.CodecCapabilities;
import android.media.MediaCodecInfo.CodecProfileLevel;

import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.concurrent.atomic.AtomicReference;
import java.util.concurrent.CountDownLatch;

/**
 * Test for the integration of MediaMuxer and MediaCodec's encoder.
 *
 * <p>It uses MediaExtractor to get frames from a test stream, decodes them to a surface, uses a
 * shader to edit them, encodes them from the resulting surface, and then uses MediaMuxer to write
 * them into a file.
 *
 * <p>It does not currently check whether the result file is correct, but makes sure that nothing
 * fails along the way.
 *
 * <p>It also tests the way the codec config buffers need to be passed from the MediaCodec to the
 * MediaMuxer.
 */
@TargetApi(18)
@AppModeFull(reason = ""Instant apps cannot access the SD card"")
public class ExtractDecodeEditEncodeMuxTest
        extends ActivityInstrumentationTestCase2<MediaStubActivity> {

    private static final String TAG = ExtractDecodeEditEncodeMuxTest.class.getSimpleName();
    private static final boolean VERBOSE = false; // lots of logging
    static final String mInpPrefix = WorkDir.getMediaDirString();

    /** How long to wait for the next buffer to become available. */
    private static final int TIMEOUT_USEC = 10000;

    /** Where to output the test files. */
    private static final File OUTPUT_FILENAME_DIR = Environment.getExternalStorageDirectory();

    // parameters for the video encoder
    private static final int OUTPUT_VIDEO_BIT_RATE = 2000000;   // 2Mbps
    private static final int OUTPUT_VIDEO_FRAME_RATE = 15;      // 15fps
    private static final int OUTPUT_VIDEO_IFRAME_INTERVAL = 10; // 10 seconds between I-frames
    private static final int OUTPUT_VIDEO_COLOR_FORMAT =
            MediaCodecInfo.CodecCapabilities.COLOR_FormatSurface;

    // parameters for the audio encoder
                                                                // Advanced Audio Coding
    private static final String OUTPUT_AUDIO_MIME_TYPE = MediaFormat.MIMETYPE_AUDIO_AAC;
    private static final int OUTPUT_AUDIO_CHANNEL_COUNT = 2;    // Must match the input stream.
    private static final int OUTPUT_AUDIO_BIT_RATE = 128 * 1024;
    private static final int OUTPUT_AUDIO_AAC_PROFILE =
            MediaCodecInfo.CodecProfileLevel.AACObjectHE;
    private static final int OUTPUT_AUDIO_SAMPLE_RATE_HZ = 44100; // Must match the input stream.

    /**
     * Used for editing the frames.
     *
     * <p>Swaps green and blue channels by storing an RBGA color in an RGBA buffer.
     */
    private static final String FRAGMENT_SHADER =
            ""#extension GL_OES_EGL_image_external : require\n"" +
            ""precision mediump float;\n"" +
            ""varying vec2 vTextureCoord;\n"" +
            ""uniform samplerExternalOES sTexture;\n"" +
            ""void main() {\n"" +
            ""  gl_FragColor = texture2D(sTexture, vTextureCoord).rbga;\n"" +
            ""}\n"";

    /** Whether to copy the video from the test video. */
    private boolean mCopyVideo;
    /** Whether to copy the audio from the test video. */
    private boolean mCopyAudio;
    /** Whether to verify the audio format. */
    private boolean mVerifyAudioFormat;
    /** Width of the output frames. */
    private int mWidth = -1;
    /** Height of the output frames. */
    private int mHeight = -1;

    /** The raw resource used as the input file. */
    private String mSourceRes;

    /** The destination file for the encoded output. */
    private String mOutputFile;

    private String mOutputVideoMimeType;

    public ExtractDecodeEditEncodeMuxTest() {
        super(MediaStubActivity.class);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.ExtractDecodeEditEncodeMuxTest"	"testExtractDecodeEditEncodeMuxQCIF"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/ExtractDecodeEditEncodeMuxTest.java"	""	"public void testExtractDecodeEditEncodeMuxQCIF() throws Throwable {
        if(!setSize(176, 144)) return;
        setSource(""video_480x360_mp4_h264_500kbps_30fps_aac_stereo_128kbps_44100hz.mp4"");
        setCopyVideo();
        setVideoMimeType(MediaFormat.MIMETYPE_VIDEO_AVC);
        TestWrapper.runTest(this);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.ExtractDecodeEditEncodeMuxTest"	"testExtractDecodeEditEncodeMuxQVGA"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/ExtractDecodeEditEncodeMuxTest.java"	""	"public void testExtractDecodeEditEncodeMuxQVGA() throws Throwable {
        if(!setSize(320, 240)) return;
        setSource(""video_480x360_mp4_h264_500kbps_30fps_aac_stereo_128kbps_44100hz.mp4"");
        setCopyVideo();
        setVideoMimeType(MediaFormat.MIMETYPE_VIDEO_AVC);
        TestWrapper.runTest(this);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.ExtractDecodeEditEncodeMuxTest"	"testExtractDecodeEditEncodeMux720p"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/ExtractDecodeEditEncodeMuxTest.java"	""	"public void testExtractDecodeEditEncodeMux720p() throws Throwable {
        if(!setSize(1280, 720)) return;
        setSource(""video_480x360_mp4_h264_500kbps_30fps_aac_stereo_128kbps_44100hz.mp4"");
        setCopyVideo();
        setVideoMimeType(MediaFormat.MIMETYPE_VIDEO_AVC);
        TestWrapper.runTest(this);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.ExtractDecodeEditEncodeMuxTest"	"testExtractDecodeEditEncodeMux2160pHevc"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/ExtractDecodeEditEncodeMuxTest.java"	""	"public void testExtractDecodeEditEncodeMux2160pHevc() throws Throwable {
        if(!setSize(3840, 2160)) return;
        setSource(""video_480x360_mp4_h264_500kbps_30fps_aac_stereo_128kbps_44100hz.mp4"");
        setCopyVideo();
        setVideoMimeType(MediaFormat.MIMETYPE_VIDEO_HEVC);
        TestWrapper.runTest(this);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.ExtractDecodeEditEncodeMuxTest"	"testExtractDecodeEditEncodeMuxAudio"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/ExtractDecodeEditEncodeMuxTest.java"	""	"public void testExtractDecodeEditEncodeMuxAudio() throws Throwable {
        if(!setSize(1280, 720)) return;
        setSource(""video_480x360_mp4_h264_500kbps_30fps_aac_stereo_128kbps_44100hz.mp4"");
        setCopyAudio();
        setVerifyAudioFormat();
        TestWrapper.runTest(this);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.ExtractDecodeEditEncodeMuxTest"	"testExtractDecodeEditEncodeMuxAudioVideo"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/ExtractDecodeEditEncodeMuxTest.java"	""	"public void testExtractDecodeEditEncodeMuxAudioVideo() throws Throwable {
        if(!setSize(1280, 720)) return;
        setSource(""video_480x360_mp4_h264_500kbps_30fps_aac_stereo_128kbps_44100hz.mp4"");
        setCopyAudio();
        setCopyVideo();
        setVerifyAudioFormat();
        TestWrapper.runTest(this);
    }

    /** Wraps testExtractDecodeEditEncodeMux() */
    private static class TestWrapper implements Runnable {
        private Throwable mThrowable;
        private ExtractDecodeEditEncodeMuxTest mTest;

        private TestWrapper(ExtractDecodeEditEncodeMuxTest test) {
            mTest = test;
        }

        @Override
        public void run() {
            try {
                mTest.extractDecodeEditEncodeMux();
            } catch (Throwable th) {
                mThrowable = th;
            }
        }

        /**
         * Entry point.
         */
        public static void runTest(ExtractDecodeEditEncodeMuxTest test) throws Throwable {
            test.setOutputFile();
            TestWrapper wrapper = new TestWrapper(test);
            Thread th = new Thread(wrapper, ""codec test"");
            th.start();
            th.join();
            if (wrapper.mThrowable != null) {
                throw wrapper.mThrowable;
            }
        }
    }

    /**
     * Sets the test to copy the video stream.
     */
    private void setCopyVideo() {
        mCopyVideo = true;
    }

    /**
     * Sets the test to copy the video stream.
     */
    private void setCopyAudio() {
        mCopyAudio = true;
    }

    /**
     * Sets the test to verify the output audio format.
     */
    private void setVerifyAudioFormat() {
        mVerifyAudioFormat = true;
    }

    /**
     * Sets the desired frame size and returns whether the given resolution is
     * supported.
     *
     * <p>If decoding/encoding using AVC as the codec, checks that the resolution
     * is supported. For other codecs, always return {@code true}.
     */
    private boolean setSize(int width, int height) {
        if ((width % 16) != 0 || (height % 16) != 0) {
            Log.w(TAG, ""WARNING: width or height not multiple of 16"");
        }
        mWidth = width;
        mHeight = height;

        // TODO: remove this logic in setSize as it is now handled when configuring codecs
        return true;
    }

    /**
     * Sets the raw resource used as the source video.
     */
    private void setSource(String res) {
        mSourceRes = res;
    }

    /**
     * Sets the name of the output file based on the other parameters.
     *
     * <p>Must be called after {@link #setSize(int, int)} and {@link #setSource(String)}.
     */
    private void setOutputFile() {
        StringBuilder sb = new StringBuilder();
        sb.append(OUTPUT_FILENAME_DIR.getAbsolutePath());
        sb.append(""/cts-media-"");
        sb.append(getClass().getSimpleName());
        assertTrue(""should have called setSource() first"", mSourceRes != null);
        sb.append('-');
        sb.append(mSourceRes);
        if (mCopyVideo) {
            assertTrue(""should have called setSize() first"", mWidth != -1);
            assertTrue(""should have called setSize() first"", mHeight != -1);
            sb.append('-');
            sb.append(""video"");
            sb.append('-');
            sb.append(mWidth);
            sb.append('x');
            sb.append(mHeight);
        }
        if (mCopyAudio) {
            sb.append('-');
            sb.append(""audio"");
        }
        sb.append("".mp4"");
        mOutputFile = sb.toString();
    }

    private void setVideoMimeType(String mimeType) {
        mOutputVideoMimeType = mimeType;
    }

    /**
     * Tests encoding and subsequently decoding video from frames generated into a buffer.
     * <p>
     * We encode several frames of a video test pattern using MediaCodec, then decode the output
     * with MediaCodec and do some simple checks.
     */
    private void extractDecodeEditEncodeMux() throws Exception {
        // Exception that may be thrown during release.
        Exception exception = null;

        MediaCodecList mcl = new MediaCodecList(MediaCodecList.REGULAR_CODECS);

        // We avoid the device-specific limitations on width and height by using values
        // that are multiples of 16, which all tested devices seem to be able to handle.
        MediaFormat outputVideoFormat =
                MediaFormat.createVideoFormat(mOutputVideoMimeType, mWidth, mHeight);

        // Set some properties. Failing to specify some of these can cause the MediaCodec
        // configure() call to throw an unhelpful exception.
        outputVideoFormat.setInteger(
                MediaFormat.KEY_COLOR_FORMAT, OUTPUT_VIDEO_COLOR_FORMAT);
        outputVideoFormat.setInteger(MediaFormat.KEY_BIT_RATE, OUTPUT_VIDEO_BIT_RATE);
        outputVideoFormat.setInteger(MediaFormat.KEY_FRAME_RATE, OUTPUT_VIDEO_FRAME_RATE);
        outputVideoFormat.setInteger(
                MediaFormat.KEY_I_FRAME_INTERVAL, OUTPUT_VIDEO_IFRAME_INTERVAL);
        if (VERBOSE) Log.d(TAG, ""video format: "" + outputVideoFormat);

        String videoEncoderName = mcl.findEncoderForFormat(outputVideoFormat);
        if (videoEncoderName == null) {
            // Don't fail CTS if they don't have an AVC codec (not here, anyway).
            Log.e(TAG, ""Unable to find an appropriate codec for "" + outputVideoFormat);
            return;
        }
        if (VERBOSE) Log.d(TAG, ""video found codec: "" + videoEncoderName);

        MediaFormat outputAudioFormat =
                MediaFormat.createAudioFormat(
                        OUTPUT_AUDIO_MIME_TYPE, OUTPUT_AUDIO_SAMPLE_RATE_HZ,
                        OUTPUT_AUDIO_CHANNEL_COUNT);
        outputAudioFormat.setInteger(MediaFormat.KEY_BIT_RATE, OUTPUT_AUDIO_BIT_RATE);
        // TODO: Bug workaround --- uncomment once fixed.
        // outputAudioFormat.setInteger(MediaFormat.KEY_AAC_PROFILE, OUTPUT_AUDIO_AAC_PROFILE);

        String audioEncoderName = mcl.findEncoderForFormat(outputAudioFormat);
        if (audioEncoderName == null) {
            // Don't fail CTS if they don't have an AAC codec (not here, anyway).
            Log.e(TAG, ""Unable to find an appropriate codec for "" + outputAudioFormat);
            return;
        }
        if (VERBOSE) Log.d(TAG, ""audio found codec: "" + audioEncoderName);

        MediaExtractor videoExtractor = null;
        MediaExtractor audioExtractor = null;
        OutputSurface outputSurface = null;
        MediaCodec videoDecoder = null;
        MediaCodec audioDecoder = null;
        MediaCodec videoEncoder = null;
        MediaCodec audioEncoder = null;
        MediaMuxer muxer = null;

        InputSurface inputSurface = null;

        try {
            if (mCopyVideo) {
                videoExtractor = createExtractor();
                int videoInputTrack = getAndSelectVideoTrackIndex(videoExtractor);
                assertTrue(""missing video track in test video"", videoInputTrack != -1);
                MediaFormat inputFormat = videoExtractor.getTrackFormat(videoInputTrack);

                // Create a MediaCodec for the desired codec, then configure it as an encoder with
                // our desired properties. Request a Surface to use for input.
                AtomicReference<Surface> inputSurfaceReference = new AtomicReference<Surface>();
                videoEncoder = createVideoEncoder(
                        videoEncoderName, outputVideoFormat, inputSurfaceReference);
                inputSurface = new InputSurface(inputSurfaceReference.get());
                inputSurface.makeCurrent();
                // Create a MediaCodec for the decoder, based on the extractor's format.
                outputSurface = new OutputSurface();
                outputSurface.changeFragmentShader(FRAGMENT_SHADER);
                videoDecoder = createVideoDecoder(mcl, inputFormat, outputSurface.getSurface());
            }

            if (mCopyAudio) {
                audioExtractor = createExtractor();
                int audioInputTrack = getAndSelectAudioTrackIndex(audioExtractor);
                assertTrue(""missing audio track in test video"", audioInputTrack != -1);
                MediaFormat inputFormat = audioExtractor.getTrackFormat(audioInputTrack);

                // Create a MediaCodec for the desired codec, then configure it as an encoder with
                // our desired properties. Request a Surface to use for input.
                audioEncoder = createAudioEncoder(audioEncoderName, outputAudioFormat);
                // Create a MediaCodec for the decoder, based on the extractor's format.
                audioDecoder = createAudioDecoder(mcl, inputFormat);
            }

            // Creates a muxer but do not start or add tracks just yet.
            muxer = createMuxer();

            doExtractDecodeEditEncodeMux(
                    videoExtractor,
                    audioExtractor,
                    videoDecoder,
                    videoEncoder,
                    audioDecoder,
                    audioEncoder,
                    muxer,
                    inputSurface,
                    outputSurface);
        } finally {
            if (VERBOSE) Log.d(TAG, ""releasing extractor, decoder, encoder, and muxer"");
            // Try to release everything we acquired, even if one of the releases fails, in which
            // case we save the first exception we got and re-throw at the end (unless something
            // other exception has already been thrown). This guarantees the first exception thrown
            // is reported as the cause of the error, everything is (attempted) to be released, and
            // all other exceptions appear in the logs.
            try {
                if (videoExtractor != null) {
                    videoExtractor.release();
                }
            } catch(Exception e) {
                Log.e(TAG, ""error while releasing videoExtractor"", e);
                if (exception == null) {
                    exception = e;
                }
            }
            try {
                if (audioExtractor != null) {
                    audioExtractor.release();
                }
            } catch(Exception e) {
                Log.e(TAG, ""error while releasing audioExtractor"", e);
                if (exception == null) {
                    exception = e;
                }
            }
            try {
                if (videoDecoder != null) {
                    videoDecoder.stop();
                    videoDecoder.release();
                }
            } catch(Exception e) {
                Log.e(TAG, ""error while releasing videoDecoder"", e);
                if (exception == null) {
                    exception = e;
                }
            }
            try {
                if (outputSurface != null) {
                    outputSurface.release();
                }
            } catch(Exception e) {
                Log.e(TAG, ""error while releasing outputSurface"", e);
                if (exception == null) {
                    exception = e;
                }
            }
            try {
                if (videoEncoder != null) {
                    videoEncoder.stop();
                    videoEncoder.release();
                }
            } catch(Exception e) {
                Log.e(TAG, ""error while releasing videoEncoder"", e);
                if (exception == null) {
                    exception = e;
                }
            }
            try {
                if (audioDecoder != null) {
                    audioDecoder.stop();
                    audioDecoder.release();
                }
            } catch(Exception e) {
                Log.e(TAG, ""error while releasing audioDecoder"", e);
                if (exception == null) {
                    exception = e;
                }
            }
            try {
                if (audioEncoder != null) {
                    audioEncoder.stop();
                    audioEncoder.release();
                }
            } catch(Exception e) {
                Log.e(TAG, ""error while releasing audioEncoder"", e);
                if (exception == null) {
                    exception = e;
                }
            }
            try {
                if (muxer != null) {
                    muxer.stop();
                    muxer.release();
                }
            } catch(Exception e) {
                Log.e(TAG, ""error while releasing muxer"", e);
                if (exception == null) {
                    exception = e;
                }
            }
            try {
                if (inputSurface != null) {
                    inputSurface.release();
                }
            } catch(Exception e) {
                Log.e(TAG, ""error while releasing inputSurface"", e);
                if (exception == null) {
                    exception = e;
                }
            }
        }
        if (exception != null) {
            throw exception;
        }

        MediaExtractor mediaExtractor = null;
        try {
            mediaExtractor = new MediaExtractor();
            mediaExtractor.setDataSource(mOutputFile);

            assertEquals(""incorrect number of tracks"", (mCopyAudio ? 1 : 0) + (mCopyVideo ? 1 : 0),
                    mediaExtractor.getTrackCount());
            if (mVerifyAudioFormat) {
                boolean foundAudio = false;
                for (int i = 0; i < mediaExtractor.getTrackCount(); i++) {
                    MediaFormat trackFormat = mediaExtractor.getTrackFormat(i);
                    if (isAudioFormat(trackFormat)) {
                        foundAudio = true;
                        int expectedSampleRate = OUTPUT_AUDIO_SAMPLE_RATE_HZ;

                        // SBR mode halves the sample rate in the format.
                        if (OUTPUT_AUDIO_AAC_PROFILE ==
                                MediaCodecInfo.CodecProfileLevel.AACObjectHE) {
                            expectedSampleRate /= 2;
                        }
                        assertEquals(""sample rates should match"", expectedSampleRate,
                                trackFormat.getInteger(MediaFormat.KEY_SAMPLE_RATE));
                    }
                }

                assertTrue(""output should have an audio track"", foundAudio || !mCopyAudio);
            }
        } catch (IOException e) {
            throw new IllegalStateException(""exception verifying output file"", e);
        } finally {
            if (mediaExtractor != null) {
                mediaExtractor.release();
            }
        }

        // TODO: Check the generated output file's video format and sample data.

        MediaStubActivity activity = getActivity();
        final MediaPlayer mp = new MediaPlayer();
        final Exception[] exceptionHolder = { null };
        final CountDownLatch playbackEndSignal = new CountDownLatch(1);
        mp.setOnErrorListener(new MediaPlayer.OnErrorListener() {
            @Override
            public boolean onError(MediaPlayer origin, int what, int extra) {
                exceptionHolder[0] = new RuntimeException(""error playing output file: what="" + what
                        + "" extra="" + extra);
                // Returning false would trigger onCompletion() so that
                // playbackEndSignal.await() can stop waiting.
                return false;
            }
        });
        mp.setOnCompletionListener(new MediaPlayer.OnCompletionListener() {
            @Override
            public void onCompletion(MediaPlayer origin) {
                playbackEndSignal.countDown();
            }
        });
        try {
            mp.setDataSource(mOutputFile);
            mp.setDisplay(activity.getSurfaceHolder());
            mp.prepare();
            mp.start();
            playbackEndSignal.await();
        } catch (Exception e) {
            exceptionHolder[0] = e;
        } finally {
            mp.release();
        }

        if (exceptionHolder[0] != null) {
            throw exceptionHolder[0];
        }
    }

    protected AssetFileDescriptor getAssetFileDescriptorFor(final String res)
            throws FileNotFoundException {
        Preconditions.assertTestFileExists(mInpPrefix + res);
        File inpFile = new File(mInpPrefix + res);
        ParcelFileDescriptor parcelFD =
                ParcelFileDescriptor.open(inpFile, ParcelFileDescriptor.MODE_READ_ONLY);
        return new AssetFileDescriptor(parcelFD, 0, parcelFD.getStatSize());
    }

    /**
     * Creates an extractor that reads its frames from {@link #mSourceRes}.
     */
    private MediaExtractor createExtractor() throws IOException {
        MediaExtractor extractor;
        AssetFileDescriptor srcFd = getAssetFileDescriptorFor(mSourceRes);
        extractor = new MediaExtractor();
        extractor.setDataSource(srcFd.getFileDescriptor(), srcFd.getStartOffset(),
                srcFd.getLength());
        return extractor;
    }

    /**
     * Creates a decoder for the given format, which outputs to the given surface.
     *
     * @param inputFormat the format of the stream to decode
     * @param surface into which to decode the frames
     */
    private MediaCodec createVideoDecoder(
            MediaCodecList mcl, MediaFormat inputFormat, Surface surface) throws IOException {
        MediaCodec decoder = MediaCodec.createByCodecName(mcl.findDecoderForFormat(inputFormat));
        decoder.configure(inputFormat, surface, null, 0);
        decoder.start();
        return decoder;
    }

    /**
     * Creates an encoder for the given format using the specified codec, taking input from a
     * surface.
     *
     * <p>The surface to use as input is stored in the given reference.
     *
     * @param codecInfo of the codec to use
     * @param format of the stream to be produced
     * @param surfaceReference to store the surface to use as input
     */
    private MediaCodec createVideoEncoder(
            String codecName,
            MediaFormat format,
            AtomicReference<Surface> surfaceReference)
            throws IOException {
        MediaCodec encoder = MediaCodec.createByCodecName(codecName);
        encoder.configure(format, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE);
        // Must be called before start() is.
        surfaceReference.set(encoder.createInputSurface());
        encoder.start();
        return encoder;
    }

    /**
     * Creates a decoder for the given format.
     *
     * @param inputFormat the format of the stream to decode
     */
    private MediaCodec createAudioDecoder(
            MediaCodecList mcl, MediaFormat inputFormat) throws IOException {
        MediaCodec decoder = MediaCodec.createByCodecName(mcl.findDecoderForFormat(inputFormat));
        decoder.configure(inputFormat, null, null, 0);
        decoder.start();
        return decoder;
    }

    /**
     * Creates an encoder for the given format using the specified codec.
     *
     * @param codecInfo of the codec to use
     * @param format of the stream to be produced
     */
    private MediaCodec createAudioEncoder(String codecName, MediaFormat format)
            throws IOException {
        MediaCodec encoder = MediaCodec.createByCodecName(codecName);
        encoder.configure(format, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE);
        encoder.start();
        return encoder;
    }

    /**
     * Creates a muxer to write the encoded frames.
     *
     * <p>The muxer is not started as it needs to be started only after all streams have been added.
     */
    private MediaMuxer createMuxer() throws IOException {
        return new MediaMuxer(mOutputFile, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
    }

    private int getAndSelectVideoTrackIndex(MediaExtractor extractor) {
        for (int index = 0; index < extractor.getTrackCount(); ++index) {
            if (VERBOSE) {
                Log.d(TAG, ""format for track "" + index + "" is ""
                        + getMimeTypeFor(extractor.getTrackFormat(index)));
            }
            if (isVideoFormat(extractor.getTrackFormat(index))) {
                extractor.selectTrack(index);
                return index;
            }
        }
        return -1;
    }

    private int getAndSelectAudioTrackIndex(MediaExtractor extractor) {
        for (int index = 0; index < extractor.getTrackCount(); ++index) {
            if (VERBOSE) {
                Log.d(TAG, ""format for track "" + index + "" is ""
                        + getMimeTypeFor(extractor.getTrackFormat(index)));
            }
            if (isAudioFormat(extractor.getTrackFormat(index))) {
                extractor.selectTrack(index);
                return index;
            }
        }
        return -1;
    }

    /**
     * Does the actual work for extracting, decoding, encoding and muxing.
     */
    private void doExtractDecodeEditEncodeMux(
            MediaExtractor videoExtractor,
            MediaExtractor audioExtractor,
            MediaCodec videoDecoder,
            MediaCodec videoEncoder,
            MediaCodec audioDecoder,
            MediaCodec audioEncoder,
            MediaMuxer muxer,
            InputSurface inputSurface,
            OutputSurface outputSurface) {
        ByteBuffer[] videoDecoderInputBuffers = null;
        ByteBuffer[] videoDecoderOutputBuffers = null;
        ByteBuffer[] videoEncoderOutputBuffers = null;
        MediaCodec.BufferInfo videoDecoderOutputBufferInfo = null;
        MediaCodec.BufferInfo videoEncoderOutputBufferInfo = null;
        if (mCopyVideo) {
            videoDecoderInputBuffers = videoDecoder.getInputBuffers();
            videoDecoderOutputBuffers = videoDecoder.getOutputBuffers();
            videoEncoderOutputBuffers = videoEncoder.getOutputBuffers();
            videoDecoderOutputBufferInfo = new MediaCodec.BufferInfo();
            videoEncoderOutputBufferInfo = new MediaCodec.BufferInfo();
        }
        ByteBuffer[] audioDecoderInputBuffers = null;
        ByteBuffer[] audioDecoderOutputBuffers = null;
        ByteBuffer[] audioEncoderInputBuffers = null;
        ByteBuffer[] audioEncoderOutputBuffers = null;
        MediaCodec.BufferInfo audioDecoderOutputBufferInfo = null;
        MediaCodec.BufferInfo audioEncoderOutputBufferInfo = null;
        if (mCopyAudio) {
            audioDecoderInputBuffers = audioDecoder.getInputBuffers();
            audioDecoderOutputBuffers =  audioDecoder.getOutputBuffers();
            audioEncoderInputBuffers = audioEncoder.getInputBuffers();
            audioEncoderOutputBuffers = audioEncoder.getOutputBuffers();
            audioDecoderOutputBufferInfo = new MediaCodec.BufferInfo();
            audioEncoderOutputBufferInfo = new MediaCodec.BufferInfo();
        }
        // We will get these from the decoders when notified of a format change.
        MediaFormat decoderOutputVideoFormat = null;
        MediaFormat decoderOutputAudioFormat = null;
        // We will get these from the encoders when notified of a format change.
        MediaFormat encoderOutputVideoFormat = null;
        MediaFormat encoderOutputAudioFormat = null;
        // We will determine these once we have the output format.
        int outputVideoTrack = -1;
        int outputAudioTrack = -1;
        // Whether things are done on the video side.
        boolean videoExtractorDone = false;
        boolean videoDecoderDone = false;
        boolean videoEncoderDone = false;
        // Whether things are done on the audio side.
        boolean audioExtractorDone = false;
        boolean audioDecoderDone = false;
        boolean audioEncoderDone = false;
        // The audio decoder output buffer to process, -1 if none.
        int pendingAudioDecoderOutputBufferIndex = -1;

        boolean muxing = false;

        int videoExtractedFrameCount = 0;
        int videoDecodedFrameCount = 0;
        int videoEncodedFrameCount = 0;

        int audioExtractedFrameCount = 0;
        int audioDecodedFrameCount = 0;
        int audioEncodedFrameCount = 0;

        while ((mCopyVideo && !videoEncoderDone) || (mCopyAudio && !audioEncoderDone)) {
            if (VERBOSE) {
                Log.d(TAG, String.format(
                        ""loop: ""

                        + ""V(%b){""
                        + ""extracted:%d(done:%b) ""
                        + ""decoded:%d(done:%b) ""
                        + ""encoded:%d(done:%b)} ""

                        + ""A(%b){""
                        + ""extracted:%d(done:%b) ""
                        + ""decoded:%d(done:%b) ""
                        + ""encoded:%d(done:%b) ""
                        + ""pending:%d} ""

                        + ""muxing:%b(V:%d,A:%d)"",

                        mCopyVideo,
                        videoExtractedFrameCount, videoExtractorDone,
                        videoDecodedFrameCount, videoDecoderDone,
                        videoEncodedFrameCount, videoEncoderDone,

                        mCopyAudio,
                        audioExtractedFrameCount, audioExtractorDone,
                        audioDecodedFrameCount, audioDecoderDone,
                        audioEncodedFrameCount, audioEncoderDone,
                        pendingAudioDecoderOutputBufferIndex,

                        muxing, outputVideoTrack, outputAudioTrack));
            }

            // Extract video from file and feed to decoder.
            // Do not extract video if we have determined the output format but we are not yet
            // ready to mux the frames.
            while (mCopyVideo && !videoExtractorDone
                    && (encoderOutputVideoFormat == null || muxing)) {
                int decoderInputBufferIndex = videoDecoder.dequeueInputBuffer(TIMEOUT_USEC);
                if (decoderInputBufferIndex == MediaCodec.INFO_TRY_AGAIN_LATER) {
                    if (VERBOSE) Log.d(TAG, ""no video decoder input buffer"");
                    break;
                }
                if (VERBOSE) {
                    Log.d(TAG, ""video decoder: returned input buffer: "" + decoderInputBufferIndex);
                }
                ByteBuffer decoderInputBuffer = videoDecoderInputBuffers[decoderInputBufferIndex];
                int size = videoExtractor.readSampleData(decoderInputBuffer, 0);
                long presentationTime = videoExtractor.getSampleTime();
                int flags = videoExtractor.getSampleFlags();
                if (VERBOSE) {
                    Log.d(TAG, ""video extractor: returned buffer of size "" + size);
                    Log.d(TAG, ""video extractor: returned buffer for time "" + presentationTime);
                }
                videoExtractorDone = !videoExtractor.advance();
                if (videoExtractorDone) {
                    if (VERBOSE) Log.d(TAG, ""video extractor: EOS"");
                    flags = flags | MediaCodec.BUFFER_FLAG_END_OF_STREAM;
                }
                if (size >= 0) {
                    videoDecoder.queueInputBuffer(
                            decoderInputBufferIndex,
                            0,
                            size,
                            presentationTime,
                            flags);
                    videoExtractedFrameCount++;
                }
                // We extracted a frame, let's try something else next.
                break;
            }

            // Extract audio from file and feed to decoder.
            // Do not extract audio if we have determined the output format but we are not yet
            // ready to mux the frames.
            while (mCopyAudio && !audioExtractorDone
                    && (encoderOutputAudioFormat == null || muxing)) {
                int decoderInputBufferIndex = audioDecoder.dequeueInputBuffer(TIMEOUT_USEC);
                if (decoderInputBufferIndex == MediaCodec.INFO_TRY_AGAIN_LATER) {
                    if (VERBOSE) Log.d(TAG, ""no audio decoder input buffer"");
                    break;
                }
                if (VERBOSE) {
                    Log.d(TAG, ""audio decoder: returned input buffer: "" + decoderInputBufferIndex);
                }
                ByteBuffer decoderInputBuffer = audioDecoderInputBuffers[decoderInputBufferIndex];
                int size = audioExtractor.readSampleData(decoderInputBuffer, 0);
                long presentationTime = audioExtractor.getSampleTime();
                if (VERBOSE) {
                    Log.d(TAG, ""audio extractor: returned buffer of size "" + size);
                    Log.d(TAG, ""audio extractor: returned buffer for time "" + presentationTime);
                }
                if (size >= 0) {
                    audioDecoder.queueInputBuffer(
                            decoderInputBufferIndex,
                            0,
                            size,
                            presentationTime,
                            audioExtractor.getSampleFlags());
                }
                audioExtractorDone = !audioExtractor.advance();
                if (audioExtractorDone) {
                    if (VERBOSE) Log.d(TAG, ""audio extractor: EOS"");
                    audioDecoder.queueInputBuffer(
                            decoderInputBufferIndex,
                            0,
                            0,
                            0,
                            MediaCodec.BUFFER_FLAG_END_OF_STREAM);
                }
                audioExtractedFrameCount++;
                // We extracted a frame, let's try something else next.
                break;
            }

            // Poll output frames from the video decoder and feed the encoder.
            while (mCopyVideo && !videoDecoderDone
                    && (encoderOutputVideoFormat == null || muxing)) {
                int decoderOutputBufferIndex =
                        videoDecoder.dequeueOutputBuffer(
                                videoDecoderOutputBufferInfo, TIMEOUT_USEC);
                if (decoderOutputBufferIndex == MediaCodec.INFO_TRY_AGAIN_LATER) {
                    if (VERBOSE) Log.d(TAG, ""no video decoder output buffer"");
                    break;
                }
                if (decoderOutputBufferIndex == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                    if (VERBOSE) Log.d(TAG, ""video decoder: output buffers changed"");
                    videoDecoderOutputBuffers = videoDecoder.getOutputBuffers();
                    break;
                }
                if (decoderOutputBufferIndex == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                    decoderOutputVideoFormat = videoDecoder.getOutputFormat();
                    if (VERBOSE) {
                        Log.d(TAG, ""video decoder: output format changed: ""
                                + decoderOutputVideoFormat);
                    }
                    break;
                }
                if (VERBOSE) {
                    Log.d(TAG, ""video decoder: returned output buffer: ""
                            + decoderOutputBufferIndex);
                    Log.d(TAG, ""video decoder: returned buffer of size ""
                            + videoDecoderOutputBufferInfo.size);
                }
                ByteBuffer decoderOutputBuffer =
                        videoDecoderOutputBuffers[decoderOutputBufferIndex];
                if ((videoDecoderOutputBufferInfo.flags & MediaCodec.BUFFER_FLAG_CODEC_CONFIG)
                        != 0) {
                    if (VERBOSE) Log.d(TAG, ""video decoder: codec config buffer"");
                    videoDecoder.releaseOutputBuffer(decoderOutputBufferIndex, false);
                    break;
                }
                if (VERBOSE) {
                    Log.d(TAG, ""video decoder: returned buffer for time ""
                            + videoDecoderOutputBufferInfo.presentationTimeUs);
                }
                boolean render = videoDecoderOutputBufferInfo.size != 0;
                videoDecoder.releaseOutputBuffer(decoderOutputBufferIndex, render);
                if (render) {
                    if (VERBOSE) Log.d(TAG, ""output surface: await new image"");
                    outputSurface.awaitNewImage();
                    // Edit the frame and send it to the encoder.
                    if (VERBOSE) Log.d(TAG, ""output surface: draw image"");
                    outputSurface.drawImage();
                    inputSurface.setPresentationTime(
                            videoDecoderOutputBufferInfo.presentationTimeUs * 1000);
                    if (VERBOSE) Log.d(TAG, ""input surface: swap buffers"");
                    inputSurface.swapBuffers();
                    if (VERBOSE) Log.d(TAG, ""video encoder: notified of new frame"");
                    videoDecodedFrameCount++;
                }
                if ((videoDecoderOutputBufferInfo.flags
                        & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                    if (VERBOSE) Log.d(TAG, ""video decoder: EOS"");
                    videoDecoderDone = true;
                    videoEncoder.signalEndOfInputStream();
                }
                // We extracted a pending frame, let's try something else next.
                break;
            }

            // Poll output frames from the audio decoder.
            // Do not poll if we already have a pending buffer to feed to the encoder.
            while (mCopyAudio && !audioDecoderDone && pendingAudioDecoderOutputBufferIndex == -1
                    && (encoderOutputAudioFormat == null || muxing)) {
                int decoderOutputBufferIndex =
                        audioDecoder.dequeueOutputBuffer(
                                audioDecoderOutputBufferInfo, TIMEOUT_USEC);
                if (decoderOutputBufferIndex == MediaCodec.INFO_TRY_AGAIN_LATER) {
                    if (VERBOSE) Log.d(TAG, ""no audio decoder output buffer"");
                    break;
                }
                if (decoderOutputBufferIndex == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                    if (VERBOSE) Log.d(TAG, ""audio decoder: output buffers changed"");
                    audioDecoderOutputBuffers = audioDecoder.getOutputBuffers();
                    break;
                }
                if (decoderOutputBufferIndex == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                    decoderOutputAudioFormat = audioDecoder.getOutputFormat();
                    if (VERBOSE) {
                        Log.d(TAG, ""audio decoder: output format changed: ""
                                + decoderOutputAudioFormat);
                    }
                    break;
                }
                if (VERBOSE) {
                    Log.d(TAG, ""audio decoder: returned output buffer: ""
                            + decoderOutputBufferIndex);
                }
                if (VERBOSE) {
                    Log.d(TAG, ""audio decoder: returned buffer of size ""
                            + audioDecoderOutputBufferInfo.size);
                }
                ByteBuffer decoderOutputBuffer =
                        audioDecoderOutputBuffers[decoderOutputBufferIndex];
                if ((audioDecoderOutputBufferInfo.flags & MediaCodec.BUFFER_FLAG_CODEC_CONFIG)
                        != 0) {
                    if (VERBOSE) Log.d(TAG, ""audio decoder: codec config buffer"");
                    audioDecoder.releaseOutputBuffer(decoderOutputBufferIndex, false);
                    break;
                }
                if (VERBOSE) {
                    Log.d(TAG, ""audio decoder: returned buffer for time ""
                            + audioDecoderOutputBufferInfo.presentationTimeUs);
                }
                if (VERBOSE) {
                    Log.d(TAG, ""audio decoder: output buffer is now pending: ""
                            + pendingAudioDecoderOutputBufferIndex);
                }
                pendingAudioDecoderOutputBufferIndex = decoderOutputBufferIndex;
                audioDecodedFrameCount++;
                // We extracted a pending frame, let's try something else next.
                break;
            }

            // Feed the pending decoded audio buffer to the audio encoder.
            while (mCopyAudio && pendingAudioDecoderOutputBufferIndex != -1) {
                if (VERBOSE) {
                    Log.d(TAG, ""audio decoder: attempting to process pending buffer: ""
                            + pendingAudioDecoderOutputBufferIndex);
                }
                int encoderInputBufferIndex = audioEncoder.dequeueInputBuffer(TIMEOUT_USEC);
                if (encoderInputBufferIndex == MediaCodec.INFO_TRY_AGAIN_LATER) {
                    if (VERBOSE) Log.d(TAG, ""no audio encoder input buffer"");
                    break;
                }
                if (VERBOSE) {
                    Log.d(TAG, ""audio encoder: returned input buffer: "" + encoderInputBufferIndex);
                }
                ByteBuffer encoderInputBuffer = audioEncoderInputBuffers[encoderInputBufferIndex];
                int size = audioDecoderOutputBufferInfo.size;
                long presentationTime = audioDecoderOutputBufferInfo.presentationTimeUs;
                if (VERBOSE) {
                    Log.d(TAG, ""audio decoder: processing pending buffer: ""
                            + pendingAudioDecoderOutputBufferIndex);
                }
                if (VERBOSE) {
                    Log.d(TAG, ""audio decoder: pending buffer of size "" + size);
                    Log.d(TAG, ""audio decoder: pending buffer for time "" + presentationTime);
                }
                if (size >= 0) {
                    ByteBuffer decoderOutputBuffer =
                            audioDecoderOutputBuffers[pendingAudioDecoderOutputBufferIndex]
                                    .duplicate();
                    decoderOutputBuffer.position(audioDecoderOutputBufferInfo.offset);
                    decoderOutputBuffer.limit(audioDecoderOutputBufferInfo.offset + size);
                    encoderInputBuffer.position(0);
                    encoderInputBuffer.put(decoderOutputBuffer);

                    audioEncoder.queueInputBuffer(
                            encoderInputBufferIndex,
                            0,
                            size,
                            presentationTime,
                            audioDecoderOutputBufferInfo.flags);
                }
                audioDecoder.releaseOutputBuffer(pendingAudioDecoderOutputBufferIndex, false);
                pendingAudioDecoderOutputBufferIndex = -1;
                if ((audioDecoderOutputBufferInfo.flags
                        & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                    if (VERBOSE) Log.d(TAG, ""audio decoder: EOS"");
                    audioDecoderDone = true;
                }
                // We enqueued a pending frame, let's try something else next.
                break;
            }

            // Poll frames from the video encoder and send them to the muxer.
            while (mCopyVideo && !videoEncoderDone
                    && (encoderOutputVideoFormat == null || muxing)) {
                int encoderOutputBufferIndex = videoEncoder.dequeueOutputBuffer(
                        videoEncoderOutputBufferInfo, TIMEOUT_USEC);
                if (encoderOutputBufferIndex == MediaCodec.INFO_TRY_AGAIN_LATER) {
                    if (VERBOSE) Log.d(TAG, ""no video encoder output buffer"");
                    break;
                }
                if (encoderOutputBufferIndex == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                    if (VERBOSE) Log.d(TAG, ""video encoder: output buffers changed"");
                    videoEncoderOutputBuffers = videoEncoder.getOutputBuffers();
                    break;
                }
                if (encoderOutputBufferIndex == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                    if (VERBOSE) Log.d(TAG, ""video encoder: output format changed"");
                    if (outputVideoTrack >= 0) {
                        fail(""video encoder changed its output format again?"");
                    }
                    encoderOutputVideoFormat = videoEncoder.getOutputFormat();
                    break;
                }
                assertTrue(""should have added track before processing output"", muxing);
                if (VERBOSE) {
                    Log.d(TAG, ""video encoder: returned output buffer: ""
                            + encoderOutputBufferIndex);
                    Log.d(TAG, ""video encoder: returned buffer of size ""
                            + videoEncoderOutputBufferInfo.size);
                }
                ByteBuffer encoderOutputBuffer =
                        videoEncoderOutputBuffers[encoderOutputBufferIndex];
                if ((videoEncoderOutputBufferInfo.flags & MediaCodec.BUFFER_FLAG_CODEC_CONFIG)
                        != 0) {
                    if (VERBOSE) Log.d(TAG, ""video encoder: codec config buffer"");
                    // Simply ignore codec config buffers.
                    videoEncoder.releaseOutputBuffer(encoderOutputBufferIndex, false);
                    break;
                }
                if (VERBOSE) {
                    Log.d(TAG, ""video encoder: returned buffer for time ""
                            + videoEncoderOutputBufferInfo.presentationTimeUs);
                }
                if (videoEncoderOutputBufferInfo.size != 0) {
                    muxer.writeSampleData(
                            outputVideoTrack, encoderOutputBuffer, videoEncoderOutputBufferInfo);
                    videoEncodedFrameCount++;
                }
                if ((videoEncoderOutputBufferInfo.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM)
                        != 0) {
                    if (VERBOSE) Log.d(TAG, ""video encoder: EOS"");
                    videoEncoderDone = true;
                }
                videoEncoder.releaseOutputBuffer(encoderOutputBufferIndex, false);
                // We enqueued an encoded frame, let's try something else next.
                break;
            }

            // Poll frames from the audio encoder and send them to the muxer.
            while (mCopyAudio && !audioEncoderDone
                    && (encoderOutputAudioFormat == null || muxing)) {
                int encoderOutputBufferIndex = audioEncoder.dequeueOutputBuffer(
                        audioEncoderOutputBufferInfo, TIMEOUT_USEC);
                if (encoderOutputBufferIndex == MediaCodec.INFO_TRY_AGAIN_LATER) {
                    if (VERBOSE) Log.d(TAG, ""no audio encoder output buffer"");
                    break;
                }
                if (encoderOutputBufferIndex == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                    if (VERBOSE) Log.d(TAG, ""audio encoder: output buffers changed"");
                    audioEncoderOutputBuffers = audioEncoder.getOutputBuffers();
                    break;
                }
                if (encoderOutputBufferIndex == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                    if (VERBOSE) Log.d(TAG, ""audio encoder: output format changed"");
                    if (outputAudioTrack >= 0) {
                        fail(""audio encoder changed its output format again?"");
                    }

                    encoderOutputAudioFormat = audioEncoder.getOutputFormat();
                    break;
                }
                assertTrue(""should have added track before processing output"", muxing);
                if (VERBOSE) {
                    Log.d(TAG, ""audio encoder: returned output buffer: ""
                            + encoderOutputBufferIndex);
                    Log.d(TAG, ""audio encoder: returned buffer of size ""
                            + audioEncoderOutputBufferInfo.size);
                }
                ByteBuffer encoderOutputBuffer =
                        audioEncoderOutputBuffers[encoderOutputBufferIndex];
                if ((audioEncoderOutputBufferInfo.flags & MediaCodec.BUFFER_FLAG_CODEC_CONFIG)
                        != 0) {
                    if (VERBOSE) Log.d(TAG, ""audio encoder: codec config buffer"");
                    // Simply ignore codec config buffers.
                    audioEncoder.releaseOutputBuffer(encoderOutputBufferIndex, false);
                    break;
                }
                if (VERBOSE) {
                    Log.d(TAG, ""audio encoder: returned buffer for time ""
                            + audioEncoderOutputBufferInfo.presentationTimeUs);
                }
                if (audioEncoderOutputBufferInfo.size != 0) {
                    muxer.writeSampleData(
                            outputAudioTrack, encoderOutputBuffer, audioEncoderOutputBufferInfo);
                }
                if ((audioEncoderOutputBufferInfo.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM)
                        != 0) {
                    if (VERBOSE) Log.d(TAG, ""audio encoder: EOS"");
                    audioEncoderDone = true;
                }
                audioEncoder.releaseOutputBuffer(encoderOutputBufferIndex, false);
                audioEncodedFrameCount++;
                // We enqueued an encoded frame, let's try something else next.
                break;
            }

            if (!muxing
                    && (!mCopyAudio || encoderOutputAudioFormat != null)
                    && (!mCopyVideo || encoderOutputVideoFormat != null)) {
                if (mCopyVideo) {
                    Log.d(TAG, ""muxer: adding video track."");
                    outputVideoTrack = muxer.addTrack(encoderOutputVideoFormat);
                }
                if (mCopyAudio) {
                    Log.d(TAG, ""muxer: adding audio track."");
                    outputAudioTrack = muxer.addTrack(encoderOutputAudioFormat);
                }
                Log.d(TAG, ""muxer: starting"");
                muxer.start();
                muxing = true;
            }
        }

        // Basic validation checks.
        if (mCopyVideo) {
            assertEquals(""encoded and decoded video frame counts should match"",
                    videoDecodedFrameCount, videoEncodedFrameCount);
            assertTrue(""decoded frame count should be less than extracted frame count"",
                    videoDecodedFrameCount <= videoExtractedFrameCount);
        }
        if (mCopyAudio) {
            assertEquals(""no frame should be pending"", -1, pendingAudioDecoderOutputBufferIndex);
        }
    }

    private static boolean isVideoFormat(MediaFormat format) {
        return getMimeTypeFor(format).startsWith(""video/"");
    }

    private static boolean isAudioFormat(MediaFormat format) {
        return getMimeTypeFor(format).startsWith(""audio/"");
    }

    private static String getMimeTypeFor(MediaFormat format) {
        return format.getString(MediaFormat.KEY_MIME);
    }

    /**
     * Returns the first codec capable of encoding the specified MIME type, or null if no match was
     * found.
     */
    private static MediaCodecInfo selectCodec(String mimeType) {
        int numCodecs = MediaCodecList.getCodecCount();
        for (int i = 0; i < numCodecs; i++) {
            MediaCodecInfo codecInfo = MediaCodecList.getCodecInfoAt(i);

            if (codecInfo.isAlias()) {
                continue;
            }
            if (!codecInfo.isEncoder()) {
                continue;
            }

            String[] types = codecInfo.getSupportedTypes();
            for (int j = 0; j < types.length; j++) {
                if (types[j].equalsIgnoreCase(mimeType)) {
                    return codecInfo;
                }
            }
        }
        return null;
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.assist.common.Utils"	"getExtraAssistBundle"	""	"/home/gpoor/cts-12-source/cts/tests/tests/assist/common/src/android/assist/common/Utils.java"	""	"public void test/*
 *.
 */
package android.assist.common;

import android.content.ComponentName;
import android.content.Context;
import android.content.Intent;
import android.content.pm.PackageManager;
import android.os.Bundle;
import android.os.LocaleList;
import android.os.Process;
import android.util.Log;

import org.json.JSONObject;

import java.util.ArrayList;
import java.util.Locale;

public class Utils {
    private static final String TAG = Utils.class.getSimpleName();
    public static final String TESTCASE_TYPE = ""testcase_type"";
    public static final String TESTINFO = ""testinfo"";
    public static final String ACTION_PREFIX = ""android.intent.action."";
    public static final String BROADCAST_INTENT = ACTION_PREFIX + ""ASSIST_TESTAPP"";
    public static final String BROADCAST_ASSIST_DATA_INTENT = ACTION_PREFIX + ""ASSIST_DATA"";
    public static final String BROADCAST_INTENT_START_ASSIST = ACTION_PREFIX + ""START_ASSIST"";
    public static final String ASSIST_RECEIVER_REGISTERED = ACTION_PREFIX + ""ASSIST_READY"";
    public static final String ACTION_END_OF_TEST = ACTION_PREFIX + ""END_OF_TEST"";

    public static final String ACTION_INVALIDATE = ""invalidate_action"";
    public static final String GET_CONTENT_VIEW_HEIGHT = ACTION_PREFIX + ""GET_CONTENT_VIEW_HEIGHT"";
    public static final String BROADCAST_CONTENT_VIEW_HEIGHT = ACTION_PREFIX + ""VIEW_HEIGHT"";
    public static final String SCROLL_TEXTVIEW_ACTION = ACTION_PREFIX + ""TEXTVIEW_SCROLL"";
    public static final String SCROLL_SCROLLVIEW_ACTION = ACTION_PREFIX + ""SCROLLVIEW_SCROLL"";
    public static final String TEST_ERROR = ""Error In Test:"";

    public static final String ASSIST_STRUCTURE_KEY = ""assist_structure"";
    public static final String ASSIST_CONTENT_KEY = ""assist_content"";
    public static final String ASSIST_BUNDLE_KEY = ""assist_bundle"";
    public static final String ASSIST_IS_ACTIVITY_ID_NULL = ""assist_is_activity_id_null"";
    public static final String ASSIST_SCREENSHOT_KEY = ""assist_screenshot"";
    public static final String SCREENSHOT_COLOR_KEY = ""set_screenshot_color"";
    public static final String COMPARE_SCREENSHOT_KEY = ""compare_screenshot"";
    public static final String DISPLAY_WIDTH_KEY = ""display_width"";
    public static final String DISPLAY_HEIGHT_KEY = ""dislay_height"";
    public static final String SCROLL_X_POSITION = ""scroll_x_position"";
    public static final String SCROLL_Y_POSITION = ""scroll_y_position"";
    public static final String SHOW_SESSION_FLAGS_TO_SET = ""show_session_flags_to_set"";

    /** Lifecycle Test intent constants */
    public static final String LIFECYCLE_PREFIX = ACTION_PREFIX + ""lifecycle_"";
    public static final String LIFECYCLE_HASRESUMED = LIFECYCLE_PREFIX + ""hasResumed"";
    public static final String LIFECYCLE_HASFOCUS = LIFECYCLE_PREFIX + ""hasFocus"";
    public static final String LIFECYCLE_LOSTFOCUS = LIFECYCLE_PREFIX + ""lostFocus"";
    public static final String LIFECYCLE_ONPAUSE = LIFECYCLE_PREFIX + ""onpause"";
    public static final String LIFECYCLE_ONSTOP = LIFECYCLE_PREFIX + ""onstop"";
    public static final String LIFECYCLE_ONDESTROY = LIFECYCLE_PREFIX + ""ondestroy"";

    /** Focus Change Test intent constants */
    public static final String GAINED_FOCUS = ACTION_PREFIX + ""focus_changed"";
    public static final String LOST_FOCUS = ACTION_PREFIX + ""lost_focus"";

    public static final String APP_3P_HASRESUMED = ACTION_PREFIX + ""app_3p_hasResumed"";
    public static final String APP_3P_HASDRAWED = ACTION_PREFIX + ""app_3p_hasDrawed"";
    public static final String TEST_ACTIVITY_DESTROY = ACTION_PREFIX + ""test_activity_destroy"";
    public static final String TEST_ACTIVITY_WEBVIEW_LOADED = ACTION_PREFIX + ""test_activity_webview_hasResumed"";

    // Notice: timeout belows have to be long because some devices / form factors (like car) are
    // slower.

    /** Timeout for getting back assist context */
    public static final int TIMEOUT_MS = 4 * 1_000;
    /** Timeout for an activity to resume */
    public static final int ACTIVITY_ONRESUME_TIMEOUT_MS = 8 * 1_000;

    public static final String EXTRA_REGISTER_RECEIVER = ""register_receiver"";

    /** Extras for passing the Assistant's ContentView's dimensions*/
    public static final String EXTRA_CONTENT_VIEW_HEIGHT = ""extra_content_view_height"";
    public static final String EXTRA_CONTENT_VIEW_WIDTH = ""extra_content_view_width"";
    public static final String EXTRA_DISPLAY_POINT = ""extra_display_point"";

    /*
     * Extras used to pass RemoteCallback objects responsible for IPC between test, app, and
     * service.
     */
    public static final String EXTRA_REMOTE_CALLBACK = ""extra_remote_callback"";
    public static final String EXTRA_REMOTE_CALLBACK_ACTION = ""extra_remote_callback_action"";

    public static final String EXTRA_REMOTE_CALLBACK_RECEIVING = ""extra_remote_callback_receiving"";
    public static final String EXTRA_REMOTE_CALLBACK_RECEIVING_ACTION = ""extra_remote_callback_receiving_action"";

    /** Test name suffixes */
    public static final String ASSIST_STRUCTURE = ""ASSIST_STRUCTURE"";
    public static final String DISABLE_CONTEXT = ""DISABLE_CONTEXT"";
    public static final String FLAG_SECURE = ""FLAG_SECURE"";
    public static final String LIFECYCLE = ""LIFECYCLE"";
    public static final String LIFECYCLE_NOUI = ""LIFECYCLE_NOUI"";
    public static final String SCREENSHOT = ""SCREENSHOT"";
    public static final String EXTRA_ASSIST = ""EXTRA_ASSIST"";
    public static final String VERIFY_CONTENT_VIEW = ""VERIFY_CONTENT_VIEW"";
    public static final String TEXTVIEW = ""TEXTVIEW"";
    public static final String LARGE_VIEW_HIERARCHY = ""LARGE_VIEW_HIERARCHY"";
    public static final String WEBVIEW = ""WEBVIEW"";
    public static final String FOCUS_CHANGE = ""FOCUS_CHANGE"";

    /** Session intent constants */
    public static final String HIDE_SESSION = ""android.intent.action.hide_session"";
    public static final String HIDE_SESSION_COMPLETE = ""android.intent.action.hide_session_complete"";

    /** Lifecycle activity intent constants */
    /** Session intent constants */
    public static final String HIDE_LIFECYCLE_ACTIVITY
            = ""android.intent.action.hide_lifecycle_activity"";

    /** Stub html view to load into WebView */
    public static final String WEBVIEW_HTML_URL = ""http://dev.null/thou/should?not=pass"";
    public static final String WEBVIEW_HTML_DOMAIN = ""dev.null"";
    public static final LocaleList WEBVIEW_LOCALE_LIST = new LocaleList(Locale.ROOT, Locale.US);
    public static final String WEBVIEW_HTML_GREETING = ""Hello WebView!"";
    public static final String WEBVIEW_HTML = ""<html><body><div><p>"" + WEBVIEW_HTML_GREETING
            + ""</p></div></body></html>"";

    /** Extra data to add to assist data and assist content */
    private static Bundle EXTRA_ASSIST_BUNDLE;
    private static String STRUCTURED_JSON;

    private static String MY_UID_EXTRA = ""my_uid"";

    public static final String getStructuredJSON() throws Exception {
        if (STRUCTURED_JSON == null) {
            STRUCTURED_JSON = new JSONObject()
                    .put(""@type"", ""MusicRecording"")
                    .put(""@id"", ""https://example/music/recording"")
                    .put(""url"", ""android-app://com.example/https/music/album"")
                    .put(""name"", ""Album Title"")
                    .put(""hello"", ""hi there"")
                    .put(""knownNull"", null)
                    .put(""unicode value"", ""\ud800\udc35"")
                    .put(""empty string"", """")
                    .put(""LongString"",
                        ""lkasdjfalsdkfjalsdjfalskj9i9234jl1w23j4o123j412l3j421l3kj412l3kj1l3k4j32"")
                    .put(""\ud800\udc35"", ""any-value"")
                    .put(""key with spaces"", ""any-value"")
                    .toString();
        }
        return STRUCTURED_JSON;
    }

    public static final Bundle getExtraAssistBundle() {
        if (EXTRA_ASSIST_BUNDLE == null) {
            EXTRA_ASSIST_BUNDLE = new Bundle();
            addExtraAssistDataToBundle(EXTRA_ASSIST_BUNDLE, /* addMyUid= */ false);
        }
        return EXTRA_ASSIST_BUNDLE;
    }

    public static void addExtraAssistDataToBundle(Bundle data) {
        addExtraAssistDataToBundle(data, /* addMyUid= */ true);

    }

    private static void addExtraAssistDataToBundle(Bundle data, boolean addMyUid) {
        data.putString(""hello"", ""there"");
        data.putBoolean(""isthis_true_or_false"", true);
        data.putInt(""number"", 123);
        if (addMyUid) {
            Log.i(TAG, ""adding "" + MY_UID_EXTRA + ""="" + Process.myUid());
            data.putInt(MY_UID_EXTRA, Process.myUid());
        }
    }

    /**
     * The test app associated with each test.
     */
    public static final ComponentName getTestAppComponent(String testCaseType) {
        switch (testCaseType) {
            case ASSIST_STRUCTURE:
            case LARGE_VIEW_HIERARCHY:
            case DISABLE_CONTEXT:
                return new ComponentName(
                        ""android.assist.testapp"", ""android.assist.testapp.TestApp"");
            case FLAG_SECURE:
                return new ComponentName(
                        ""android.assist.testapp"", ""android.assist.testapp.SecureActivity"");
            case LIFECYCLE:
            case LIFECYCLE_NOUI:
                return new ComponentName(
                        ""android.assist.testapp"", ""android.assist.testapp.LifecycleActivity"");
            case SCREENSHOT:
                return new ComponentName(
                        ""android.assist.testapp"", ""android.assist.testapp.ScreenshotActivity"");
            case EXTRA_ASSIST:
                return new ComponentName(
                        ""android.assist.testapp"", ""android.assist.testapp.ExtraAssistDataActivity"");
            case TEXTVIEW:
                return new ComponentName(
                        ""android.assist.testapp"", ""android.assist.testapp.TextViewActivity"");
            case WEBVIEW:
                return new ComponentName(
                        ""android.assist.testapp"", ""android.assist.testapp.WebViewActivity"");
            case FOCUS_CHANGE:
                return new ComponentName(
                        ""android.assist.testapp"", ""android.assist.testapp.FocusChangeActivity"");
            default:
                return new ComponentName("""","""");
        }
    }

    /**
     * Sets the proper action used to launch an activity in the testapp package.
     */
    public static void setTestAppAction(Intent intent, String testCaseName) {
        intent.putExtra(Utils.TESTCASE_TYPE, testCaseName);
        intent.setAction(""android.intent.action.TEST_APP_"" + testCaseName);
    }

    /**
     * Returns the amount of time to wait for assist data.
     */
    public static final int getAssistDataTimeout(String testCaseType) {
        switch (testCaseType) {
            case SCREENSHOT:
                // needs to wait for 3p activity to resume before receiving assist data.
                return TIMEOUT_MS + ACTIVITY_ONRESUME_TIMEOUT_MS;
            default:
                return TIMEOUT_MS;
        }
    }

    public static final String toBundleString(Bundle bundle) {
        if (bundle == null) {
            return ""*** Bundle is null ****"";
        }
        StringBuffer buf = new StringBuffer(""Bundle is: "");
        String testType = bundle.getString(TESTCASE_TYPE);
        if (testType != null) {
            buf.append(""testcase type = "" + testType);
        }
        ArrayList<String> info = bundle.getStringArrayList(TESTINFO);
        if (info != null) {
            for (String s : info) {
                buf.append(s + ""\n\t\t"");
            }
        }
        return buf.toString();
    }

    public static final void addErrorResult(final Bundle testinfo, final String msg) {
        testinfo.getStringArrayList(testinfo.getString(Utils.TESTCASE_TYPE))
            .add(TEST_ERROR + "" "" + msg);
    }

    public static int getExpectedUid(Bundle extras) {
        return extras.getInt(MY_UID_EXTRA);
    }

    public static Bundle bundleOfRemoteAction(String action) {
        Bundle bundle = new Bundle();
        bundle.putString(Utils.EXTRA_REMOTE_CALLBACK_ACTION, action);
        return bundle;
    }

    public static boolean isAutomotive(Context context) {
        return context.getPackageManager().hasSystemFeature(PackageManager.FEATURE_AUTOMOTIVE);
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.view.cts.TextureViewCameraActivity"	"isAvailable"	"CtsViewTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/view/src/android/view/cts/TextureViewCameraActivity.java"	""	"public void test/*
 *.
 */

package android.view.cts;

import android.app.Activity;
import android.graphics.Bitmap;
import android.graphics.Matrix;
import android.graphics.SurfaceTexture;
import android.hardware.Camera;
import android.os.Bundle;
import android.util.Log;
import android.view.TextureView;
import android.view.View;

import junit.framework.Assert;

import java.io.IOException;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;


public class TextureViewCameraActivity extends Activity implements
        TextureView.SurfaceTextureListener {
    private static final String TAG = ""TextureViewCameraActivity"";

    private static final int CAPTURE_SCREEN_INTERVAL = 10;
    private static final float SCREEN_ROTATION_RATE = 1.0f;
    private static final int MAX_FRAME_UPDATE = 40;

    private Camera mCamera;
    private TextureView mTextureView;
    private int mUpdateCounter = 0;
    private int mWidth;
    private int mHeight;
    private float mRotation = 0f;
    private final CountDownLatch mLatch = new CountDownLatch(1);


    public void onSurfaceTextureAvailable(SurfaceTexture surface, int width, int height) {
        Assert.assertTrue(mTextureView.getLayerType() == View.LAYER_TYPE_HARDWARE);
        Assert.assertTrue(mTextureView.isAvailable());
        Assert.assertNotNull(mTextureView.getSurfaceTexture());
        Assert.assertTrue(mTextureView.getSurfaceTextureListener() == this);
        Assert.assertTrue(mTextureView.isOpaque());
        mWidth = width;
        mHeight = height;
        if (Camera.getNumberOfCameras() > 0) {
            Log.d(TAG, ""opening camera..."");
            mCamera = Camera.open(0);
            Log.d(TAG, ""surface texture available, opening a camera"");
        } else {
            // no camera, and no frame update, so just complete here.
            Log.d(TAG, ""no camera, test aborting"");
            mLatch.countDown();
            return;
        }

        try {
            mCamera.setPreviewTexture(surface);
            mCamera.startPreview();
        } catch (IOException ioe) {
            // Something bad happened
            Assert.fail();
        }
    }

    public void onSurfaceTextureSizeChanged(SurfaceTexture surface, int width, int height) {
        Log.d(TAG, ""surface texture size change "" + width + "" x "" + height);
        mWidth = width;
        mHeight = height;
    }

    public boolean onSurfaceTextureDestroyed(SurfaceTexture surface) {
        Log.d(TAG, ""surface texture destroyed"");
        if (mCamera != null) {
            Log.d(TAG, ""stopping camera"");
            mCamera.stopPreview();
            mCamera.release();
        }
        return true;
    }

    public void onSurfaceTextureUpdated(SurfaceTexture surface) {
        Log.d(TAG, ""surface texture updated"");
        mUpdateCounter++;
        if (mUpdateCounter % CAPTURE_SCREEN_INTERVAL == 0) {
            Bitmap bitmap = mTextureView.getBitmap();
            Log.d(TAG, ""acquired bitmap"");
            Assert.assertEquals(mHeight, bitmap.getHeight());
            Assert.assertEquals(mWidth, bitmap.getWidth());
            bitmap.recycle();
            if (mUpdateCounter >= MAX_FRAME_UPDATE) {
                Log.d(TAG, ""seen "" + MAX_FRAME_UPDATE + "" frames, test complete"");
                mLatch.countDown();
            }
        }
        Matrix transformMatrix =  mTextureView.getTransform(null);
        mRotation += SCREEN_ROTATION_RATE;
        transformMatrix.setRotate(mRotation, mWidth / 2, mHeight / 2);
        mTextureView.setTransform(transformMatrix);
    }

    public boolean waitForCompletion(long timeoutInSecs) throws InterruptedException {
        return mLatch.await(timeoutInSecs, TimeUnit.SECONDS);
    }

    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        mTextureView = new TextureView(this);
        mTextureView.setSurfaceTextureListener(this);
        mTextureView.setOpaque(true);
        setContentView(mTextureView);
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaSyncTest"	"MediaSyncTest"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaSyncTest.java"	""	"public void test/*
 *.
 */
package android.media.cts;

import android.content.Context;
import android.content.pm.PackageManager;
import android.media.AudioFormat;
import android.media.AudioManager;
import android.media.AudioTrack;
import android.media.MediaCodec;
import android.media.MediaExtractor;
import android.media.MediaFormat;
import android.media.MediaSync;
import android.media.MediaTimestamp;
import android.media.PlaybackParams;
import android.media.SyncParams;
import android.os.Handler;
import android.os.HandlerThread;
import android.platform.test.annotations.AppModeFull;
import android.platform.test.annotations.RequiresDevice;
import android.test.ActivityInstrumentationTestCase2;
import android.util.Log;
import android.view.Surface;

import androidx.test.filters.SmallTest;

import com.android.compatibility.common.util.MediaUtils;

import java.io.IOException;
import java.lang.Long;
import java.lang.Math;
import java.nio.ByteBuffer;
import java.util.LinkedList;
import java.util.List;
import java.util.concurrent.atomic.AtomicBoolean;

/**
 * Tests for the MediaSync API and local video/audio playback.
 *
 * <p>The file in res/raw used by all tests are (c) copyright 2008,
 * Blender Foundation / www.bigbuckbunny.org, and are licensed under the Creative Commons
 * Attribution 3.0 License at http://creativecommons.org/licenses/by/3.0/us/.
 */
@NonMediaMainlineTest
@SmallTest
@RequiresDevice
@AppModeFull(reason = ""TODO: evaluate and port to instant"")
public class MediaSyncTest extends ActivityInstrumentationTestCase2<MediaStubActivity> {
    private static final String LOG_TAG = ""MediaSyncTest"";

    static final String mInpPrefix = WorkDir.getMediaDirString();
    private final long NO_TIMESTAMP = -1;
    private final float FLOAT_PLAYBACK_RATE_TOLERANCE = .02f;
    private final long TIME_MEASUREMENT_TOLERANCE_US = 20000;
    final String INPUT_RESOURCE =
            mInpPrefix + ""video_480x360_mp4_h264_1350kbps_30fps_aac_stereo_192kbps_44100hz.mp4"";
    private final int APPLICATION_AUDIO_PERIOD_MS = 200;
    private final int TEST_MAX_SPEED = 2;
    private static final float FLOAT_TOLERANCE = .00001f;

    private Context mContext;

    private MediaStubActivity mActivity;

    private MediaSync mMediaSync = null;
    private Surface mSurface = null;

    private Decoder mDecoderVideo = null;
    private Decoder mDecoderAudio = null;
    private boolean mHasAudio = false;
    private boolean mHasVideo = false;
    private boolean mEosAudio = false;
    private boolean mEosVideo = false;
    private int mTaggedAudioBufferIndex = -1;
    private final Object mConditionEos = new Object();
    private final Object mConditionEosAudio = new Object();
    private final Object mConditionTaggedAudioBufferIndex = new Object();

    private int mNumBuffersReturned = 0;

    public MediaSyncTest() {
        super(MediaStubActivity.class);
    }

    @Override
    protected void setUp() throws Exception {
        super.setUp();
        mActivity = getActivity();
        getInstrumentation().waitForIdleSync();
        try {
            runTestOnUiThread(new Runnable() {
                public void run() {
                    mMediaSync = new MediaSync();
                }
            });
        } catch (Throwable e) {
            e.printStackTrace();
            fail();
        }
        mContext = getInstrumentation().getTargetContext();
        mDecoderVideo = new Decoder(this, mMediaSync, false);
        mDecoderAudio = new Decoder(this, mMediaSync, true);
    }

    @Override
    protected void tearDown() throws Exception {
        if (mMediaSync != null) {
            mMediaSync.release();
            mMediaSync = null;
        }
        if (mDecoderAudio != null) {
            mDecoderAudio.release();
            mDecoderAudio = null;
        }
        if (mDecoderVideo != null) {
            mDecoderVideo.release();
            mDecoderVideo = null;
        }
        if (mSurface != null) {
            mSurface.release();
            mSurface = null;
        }
        mActivity = null;
        mHasAudio = false;
        mHasVideo = false;
        mEosAudio = false;
        mEosVideo = false;
        mTaggedAudioBufferIndex = -1;
        super.tearDown();
    }

    private boolean reachedEos_l() {
        return ((!mHasVideo || mEosVideo) && (!mHasAudio || mEosAudio));
    }

    public void onTaggedAudioBufferIndex(Decoder decoder, int index) {
        synchronized (mConditionTaggedAudioBufferIndex) {
            if (decoder == mDecoderAudio) {
                mTaggedAudioBufferIndex = index;
            }
        }
    }

    public void onEos(Decoder decoder) {
        synchronized (mConditionEosAudio) {
            if (decoder == mDecoderAudio) {
                mEosAudio = true;
                mConditionEosAudio.notify();
            }
        }

        synchronized (mConditionEos) {
            if (decoder == mDecoderVideo) {
                mEosVideo = true;
            }
            if (reachedEos_l()) {
                mConditionEos.notify();
            }
        }
    }

    private boolean hasAudioOutput() {
        return mActivity.getPackageManager()
            .hasSystemFeature(PackageManager.FEATURE_AUDIO_OUTPUT);
    }

    /**
     * Tests setPlaybackParams is handled correctly for wrong rate.
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.classloadersplitapp.SplitAppTest"	"AppContextTestRule"	""	"/home/gpoor/cts-12-source/cts/hostsidetests/classloaders/splits/apps/src/com/android/cts/classloadersplitapp/SplitAppTest.java"	""	"/*
 *.
 */

package com.android.cts.classloadersplitapp;

import static org.hamcrest.CoreMatchers.*;
import static org.junit.Assert.*;

import android.app.Activity;
import android.app.Service;
import android.content.BroadcastReceiver;
import android.content.ComponentName;
import android.content.Context;
import android.content.Intent;
import android.os.Bundle;

import androidx.test.InstrumentationRegistry;
import androidx.test.rule.ActivityTestRule;
import androidx.test.rule.ServiceTestRule;
import androidx.test.runner.AndroidJUnit4;

import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.TestRule;
import org.junit.runner.Description;
import org.junit.runner.RunWith;
import org.junit.runners.model.Statement;

import java.util.concurrent.CompletableFuture;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;

@RunWith(AndroidJUnit4.class)
public class SplitAppTest {
    /* The feature hierarchy looks like this:

        APK_BASE (PathClassLoader)
          ^
          |
        APK_FEATURE_A (DelegateLastClassLoader)
          ^
          |
        APK_FEATURE_B (PathClassLoader)

     */

    private static final String PACKAGE = ""com.android.cts.classloadersplitapp"";
    private static final ComponentName FEATURE_A_ACTIVITY =
            ComponentName.createRelative(PACKAGE, "".feature_a.FeatureAActivity"");
    private static final ComponentName FEATURE_B_ACTIVITY =
            ComponentName.createRelative(PACKAGE, "".feature_b.FeatureBActivity"");
    private static final ComponentName FEATURE_A_SERVICE =
            ComponentName.createRelative(PACKAGE, "".feature_a.FeatureAService"");
    private static final ComponentName FEATURE_B_SERVICE =
            ComponentName.createRelative(PACKAGE, "".feature_b.FeatureBService"");

    @Rule
    public ActivityTestRule<BaseActivity> mBaseActivityRule =
            new ActivityTestRule<>(BaseActivity.class);

    // Do not launch this activity lazily. We use this rule to launch all feature Activities,
    // so we use #launchActivity() with the correct Intent.
    @Rule
    public ActivityTestRule<Activity> mFeatureActivityRule =
            new ActivityTestRule<>(Activity.class, true /*initialTouchMode*/,
                    false /*launchActivity*/);

    @Rule
    public AppContextTestRule mAppContextTestRule = new AppContextTestRule();

    @Rule
    public ServiceTestRule mServiceTestRule = new ServiceTestRule();"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaFormatTest"	"testMediaFormatConstructors"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaFormatTest.java"	""	"public void testMediaFormatConstructors() {
        MediaFormat format;
        {
            String[] audioMimeTypes = { MediaFormat.MIMETYPE_AUDIO_AAC,
                    MediaFormat.MIMETYPE_AUDIO_MPEGH_MHA1, MediaFormat.MIMETYPE_AUDIO_MPEGH_MHM1 };
            for (String mime : audioMimeTypes) {
                format = MediaFormat.createAudioFormat(mime, 48000, 6);
                assertEquals(mime, format.getString(MediaFormat.KEY_MIME));
                assertEquals(48000, format.getInteger(MediaFormat.KEY_SAMPLE_RATE));
                assertEquals(6, format.getInteger(MediaFormat.KEY_CHANNEL_COUNT));
                assertEquals(3, format.getKeys().size());
                assertEquals(0, format.getFeatures().size());
            }
        }

        {
            format = MediaFormat.createVideoFormat(MediaFormat.MIMETYPE_VIDEO_AVC, 1920, 1080);
            assertEquals(MediaFormat.MIMETYPE_VIDEO_AVC, format.getString(MediaFormat.KEY_MIME));
            assertEquals(1920, format.getInteger(MediaFormat.KEY_WIDTH));
            assertEquals(1080, format.getInteger(MediaFormat.KEY_HEIGHT));
            assertEquals(3, format.getKeys().size());
            assertEquals(0, format.getFeatures().size());
        }

        {
            format = MediaFormat.createSubtitleFormat(MediaFormat.MIMETYPE_TEXT_VTT, ""und"");
            assertEquals(MediaFormat.MIMETYPE_TEXT_VTT, format.getString(MediaFormat.KEY_MIME));
            assertEquals(""und"", format.getString(MediaFormat.KEY_LANGUAGE));
            assertEquals(2, format.getKeys().size());
            assertEquals(0, format.getFeatures().size());

            format.setFeatureEnabled(""feature1"", false);

            // also test dup
            MediaFormat other = new MediaFormat(format);
            format.setString(MediaFormat.KEY_LANGUAGE, ""un"");
            other.setInteger(MediaFormat.KEY_IS_DEFAULT, 1);
            other.setFeatureEnabled(""feature1"", true);

            assertEquals(MediaFormat.MIMETYPE_TEXT_VTT, format.getString(MediaFormat.KEY_MIME));
            assertEquals(""un"", format.getString(MediaFormat.KEY_LANGUAGE));
            assertEquals(2, format.getKeys().size());
            assertFalse(format.getFeatureEnabled(""feature1""));
            assertEquals(1, format.getFeatures().size());

            assertEquals(MediaFormat.MIMETYPE_TEXT_VTT, other.getString(MediaFormat.KEY_MIME));
            assertEquals(""und"", other.getString(MediaFormat.KEY_LANGUAGE));
            assertEquals(1, other.getInteger(MediaFormat.KEY_IS_DEFAULT));
            assertEquals(3, other.getKeys().size());
            assertTrue(other.getFeatureEnabled(""feature1""));
            assertEquals(1, other.getFeatures().size());
        }
    }

    /**
     * Check MediaFormat key name and string value consistency.
     *
     * The canonical key reads something like this:
     * KEY_SOMETHING_HERE = ""something-here"";
     *
     * An exclusion list allows arbitrary keys as needed.
     *
     * This test uses introspection to find the key fields.
     *
     * @throws Exception
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaFormatTest"	"testKeyConsistency"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaFormatTest.java"	""	"public void testKeyConsistency() throws Exception {
        // Legacy MediaFormat keys inconsistent with the canonical format.
        final Set<String> exclusions = Stream.of(
            // <aac-drc-[cut-level]>
            ""KEY_AAC_DRC_ATTENUATION_FACTOR"",
            // <aac-drc-boost-[level]>
            ""KEY_AAC_DRC_BOOST_FACTOR"",
            // <aac-[target-ref]-level>
            ""KEY_AAC_DRC_TARGET_REFERENCE_LEVEL"",
            // <...c-max-output-channel[_]count>
            ""KEY_AAC_MAX_OUTPUT_CHANNEL_COUNT"",
            // <bit[]rate>
            ""KEY_BIT_RATE"",
            // <create-input-[buffers]-suspended>
            ""KEY_CREATE_INPUT_SURFACE_SUSPENDED"",
            // <duration[u]s>
            ""KEY_DURATION"",
            // <grid-col[]s>
            ""KEY_GRID_COLUMNS"",
            // <h[w]-av-sync-id>
            ""KEY_HARDWARE_AV_SYNC_ID"",
            // <max-bit[]rate>
            ""KEY_MAX_BIT_RATE"",
            // <max-b[]frames>
            ""KEY_MAX_B_FRAMES"",
            // <[sar]-height>
            ""KEY_PIXEL_ASPECT_RATIO_HEIGHT"",
            // <[sar]-width>
            ""KEY_PIXEL_ASPECT_RATIO_WIDTH"",
            // <prepend-[sps-pps-to-idr]-frames>
            ""KEY_PREPEND_HEADER_TO_SYNC_FRAMES"",
            // <...h-blank-buffers-on-s[hutdown]>
            ""KEY_PUSH_BLANK_BUFFERS_ON_STOP"",
            // <rotation[-degrees]>
            ""KEY_ROTATION"",
            // <t[s-schema]>
            ""KEY_TEMPORAL_LAYERING""
            ).collect(Collectors.toCollection(HashSet::new));

        ArrayList<String> failures = new ArrayList<>();
        final Field[] fields = MediaFormat.class.getFields();
        for (Field field : fields) {
            final String key = field.getName();
            if (!key.startsWith(""KEY_"")) continue;
            if (exclusions.contains(key)) continue;

            if (!key.equals(key.toUpperCase())) {
                failures.add(""Key field "" + key + "" must be upper case"");
            }
            final String value = (String)field.get(null);
            assertEquals(""String value "" + value + "" must be lower case"",
                    value.toLowerCase(), value);

            // What do we expect the key should look like for the value?
            final String checkKey = ""KEY_"" + value.toUpperCase().replace('-', '_');
            if (!checkKey.equals(key)) {
                failures.add(""Key field "" + key + "" should represent value "" + value
                        + "" expected("" + checkKey + "")"");
            }
        }
        // There may be special vendor keys that are public.
        // Log failures but don't fail test.
        logFailures(""testKeyConsistency"", failures);
    }

    /**
     * Check MediaFormat mime type field name and string value consistency.
     *
     * The typical mime type field reads as follows:
     * MIMETYPE_CATEGORY_ANYCASE_HERE = ""category/anYCaSE[-.+]HeRE"";
     *
     * See here for the Internet Assigned Numbers Authority (IANA) list of media mime types:
     * https://www.iana.org/assignments/media-types/media-types.xhtml
     *
     * An exclusion list allows arbitrary keys as needed.
     *
     * This test uses introspection to find the mime type fields.
     *
     * @throws Exception
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.MediaFormatTest"	"testMimeTypeConsistency"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/MediaFormatTest.java"	""	"public void testMimeTypeConsistency() throws Exception {
        // Legacy inconsistent mime types with the exception
        final Set<String> exclusions = Stream.of(
                // <audio/[mp4a-latm]>
                ""MIMETYPE_AUDIO_AAC"",
                // <audio/[3gpp]>
                ""MIMETYPE_AUDIO_AMR_NB"",
                // audio/mhm1
                ""MIMETYPE_AUDIO_MPEGH_MHM1"",
                // audio/mha1
                ""MIMETYPE_AUDIO_MPEGH_MHA1"",
                // <audio/[]gsm>
                ""MIMETYPE_AUDIO_MSGSM"",
                // <image/[vnd.android.]heic>
                ""MIMETYPE_IMAGE_ANDROID_HEIC"",
                // <[application/x-]subrip>
                ""MIMETYPE_TEXT_SUBRIP"",
                // <video/av[0]1>
                ""MIMETYPE_VIDEO_AV1"",
                // <video/[3gpp]>
                ""MIMETYPE_VIDEO_H263"",
                // <video/mp[4v-es]>
                ""MIMETYPE_VIDEO_MPEG4"",
                // <video/[x-vnd.on2.]vp8>
                ""MIMETYPE_VIDEO_VP8"",
                // <video/[x-vnd.on2.]vp9>
                ""MIMETYPE_VIDEO_VP9""
        ).collect(Collectors.toCollection(HashSet::new));

        ArrayList<String> failures = new ArrayList<>();
        final Field[] fields = MediaFormat.class.getFields();
        for (Field field : fields) {
            final String mimeType = field.getName();

            if (!mimeType.startsWith(""MIMETYPE_"")) continue;
            if (exclusions.contains(mimeType)) continue;

            if (!mimeType.equals(mimeType.toUpperCase())) {
                failures.add(""mimeType field "" + mimeType + "" must be upper case"");
                continue;
            }
            final String value = (String)field.get(null);

            // What do we expect the mime type field should be for the value?
            final String checkMime = ""MIMETYPE_""
                    + value.toUpperCase().replace('/', '_').replace('-', '_')
                            .replace('.', '_').replace('+', '_');
            if (!mimeType.equals(checkMime)) {
                failures.add(""Mime type "" + mimeType
                        + "" should represent value "" + value
                        + "" expected("" + checkMime + "")"");
            }
        }
        // There may be special vendor keys that are public.
        // Log failures but don't fail test.
        logFailures(""testMimeTypeConsistency"", failures);
    }

    private static final String REPORT_LOG_NAME = ""CtsMediaTestCases"";
    private static final int REPORT_SUMMARY_MAX_KEY_LEN = 240;

    /**
     * Log failures on atest, but don't raise an exception or fail CTS.
     *
     * This part is tricky:
     * 1) We create a device report log so it is visible on the host.
     * 2) We also write to logcat.
     */
    private static void logFailures(@NonNull String logName, @NonNull List<String> failures) {
        if (failures.size() > 0) {
            DeviceReportLog log = new DeviceReportLog(REPORT_LOG_NAME, logName);
            StringBuilder sb = new StringBuilder(""FAILED ON: "");
            int i = 0;
            for (String failure : failures) {
                Log.w(TAG, failure);
                log.addValue(""failure_"" + i++, failure, ResultType.NEUTRAL, ResultUnit.NONE);
                sb.append(""["" + failure + ""] "");
            }
            if (sb.length() > REPORT_SUMMARY_MAX_KEY_LEN) {
                sb.setLength(REPORT_SUMMARY_MAX_KEY_LEN);
            }
            log.setSummary(sb.toString(), failures.size(),
                    ResultType.LOWER_BETTER, ResultUnit.COUNT);
            log.submit(InstrumentationRegistry.getInstrumentation());
        }
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.media.cts.DecoderTestAacFormat"	"testHeAacM4aMultichannelDownmix"	"CtsMediaTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/media/src/android/media/cts/DecoderTestAacFormat.java"	""	"public void testHeAacM4aMultichannelDownmix() throws Exception {
        Log.i(TAG, ""START testDecodeHeAacMcM4a"");

        if (!MediaUtils.check(sIsAndroidRAndAbove, ""M-chan downmix fixed in Android R""))
            return;

        // array of multichannel resources with their expected number of channels without downmixing
        Object [][] samples = {
                //  {resource, numChannels},
                {""noise_5ch_48khz_aot5_dr_sbr_sig1_mp4.m4a"", 5},
                {""noise_6ch_44khz_aot5_dr_sbr_sig2_mp4.m4a"", 6},
        };
        for (Object [] sample: samples) {
            for (String codecName : DecoderTest.codecsFor((String)sample[0] /* resource */)) {
                // verify correct number of channels is observed without downmixing
                AudioParameter chanParams = new AudioParameter();
                decodeUpdateFormat(codecName, (String) sample[0] /*resource*/, chanParams,
                        0 /*no downmix*/);
                assertEquals(""Number of channels differs for codec:"" + codecName,
                        sample[1], chanParams.getNumChannels());

                // verify correct number of channels is observed when downmixing to stereo
                AudioParameter downmixParams = new AudioParameter();
                decodeUpdateFormat(codecName, (String) sample[0] /* resource */, downmixParams,
                        2 /*stereo downmix*/);
                assertEquals(""Number of channels differs for codec:"" + codecName,
                        2, downmixParams.getNumChannels());

            }
        }
    }

    /**
     *
     * @param decoderName
     * @param testInput
     * @param audioParams
     * @param downmixChannelCount 0 if no downmix requested,
     *                           positive number for number of channels in requested downmix
     * @throws IOException
     */
    private void decodeUpdateFormat(String decoderName, final String testInput,
            AudioParameter audioParams, int downmixChannelCount)
            throws IOException
    {
        Preconditions.assertTestFileExists(mInpPrefix + testInput);
        File inpFile = new File(mInpPrefix + testInput);
        ParcelFileDescriptor parcelFD =
                ParcelFileDescriptor.open(inpFile, ParcelFileDescriptor.MODE_READ_ONLY);
        AssetFileDescriptor testFd = new AssetFileDescriptor(parcelFD, 0, parcelFD.getStatSize());

        MediaExtractor extractor = new MediaExtractor();
        extractor.setDataSource(testFd.getFileDescriptor(), testFd.getStartOffset(),
                testFd.getLength());
        testFd.close();

        assertEquals(""wrong number of tracks"", 1, extractor.getTrackCount());
        MediaFormat format = extractor.getTrackFormat(0);
        String mime = format.getString(MediaFormat.KEY_MIME);
        assertTrue(""not an audio file"", mime.startsWith(""audio/""));

        MediaCodec decoder;
        if (decoderName == null) {
            decoder = MediaCodec.createDecoderByType(mime);
        } else {
            decoder = MediaCodec.createByCodecName(decoderName);
        }

        MediaFormat configFormat = format;
        if (downmixChannelCount > 0) {
            configFormat.setInteger(
                    MediaFormat.KEY_AAC_MAX_OUTPUT_CHANNEL_COUNT, downmixChannelCount);
        }

        Log.v(TAG, ""configuring with "" + configFormat);
        decoder.configure(configFormat, null /* surface */, null /* crypto */, 0 /* flags */);

        decoder.start();
        ByteBuffer[] codecInputBuffers = decoder.getInputBuffers();
        ByteBuffer[] codecOutputBuffers = decoder.getOutputBuffers();

        extractor.selectTrack(0);

        // start decoding
        final long kTimeOutUs = 5000;
        MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
        boolean sawInputEOS = false;
        boolean sawOutputEOS = false;
        int noOutputCounter = 0;
        int samplecounter = 0;
        short[] decoded = new short[0];
        int decodedIdx = 0;
        while (!sawOutputEOS && noOutputCounter < 50) {
            noOutputCounter++;
            if (!sawInputEOS) {
                int inputBufIndex = decoder.dequeueInputBuffer(kTimeOutUs);

                if (inputBufIndex >= 0) {
                    ByteBuffer dstBuf = codecInputBuffers[inputBufIndex];

                    int sampleSize =
                            extractor.readSampleData(dstBuf, 0 /* offset */);

                    long presentationTimeUs = 0;

                    if (sampleSize < 0) {
                        Log.d(TAG, ""saw input EOS."");
                        sawInputEOS = true;
                        sampleSize = 0;
                    } else {
                        samplecounter++;
                        presentationTimeUs = extractor.getSampleTime();
                    }
                    decoder.queueInputBuffer(
                            inputBufIndex,
                            0 /* offset */,
                            sampleSize,
                            presentationTimeUs,
                            sawInputEOS ? MediaCodec.BUFFER_FLAG_END_OF_STREAM : 0);

                    if (!sawInputEOS) {
                        extractor.advance();
                    }
                }
            }

            int res = decoder.dequeueOutputBuffer(info, kTimeOutUs);

            if (res >= 0) {
                if (info.size > 0) {
                    noOutputCounter = 0;
                }

                int outputBufIndex = res;
                ByteBuffer buf = codecOutputBuffers[outputBufIndex];

                if (decodedIdx + (info.size / 2) >= decoded.length) {
                    decoded = Arrays.copyOf(decoded, decodedIdx + (info.size / 2));
                }

                buf.position(info.offset);
                for (int i = 0; i < info.size; i += 2) {
                    decoded[decodedIdx++] = buf.getShort();
                }

                decoder.releaseOutputBuffer(outputBufIndex, false /* render */);

                if ((info.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                    Log.d(TAG, ""saw output EOS."");
                    sawOutputEOS = true;
                }
            } else if (res == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                codecOutputBuffers = decoder.getOutputBuffers();
                Log.d(TAG, ""output buffers have changed."");
            } else if (res == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                MediaFormat outputFormat = decoder.getOutputFormat();
                audioParams.setNumChannels(outputFormat.getInteger(MediaFormat.KEY_CHANNEL_COUNT));
                audioParams.setSamplingRate(outputFormat.getInteger(MediaFormat.KEY_SAMPLE_RATE));
                Log.i(TAG, ""output format has changed to "" + outputFormat);
            } else {
                Log.d(TAG, ""dequeueOutputBuffer returned "" + res);
            }
        }
        if (noOutputCounter >= 50) {
            fail(""decoder stopped outputing data"");
        }
        decoder.stop();
        decoder.release();
        extractor.release();
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.keystore.cts.CipherTest"	"isDeviceLocked"	"CtsKeystoreTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/keystore/src/android/keystore/cts/CipherTest.java"	""	"/*
 *.
 */

package android.keystore.cts;

import static org.junit.Assert.assertArrayEquals;
import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertFalse;
import static org.junit.Assert.assertNotNull;
import static org.junit.Assert.assertSame;
import static org.junit.Assert.assertTrue;
import static org.junit.Assert.fail;

import android.app.KeyguardManager;
import android.content.Context;
import android.content.pm.PackageManager;
import android.keystore.cts.util.EmptyArray;
import android.keystore.cts.util.ImportedKey;
import android.keystore.cts.util.TestUtils;
import android.os.SystemClock;
import android.platform.test.annotations.Presubmit;
import android.security.keystore.KeyProperties;
import android.security.keystore.KeyProtection;
import android.server.wm.ActivityManagerTestBase;
import android.server.wm.UiDeviceUtils;

import androidx.test.InstrumentationRegistry;
import androidx.test.runner.AndroidJUnit4;

import com.google.common.collect.ObjectArrays;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.security.AlgorithmParameters;
import java.security.InvalidKeyException;
import java.security.Key;
import java.security.KeyStoreException;
import java.security.Provider;
import java.security.Security;
import java.security.spec.AlgorithmParameterSpec;
import java.security.spec.MGF1ParameterSpec;
import java.security.Provider.Service;
import java.util.Arrays;
import java.util.Collection;
import java.util.Date;
import java.util.HashSet;
import java.util.Locale;
import java.util.Map;
import java.util.Random;
import java.util.Set;
import java.util.TreeMap;

import javax.crypto.BadPaddingException;
import javax.crypto.Cipher;
import javax.crypto.CipherInputStream;
import javax.crypto.CipherOutputStream;
import javax.crypto.IllegalBlockSizeException;
import javax.crypto.spec.GCMParameterSpec;
import javax.crypto.spec.IvParameterSpec;
import javax.crypto.spec.OAEPParameterSpec;
import javax.crypto.spec.PSource;
import javax.crypto.spec.SecretKeySpec;

import org.junit.Test;
import org.junit.runner.RunWith;

/**
 * Tests for algorithm-agnostic functionality of {@code Cipher} implementations backed by Android
 * Keystore.
 */
@RunWith(AndroidJUnit4.class)
public class CipherTest {

    private static final String EXPECTED_PROVIDER_NAME = TestUtils.EXPECTED_CRYPTO_OP_PROVIDER_NAME;

    private static final String[] BASE_EXPECTED_ALGORITHMS = {
        ""AES/ECB/NoPadding"",
        ""AES/ECB/PKCS7Padding"",
        ""AES/CBC/NoPadding"",
        ""AES/CBC/PKCS7Padding"",
        ""AES/CTR/NoPadding"",
        ""AES/GCM/NoPadding"",
        ""RSA/ECB/NoPadding"",
        ""RSA/ECB/PKCS1Padding"",
        ""RSA/ECB/OAEPPadding"",
        ""RSA/ECB/OAEPWithSHA-1AndMGF1Padding"",
        ""RSA/ECB/OAEPWithSHA-224AndMGF1Padding"",
        ""RSA/ECB/OAEPWithSHA-256AndMGF1Padding"",
        ""RSA/ECB/OAEPWithSHA-384AndMGF1Padding"",
        ""RSA/ECB/OAEPWithSHA-512AndMGF1Padding""
    };

    private static final String[] DESEDE_ALGORITHMS = {
        ""DESede/CBC/NoPadding"",
        ""DESede/CBC/PKCS7Padding"",
        ""DESede/ECB/NoPadding"",
        ""DESede/ECB/PKCS7Padding"",
    };

    private static String[] EXPECTED_ALGORITHMS = BASE_EXPECTED_ALGORITHMS;

    // For tests of behavior largely unrelated to the selected algorithm, such as
    // unlockedDeviceRequired
    private static final String[] BASIC_ALGORITHMS = {
            ""AES/GCM/NoPadding"",
            ""RSA/ECB/OAEPWithSHA-256AndMGF1Padding"",
    };

    static {
      if (TestUtils.supports3DES()) {
        EXPECTED_ALGORITHMS = ObjectArrays
            .concat(BASE_EXPECTED_ALGORITHMS, DESEDE_ALGORITHMS, String.class);
      }
    }

    private static class KatVector {
        private final byte[] plaintext;
        private final byte[] ciphertext;
        private final AlgorithmParameterSpec params;

        private KatVector(String plaintextHex, String ciphertextHex) {
            this(plaintextHex, null, ciphertextHex);
        }

        private KatVector(String plaintextHex, AlgorithmParameterSpec params,
                String ciphertextHex) {
            this(HexEncoding.decode(plaintextHex), params, HexEncoding.decode(ciphertextHex));
        }

        private KatVector(byte[] plaintext, byte[] ciphertext) {
            this(plaintext, null, ciphertext);
        }

        private KatVector(byte[] plaintext, AlgorithmParameterSpec params, byte[] ciphertext) {
            this.plaintext = plaintext;
            this.ciphertext = ciphertext;
            this.params = params;
        }
    }
    private static final Map<String, KatVector> KAT_VECTORS =
            new TreeMap<String, KatVector>(String.CASE_INSENSITIVE_ORDER);
    static {
        // From RI
        KAT_VECTORS.put(""AES/ECB/NoPadding"", new KatVector(
                ""0383911bb1519d58e6656f3fd35639c502dbeb2196cea937fca272666cb4a80b"",
                ""6574c5065283b89e0c930019e4655d8516b98170db6516cd83e589bd9c5e5adc""));
        KAT_VECTORS.put(""AES/ECB/PKCS7Padding"", new KatVector(
                ""1ad3d73a3cfa66dac78a51a95c2cb2125ea701e6e9ecbca2415b436f0258e2ba7439b67545"",
                ""920f873f2f9e91bac4c9c948d66496a21b8b2606850490dac7abecae83317488ee550b9973ac5cd142""
                + ""f387d7d2a12752""));
        KAT_VECTORS.put(""AES/CBC/NoPadding"", new KatVector(
                ""1dffe21c8f18276c3a39ed0c53ab257b84efcedab60095c4cadd131143058cf7"",
                new IvParameterSpec(HexEncoding.decode(""10b3eea6cc8a7d6f48337e9b6987d28c"")),
                ""47ab115bfadca91eaebec73ab942a06f3121fdd5aa55d223bd2cbcc3855e1ef8""));
        KAT_VECTORS.put(""AES/CBC/PKCS7Padding"", new KatVector(
                ""9d49fb970b23bfe742ae7c45a773ada9faad84708c8858a06e4a192e0a90e2f6083548e0bf3f67"",
                new IvParameterSpec(HexEncoding.decode(""ecd87bf9c49f37dcd2294e309192289a"")),
                ""aeb64f48ec18a086eda7ee080948651a50b6f582ab54aac5454c9ab0a4de5b4a4abac526a4307011d1""
                + ""2881f1849c32ae""));
        KAT_VECTORS.put(""AES/CTR/NoPadding"", new KatVector(
                ""b4e786cab9df48d2fce0c7872651314db1318d1f31a1b10a2c334d2555b4117668"",
                new IvParameterSpec(HexEncoding.decode(""94d9f7a6d16f58018819b668020b68cc"")),
                ""022e74572a70be57a0b65b2fb5bc9b803ce48973b6163f528bbe1fd001e29d330a""));
        KAT_VECTORS.put(""AES/GCM/NoPadding"", new KatVector(
                ""03889a6ca811e3fd7e78467e3dae587d2110e80e98edbc9dfe17afba238c4c493186"",
                new GCMParameterSpec(128, HexEncoding.decode(""f67aaf97cdec65b12188315e"")),
                ""159eb1ffc86589b38f18097c32db646c7de3525b603876c3ae671bc2ca52a5395a374b377a915c9ed1""
                + ""a349abf9fc54c9ca81""));
        KAT_VECTORS.put(""RSA/ECB/NoPadding"", new KatVector(
                ""50c499d558c38fd48ea76832887db2abc76e4e153a98fd4323ccb8006d34f11724a5692fb101b0eb96""
                + ""060eb9d15222"",
                ""349b1d5061e98d0ab3f2327680bbc0cbb1b8ef8ee26148d7c67cf535223e3f78d822d369592ede29b1""
                + ""654aab25e6ae5e098318e55c13dc405f5ba27e5cc69ced32778592a51e6293a03f95e14ed17099fb""
                + ""0ac585e41297b87c3432953df0d98be7e505dc7de7bfe9d9ec750f475afeba4cc2dd78838c0d4399""
                + ""d8de02b07f00b292dc3d32d2a2f98ea5a5dac1a0fec4d01e5c3aea8c56eeff264896fb6cf2144401""
                + ""278c6663417bc00aafbb9eb97c056573cdec88d6ac6fd6c333d131337b16031da229029e3b6fe6f8""
                + ""ee427f2e90041e9636d67cddac75845914ce4be56092eed7188fe7e2bb33769efdeed86a7acbe15d""
                + ""debf92d9fbaaddede206acfa650697""));
        KAT_VECTORS.put(""RSA/ECB/PKCS1Padding"", new KatVector(
                ""aed8cd94f35b2a54cdd3ed771482bd87e256b995408558fb82e5d475d1ee54711472f899ad6cbb6847""
                + ""99e52ff1d57cbc39f4"",
                ""64148dee294dd3ea31d2b595ea661318cf90c89f71393cf6559087d6e8993e73eb1e6b5f4d3cfde3cb""
                + ""267938c5eca522b95a2df02df9c703dbe3103c157af0d2ed5b70da51cb4caa49061319420d0ea433""
                + ""f24b727530c162226bc806b7f39079cd494a5c8a242737413d27063f9fb74aadd20f521211316719""
                + ""c628fd4351d0608928949b6f59f351d9ccec4c596514335010834fcabd53a2cbb2642e0f83c4f89c""
                + ""199ee2c68ace9182cf484d99e86b0b2213c1cc113d24891958e5a0774b7486abae1475e46a939a94""
                + ""5d6491b98ad7979fd6e752b47e43e960557a0c0589d7d0444b011d75c9f5b143da6e1dcf7b678a2e""
                + ""f82fbe37a74df3e20fb1a9dbfd5978""));
        KAT_VECTORS.put(""RSA/ECB/OAEPPadding"", new KatVector(
                ""c219f4e3e37eae2315f0fa4ebc4b46ef0c6befbb43a51ceda07435fc88a9"",
                ""7a9bcfd0d02b6434025bbf5ba09c2dad118a4a3bca7cced8b404bc0fc2f17ddee13de82c8324294bf2""
                + ""60ad6e5171c2c3728a0c0fab20dd60e4e56cfef3e66239439ed2eddcc83ac8eeaedfd970e9966de3""
                + ""94ad1df0df503a0a640a49e10885b3a4115c3e94e893fff87bf9a5808350f957d6bc556ca6b08f81""
                + ""bf697704a3eb3db774797f883af0dcdc9bd9196d7595bab5e87d3187eb45b5771abe4e4dc70c25fa""
                + ""b9e3cddb6ae453a1d8e517d000779472e1376e5848b1654a51a9e90be4a4a6d0f6b8723c6e93c471""
                + ""313ea94f24504ca377b502057331355965a7e0b9c3b1d1fbd24ab5a4167f721d1ddac4d3c094d5c9""
                + ""0d2e277e9b5617cbf2770186323e89""));
        KAT_VECTORS.put(""RSA/ECB/OAEPWithSHA-1AndMGF1Padding"", new KatVector(
                ""bb2854620bb0e361d1384703dda12acee1fefc22024bcfc40a86390d5342c693aab8c7ed6517d8da86""
                + ""04492c9d"",
                ""77033c578f24ef0ed93bfe6dc6f7c3f9f0505e7562f67ce987a269cabaa8a3ae7dd5e567a8b37db42d""
                + ""a79aa86ea2e189af5b9560b39407ff86f2785cdaf660fc7c93649bc24a818de564cb0d03e7681fa8""
                + ""f3cd42b3bfc58c49d3f049e0c98b07aff95876f05ddc45ebaa7127a198f27ae0cfd161c5598ac795""
                + ""8ed386d98b13d45730e6dc16313fe012af27d7be0e45215040bbfb07f2d35e34291fe4335a68175a""
                + ""46be99a15c1ccf673659157e1f52105de5a0a6f8c9d946740216eefe2a01a37b0ab144a44ff0d800""
                + ""be713b5b44acf4fcb1a60d5db977af4d77fa77bdb8594032b2f5bbdd49346b08e0e98ab1051b462e""
                + ""160c1bff62b927cd26c936948b723a""));
        KAT_VECTORS.put(""RSA/ECB/OAEPWithSHA-224AndMGF1Padding"", new KatVector(
                ""1bae19434be6599d1987b1ed866dd6b684dcd908bd98d797250be545eafea46d05ebdf9018"",
                ""0f18b4a1153c6f8821e18a4275e4b570d540c8ad86bfc99146e5475238a43ecbe63bc81368cd64b9a2""
                + ""ab3ccd586e6afaad054c9d7bdc986adf022ec86335d110c53ebd5f2f2bd49d48d6da9541312c9b1b""
                + ""cc299ca4f59475869e4ec2253c91b137eae274a245fc9ee6262f74754bbda55d8bd25bfa4c1698f3""
                + ""a22d2d8d7fc6e9fbb56d828e61912b3085d82cceaeb1d2da425871575e7ba31a3d47b1b7d7df0bda""
                + ""81d62c75a9887bbc528fc6bb51db09884bb513b4cc94ca4a5fe0b370ca548dcdf60eebbf61e7efe7""
                + ""630fc47256d6d617fc1c2c774405f385650898abea03502cfbdcb53579fd18d896490e67aecdb7c7""
                + ""b7b950dc7ddba5c64188494c1a177b""));
        KAT_VECTORS.put(""RSA/ECB/OAEPWithSHA-256AndMGF1Padding"", new KatVector(
                ""332c2f2fc066fb29ec0928a52b5111ce6965546ce73927340c42d33b56b6ba547b77ac361ac0d13316""
                + ""345ca953840023d892fa4ff1aa32cc66d5aa88b79867"",
                ""942c0ba1c67a34a7e116d9281b1df5084c66bc1458faf1b26d4f0f63a57307a9addcd3e5d2f3320071""
                + ""5a3d95ae84fb40a8dfe4cb0a28873fd5883ff8ee6efbfe38c460c755577b34fcf05bb2077afec7b2""
                + ""203799022be6a0903915e01e94abc51efe9c5548eb86bbbb4fd7f3bfc7b86f388128b6df1e6ce651""
                + ""230c6bc18bbf55b029f1e31da880c27d947ff97519df66a57ead6db791c4978f1d62edec0d89bb16""
                + ""83d237213f3f24271ddb8c4b50a82527954f0e49ae44d3acd8ddd3a57cfbfa456dd40675d5d75542""
                + ""31c6b79c7fb3500b1631be1d100e67d85ce423845fdc7c7f45e346a8ba573f5d11de9009069468dd""
                + ""8d517ad4adb1509dd5173ee1862d74""));
        KAT_VECTORS.put(""RSA/ECB/OAEPWithSHA-384AndMGF1Padding"", new KatVector(
                ""f51f158cbad4dbab38403b839c724f09a480c49be29c0e72615539dbe57ec86143f31f19392f419b5b""
                + ""e4ba9e3c6f1e870d307a7cf1a9e2"",
                ""944243f35f534e7a273e94986b6835a4f5cdc5bc4efb9970d4760986599a02f652a848fcae333ff25a""
                + ""64108c9b900aaf002688398ad9fc17c73be52726306af9c13540df9d1765336b6f09ba4cb8a54d72""
                + ""5a4e45854bfa3802cfb110a6d7f7054e6072440ec00da62828cb75fe2566ec5be79eb8a3d1fbe2c2""
                + ""4439c107e5018e445e201ad80725755543c00dec50bb464c6ca897600eb3cda51fcef8161ac13d75""
                + ""a3eb30d385a1e718a61ae1b5d47aadb966fc007becc84db397d0b3cd983121872f9975995153e869""
                + ""9e24554a3c5e885f0ed8cd03e916da5ed541f1598da9bd6209447301d00f086153da353deff9d045""
                + ""8976ff7570410f0bdcfb3f56b782f5""));
        KAT_VECTORS.put(""RSA/ECB/OAEPWithSHA-512AndMGF1Padding"", new KatVector(
                ""d45f6ccc7e663957f234c237c1f09bf7791f6f5c1b9ef4fefb16e55ded0d96112e590f1bb08a60f85c""
                + ""2d0d2533f1d69792dfd8d647d880b18f87cfe32488c73613a3d535da7d776d90d9a4ba6a0311f456""
                + ""8511da49107c"",
                ""5a037df3e5d6f3f703541e2db2aef7c69985e513bdff67c8ade6a09f50e27267bfb444f6c69b40a77a""
                + ""9136a27b29876af9d2bf4e7099863445d35b188d31f376b89fbd196059667ca657e10b9454c2b25f""
                + ""046fc9f7b42506e382e6b6fd99409cf97e865e65f8dce5d14a06b8aa8833c4bc72c8764467758f2d""
                + ""7960243161dce4ca8231e91bfcd3c933a80bc703ceab976224c876b1f550f91a6c2a0332d4377bd8""
                + ""dfe4b1283ab114e517b7b9e4a6e0bf166d5b506e7a3b7328078e12cb23b1d938760767dc9b3c3eb0""
                + ""848ddda101792aca9273ad414314c13fc511ffa0358a8f4c5f38edded3a2dc111fa62c80e6032c32""
                + ""ae04aeac7729f16a6310f1f6785c27""));


        KAT_VECTORS.put(""DESede/CBC/NoPadding"",
                new KatVector(""eac1b7959e1e23c11dc4a0e233eedd99e5bf5dd391a5f107d006133a9af3e385"",
                        new IvParameterSpec(HexEncoding.decode(""ecd87bf9c49f37dc"")),
                        ""632511c46680d60883a228e62cd31244ad61b987e8df7901dae0eb220c839689""));
        KAT_VECTORS.put(""DESede/CBC/PKCS7Padding"",
                new KatVector(""31323334353637383132333435363738"",
                        new IvParameterSpec(HexEncoding.decode(""DFCA366848DEA6BB"")),
                        ""e70bb5761d796d7b0eb40b5b60deb6a9726f72d97cf2ada4""));
        KAT_VECTORS.put(""DESede/ECB/NoPadding"",
                new KatVector(""31323334353637383132333435363738"",
                        ""ade119f9e35ab3e9ade119f9e35ab3e9""));
        KAT_VECTORS.put(""DESede/ECB/PKCS7Padding"",
                new KatVector(""31323334353637383132333435363738"",
                        ""ade119f9e35ab3e9ade119f9e35ab3e94bcb01bbc0d05526""));
    }

    private static final long DAY_IN_MILLIS = TestUtils.DAY_IN_MILLIS;

    private static final byte[] AES128_KAT_KEY_BYTES =
            HexEncoding.decode(""7d9f11a0da111e9d8bdd14f04648ed91"");

    private static final byte[] AES192_KAT_KEY_BYTES =
            HexEncoding.decode(""69ef2c44a48d3dc4d5744a281f7ebb5ca976c2202f91e10c"");

    private static final byte[] AES256_KAT_KEY_BYTES =
            HexEncoding.decode(""cf601cc10aaf434d1f01747136aff222af7fb426d101901712214c3fea18125f"");

    private static final byte[] DESede_KAT_KEY_BYTES = HexEncoding.decode(
            ""5EBE2294ECD0E0F08EAB7690D2A6EE6926AE5CC854E36B6B"");

    private Context getContext() {
        return InstrumentationRegistry.getInstrumentation().getTargetContext();
    }

    private class DeviceLockSession extends ActivityManagerTestBase implements AutoCloseable {

        private LockScreenSession mLockCredential;

        public DeviceLockSession() throws Exception {
            setUp();
            mLockCredential = new LockScreenSession();
            mLockCredential.setLockCredential();
        }

        public void performDeviceLock() {
            mLockCredential.sleepDevice();
            KeyguardManager keyguardManager = (KeyguardManager)getContext().getSystemService(Context.KEYGUARD_SERVICE);
            for (int i = 0; i < 25 && !keyguardManager.isDeviceLocked(); i++) {
                SystemClock.sleep(200);
            }
        }

        public void performDeviceUnlock() throws Exception {
            mLockCredential.gotoKeyguard();
            UiDeviceUtils.pressUnlockButton();
            mLockCredential.enterAndConfirmLockCredential();
            launchHomeActivity();
            KeyguardManager keyguardManager = (KeyguardManager)getContext().getSystemService(
                    Context.KEYGUARD_SERVICE);
            for (int i = 0; i < 25 && keyguardManager.isDeviceLocked(); i++) {
                SystemClock.sleep(200);
            }
            assertFalse(keyguardManager.isDeviceLocked());
        }

        @Override
        public void close() throws Exception {
            mLockCredential.close();
        }
    }

    @Presubmit"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.ExtractorTest"	"isClosed"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/ExtractorTest.java"	""	"/*
 *.
 */

package android.mediav2.cts;

import android.content.Context;
import android.content.res.AssetFileDescriptor;
import android.media.MediaCodec;
import android.media.MediaCodecInfo;
import android.media.MediaDataSource;
import android.media.MediaExtractor;
import android.media.MediaFormat;
import android.net.Uri;
import android.os.ParcelFileDescriptor;
import android.os.PersistableBundle;
import android.util.Log;
import android.webkit.cts.CtsTestServer;

import androidx.test.filters.LargeTest;
import androidx.test.filters.SmallTest;
import androidx.test.platform.app.InstrumentationRegistry;

import org.apache.http.Header;
import org.apache.http.HttpRequest;
import org.junit.After;
import org.junit.Before;
import org.junit.Ignore;
import org.junit.Rule;
import org.junit.Test;
import org.junit.experimental.runners.Enclosed;
import org.junit.rules.TestName;
import org.junit.runner.RunWith;
import org.junit.runners.Parameterized;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.Reader;
import java.io.StreamTokenizer;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.zip.CRC32;

import static android.mediav2.cts.CodecTestBase.hasDecoder;
import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;
import static org.junit.Assert.fail;
import static org.junit.Assume.assumeTrue;

class TestMediaDataSource extends MediaDataSource {
    private static final String LOG_TAG = TestMediaDataSource.class.getSimpleName();
    private static final boolean ENABLE_LOGS = false;
    private byte[] mData;
    private boolean mFatalGetSize;
    private boolean mFatalReadAt;
    private boolean mIsClosed = false;

    static TestMediaDataSource fromString(String inpPath, boolean failSize, boolean failRead)
            throws IOException {
        try (FileInputStream fInp = new FileInputStream(inpPath)) {
            int size = (int) new File(inpPath).length();
            byte[] data = new byte[size];
            fInp.read(data, 0, size);
            return new TestMediaDataSource(data, failSize, failRead);
        }
    }

    private TestMediaDataSource(byte[] data, boolean fatalGetSize, boolean fatalReadAt) {
        mData = data;
        mFatalGetSize = fatalGetSize;
        mFatalReadAt = fatalReadAt;
    }

    @Override
    public synchronized int readAt(long srcOffset, byte[] buffer, int dstOffset, int size)
            throws IOException {
        if (mFatalReadAt) {
            throw new IOException(""malformed media data source"");
        }
        if (srcOffset >= mData.length) {
            return -1;
        }
        if (srcOffset + size > mData.length) {
            size = mData.length - (int) srcOffset;
        }
        System.arraycopy(mData, (int) srcOffset, buffer, dstOffset, size);
        return size;
    }

    @Override
    public synchronized long getSize() throws IOException {
        if (mFatalGetSize) {
            throw new IOException(""malformed media data source"");
        }
        if (ENABLE_LOGS) {
            Log.v(LOG_TAG, ""getSize: "" + mData.length);
        }
        return mData.length;
    }

    @Override
    public synchronized void close() {
        mIsClosed = true;
    }

    public boolean isClosed() {
        return mIsClosed;
    }
}

@RunWith(Enclosed.class)
public class ExtractorTest {
    private static final String LOG_TAG = ExtractorTest.class.getSimpleName();
    private static final boolean ENABLE_LOGS = false;
    private static final int MAX_SAMPLE_SIZE = 4 * 1024 * 1024;
    private static final String EXT_SEL_KEY = ""ext-sel"";
    static private final List<String> codecListforTypeMp4 =
            Arrays.asList(MediaFormat.MIMETYPE_AUDIO_MPEG, MediaFormat.MIMETYPE_AUDIO_AAC,
                    MediaFormat.MIMETYPE_AUDIO_FLAC, MediaFormat.MIMETYPE_AUDIO_VORBIS,
                    MediaFormat.MIMETYPE_AUDIO_OPUS, MediaFormat.MIMETYPE_VIDEO_MPEG2,
                    MediaFormat.MIMETYPE_VIDEO_MPEG4, MediaFormat.MIMETYPE_VIDEO_H263,
                    MediaFormat.MIMETYPE_VIDEO_AVC, MediaFormat.MIMETYPE_VIDEO_HEVC);
    static private final List<String> codecListforTypeWebm =
            Arrays.asList(MediaFormat.MIMETYPE_AUDIO_VORBIS, MediaFormat.MIMETYPE_AUDIO_OPUS,
                    MediaFormat.MIMETYPE_VIDEO_VP8, MediaFormat.MIMETYPE_VIDEO_VP9);
    static private final List<String> codecListforType3gp =
            Arrays.asList(MediaFormat.MIMETYPE_AUDIO_AAC, MediaFormat.MIMETYPE_AUDIO_AMR_NB,
                    MediaFormat.MIMETYPE_AUDIO_AMR_WB, MediaFormat.MIMETYPE_VIDEO_MPEG4,
                    MediaFormat.MIMETYPE_VIDEO_H263, MediaFormat.MIMETYPE_VIDEO_AVC);
    static private final List<String> codecListforTypeMkv =
            Arrays.asList(MediaFormat.MIMETYPE_AUDIO_MPEG, MediaFormat.MIMETYPE_AUDIO_AAC,
                    MediaFormat.MIMETYPE_AUDIO_FLAC, MediaFormat.MIMETYPE_AUDIO_VORBIS,
                    MediaFormat.MIMETYPE_AUDIO_OPUS, MediaFormat.MIMETYPE_VIDEO_MPEG2,
                    MediaFormat.MIMETYPE_VIDEO_MPEG4, MediaFormat.MIMETYPE_VIDEO_H263,
                    MediaFormat.MIMETYPE_VIDEO_AVC, MediaFormat.MIMETYPE_VIDEO_HEVC,
                    MediaFormat.MIMETYPE_VIDEO_VP8, MediaFormat.MIMETYPE_VIDEO_VP9);
    static private final List<String> codecListforTypeOgg =
            Arrays.asList(MediaFormat.MIMETYPE_AUDIO_VORBIS, MediaFormat.MIMETYPE_AUDIO_OPUS);
    static private final List<String> codecListforTypeTs =
            Arrays.asList(MediaFormat.MIMETYPE_AUDIO_AAC, MediaFormat.MIMETYPE_VIDEO_MPEG2,
                    MediaFormat.MIMETYPE_VIDEO_AVC);
    static private final List<String> codecListforTypePs =
            Arrays.asList(MediaFormat.MIMETYPE_VIDEO_MPEG2);
    static private final List<String> codecListforTypeRaw =
            Arrays.asList(MediaFormat.MIMETYPE_AUDIO_AAC, MediaFormat.MIMETYPE_AUDIO_FLAC,
                    MediaFormat.MIMETYPE_AUDIO_MPEG, MediaFormat.MIMETYPE_AUDIO_AMR_NB,
                    MediaFormat.MIMETYPE_AUDIO_AMR_WB, MediaFormat.MIMETYPE_AUDIO_RAW);
    static private final List<String> codecListforTypeWav =
            Arrays.asList(MediaFormat.MIMETYPE_AUDIO_RAW,  MediaFormat.MIMETYPE_AUDIO_G711_ALAW,
                    MediaFormat.MIMETYPE_AUDIO_G711_MLAW,  MediaFormat.MIMETYPE_AUDIO_MSGSM);
    // List of codecs that are not required to be supported as per CDD but are tested
    static private final List<String> codecListSupp =
            Arrays.asList(MediaFormat.MIMETYPE_VIDEO_AV1, MediaFormat.MIMETYPE_AUDIO_AC3,
                    MediaFormat.MIMETYPE_AUDIO_AC4, MediaFormat.MIMETYPE_AUDIO_EAC3);
    private static String mInpPrefix = WorkDir.getMediaDirString();
    private static String extSel;

    static {
        android.os.Bundle args = InstrumentationRegistry.getArguments();
        final String defSel = ""mp4;webm;3gp;mkv;ogg;supp;raw;ts;ps;wav"";
        extSel = (null == args.getString(EXT_SEL_KEY)) ? defSel : args.getString(EXT_SEL_KEY);
    }

    static private boolean shouldRunTest(String mime) {
        boolean result = false;
        if ((extSel.contains(""mp4"") && codecListforTypeMp4.contains(mime)) ||
                (extSel.contains(""webm"") && codecListforTypeWebm.contains(mime)) ||
                (extSel.contains(""3gp"") && codecListforType3gp.contains(mime)) ||
                (extSel.contains(""mkv"") && codecListforTypeMkv.contains(mime)) ||
                (extSel.contains(""ogg"") && codecListforTypeOgg.contains(mime)) ||
                (extSel.contains(""ts"") && codecListforTypeTs.contains(mime)) ||
                (extSel.contains(""ps"") && codecListforTypePs.contains(mime)) ||
                (extSel.contains(""raw"") && codecListforTypeRaw.contains(mime)) ||
                (extSel.contains(""wav"") && codecListforTypeWav.contains(mime)) ||
                (extSel.contains(""supp"") && codecListSupp.contains(mime)))
            result = true;
        return result;
    }

    private static boolean isExtractorOKonEOS(MediaExtractor extractor) {
        return extractor.getSampleTrackIndex() < 0 && extractor.getSampleSize() < 0 &&
                extractor.getSampleFlags() < 0 && extractor.getSampleTime() < 0;
    }

    private static boolean isSampleInfoIdentical(MediaCodec.BufferInfo refSample,
            MediaCodec.BufferInfo testSample) {
        return refSample.flags == testSample.flags && refSample.size == testSample.size &&
                refSample.presentationTimeUs == testSample.presentationTimeUs;
    }

    private static boolean isSampleInfoValidAndIdentical(MediaCodec.BufferInfo refSample,
            MediaCodec.BufferInfo testSample) {
        return refSample.flags == testSample.flags && refSample.size == testSample.size &&
                Math.abs(refSample.presentationTimeUs - testSample.presentationTimeUs) <= 1 &&
                refSample.flags >= 0 && refSample.size >= 0 && refSample.presentationTimeUs >= 0;
    }

    static boolean isCSDIdentical(MediaFormat refFormat, MediaFormat testFormat) {
        String mime = refFormat.getString(MediaFormat.KEY_MIME);
        for (int i = 0; ; i++) {
            String csdKey = ""csd-"" + i;
            boolean refHasCSD = refFormat.containsKey(csdKey);
            boolean testHasCSD = testFormat.containsKey(csdKey);
            if (refHasCSD != testHasCSD) {
                if (ENABLE_LOGS) {
                    Log.w(LOG_TAG, ""error, ref fmt has CSD: "" + refHasCSD + "" test fmt has CSD: "" +
                            testHasCSD);
                }
                return false;
            }
            if (refHasCSD) {
                Log.v(LOG_TAG, mime + "" has "" + csdKey);
                ByteBuffer r = refFormat.getByteBuffer(csdKey);
                ByteBuffer t = testFormat.getByteBuffer(csdKey);
                if (!r.equals(t)) {
                    if (ENABLE_LOGS) {
                        Log.w(LOG_TAG, ""ref CSD and test CSD buffers are not identical"");
                    }
                    return false;
                }
            } else break;
        }
        return true;
    }

    static boolean isFormatSimilar(MediaFormat refFormat, MediaFormat testFormat) {
        String refMime = refFormat.getString(MediaFormat.KEY_MIME);
        String testMime = testFormat.getString(MediaFormat.KEY_MIME);

        if (!refMime.equals(testMime)) return false;
        if (refFormat.getLong(MediaFormat.KEY_DURATION) !=
                    testFormat.getLong(MediaFormat.KEY_DURATION)) {
            Log.w(LOG_TAG, ""Duration mismatches ref / test = "" +
                                   refFormat.getLong(MediaFormat.KEY_DURATION) + "" / "" +
                                   testFormat.getLong(MediaFormat.KEY_DURATION));
            // TODO (b/163477410)(b/163478168)
//            return false;
        }
        if (!isCSDIdentical(refFormat, testFormat)) return false;
        if (refMime.startsWith(""audio/"")) {
            if (refFormat.getInteger(MediaFormat.KEY_CHANNEL_COUNT) !=
                        testFormat.getInteger(MediaFormat.KEY_CHANNEL_COUNT)) return false;
            if (refFormat.getInteger(MediaFormat.KEY_SAMPLE_RATE) !=
                        testFormat.getInteger(MediaFormat.KEY_SAMPLE_RATE)) return false;
        } else if (refMime.startsWith(""video/"")) {
            if (refFormat.getInteger(MediaFormat.KEY_WIDTH) !=
                        testFormat.getInteger(MediaFormat.KEY_WIDTH)) return false;
            if (refFormat.getInteger(MediaFormat.KEY_HEIGHT) !=
                        testFormat.getInteger(MediaFormat.KEY_HEIGHT)) return false;
        }
        return true;
    }

    private static boolean isMediaSimilar(MediaExtractor refExtractor, MediaExtractor testExtractor,
            String mime, int sampleLimit) {
        ByteBuffer refBuffer = ByteBuffer.allocate(MAX_SAMPLE_SIZE);
        ByteBuffer testBuffer = ByteBuffer.allocate(MAX_SAMPLE_SIZE);

        int noOfTracksMatched = 0;
        for (int refTrackID = 0; refTrackID < refExtractor.getTrackCount(); refTrackID++) {
            MediaFormat refFormat = refExtractor.getTrackFormat(refTrackID);
            String refMime = refFormat.getString(MediaFormat.KEY_MIME);
            if (mime != null && !refMime.equals(mime)) {
                continue;
            }
            for (int testTrackID = 0; testTrackID < testExtractor.getTrackCount(); testTrackID++) {
                MediaFormat testFormat = testExtractor.getTrackFormat(testTrackID);
                if (!isFormatSimilar(refFormat, testFormat)) {
                    continue;
                }
                refExtractor.selectTrack(refTrackID);
                testExtractor.selectTrack(testTrackID);

                MediaCodec.BufferInfo refSampleInfo = new MediaCodec.BufferInfo();
                MediaCodec.BufferInfo testSampleInfo = new MediaCodec.BufferInfo();
                boolean areTracksIdentical = true;
                for (int frameCount = 0; ; frameCount++) {
                    refSampleInfo.set(0, (int) refExtractor.getSampleSize(),
                            refExtractor.getSampleTime(), refExtractor.getSampleFlags());
                    testSampleInfo.set(0, (int) testExtractor.getSampleSize(),
                            testExtractor.getSampleTime(), testExtractor.getSampleFlags());
                    if (!isSampleInfoValidAndIdentical(refSampleInfo, testSampleInfo)) {
                        if (ENABLE_LOGS) {
                            Log.d(LOG_TAG,
                                    "" Mime: "" + refMime + "" mismatch for sample: "" + frameCount);
                            Log.d(LOG_TAG, "" flags exp/got: "" +
                                    refSampleInfo.flags + '/' + testSampleInfo.flags);
                            Log.d(LOG_TAG, "" size exp/got: "" +
                                    refSampleInfo.size + '/' + testSampleInfo.size);
                            Log.d(LOG_TAG, "" ts exp/got: "" + refSampleInfo.presentationTimeUs +
                                    '/' + testSampleInfo.presentationTimeUs);
                        }
                        areTracksIdentical = false;
                        break;
                    }
                    int refSz = refExtractor.readSampleData(refBuffer, 0);
                    if (refSz != refSampleInfo.size) {
                        if (ENABLE_LOGS) {
                            Log.d(LOG_TAG, ""Mime: "" + refMime + "" Size exp/got: "" +
                                    refSampleInfo.size + '/' + refSz);
                        }
                        areTracksIdentical = false;
                        break;
                    }
                    int testSz = testExtractor.readSampleData(testBuffer, 0);
                    if (testSz != testSampleInfo.size) {
                        if (ENABLE_LOGS) {
                            Log.d(LOG_TAG, ""Mime: "" + refMime + "" Size exp/got: "" +
                                    testSampleInfo.size + '/' + testSz);
                        }
                        areTracksIdentical = false;
                        break;
                    }
                    int trackIndex = refExtractor.getSampleTrackIndex();
                    if (trackIndex != refTrackID) {
                        if (ENABLE_LOGS) {
                            Log.d(LOG_TAG, ""Mime: "" + refMime +
                                    "" TrackID exp/got: "" + refTrackID + '/' + trackIndex);
                        }
                        areTracksIdentical = false;
                        break;
                    }
                    trackIndex = testExtractor.getSampleTrackIndex();
                    if (trackIndex != testTrackID) {
                        if (ENABLE_LOGS) {
                            Log.d(LOG_TAG, ""Mime: "" + refMime +
                                    "" TrackID exp/got: "" + testTrackID + '/' + trackIndex);
                        }
                        areTracksIdentical = false;
                        break;
                    }
                    if (!testBuffer.equals(refBuffer)) {
                        if (ENABLE_LOGS) {
                            Log.d(LOG_TAG, ""Mime: "" + refMime + "" sample data is not identical"");
                        }
                        areTracksIdentical = false;
                        break;
                    }
                    boolean haveRefSamples = refExtractor.advance();
                    boolean haveTestSamples = testExtractor.advance();
                    if (haveRefSamples != haveTestSamples) {
                        if (ENABLE_LOGS) {
                            Log.d(LOG_TAG, ""Mime: "" + refMime + "" Mismatch in sampleCount"");
                        }
                        areTracksIdentical = false;
                        break;
                    }

                    if (!haveRefSamples && !isExtractorOKonEOS(refExtractor)) {
                        if (ENABLE_LOGS) {
                            Log.d(LOG_TAG, ""Mime: "" + refMime + "" calls post advance() are not OK"");
                        }
                        areTracksIdentical = false;
                        break;
                    }
                    if (!haveTestSamples && !isExtractorOKonEOS(testExtractor)) {
                        if (ENABLE_LOGS) {
                            Log.d(LOG_TAG, ""Mime: "" + refMime + "" calls post advance() are not OK"");
                        }
                        areTracksIdentical = false;
                        break;
                    }
                    if (ENABLE_LOGS) {
                        Log.v(LOG_TAG, ""Mime: "" + refMime + "" Sample: "" + frameCount +
                                "" flags: "" + refSampleInfo.flags +
                                "" size: "" + refSampleInfo.size +
                                "" ts: "" + refSampleInfo.presentationTimeUs);
                    }
                    if (!haveRefSamples || frameCount >= sampleLimit) {
                        break;
                    }
                }
                testExtractor.unselectTrack(testTrackID);
                refExtractor.unselectTrack(refTrackID);
                if (areTracksIdentical) {
                    noOfTracksMatched++;
                    break;
                }
            }
            if (mime != null && noOfTracksMatched > 0) break;
        }
        if (mime == null) {
            return noOfTracksMatched == refExtractor.getTrackCount();
        } else {
            return noOfTracksMatched > 0;
        }
    }

    private static long readAllData(MediaExtractor extractor, String mime, int sampleLimit) {
        CRC32 checksum = new CRC32();
        ByteBuffer buffer = ByteBuffer.allocate(MAX_SAMPLE_SIZE);
        int tracksSelected = 0;
        for (int trackID = 0; trackID < extractor.getTrackCount(); trackID++) {
            MediaFormat format = extractor.getTrackFormat(trackID);
            String srcMime = format.getString(MediaFormat.KEY_MIME);
            if (mime != null && !srcMime.equals(mime)) {
                continue;
            }
            extractor.selectTrack(trackID);
            tracksSelected++;
            if (srcMime.startsWith(""audio/"")) {
                buffer.putInt(0);
                buffer.putInt(format.getInteger(MediaFormat.KEY_SAMPLE_RATE));
                buffer.putInt(format.getInteger(MediaFormat.KEY_CHANNEL_COUNT));
            } else if (srcMime.startsWith(""video/"")) {
                buffer.putInt(1);
                buffer.putInt(format.getInteger(MediaFormat.KEY_WIDTH));
                buffer.putInt(format.getInteger(MediaFormat.KEY_HEIGHT));
            } else {
                buffer.putInt(2);
            }
            buffer.putLong(format.getLong(MediaFormat.KEY_DURATION));
            for (int i = 0; ; i++) {
                String csdKey = ""csd-"" + i;
                if (format.containsKey(csdKey)) {
                    checksum.update(format.getByteBuffer(csdKey));
                } else break;
            }
        }
        assertTrue(tracksSelected > 0);
        buffer.flip();
        checksum.update(buffer);

        MediaCodec.BufferInfo sampleInfo = new MediaCodec.BufferInfo();
        for (int sampleCount = 0; sampleCount < sampleLimit; sampleCount++) {
            sampleInfo.set(0, (int) extractor.getSampleSize(), extractor.getSampleTime(),
                    extractor.getSampleFlags());
            extractor.readSampleData(buffer, 0);
            checksum.update(buffer);
            assertEquals(sampleInfo.size, buffer.limit());
            assertTrue(sampleInfo.flags != -1);
            assertTrue(sampleInfo.presentationTimeUs != -1);
            buffer.position(0);
            buffer.putInt(sampleInfo.size)
                    .putInt(sampleInfo.flags)
                    .putLong(sampleInfo.presentationTimeUs);
            buffer.flip();
            checksum.update(buffer);
            sampleCount++;
            if (!extractor.advance()) {
                assertTrue(isExtractorOKonEOS(extractor));
                break;
            }
        }
        for (int trackID = 0; trackID < extractor.getTrackCount(); trackID++) {
            extractor.unselectTrack(trackID);
        }
        return checksum.getValue();
    }

    private static native long nativeReadAllData(String srcPath, String mime, int sampleLimit,
            String[] keys, String[] values, boolean isSrcUrl);

    /**
     * Tests setDataSource(...) Api by observing the extractor behavior after its successful
     * instantiation using a media stream.
     */
    @SmallTest
    public static class SetDataSourceTest {
        @Rule
        public TestName testName = new TestName();

        private static final String mInpMedia = ""ForBiggerEscapes.mp4"";
        private static final String mResString = ""raw/forbiggerescapes"";
        private CtsTestServer mWebServer;
        private String mInpMediaUrl;
        private MediaExtractor mRefExtractor;

        static {
            System.loadLibrary(""ctsmediav2extractor_jni"");
        }

        @Before
        public void setUp() throws IOException {
            mRefExtractor = new MediaExtractor();
            Preconditions.assertTestFileExists(mInpPrefix + mInpMedia);
            mRefExtractor.setDataSource(mInpPrefix + mInpMedia);
            try {
                Context context = InstrumentationRegistry.getInstrumentation().getTargetContext();
                mWebServer = new CtsTestServer(context);
                mInpMediaUrl = mWebServer.getAssetUrl(mResString);
            } catch (Exception e) {
                fail(e.getMessage());
            }
        }

        @After
        public void tearDown() {
            mRefExtractor.release();
            mRefExtractor = null;
            mWebServer.shutdown();
        }

        private static boolean areMetricsIdentical(MediaExtractor refExtractor,
                MediaExtractor testExtractor) {
            PersistableBundle bundle = refExtractor.getMetrics();
            int refNumTracks = bundle.getInt(MediaExtractor.MetricsConstants.TRACKS);
            String refFormat = bundle.getString(MediaExtractor.MetricsConstants.FORMAT);
            String refMime = bundle.getString(MediaExtractor.MetricsConstants.MIME_TYPE);
            bundle = testExtractor.getMetrics();
            int testNumTracks = bundle.getInt(MediaExtractor.MetricsConstants.TRACKS);
            String testFormat = bundle.getString(MediaExtractor.MetricsConstants.FORMAT);
            String testMime = bundle.getString(MediaExtractor.MetricsConstants.MIME_TYPE);
            boolean result = testNumTracks == refNumTracks && testFormat.equals(refFormat) &&
                    testMime.equals(refMime);
            if (ENABLE_LOGS) {
                Log.d(LOG_TAG, "" NumTracks exp/got: "" + refNumTracks + '/' + testNumTracks);
                Log.d(LOG_TAG, "" Format exp/got: "" + refFormat + '/' + testFormat);
                Log.d(LOG_TAG, "" Mime exp/got: "" + refMime + '/' + testMime);
            }
            return result;
        }

        private static boolean isSeekOk(MediaExtractor refExtractor, MediaExtractor testExtractor) {
            final long maxEstDuration = 14000000;
            final int MAX_SEEK_POINTS = 7;
            final long mSeed = 0x12b9b0a1;
            final Random randNum = new Random(mSeed);
            MediaCodec.BufferInfo refSampleInfo = new MediaCodec.BufferInfo();
            MediaCodec.BufferInfo testSampleInfo = new MediaCodec.BufferInfo();
            boolean result = true;
            for (int trackID = 0; trackID < refExtractor.getTrackCount() && result; trackID++) {
                refExtractor.selectTrack(trackID);
                testExtractor.selectTrack(trackID);
                for (int i = 0; i < MAX_SEEK_POINTS && result; i++) {
                    long pts = (long) (randNum.nextDouble() * maxEstDuration);
                    for (int mode = MediaExtractor.SEEK_TO_PREVIOUS_SYNC;
                         mode <= MediaExtractor.SEEK_TO_CLOSEST_SYNC; mode++) {
                        refExtractor.seekTo(pts, mode);
                        testExtractor.seekTo(pts, mode);
                        refSampleInfo.set(0, (int) refExtractor.getSampleSize(),
                                refExtractor.getSampleTime(), refExtractor.getSampleFlags());
                        testSampleInfo.set(0, (int) testExtractor.getSampleSize(),
                                testExtractor.getSampleTime(), testExtractor.getSampleFlags());
                        result = isSampleInfoIdentical(refSampleInfo, testSampleInfo);
                        int refTrackIdx = refExtractor.getSampleTrackIndex();
                        int testTrackIdx = testExtractor.getSampleTrackIndex();
                        result &= (refTrackIdx == testTrackIdx);
                        if (ENABLE_LOGS) {
                            Log.d(LOG_TAG, "" mode/pts/trackId:"" + mode + ""/"" + pts + ""/"" + trackID);
                            Log.d(LOG_TAG, "" trackId exp/got: "" + refTrackIdx + '/' + testTrackIdx);
                            Log.d(LOG_TAG, "" flags exp/got: "" +
                                    refSampleInfo.flags + '/' + testSampleInfo.flags);
                            Log.d(LOG_TAG, "" size exp/got: "" +
                                    refSampleInfo.size + '/' + testSampleInfo.size);
                            Log.d(LOG_TAG, "" ts exp/got: "" + refSampleInfo.presentationTimeUs +
                                    '/' + testSampleInfo.presentationTimeUs);
                        }
                    }
                }
                refExtractor.unselectTrack(trackID);
                testExtractor.unselectTrack(trackID);
            }
            return result;
        }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.ExtractorTest"	"testDataSourceNative"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/ExtractorTest.java"	""	"public void testDataSourceNative() {
            Preconditions.assertTestFileExists(mInpPrefix + mInpMedia);
            assertTrue(testName.getMethodName() + "" failed "",
                    nativeTestDataSource(mInpPrefix + mInpMedia, mInpMediaUrl));
        }
    }

    /**
     * Encloses extractor functionality tests
     */
    @RunWith(Parameterized.class)
    public static class FunctionalityTest {
        private static final int MAX_SEEK_POINTS = 7;
        private static final long mSeed = 0x12b9b0a1;
        private final Random mRandNum = new Random(mSeed);
        private String[] mSrcFiles;
        private String mMime;

        static {
            System.loadLibrary(""ctsmediav2extractor_jni"");
        }

        @Rule
        public TestName testName = new TestName();

        @Parameterized.Parameters(name = ""{index}({0})"")
        public static Collection<Object[]> input() {
            return Arrays.asList(new Object[][]{
                    {MediaFormat.MIMETYPE_VIDEO_MPEG2, new String[]{
                            ""bbb_cif_768kbps_30fps_mpeg2_stereo_48kHz_192kbps_mp3.mp4"",
                            ""bbb_cif_768kbps_30fps_mpeg2.mkv"",
                            /* TODO(b/162919907)
                            ""bbb_cif_768kbps_30fps_mpeg2.vob"",*/
                            /* TODO(b/162715861)
                            ""bbb_cif_768kbps_30fps_mpeg2.ts"" */}},
                    {MediaFormat.MIMETYPE_VIDEO_H263, new String[]{
                            ""bbb_cif_768kbps_30fps_h263.mp4"",
                            ""bbb_cif_768kbps_30fps_h263_stereo_48kHz_192kbps_flac.mkv"",
                            ""bbb_cif_768kbps_30fps_h263_mono_8kHz_12kbps_amrnb.3gp"",}},
                    {MediaFormat.MIMETYPE_VIDEO_MPEG4, new String[]{
                            ""bbb_cif_768kbps_30fps_mpeg4.mkv"",
                            ""bbb_cif_768kbps_30fps_mpeg4_stereo_48kHz_192kbps_flac.mp4"",
                            ""bbb_cif_768kbps_30fps_mpeg4_mono_16kHz_20kbps_amrwb.3gp"",}},
                    {MediaFormat.MIMETYPE_VIDEO_AVC, new String[]{
                            ""bbb_cif_768kbps_30fps_avc_stereo_48kHz_192kbps_vorbis.mp4"",
                            ""bbb_cif_768kbps_30fps_avc_stereo_48kHz_192kbps_aac.mkv"",
                            ""bbb_cif_768kbps_30fps_avc_stereo_48kHz_192kbps_aac.3gp"",
                            /* TODO(b/162715861)
                            ""bbb_cif_768kbps_30fps_avc.ts"",*/}},
                    {MediaFormat.MIMETYPE_VIDEO_HEVC, new String[]{
                            ""bbb_cif_768kbps_30fps_hevc_stereo_48kHz_192kbps_opus.mp4"",
                            ""bbb_cif_768kbps_30fps_hevc_stereo_48kHz_192kbps_mp3.mkv"",}},
                    {MediaFormat.MIMETYPE_VIDEO_VP8, new String[]{
                            ""bbb_cif_768kbps_30fps_vp8_stereo_48kHz_192kbps_vorbis.webm"",
                            ""bbb_cif_768kbps_30fps_vp8_stereo_48kHz_192kbps_vorbis.mkv""}},
                    {MediaFormat.MIMETYPE_VIDEO_VP9, new String[]{
                            ""bbb_cif_768kbps_30fps_vp9_stereo_48kHz_192kbps_opus.webm"",
                            ""bbb_cif_768kbps_30fps_vp9_stereo_48kHz_192kbps_opus.mkv"",}},
                    {MediaFormat.MIMETYPE_VIDEO_AV1, new String[]{
                            ""bbb_cif_768kbps_30fps_av1.mp4"",
                            ""bbb_cif_768kbps_30fps_av1.webm"",
                            ""bbb_cif_768kbps_30fps_av1.mkv"",}},
                    {MediaFormat.MIMETYPE_AUDIO_VORBIS, new String[]{
                            ""bbb_cif_768kbps_30fps_avc_stereo_48kHz_192kbps_vorbis.mp4"",
                            ""bbb_cif_768kbps_30fps_vp8_stereo_48kHz_192kbps_vorbis.mkv"",
                            ""bbb_cif_768kbps_30fps_vp8_stereo_48kHz_192kbps_vorbis.webm"",
                            ""bbb_stereo_48kHz_192kbps_vorbis.ogg"",}},
                    {MediaFormat.MIMETYPE_AUDIO_OPUS, new String[]{
                            ""bbb_cif_768kbps_30fps_vp9_stereo_48kHz_192kbps_opus.webm"",
                            ""bbb_cif_768kbps_30fps_vp9_stereo_48kHz_192kbps_opus.mkv"",
                            ""bbb_cif_768kbps_30fps_hevc_stereo_48kHz_192kbps_opus.mp4"",
                            ""bbb_stereo_48kHz_192kbps_opus.ogg"",}},
                    {MediaFormat.MIMETYPE_AUDIO_MPEG, new String[]{
                            ""bbb_stereo_48kHz_192kbps_mp3.mp3"",
                            ""bbb_cif_768kbps_30fps_mpeg2_stereo_48kHz_192kbps_mp3.mp4"",
                            ""bbb_cif_768kbps_30fps_hevc_stereo_48kHz_192kbps_mp3.mkv"",}},
                    {MediaFormat.MIMETYPE_AUDIO_AAC, new String[]{
                            ""bbb_stereo_48kHz_192kbps_aac.mp4"",
                            ""bbb_cif_768kbps_30fps_avc_stereo_48kHz_192kbps_aac.3gp"",
                            ""bbb_cif_768kbps_30fps_avc_stereo_48kHz_192kbps_aac.mkv"",
                            ""bbb_stereo_48kHz_128kbps_aac.ts"",}},
                    {MediaFormat.MIMETYPE_AUDIO_AMR_NB, new String[]{
                            ""bbb_cif_768kbps_30fps_h263_mono_8kHz_12kbps_amrnb.3gp"",
                            ""bbb_mono_8kHz_12kbps_amrnb.amr"",}},
                    {MediaFormat.MIMETYPE_AUDIO_AMR_WB, new String[]{
                            ""bbb_cif_768kbps_30fps_mpeg4_mono_16kHz_20kbps_amrwb.3gp"",
                            ""bbb_mono_16kHz_20kbps_amrwb.amr""}},
                    {MediaFormat.MIMETYPE_AUDIO_FLAC, new String[]{
                            ""bbb_cif_768kbps_30fps_mpeg4_stereo_48kHz_192kbps_flac.mp4"",
                            ""bbb_cif_768kbps_30fps_h263_stereo_48kHz_192kbps_flac.mkv"",}},
                    {MediaFormat.MIMETYPE_AUDIO_RAW, new String[]{""canon.mid"",}},
                    {MediaFormat.MIMETYPE_AUDIO_AC3, new String[]{
                            ""testac3mp4.mp4"", ""testac3ts.ts"",}},
                    {MediaFormat.MIMETYPE_AUDIO_AC4, new String[]{""multi0.mp4"",}},
                    {MediaFormat.MIMETYPE_AUDIO_EAC3, new String[]{
                            ""testeac3mp4.mp4"", ""testeac3ts.ts"",}},
                    {MediaFormat.MIMETYPE_AUDIO_RAW, new String[]{""bbb_1ch_16kHz.wav"",}},
                    {MediaFormat.MIMETYPE_AUDIO_G711_ALAW, new String[]{""bbb_2ch_8kHz_alaw.wav"",}},
                    {MediaFormat.MIMETYPE_AUDIO_G711_MLAW, new String[]{""bbb_2ch_8kHz_mulaw.wav"",}},
                    {MediaFormat.MIMETYPE_AUDIO_MSGSM, new String[]{""bbb_1ch_8kHz_gsm.wav"",}},
            });
        }

        private native boolean nativeTestExtract(String srcPath, String refPath, String mime);

        private native boolean nativeTestSeek(String srcPath, String mime);

        private native boolean nativeTestSeekFlakiness(String srcPath, String mime);

        private native boolean nativeTestSeekToZero(String srcPath, String mime);

        private native boolean nativeTestFileFormat(String srcPath);

        public FunctionalityTest(String mime, String[] srcFiles) {
            mMime = mime;
            mSrcFiles = srcFiles;
        }

        // content necessary for testing seek are grouped in this class
        private class SeekTestParams {
            MediaCodec.BufferInfo mExpected;
            long mTimeStamp;
            int mMode;

            SeekTestParams(MediaCodec.BufferInfo expected, long timeStamp, int mode) {
                mExpected = expected;
                mTimeStamp = timeStamp;
                mMode = mode;
            }
        }

        private ArrayList<MediaCodec.BufferInfo> getSeekablePoints(String srcFile, String mime)
                throws IOException {
            ArrayList<MediaCodec.BufferInfo> bookmarks = null;
            if (mime == null) return null;
            MediaExtractor extractor = new MediaExtractor();
            Preconditions.assertTestFileExists(mInpPrefix + srcFile);
            extractor.setDataSource(mInpPrefix + srcFile);
            for (int trackID = 0; trackID < extractor.getTrackCount(); trackID++) {
                MediaFormat format = extractor.getTrackFormat(trackID);
                if (!mime.equals(format.getString(MediaFormat.KEY_MIME))) continue;
                extractor.selectTrack(trackID);
                bookmarks = new ArrayList<>();
                do {
                    int sampleFlags = extractor.getSampleFlags();
                    if ((sampleFlags & MediaExtractor.SAMPLE_FLAG_SYNC) != 0) {
                        MediaCodec.BufferInfo sampleInfo = new MediaCodec.BufferInfo();
                        sampleInfo.set(0, (int) extractor.getSampleSize(),
                                extractor.getSampleTime(), extractor.getSampleFlags());
                        bookmarks.add(sampleInfo);
                    }
                } while (extractor.advance());
                extractor.unselectTrack(trackID);
                break;
            }
            extractor.release();
            return bookmarks;
        }

        private ArrayList<SeekTestParams> generateSeekTestArgs(String srcFile, String mime,
                boolean isRandom) throws IOException {
            ArrayList<SeekTestParams> testArgs = new ArrayList<>();
            if (mime == null) return null;
            Preconditions.assertTestFileExists(mInpPrefix + srcFile);
            if (isRandom) {
                MediaExtractor extractor = new MediaExtractor();
                extractor.setDataSource(mInpPrefix + srcFile);
                final long maxEstDuration = 4000000;
                for (int trackID = 0; trackID < extractor.getTrackCount(); trackID++) {
                    MediaFormat format = extractor.getTrackFormat(trackID);
                    if (!mime.equals(format.getString(MediaFormat.KEY_MIME))) continue;
                    extractor.selectTrack(trackID);
                    for (int i = 0; i < MAX_SEEK_POINTS; i++) {
                        long pts = (long) (mRandNum.nextDouble() * maxEstDuration);
                        for (int mode = MediaExtractor.SEEK_TO_PREVIOUS_SYNC;
                             mode <= MediaExtractor.SEEK_TO_CLOSEST_SYNC; mode++) {
                            MediaCodec.BufferInfo currInfo = new MediaCodec.BufferInfo();
                            extractor.seekTo(pts, mode);
                            currInfo.set(0, (int) extractor.getSampleSize(),
                                    extractor.getSampleTime(), extractor.getSampleFlags());
                            testArgs.add(new SeekTestParams(currInfo, pts, mode));
                        }
                    }
                    extractor.unselectTrack(trackID);
                    break;
                }
                extractor.release();
            } else {
                ArrayList<MediaCodec.BufferInfo> bookmarks = getSeekablePoints(srcFile, mime);
                if (bookmarks == null) return null;
                int size = bookmarks.size();
                int[] indices;
                if (size > MAX_SEEK_POINTS) {
                    indices = new int[MAX_SEEK_POINTS];
                    indices[0] = 0;
                    indices[MAX_SEEK_POINTS - 1] = size - 1;
                    for (int i = 1; i < MAX_SEEK_POINTS - 1; i++) {
                        indices[i] = (int) (mRandNum.nextDouble() * (MAX_SEEK_POINTS - 1) + 1);
                    }
                } else {
                    indices = new int[size];
                    for (int i = 0; i < size; i++) indices[i] = i;
                }
                // closest sync : Seek to the sync sample CLOSEST to the specified time
                // previous sync : Seek to a sync sample AT or AFTER the specified time
                // next sync : Seek to a sync sample AT or BEFORE the specified time
                for (int i : indices) {
                    MediaCodec.BufferInfo currInfo = bookmarks.get(i);
                    long pts = currInfo.presentationTimeUs;
                    testArgs.add(
                            new SeekTestParams(currInfo, pts, MediaExtractor.SEEK_TO_CLOSEST_SYNC));
                    testArgs.add(
                            new SeekTestParams(currInfo, pts, MediaExtractor.SEEK_TO_NEXT_SYNC));
                    testArgs.add(
                            new SeekTestParams(currInfo, pts,
                                    MediaExtractor.SEEK_TO_PREVIOUS_SYNC));
                    if (i > 0) {
                        MediaCodec.BufferInfo prevInfo = bookmarks.get(i - 1);
                        long ptsMinus = prevInfo.presentationTimeUs;
                        ptsMinus = pts - ((pts - ptsMinus) >> 3);
                        testArgs.add(new SeekTestParams(currInfo, ptsMinus,
                                MediaExtractor.SEEK_TO_CLOSEST_SYNC));
                        testArgs.add(new SeekTestParams(currInfo, ptsMinus,
                                MediaExtractor.SEEK_TO_NEXT_SYNC));
                        testArgs.add(new SeekTestParams(prevInfo, ptsMinus,
                                MediaExtractor.SEEK_TO_PREVIOUS_SYNC));
                    }
                    if (i < size - 1) {
                        MediaCodec.BufferInfo nextInfo = bookmarks.get(i + 1);
                        long ptsPlus = nextInfo.presentationTimeUs;
                        ptsPlus = pts + ((ptsPlus - pts) >> 3);
                        testArgs.add(new SeekTestParams(currInfo, ptsPlus,
                                MediaExtractor.SEEK_TO_CLOSEST_SYNC));
                        testArgs.add(new SeekTestParams(nextInfo, ptsPlus,
                                MediaExtractor.SEEK_TO_NEXT_SYNC));
                        testArgs.add(new SeekTestParams(currInfo, ptsPlus,
                                MediaExtractor.SEEK_TO_PREVIOUS_SYNC));
                    }
                }
            }
            return testArgs;
        }

        int checkSeekPoints(String srcFile, String mime,
                ArrayList<SeekTestParams> seekTestArgs) throws IOException {
            int errCnt = 0;
            Preconditions.assertTestFileExists(mInpPrefix + srcFile);
            MediaExtractor extractor = new MediaExtractor();
            extractor.setDataSource(mInpPrefix + srcFile);
            for (int trackID = 0; trackID < extractor.getTrackCount(); trackID++) {
                MediaFormat format = extractor.getTrackFormat(trackID);
                if (!format.getString(MediaFormat.KEY_MIME).equals(mime)) continue;
                extractor.selectTrack(trackID);
                MediaCodec.BufferInfo received = new MediaCodec.BufferInfo();
                for (SeekTestParams arg : seekTestArgs) {
                    extractor.seekTo(arg.mTimeStamp, arg.mMode);
                    received.set(0, (int) extractor.getSampleSize(), extractor.getSampleTime(),
                            extractor.getSampleFlags());
                    if (!isSampleInfoIdentical(arg.mExpected, received)) {
                        errCnt++;
                        if (ENABLE_LOGS) {
                            Log.d(LOG_TAG, "" flags exp/got: "" + arg.mExpected.flags + '/' +
                                    received.flags);
                            Log.d(LOG_TAG,
                                    "" size exp/got: "" + arg.mExpected.size + '/' + received.size);
                            Log.d(LOG_TAG,
                                    "" ts exp/got: "" + arg.mExpected.presentationTimeUs + '/' +
                                            received.presentationTimeUs);
                        }
                    }
                }
                extractor.unselectTrack(trackID);
                break;
            }
            extractor.release();
            return errCnt;
        }

        private boolean isFileSeekable(String srcFile) throws IOException {
            MediaExtractor ext = new MediaExtractor();
            Preconditions.assertTestFileExists(mInpPrefix + srcFile);
            ext.setDataSource(mInpPrefix + srcFile);
            String format = ext.getMetrics().getString(MediaExtractor.MetricsConstants.FORMAT);
            ext.release();
            // MPEG2TS and MPEG2PS files are non-seekable
            return !(format.equalsIgnoreCase(""MPEG2TSExtractor"") ||
                    format.equalsIgnoreCase(""MPEG2PSExtractor""));
        }

        /**
         * Audio, Video codecs support a variety of file-types/container formats. For example,
         * Vorbis supports OGG, MP4, WEBM and MKV. H.263 supports 3GPP, WEBM and MKV. For every
         * mime, a list of test vectors are provided one for each container) but underlying
         * elementary stream is the same for all. The streams of a mime are extracted and
         * compared with each other for similarity.
         */
        @LargeTest"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.ExtractorTest"	"testExtract"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/ExtractorTest.java"	""	"public void testExtract() throws IOException {
            assumeTrue(shouldRunTest(mMime));
            Preconditions.assertTestFileExists(mInpPrefix + mSrcFiles[0]);
            MediaExtractor refExtractor = new MediaExtractor();
            refExtractor.setDataSource(mInpPrefix + mSrcFiles[0]);
            long sdkChecksum = readAllData(refExtractor, mMime, Integer.MAX_VALUE);
            long ndkChecksum = nativeReadAllData(mInpPrefix + mSrcFiles[0], mMime,
                    Integer.MAX_VALUE, null, null, false);
            assertEquals(""SDK and NDK checksums mismatch"", sdkChecksum, ndkChecksum);
            if (mSrcFiles.length == 1) {
                refExtractor.release();
                return;
            }
            assumeTrue(""TODO(b/146925481)"", !mMime.equals(MediaFormat.MIMETYPE_AUDIO_VORBIS));
            assumeTrue(""TODO(b/146925481)"", !mMime.equals(MediaFormat.MIMETYPE_AUDIO_OPUS));
            assumeTrue(""TODO(b/146925481)"", !mMime.equals(MediaFormat.MIMETYPE_AUDIO_MPEG));
            assumeTrue(""TODO(b/146925481)"", !mMime.equals(MediaFormat.MIMETYPE_AUDIO_AAC));
            boolean isOk = true;
            for (int i = 1; i < mSrcFiles.length && isOk; i++) {
                MediaExtractor testExtractor = new MediaExtractor();
                Preconditions.assertTestFileExists(mInpPrefix + mSrcFiles[i]);
                testExtractor.setDataSource(mInpPrefix + mSrcFiles[i]);
                if (!isMediaSimilar(refExtractor, testExtractor, mMime, Integer.MAX_VALUE)) {
                    if (ENABLE_LOGS) {
                        Log.d(LOG_TAG, ""Files: "" + mSrcFiles[0] + "", "" + mSrcFiles[i] +
                                "" are different from extractor perspective"");
                    }
                    if (!codecListSupp.contains(mMime)) {
                        isOk = false;
                    }
                }
                testExtractor.release();
            }
            refExtractor.release();
            assertTrue(testName.getMethodName() + "" failed for mime: "" + mMime, isOk);
        }

        /**
         * Tests seek functionality, verifies if we seek to most accurate point for a given
         * choice of timestamp and mode.
         */
        @LargeTest"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.ExtractorTest"	"testSeekToZero"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/ExtractorTest.java"	""	"public void testSeekToZero() throws IOException {
            assumeTrue(shouldRunTest(mMime));
            assumeTrue(""TODO(b/146925481)"", !mMime.equals(MediaFormat.MIMETYPE_AUDIO_VORBIS));
            assumeTrue(""TODO(b/146925481)"", !mMime.equals(MediaFormat.MIMETYPE_AUDIO_MPEG));
            assumeTrue(""TODO(b/146925481)"", !mMime.equals(MediaFormat.MIMETYPE_AUDIO_AAC));
            boolean isOk = true;
            for (String srcFile : mSrcFiles) {
                if (!isFileSeekable(srcFile)) continue;
                MediaExtractor extractor = new MediaExtractor();
                Preconditions.assertTestFileExists(mInpPrefix + srcFile);
                extractor.setDataSource(mInpPrefix + srcFile);
                MediaCodec.BufferInfo sampleInfoAtZero = new MediaCodec.BufferInfo();
                MediaCodec.BufferInfo currInfo = new MediaCodec.BufferInfo();
                final long randomSeekPts = 1 << 20;
                for (int trackID = 0; trackID < extractor.getTrackCount(); trackID++) {
                    MediaFormat format = extractor.getTrackFormat(trackID);
                    if (!mMime.equals(format.getString(MediaFormat.KEY_MIME))) continue;
                    extractor.selectTrack(trackID);
                    sampleInfoAtZero.set(0, (int) extractor.getSampleSize(),
                            extractor.getSampleTime(), extractor.getSampleFlags());
                    extractor.seekTo(randomSeekPts, MediaExtractor.SEEK_TO_NEXT_SYNC);
                    extractor.seekTo(0, MediaExtractor.SEEK_TO_CLOSEST_SYNC);
                    currInfo.set(0, (int) extractor.getSampleSize(),
                            extractor.getSampleTime(), extractor.getSampleFlags());
                    if (!isSampleInfoIdentical(sampleInfoAtZero, currInfo)) {
                        if (!codecListSupp.contains(mMime)) {
                            if (ENABLE_LOGS) {
                                Log.d(LOG_TAG, ""seen mismatch seekTo(0, SEEK_TO_CLOSEST_SYNC)"");
                                Log.d(LOG_TAG, "" flags exp/got: "" + sampleInfoAtZero.flags + '/' +
                                        currInfo.flags);
                                Log.d(LOG_TAG, "" size exp/got: "" + sampleInfoAtZero.size + '/' +
                                        currInfo.size);
                                Log.d(LOG_TAG,
                                        "" ts exp/got: "" + sampleInfoAtZero.presentationTimeUs +
                                                '/' + currInfo.presentationTimeUs);
                            }
                            isOk = false;
                            break;
                        }
                    }
                    extractor.seekTo(-1L, MediaExtractor.SEEK_TO_CLOSEST_SYNC);
                    currInfo.set(0, (int) extractor.getSampleSize(),
                            extractor.getSampleTime(), extractor.getSampleFlags());
                    if (!isSampleInfoIdentical(sampleInfoAtZero, currInfo)) {
                        if (!codecListSupp.contains(mMime)) {
                            if (ENABLE_LOGS) {
                                Log.d(LOG_TAG, ""seen mismatch seekTo(-1, SEEK_TO_CLOSEST_SYNC)"");
                                Log.d(LOG_TAG, "" flags exp/got: "" + sampleInfoAtZero.flags + '/' +
                                        currInfo.flags);
                                Log.d(LOG_TAG, "" size exp/got: "" + sampleInfoAtZero.size + '/' +
                                        currInfo.size);
                                Log.d(LOG_TAG,
                                        "" ts exp/got: "" + sampleInfoAtZero.presentationTimeUs +
                                                '/' + currInfo.presentationTimeUs);
                            }
                            isOk = false;
                            break;
                        }
                    }
                    extractor.unselectTrack(trackID);
                }
                extractor.release();
            }
            assertTrue(testName.getMethodName() + "" failed for mime: "" + mMime, isOk);
        }

        @SmallTest"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.ExtractorTest"	"testExtractNative"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/ExtractorTest.java"	""	"public void testExtractNative() {
            assumeTrue(shouldRunTest(mMime));
            if (mSrcFiles.length == 1) return;
            assumeTrue(""TODO(b/146925481)"", !mMime.equals(MediaFormat.MIMETYPE_AUDIO_VORBIS));
            assumeTrue(""TODO(b/146925481)"", !mMime.equals(MediaFormat.MIMETYPE_AUDIO_OPUS));
            assumeTrue(""TODO(b/146925481)"", !mMime.equals(MediaFormat.MIMETYPE_AUDIO_MPEG));
            assumeTrue(""TODO(b/146925481)"", !mMime.equals(MediaFormat.MIMETYPE_AUDIO_AAC));
            boolean isOk = true;
            Preconditions.assertTestFileExists(mInpPrefix + mSrcFiles[0]);
            for (int i = 1; i < mSrcFiles.length; i++) {
                Preconditions.assertTestFileExists(mInpPrefix + mSrcFiles[i]);
                if (!nativeTestExtract(mInpPrefix + mSrcFiles[0], mInpPrefix + mSrcFiles[i],
                        mMime)) {
                    Log.d(LOG_TAG, ""Files: "" + mSrcFiles[0] + "", "" + mSrcFiles[i] +
                            "" are different from extractor perpsective"");
                    if (!codecListSupp.contains(mMime)) {
                        isOk = false;
                        break;
                    }
                }
            }
            assertTrue(testName.getMethodName() + "" failed for mime: "" + mMime, isOk);
        }

        @LargeTest"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.ExtractorTest"	"testSeekToZeroNative"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/ExtractorTest.java"	""	"public void testSeekToZeroNative() throws IOException {
            assumeTrue(shouldRunTest(mMime));
            assumeTrue(""TODO(b/146925481)"", !mMime.equals(MediaFormat.MIMETYPE_AUDIO_VORBIS));
            assumeTrue(""TODO(b/146925481)"", !mMime.equals(MediaFormat.MIMETYPE_AUDIO_MPEG));
            assumeTrue(""TODO(b/146925481)"", !mMime.equals(MediaFormat.MIMETYPE_AUDIO_AAC));
            boolean isOk = true;
            for (String srcFile : mSrcFiles) {
                Preconditions.assertTestFileExists(mInpPrefix + srcFile);
                if (!isFileSeekable(srcFile)) continue;
                if (!nativeTestSeekToZero(mInpPrefix + srcFile, mMime)) {
                    if (!codecListSupp.contains(mMime)) {
                        isOk = false;
                        break;
                    }
                }
            }
            assertTrue(testName.getMethodName() + "" failed for mime: "" + mMime, isOk);
        }

        @SmallTest"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.ExtractorTest"	"testExtractDecodeAndValidate"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/ExtractorTest.java"	""	"public void testExtractDecodeAndValidate() throws IOException, InterruptedException {
            MediaExtractor testExtractor = new MediaExtractor();
            testExtractor.setDataSource(mInpPrefix + mTestFile);
            MediaFormat format = testExtractor.getTrackFormat(0);
            String mime = format.getString(MediaFormat.KEY_MIME);
            if (mime.equals(MediaFormat.MIMETYPE_AUDIO_RAW)) {
                ArrayList<String> listOfDecoders =
                        CodecTestBase.selectCodecs(mMime, null, null, false);
                assertTrue(""no suitable codecs found for mime: "" + mMime,
                        !listOfDecoders.isEmpty());
                CodecDecoderTestBase cdtb =
                        new CodecDecoderTestBase(listOfDecoders.get(0), mMime, mRefFile);
                cdtb.decodeToMemory(mRefFile, listOfDecoders.get(0), 0,
                        MediaExtractor.SEEK_TO_CLOSEST_SYNC, Integer.MAX_VALUE);
                String log = String.format(""test file: %s, ref file: %s:: "", mTestFile, mRefFile);
                assertTrue(log + ""no output received"", 0 != cdtb.mOutputCount);
                final ByteBuffer refBuffer = cdtb.mOutputBuff.getBuffer();

                testExtractor.selectTrack(0);
                ByteBuffer testBuffer = ByteBuffer.allocate(refBuffer.limit());
                int bufOffset = 0;
                while (true) {
                    long bytesRead = testExtractor.readSampleData(testBuffer, bufOffset);
                    if (bytesRead == -1) break;
                    bufOffset += bytesRead;
                    testExtractor.advance();
                }
                testBuffer.rewind();
                assertEquals(log + ""Output mismatch"", 0, refBuffer.compareTo(testBuffer));
                assertTrue(log + ""Output formats mismatch"",
                        cdtb.isFormatSimilar(cdtb.mOutFormat, format));
            } else if (mime.equals(mMime)) {
                MediaExtractor refExtractor = new MediaExtractor();
                refExtractor.setDataSource(mInpPrefix + mRefFile);
                if (!isMediaSimilar(refExtractor, testExtractor, mMime, Integer.MAX_VALUE)) {
                    fail(""Files: "" + mRefFile + "", "" + mTestFile +
                            "" are different from extractor perspective"");
                }
                refExtractor.release();
            } else {
                fail(""unexpected mime: "" + mime);
            }
            testExtractor.release();
        }
    }

    /**
     * Test if extractor populates key-value pairs correctly
     */
    @RunWith(Parameterized.class)
    public static class ValidateKeyValuePairs {
        private static final String mInpPrefix = WorkDir.getMediaDirString();
        private final String mMime;
        private final String[] mInpFiles;
        private final int mProfile;
        private final int mLevel;
        private final int mWR;
        private final int mHCh;

        public ValidateKeyValuePairs(String mime, String[] inpFiles, int profile, int level, int wr,
                int hCh) {
            mMime = mime;
            mInpFiles = inpFiles;
            mProfile = profile;
            mLevel = level;
            mWR = wr;
            mHCh = hCh;
        }

        @Parameterized.Parameters(name = ""{index}({0})"")
        public static Collection<Object[]> input() {
            // mime, clips, profile, level, width/sample rate, height/channel count
            List<Object[]> exhaustiveArgsList = new ArrayList<>();

            if (hasDecoder(MediaFormat.MIMETYPE_VIDEO_MPEG2)) {
                // profile and level constraints as per sec 2.3.2 of cdd
                /* TODO(b/159582475)
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_MPEG2, new String[]{
                        ""bbb_1920x1080_mpeg2_main_high.mp4"",
                        ""bbb_1920x1080_mpeg2_main_high.mkv""},
                        MediaCodecInfo.CodecProfileLevel.MPEG2ProfileMain,
                        MediaCodecInfo.CodecProfileLevel.MPEG2LevelHL, 1920, 1080});*/
            }

            if (hasDecoder(MediaFormat.MIMETYPE_VIDEO_AVC)) {
                // profile and level constraints as per sec 2.3.2 of cdd
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_AVC, new String[]{
                        ""bbb_1920x1080_avc_baseline_l42.mp4"",
                        ""bbb_1920x1080_avc_baseline_l42.mkv"",
                        ""bbb_1920x1080_avc_baseline_l42.3gp""},
                        MediaCodecInfo.CodecProfileLevel.AVCProfileConstrainedBaseline,
                        MediaCodecInfo.CodecProfileLevel.AVCLevel42, 1920, 1080});
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_AVC, new String[]{
                        ""bbb_1920x1080_avc_main_l42.mp4"",
                        ""bbb_1920x1080_avc_main_l42.mkv"",
                        ""bbb_1920x1080_avc_main_l42.3gp""},
                        MediaCodecInfo.CodecProfileLevel.AVCProfileMain,
                        MediaCodecInfo.CodecProfileLevel.AVCLevel42, 1920, 1080});
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_AVC, new String[]{
                        ""bbb_1920x1080_avc_high_l42.mp4"",
                        ""bbb_1920x1080_avc_high_l42.mkv"",
                        ""bbb_1920x1080_avc_high_l42.3gp""},
                        MediaCodecInfo.CodecProfileLevel.AVCProfileHigh,
                        MediaCodecInfo.CodecProfileLevel.AVCLevel42, 1920, 1080});
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_AVC, new String[]{
                        ""video_dovi_1920x1080_60fps_dvav_09.mp4""},
                        MediaCodecInfo.CodecProfileLevel.AVCProfileHigh,
                        MediaCodecInfo.CodecProfileLevel.AVCLevel42, 1920, 1080});
                // profile/level constraints for avc as per sec 5.3.4 of cdd
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_AVC, new String[]{
                        ""bbb_1920x1080_avc_baseline_l40.mp4"",
                        ""bbb_1920x1080_avc_baseline_l40.mkv"",
                        ""bbb_1920x1080_avc_baseline_l40.3gp""},
                        MediaCodecInfo.CodecProfileLevel.AVCProfileConstrainedBaseline,
                        MediaCodecInfo.CodecProfileLevel.AVCLevel4, 1920, 1080});
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_AVC, new String[]{
                        ""bbb_1920x1080_avc_main_l40.mp4"",
                        ""bbb_1920x1080_avc_main_l40.mkv"",
                        ""bbb_1920x1080_avc_main_l40.3gp""},
                        MediaCodecInfo.CodecProfileLevel.AVCProfileMain,
                        MediaCodecInfo.CodecProfileLevel.AVCLevel4, 1920, 1080});
            }

            if (hasDecoder(MediaFormat.MIMETYPE_VIDEO_HEVC)) {
                // profile and level constraints as per sec 2.3.2 of cdd
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_HEVC, new String[]{
                        ""bbb_1920x1080_hevc_main_l41.mp4"",
                        ""bbb_1920x1080_hevc_main_l41.mkv""},
                        MediaCodecInfo.CodecProfileLevel.HEVCProfileMain,
                        MediaCodecInfo.CodecProfileLevel.HEVCMainTierLevel41, 1920, 1080});
                // profile/level constraints for hevc as per sec 5.3.5 of cdd
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_HEVC, new String[]{
                        ""bbb_1920x1080_hevc_main_l40.mp4"",
                        ""bbb_1920x1080_hevc_main_l40.mkv""},
                        MediaCodecInfo.CodecProfileLevel.HEVCProfileMain,
                        MediaCodecInfo.CodecProfileLevel.HEVCMainTierLevel4, 1920, 1080});
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_HEVC, new String[]{
                        ""video_dovi_1920x1080_30fps_dvhe_04.mp4""},
                        MediaCodecInfo.CodecProfileLevel.HEVCProfileMain10,
                        MediaCodecInfo.CodecProfileLevel.HEVCMainTierLevel4, 1920, 1080});
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_HEVC, new String[]{
                        ""video_dovi_1920x1080_60fps_dvhe_08.mp4""},
                        MediaCodecInfo.CodecProfileLevel.HEVCProfileMain10,
                        MediaCodecInfo.CodecProfileLevel.HEVCMainTierLevel41, 1920, 1080});
            }

            if (hasDecoder(MediaFormat.MIMETYPE_VIDEO_VP9)) {
                // profile and level constraints as per sec 2.3.2 of cdd
                /* TODO(b/159582475)
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_VP9, new String[]{
                        ""bbb_1920x1080_vp9_main_l41.webm"",
                        ""bbb_1920x1080_vp9_main_l41.mkv""},
                        MediaCodecInfo.CodecProfileLevel.VP9Profile0,
                        MediaCodecInfo.CodecProfileLevel.VP9Level41, 1920, 1080});*/
                // profile/level constraints for vp9 as per sec 5.3.6 of cdd
                /* TODO(b/159582475)
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_VP9, new String[]{
                        ""bbb_1920x1080_vp9_main_l40.webm"",
                        ""bbb_1920x1080_vp9_main_l40.mkv""},
                        MediaCodecInfo.CodecProfileLevel.VP9Profile0,
                        MediaCodecInfo.CodecProfileLevel.VP9Level4, 1920, 1080});*/
            }

            if (hasDecoder(MediaFormat.MIMETYPE_VIDEO_H263)) {
                // profile/level constraints for h263 as per sec 5.3.2 of cdd
                /* TODO(b/159582475)
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_H263, new String[]{
                        ""bbb_352x288_384kbps_30fps_h263_baseline_l3.3gp"",
                        ""bbb_352x288_384kbps_30fps_h263_baseline_l3.mp4"",
                        ""bbb_352x288_384kbps_30fps_h263_baseline_l3.mkv""},
                        MediaCodecInfo.CodecProfileLevel.H263ProfileBaseline,
                        MediaCodecInfo.CodecProfileLevel.H263Level30, 352, 288});*/
            }

            if (hasDecoder(MediaFormat.MIMETYPE_VIDEO_MPEG4)) {
                // profile/level constraints for mpeg4 as per sec 5.3.3 of cdd
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_MPEG4, new String[]{
                        ""bbb_352x288_384kbps_30fps_mpeg4_simple_l3.mp4"",
                        ""bbb_352x288_384kbps_30fps_mpeg4_simple_l3.3gp"",
                        ""bbb_352x288_384kbps_30fps_mpeg4_simple_l3.mkv""},
                        MediaCodecInfo.CodecProfileLevel.MPEG4ProfileSimple,
                        MediaCodecInfo.CodecProfileLevel.MPEG4Level3, 352, 288});
            }

            if (hasDecoder(MediaFormat.MIMETYPE_AUDIO_AAC)) {
                // profile and level constraints for devices that have audio output as per sec 2.2.2,
                // sec 2.3.2, sec 2.5.2, sec 5.1.2 of cdd
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_AUDIO_AAC, new String[]{
                        ""bbb_stereo_44kHz_192kbps_aac_lc.mp4"",
                        ""bbb_stereo_44kHz_192kbps_aac_lc.3gp"",
                        ""bbb_stereo_44kHz_192kbps_aac_lc.mkv""},
                        MediaCodecInfo.CodecProfileLevel.AACObjectLC, 0, 44100, 2});
                /* TODO(b/159582475)
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_AUDIO_AAC, new String[]{
                        ""bbb_stereo_44kHz_192kbps_aac_he.mp4"",
                        ""bbb_stereo_44kHz_192kbps_aac_he.3gp"",
                        ""bbb_stereo_44kHz_192kbps_aac_he.mkv""},
                        MediaCodecInfo.CodecProfileLevel.AACObjectHE, 0, 44100, 2});*/
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_AUDIO_AAC, new String[] {
                        ""bbb_stereo_44kHz_192kbps_aac_eld.mp4"",
                        ""bbb_stereo_44kHz_192kbps_aac_eld.3gp"",
                        ""bbb_stereo_44kHz_192kbps_aac_eld.mkv""},
                        MediaCodecInfo.CodecProfileLevel.AACObjectELD, 0, 44100, 2});
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_AUDIO_AAC, new String[]{
                        ""bbb_stereo_44kHz_192kbps_aac_ld.mp4"",
                        ""bbb_stereo_44kHz_192kbps_aac_ld.3gp"",
                        ""bbb_stereo_44kHz_192kbps_aac_ld.mkv""},
                        MediaCodecInfo.CodecProfileLevel.AACObjectLD, 0, 44100, 2});
                /*TODO(b/159582475)
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_AUDIO_AAC, new String[]{
                        ""bbb_stereo_44kHz_192kbps_aac_hev2.mp4"",
                        ""bbb_stereo_44kHz_192kbps_aac_hev2.3gp"",
                        ""bbb_stereo_44kHz_192kbps_aac_hev2.mkv""},
                        MediaCodecInfo.CodecProfileLevel.AACObjectHE_PS, 0, 44100, 2});*/
            }

            // Miscellaneous
            if (hasDecoder(MediaFormat.MIMETYPE_VIDEO_DOLBY_VISION)) {
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_DOLBY_VISION,
                        new String[]{""video_dovi_1920x1080_30fps_dvhe_04.mp4""},
                        MediaCodecInfo.CodecProfileLevel.DolbyVisionProfileDvheDtr,
                        MediaCodecInfo.CodecProfileLevel.DolbyVisionLevelFhd30, 1920, 1080});
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_DOLBY_VISION,
                        new String[]{""video_dovi_1920x1080_60fps_dvhe_05.mp4""},
                        MediaCodecInfo.CodecProfileLevel.DolbyVisionProfileDvheStn,
                        MediaCodecInfo.CodecProfileLevel.DolbyVisionLevelFhd60, 1920, 1080});
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_DOLBY_VISION,
                        new String[]{""video_dovi_1920x1080_60fps_dvhe_08.mp4""},
                        MediaCodecInfo.CodecProfileLevel.DolbyVisionProfileDvheSt,
                        MediaCodecInfo.CodecProfileLevel.DolbyVisionLevelFhd60, 1920, 1080});
                exhaustiveArgsList.add(new Object[]{MediaFormat.MIMETYPE_VIDEO_DOLBY_VISION,
                        new String[]{""video_dovi_1920x1080_60fps_dvav_09.mp4""},
                        MediaCodecInfo.CodecProfileLevel.DolbyVisionProfileDvavSe,
                        MediaCodecInfo.CodecProfileLevel.DolbyVisionLevelFhd60, 1920, 1080});
            }

            return exhaustiveArgsList;
        }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediav2.cts.CodecEncoderTest"	"isFormatSupported"	"CtsMediaV2TestCases"	"/home/gpoor/cts-12-source/cts/tests/media/src/android/mediav2/cts/CodecEncoderTest.java"	""	"/*
 *.
 */

package android.mediav2.cts;

import android.media.MediaCodec;
import android.media.MediaCodecInfo;
import android.media.MediaFormat;
import android.os.Bundle;
import android.util.Log;

import androidx.test.filters.LargeTest;
import androidx.test.filters.SmallTest;

import org.junit.Assume;
import org.junit.Ignore;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.junit.runners.Parameterized;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.List;

import static org.junit.Assert.assertFalse;
import static org.junit.Assert.assertTrue;
import static org.junit.Assert.fail;

/**
 * Validate encode functionality of listed encoder components
 *
 * The test aims to test all encoders advertised in MediaCodecList. Hence we are not using
 * MediaCodecList#findEncoderForFormat to create codec. Further, it can so happen that the
 * test clip chosen is not supported by component (codecCapabilities.isFormatSupported()
 * fails), then it is better to remove the format but not skip testing the component. The idea
 * of these tests are not to cover CDD requirements but to test components and their plugins
 */
@RunWith(Parameterized.class)
public class CodecEncoderTest extends CodecEncoderTestBase {
    private static final String LOG_TAG = CodecEncoderTest.class.getSimpleName();
    private int mNumSyncFramesReceived;
    private ArrayList<Integer> mSyncFramesPos;
    private static ArrayList<String> mAdaptiveBitrateMimeList = new ArrayList<>();

    static {
        mAdaptiveBitrateMimeList.add(MediaFormat.MIMETYPE_VIDEO_AVC);
        mAdaptiveBitrateMimeList.add(MediaFormat.MIMETYPE_VIDEO_HEVC);
        mAdaptiveBitrateMimeList.add(MediaFormat.MIMETYPE_VIDEO_VP8);
        mAdaptiveBitrateMimeList.add(MediaFormat.MIMETYPE_VIDEO_VP9);
    }

    public CodecEncoderTest(String encoder, String mime, int[] bitrates, int[] encoderInfo1,
            int[] encoderInfo2) {
        super(encoder, mime, bitrates, encoderInfo1, encoderInfo2);
        mSyncFramesPos = new ArrayList<>();
    }

    @Override
    void resetContext(boolean isAsync, boolean signalEOSWithLastFrame) {
        super.resetContext(isAsync, signalEOSWithLastFrame);
        mNumSyncFramesReceived = 0;
        mSyncFramesPos.clear();
    }

    @Override
    void flushCodec() {
        super.flushCodec();
        mNumSyncFramesReceived = 0;
        mSyncFramesPos.clear();
    }

    void dequeueOutput(int bufferIndex, MediaCodec.BufferInfo info) {
        if (info.size > 0 && (info.flags & MediaCodec.BUFFER_FLAG_KEY_FRAME) != 0) {
            mNumSyncFramesReceived += 1;
            mSyncFramesPos.add(mOutputCount);
        }
        super.dequeueOutput(bufferIndex, info);
    }

    private void forceSyncFrame() {
        final Bundle syncFrame = new Bundle();
        syncFrame.putInt(MediaCodec.PARAMETER_KEY_REQUEST_SYNC_FRAME, 0);
        if (ENABLE_LOGS) {
            Log.v(LOG_TAG, ""requesting key frame"");
        }
        mCodec.setParameters(syncFrame);
    }

    private void updateBitrate(int bitrate) {
        final Bundle bitrateUpdate = new Bundle();
        bitrateUpdate.putInt(MediaCodec.PARAMETER_KEY_VIDEO_BITRATE, bitrate);
        if (ENABLE_LOGS) {
            Log.v(LOG_TAG, ""requesting bitrate to be changed to "" + bitrate);
        }
        mCodec.setParameters(bitrateUpdate);
    }

    @Parameterized.Parameters(name = ""{index}({0}_{1})"")
    public static Collection<Object[]> input() {
        final boolean isEncoder = true;
        final boolean needAudio = true;
        final boolean needVideo = true;
        final List<Object[]> exhaustiveArgsList = Arrays.asList(new Object[][]{
                // Audio - CodecMime, arrays of bit-rates, sample rates, channel counts
                {MediaFormat.MIMETYPE_AUDIO_AAC, new int[]{64000, 128000}, new int[]{8000, 11025,
                        22050, 44100, 48000}, new int[]{1, 2}},
                {MediaFormat.MIMETYPE_AUDIO_OPUS, new int[]{6600, 8850, 12650, 14250, 15850,
                        18250, 19850, 23050, 23850}, new int[]{16000}, new int[]{1}},
                {MediaFormat.MIMETYPE_AUDIO_AMR_NB, new int[]{4750, 5150, 5900, 6700, 7400, 7950,
                        10200, 12200}, new int[]{8000}, new int[]{1}},
                {MediaFormat.MIMETYPE_AUDIO_AMR_WB, new int[]{6600, 8850, 12650, 14250, 15850,
                        18250, 19850, 23050, 23850}, new int[]{16000}, new int[]{1}},
                /* TODO(169310292) */
                {MediaFormat.MIMETYPE_AUDIO_FLAC, new int[]{/* 0, 1, 2, */ 3, 4, 5, 6, 7, 8},
                        new int[]{8000, 48000, 96000, 192000}, new int[]{1, 2}},

                // Video - CodecMime, arrays of bit-rates, height, width
                {MediaFormat.MIMETYPE_VIDEO_H263, new int[]{32000, 64000}, new int[]{176},
                        new int[]{144}},
                {MediaFormat.MIMETYPE_VIDEO_MPEG4, new int[]{32000, 64000}, new int[]{176},
                        new int[]{144}},
                {MediaFormat.MIMETYPE_VIDEO_AVC, new int[]{256000, 512000}, new int[]{176, 352,
                        352, 480}, new int[]{144, 240, 288, 360}},
                {MediaFormat.MIMETYPE_VIDEO_HEVC, new int[]{256000, 512000}, new int[]{176, 352,
                        352, 480}, new int[]{144, 240, 288, 360}},
                {MediaFormat.MIMETYPE_VIDEO_VP8, new int[]{256000, 512000}, new int[]{176, 352,
                        352, 480}, new int[]{144, 240, 288, 360}},
                {MediaFormat.MIMETYPE_VIDEO_VP9, new int[]{256000, 512000}, new int[]{176, 352,
                        352, 480}, new int[]{144, 240, 288, 360}},
                {MediaFormat.MIMETYPE_VIDEO_AV1, new int[]{256000, 512000}, new int[]{176, 352,
                        352, 480}, new int[]{144, 240, 288, 360}},
        });
        return prepareParamList(exhaustiveArgsList, isEncoder, needAudio, needVideo, true);
    }

    /**
     * Tests encoder for combinations:
     * 1. Codec Sync Mode, Signal Eos with Last frame
     * 2. Codec Sync Mode, Signal Eos Separately
     * 3. Codec Async Mode, Signal Eos with Last frame
     * 4. Codec Async Mode, Signal Eos Separately
     * In all these scenarios, Timestamp ordering is verified. The output has to be
     * consistent (not flaky) in all runs.
     */
    @LargeTest"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.webkit.cts.WebSettingsTest"	"WebSettingsTest"	"CtsWebkitTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/webkit/src/android/webkit/cts/WebSettingsTest.java"	""	"public void test/*
 *.
 */
package android.webkit.cts;

import static org.hamcrest.MatcherAssert.assertThat;
import static org.hamcrest.Matchers.greaterThan;
import static org.hamcrest.Matchers.lessThan;
import static org.junit.Assert.assertNotEquals;

import android.content.Context;
import android.graphics.Bitmap;
import android.graphics.Color;
import android.net.http.SslError;
import android.os.Build;
import android.os.Message;
import android.platform.test.annotations.AppModeFull;
import android.test.ActivityInstrumentationTestCase2;
import android.util.Base64;
import android.util.Log;
import android.view.ViewGroup;
import android.webkit.ConsoleMessage;
import android.webkit.SslErrorHandler;
import android.webkit.WebChromeClient;
import android.webkit.WebIconDatabase;
import android.webkit.WebResourceResponse;
import android.webkit.WebResourceRequest;
import android.webkit.WebSettings;
import android.webkit.WebSettings.TextSize;
import android.webkit.WebStorage;
import android.webkit.WebView;
import android.webkit.WebViewClient;
import android.webkit.cts.WebViewSyncLoader.WaitForLoadedClient;
import android.webkit.cts.WebViewSyncLoader.WaitForProgressClient;

import com.android.compatibility.common.util.NullWebViewUtils;
import com.android.compatibility.common.util.PollingCheck;
import com.google.common.util.concurrent.SettableFuture;

import java.io.ByteArrayInputStream;
import java.io.FileOutputStream;
import java.nio.charset.StandardCharsets;
import java.util.HashMap;
import java.util.Locale;
import java.util.Map;
import java.util.concurrent.CountDownLatch;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import java.util.concurrent.BlockingQueue;
import java.util.concurrent.LinkedBlockingQueue;

/**
 * Tests for {@link android.webkit.WebSettings}
 */
@AppModeFull
public class WebSettingsTest extends ActivityInstrumentationTestCase2<WebViewCtsActivity> {

    private static final int WEBVIEW_TIMEOUT = 5000;
    private static final String LOG_TAG = ""WebSettingsTest"";

    private final String EMPTY_IMAGE_HEIGHT = ""0"";
    private final String NETWORK_IMAGE_HEIGHT = ""48"";  // See getNetworkImageHtml()
    private final String DATA_URL_IMAGE_HTML = ""<html>"" +
            ""<head><script>function updateTitle(){"" +
            ""document.title=document.getElementById('img').naturalHeight;}</script></head>"" +
            ""<body onload='updateTitle()'>"" +
            ""<img id='img' onload='updateTitle()' src='data:image/png;base64,iVBORw0KGgoAAA"" +
            ""ANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAAAXNSR0IArs4c6QAAAA1JREFUCB0BAgD9/wAAAAIAAc3j"" +
            ""0SsAAAAASUVORK5CYII="" +
            ""'></body></html>"";
    private final String DATA_URL_IMAGE_HEIGHT = ""1"";

    private WebSettings mSettings;
    private CtsTestServer mWebServer;
    private WebViewOnUiThread mOnUiThread;
    private Context mContext;

    public WebSettingsTest() {
        super(""android.webkit.cts"", WebViewCtsActivity.class);
    }

    @Override
    protected void setUp() throws Exception {
        super.setUp();
        WebView webview = getActivity().getWebView();
        if (webview != null) {
            mOnUiThread = new WebViewOnUiThread(webview);
            mSettings = mOnUiThread.getSettings();
        }
        mContext = getInstrumentation().getTargetContext();
    }

    @Override
    protected void tearDown() throws Exception {
        if (mWebServer != null) {
            mWebServer.shutdown();
        }
        if (mOnUiThread != null) {
            mOnUiThread.cleanUp();
        }
        super.tearDown();
    }

    /**
     * Verifies that the default user agent string follows the format defined in Android
     * compatibility definition (tokens in angle brackets are variables, tokens in square
     * brackets are optional):
     * <p/>
     * Mozilla/5.0 (Linux; Android <version>; [<devicemodel>] [Build/<buildID>]; wv)
     * AppleWebKit/<major>.<minor> (KHTML, like Gecko) Version/<major>.<minor>
     * Chrome/<major>.<minor>.<branch>.<build>[ Mobile] Safari/<major>.<minor>
     */"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.telephony.cts.PhoneStateListenerTest"	"testOnDataActivity"	"CtsTelephonyTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/telephony/current/src/android/telephony/cts/PhoneStateListenerTest.java"	""	"public void testOnDataActivity() throws Throwable {
        if (mCm.getNetworkInfo(ConnectivityManager.TYPE_MOBILE) == null) {
            Log.d(TAG, ""Skipping test that requires ConnectivityManager.TYPE_MOBILE"");
            return;
        }
        assertFalse(mOnDataActivityCalled);

        mHandler.post(() -> {
            mListener = new PhoneStateListener() {
                @Override
                public void onDataActivity(int direction) {
                    synchronized (mLock) {
                        mOnDataActivityCalled = true;
                        mLock.notify();
                    }
                }
            };
            mTelephonyManager.listen(mListener, PhoneStateListener.LISTEN_DATA_ACTIVITY);
        });
        synchronized (mLock) {
            if (!mOnDataActivityCalled) {
                mLock.wait(WAIT_TIME);
            }
        }

        assertTrue(mOnDataActivityCalled);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.telephony.cts.PhoneStateListenerTest"	"testOnCellInfoChanged"	"CtsTelephonyTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/telephony/current/src/android/telephony/cts/PhoneStateListenerTest.java"	""	"public void testOnCellInfoChanged() throws Throwable {
        if (mCm.getNetworkInfo(ConnectivityManager.TYPE_MOBILE) == null) {
            Log.d(TAG, ""Skipping test that requires ConnectivityManager.TYPE_MOBILE"");
            return;
        }
        assertFalse(mOnDataActivityCalled);

        TelephonyManagerTest.grantLocationPermissions();
        mHandler.post(() -> {
            mListener = new PhoneStateListener() {
                @Override
                public void onCellInfoChanged(List<CellInfo> cellInfo) {
                    synchronized (mLock) {
                        mOnCellInfoChangedCalled = true;
                        mLock.notify();
                    }
                }
            };
            mTelephonyManager.listen(mListener, PhoneStateListener.LISTEN_CELL_INFO);
        });
        synchronized (mLock) {
            if (!mOnCellInfoChangedCalled) {
                mLock.wait(WAIT_TIME);
            }
        }

        assertTrue(mOnCellInfoChangedCalled);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediastress.cts.NativeMediaTest"	"NativeMediaTest"	"CtsMediaStressTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/mediastress/src/android/mediastress/cts/NativeMediaTest.java"	""	"public void test/*
 *.
 */
package android.mediastress.cts;

import android.app.Instrumentation;
import android.content.Intent;
import android.media.MediaFormat;
import android.media.MediaRecorder.AudioEncoder;
import android.media.MediaRecorder.VideoEncoder;
import android.os.Environment;
import android.test.ActivityInstrumentationTestCase2;
import android.util.Log;

import com.android.compatibility.common.util.MediaUtils;

import junit.framework.Assert;

public class NativeMediaTest extends ActivityInstrumentationTestCase2<NativeMediaActivity> {
    private static final String TAG = ""NativeMediaTest"";
    private static final String MIME_TYPE = MediaFormat.MIMETYPE_VIDEO_AVC;
    private static final int VIDEO_CODEC = VideoEncoder.H264;
    private static final int NUMBER_PLAY_PAUSE_REPEATITIONS = 10;
    private static final long PLAY_WAIT_TIME_MS = 4000;

    public NativeMediaTest() {
        super(NativeMediaActivity.class);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediastress.cts.NativeMediaTest"	"testDefaultPlay"	"CtsMediaStressTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/mediastress/src/android/mediastress/cts/NativeMediaTest.java"	""	"public void testDefaultPlay() throws InterruptedException {
        runPlayTest(480, 360);
    }

    private void runPlayTest(int width, int height) throws InterruptedException {
        MediaFormat format = MediaFormat.createVideoFormat(MIME_TYPE, width, height);
        // Don't run the test if the codec isn't supported.
        if (!MediaUtils.canDecode(format)) {
            return; // skip
        }

        Intent intent = new Intent();
        intent.putExtra(NativeMediaActivity.EXTRA_VIDEO_HEIGHT, height);
        setActivityIntent(intent);
        final NativeMediaActivity activity = getActivity();
        final Instrumentation instrumentation = getInstrumentation();
        waitForNativeMediaLifeCycle(activity, true);
        Thread.sleep(PLAY_WAIT_TIME_MS); // let it play for some time
        for (int i = 0; i < NUMBER_PLAY_PAUSE_REPEATITIONS; i++) {
            instrumentation.runOnMainSync(() -> {
                instrumentation.callActivityOnPause(activity);
            });
            instrumentation.waitForIdleSync();
            waitForNativeMediaLifeCycle(activity, false);
            instrumentation.runOnMainSync(() -> {
                instrumentation.callActivityOnResume(activity);
            });
            waitForNativeMediaLifeCycle(activity, true);
            Thread.sleep(PLAY_WAIT_TIME_MS); // let it play for some time
        }
    }

    /**
     * wait until life cycle change and checks if the current status is in line with expectation
     * @param activity
     * @param expectAlive expected status, true if it should be alive.
     * @throws InterruptedException
     */
    private void waitForNativeMediaLifeCycle(NativeMediaActivity activity, boolean expectAlive)
            throws InterruptedException {
        Boolean status = activity.waitForNativeMediaLifeCycle();
        Assert.assertNotNull(status); // null means time-out
        Assert.assertEquals(expectAlive, status.booleanValue());
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.fov.PhotoCaptureActivity"	"OnClickListener"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/fov/PhotoCaptureActivity.java"	""	"public void test/*
 *.
 */

package com.android.cts.verifier.camera.fov;

import android.app.Activity;
import android.app.AlertDialog;
import android.app.Dialog;
import android.content.Context;
import android.content.DialogInterface;
import android.content.Intent;
import android.graphics.Color;
import android.hardware.Camera;
import android.hardware.Camera.PictureCallback;
import android.hardware.Camera.ShutterCallback;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraManager;
import android.os.Bundle;
import android.os.PowerManager;
import android.os.PowerManager.WakeLock;
import android.util.Log;
import android.view.Surface;
import android.view.SurfaceHolder;
import android.view.SurfaceView;
import android.view.View;
import android.view.View.OnClickListener;
import android.widget.AdapterView;
import android.widget.AdapterView.OnItemSelectedListener;
import android.widget.ArrayAdapter;
import android.widget.Button;
import android.widget.Spinner;
import android.widget.TextView;
import android.widget.Toast;

import com.android.cts.verifier.R;
import com.android.cts.verifier.TestResult;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

/**
 * An activity for showing the camera preview and taking a picture.
 */
public class PhotoCaptureActivity extends Activity
        implements PictureCallback, SurfaceHolder.Callback {
    private static final String TAG = PhotoCaptureActivity.class.getSimpleName();
    private static final int FOV_REQUEST_CODE = 1006;
    private static final String PICTURE_FILENAME = ""photo.jpg"";
    private static float mReportedFovDegrees = 0;
    private float mReportedFovPrePictureTaken = -1;

    private SurfaceView mPreview;
    private SurfaceHolder mSurfaceHolder;
    private Spinner mResolutionSpinner;
    private List<SelectableResolution> mSupportedResolutions;
    private ArrayAdapter<SelectableResolution> mAdapter;

    private SelectableResolution mSelectedResolution;
    private Camera mCamera;
    private Size mSurfaceSize;
    private boolean mCameraInitialized = false;
    private boolean mPreviewActive = false;
    private boolean mTakingPicture = false;
    private int mResolutionSpinnerIndex = -1;
    private WakeLock mWakeLock;
    private long shutterStartTime;
    private int mPreviewOrientation;
    private int mJpegOrientation;

    private ArrayList<Integer> mPreviewSizeCamerasToProcess = new ArrayList<Integer>();

    private Dialog mActiveDialog;

    /**
     * Selected preview size per camera. If null, preview size should be
     * automatically detected.
     */
    private Size[] mPreviewSizes = null;

    public static File getPictureFile(Context context) {
        return new File(context.getExternalCacheDir(), PICTURE_FILENAME);
    }

    public static float getReportedFovDegrees() {
        return mReportedFovDegrees;
    }

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.camera_fov_calibration_photo_capture);

        int cameraToBeTested = 0;
        for (int cameraId = 0; cameraId < Camera.getNumberOfCameras(); ++cameraId) {
            if (!isExternalCamera(cameraId)) {
                cameraToBeTested++;
            }
        }

        mPreview = (SurfaceView) findViewById(R.id.camera_fov_camera_preview);
        mSurfaceHolder = mPreview.getHolder();
        mSurfaceHolder.addCallback(this);

        // This is required for older versions of Android hardware.
        mSurfaceHolder.setType(SurfaceHolder.SURFACE_TYPE_PUSH_BUFFERS);

        TextView textView = (TextView) findViewById(R.id.camera_fov_tap_to_take_photo);
        textView.setTextColor(Color.WHITE);

        Button setupButton = (Button) findViewById(R.id.camera_fov_settings_button);
        setupButton.setOnClickListener(new OnClickListener() {

            @Override
            public void onClick(View v) {
                startActivity(new Intent(
                        PhotoCaptureActivity.this, CalibrationPreferenceActivity.class));
            }
        });

        Button changePreviewSizeButton = (Button) findViewById(
                R.id.camera_fov_change_preview_size_button);
        changePreviewSizeButton.setOnClickListener(new OnClickListener() {
            @Override
            public void onClick(View v) {
                // Stop camera until preview sizes have been obtained.
                if (mCamera != null) {
                    mCamera.stopPreview();
                    mCamera.release();
                    mCamera = null;
                }

                mPreviewSizeCamerasToProcess.clear();
                mPreviewSizes =  new Size[Camera.getNumberOfCameras()];
                for (int cameraId = 0; cameraId < Camera.getNumberOfCameras(); ++cameraId) {
                    if (!isExternalCamera(cameraId)) {
                        mPreviewSizeCamerasToProcess.add(cameraId);
                    }
                }
                showNextDialogToChoosePreviewSize();
            }
        });

        View previewView = findViewById(R.id.camera_fov_preview_overlay);
        previewView.setOnClickListener(new OnClickListener() {
            @Override
            public void onClick(View v) {
                if (mPreviewActive && !mTakingPicture) {
                    mTakingPicture = true;
                    shutterStartTime = System.currentTimeMillis();

                    mCamera.takePicture(new ShutterCallback() {
                        @Override
                        public void onShutter() {
                            long dT = System.currentTimeMillis() - shutterStartTime;
                            Log.d(""CTS"", ""Shutter Lag: "" + dT);
                        }
                    }, null, PhotoCaptureActivity.this);
                }
            }
        });

        mResolutionSpinner = (Spinner) findViewById(R.id.camera_fov_resolution_selector);
        mResolutionSpinner.setOnItemSelectedListener(new OnItemSelectedListener() {
            @Override
            public void onItemSelected(
                    AdapterView<?> parent, View view, int position, long id) {
                if (mSupportedResolutions != null) {
                    SelectableResolution resolution = mSupportedResolutions.get(position);
                    switchToCamera(resolution, false);

                    // It should be guaranteed that the FOV is correctly updated after setParameters().
                    mReportedFovPrePictureTaken = mCamera.getParameters().getHorizontalViewAngle();

                    mResolutionSpinnerIndex = position;
                    startPreview();
                }
            }

            @Override
            public void onNothingSelected(AdapterView<?> arg0) {}
        });

        if (cameraToBeTested == 0) {
            Log.i(TAG, ""No cameras needs to be tested. Setting test pass."");
            Toast.makeText(this, ""No cameras needs to be tested. Test pass."",
                    Toast.LENGTH_LONG).show();

            TestResult.setPassedResult(this, getClass().getName(),
                    ""All cameras are external, test skipped!"");
            finish();
        }
    }

    @Override
    protected void onResume() {
        super.onResume();
        // Keep the device from going to sleep.
        PowerManager pm = (PowerManager) getSystemService(Context.POWER_SERVICE);
        mWakeLock = pm.newWakeLock(PowerManager.FULL_WAKE_LOCK, TAG);
        mWakeLock.acquire();

        if (mSupportedResolutions == null) {
            mSupportedResolutions = new ArrayList<SelectableResolution>();
            int numCameras = Camera.getNumberOfCameras();
            for (int cameraId = 0; cameraId < numCameras; ++cameraId) {
                if (isExternalCamera(cameraId)) {
                    continue;
                }

                Camera camera = Camera.open(cameraId);

                // Get the supported picture sizes and fill the spinner.
                List<Camera.Size> supportedSizes =
                        camera.getParameters().getSupportedPictureSizes();
                for (Camera.Size size : supportedSizes) {
                    mSupportedResolutions.add(
                            new SelectableResolution(cameraId, size.width, size.height));
                }
                camera.release();
            }
        }

        // Find the first untested entry.
        for (mResolutionSpinnerIndex = 0;
                mResolutionSpinnerIndex < mSupportedResolutions.size();
                mResolutionSpinnerIndex++) {
            if (!mSupportedResolutions.get(mResolutionSpinnerIndex).tested) {
                break;
            }
        }

        mAdapter = new ArrayAdapter<SelectableResolution>(
                this, android.R.layout.simple_spinner_dropdown_item,
                mSupportedResolutions);
        mResolutionSpinner.setAdapter(mAdapter);

        mResolutionSpinner.setSelection(mResolutionSpinnerIndex);
        setResult(RESULT_CANCELED);
    }

    @Override
    public void onPause() {
        if (mCamera != null) {
            if (mPreviewActive) {
                mCamera.stopPreview();
            }

            mCamera.release();
            mCamera = null;
        }
        mPreviewActive = false;
        mWakeLock.release();
        super.onPause();
    }

    @Override
    public void onPictureTaken(byte[] data, Camera camera) {
        File pictureFile = getPictureFile(this);
        Camera.Parameters params = mCamera.getParameters();
        mReportedFovDegrees = params.getHorizontalViewAngle();

        // Show error if FOV does not match the value reported before takePicture().
        if (mReportedFovPrePictureTaken != mReportedFovDegrees) {
            mSupportedResolutions.get(mResolutionSpinnerIndex).tested = true;
            mSupportedResolutions.get(mResolutionSpinnerIndex).passed = false;

            AlertDialog.Builder dialogBuilder = new AlertDialog.Builder(this);
            dialogBuilder.setTitle(R.string.camera_fov_reported_fov_problem);
            dialogBuilder.setNeutralButton(
                    android.R.string.ok, new DialogInterface.OnClickListener() {
                @Override
                public void onClick(DialogInterface dialog, int which) {
                    if (mActiveDialog != null) {
                        mActiveDialog.dismiss();
                        mActiveDialog = null;
                        initializeCamera();
                    }
                }
            });

            String message  = getResources().getString(R.string.camera_fov_reported_fov_problem_message);
            dialogBuilder.setMessage(String.format(message, mReportedFovPrePictureTaken, mReportedFovDegrees));
            mActiveDialog = dialogBuilder.show();
            mTakingPicture = false;
            return;
        }

        try {
            FileOutputStream fos = new FileOutputStream(pictureFile);
            fos.write(data);
            fos.close();
            Log.d(TAG, ""File saved to "" + pictureFile.getAbsolutePath());

            // Start activity which will use the taken picture to determine the
            // FOV.
            startActivityForResult(new Intent(this, DetermineFovActivity.class),
                    FOV_REQUEST_CODE + mResolutionSpinnerIndex, null);
        } catch (IOException e) {
            Log.e(TAG, ""Could not save picture file."", e);
            Toast.makeText(this, ""Could not save picture file: "" + e.getMessage(),
                    Toast.LENGTH_LONG).show();
        }
        mTakingPicture = false;
    }

    @Override
    protected void onActivityResult(int requestCode, int resultCode, Intent data) {
        if (resultCode != RESULT_OK) {
            return;
        }
        int testIndex = requestCode - FOV_REQUEST_CODE;
        SelectableResolution res = mSupportedResolutions.get(testIndex);
        res.tested = true;
        float reportedFOV = CtsTestHelper.getReportedFOV(data);
        float measuredFOV = CtsTestHelper.getMeasuredFOV(data);
        res.measuredFOV = measuredFOV;
        if (CtsTestHelper.isResultPassed(reportedFOV, measuredFOV)) {
            res.passed = true;
        }

        boolean allTested = true;
        for (int i = 0; i < mSupportedResolutions.size(); i++) {
            if (!mSupportedResolutions.get(i).tested) {
                allTested = false;
                break;
            }
        }
        if (!allTested) {
            mAdapter.notifyDataSetChanged();
            return;
        }

        boolean allPassed = true;
        for (int i = 0; i < mSupportedResolutions.size(); i++) {
            if (!mSupportedResolutions.get(i).passed) {
                allPassed = false;
                break;
            }
        }
        if (allPassed) {
            TestResult.setPassedResult(this, getClass().getName(),
                    CtsTestHelper.getTestDetails(mSupportedResolutions));
        } else {
            TestResult.setFailedResult(this, getClass().getName(),
                    CtsTestHelper.getTestDetails(mSupportedResolutions));
        }
        finish();
    }

    @Override
    public void surfaceChanged(
            SurfaceHolder holder, int format, int width, int height) {
        mSurfaceSize = new Size(width, height);
        initializeCamera();
    }

    @Override
    public void surfaceCreated(SurfaceHolder holder) {
        // Nothing to do.
    }

    @Override
    public void surfaceDestroyed(SurfaceHolder holder) {
        // Nothing to do.
    }

    private void showNextDialogToChoosePreviewSize() {
        final int cameraId = mPreviewSizeCamerasToProcess.remove(0);

        Camera camera = Camera.open(cameraId);
        final List<Camera.Size> sizes = camera.getParameters()
                .getSupportedPreviewSizes();
        String[] choices = new String[sizes.size()];
        for (int i = 0; i < sizes.size(); ++i) {
            Camera.Size size = sizes.get(i);
            choices[i] = size.width + "" x "" + size.height;
        }

        final AlertDialog.Builder builder = new AlertDialog.Builder(this);
        String dialogTitle = String.format(
                getResources().getString(R.string.camera_fov_choose_preview_size_for_camera),
                cameraId);
        builder.setTitle(
                dialogTitle).
                setOnCancelListener(new DialogInterface.OnCancelListener() {
                    @Override
                    public void onCancel(DialogInterface arg0) {
                        // User cancelled preview size selection.
                        mPreviewSizes = null;
                        switchToCamera(mSelectedResolution, true);
                    }
                }).
                setSingleChoiceItems(choices, 0, new DialogInterface.OnClickListener() {
                    @Override
                    public void onClick(DialogInterface dialog, int which) {
                        Camera.Size size = sizes.get(which);
                        mPreviewSizes[cameraId] = new Size(
                                size.width, size.height);
                        dialog.dismiss();

                        if (mPreviewSizeCamerasToProcess.isEmpty()) {
                            // We're done, re-initialize camera.
                            switchToCamera(mSelectedResolution, true);
                        } else {
                            // Process other cameras.
                            showNextDialogToChoosePreviewSize();
                        }
                    }
                }).create().show();
        camera.release();
    }

    private void initializeCamera() {
        initializeCamera(true);
    }

    private void initializeCamera(boolean startPreviewAfterInit) {
        if (mCamera == null || mSurfaceHolder.getSurface() == null) {
            return;
        }

        try {
            mCamera.setPreviewDisplay(mSurfaceHolder);
        } catch (Throwable t) {
            Log.e(TAG, ""Could not set preview display"", t);
            Toast.makeText(this, t.getMessage(), Toast.LENGTH_LONG).show();
            return;
        }

        calculateOrientations(this, mSelectedResolution.cameraId, mCamera);
        Camera.Parameters params = setCameraParams(mCamera);

        // Either use chosen preview size for current camera or automatically
        // choose preview size based on view dimensions.
        Size selectedPreviewSize = null;
        if (mPreviewSizes != null) {
            selectedPreviewSize = mPreviewSizes[mSelectedResolution.cameraId];
        } else if (mSurfaceSize != null) {
            selectedPreviewSize = getBestPreviewSize(
                    mSurfaceSize.width, mSurfaceSize.height, params);
        }

        if (selectedPreviewSize != null) {
            params.setPreviewSize(selectedPreviewSize.width, selectedPreviewSize.height);
            mCamera.setParameters(params);
            mCameraInitialized = true;
        }

        if (startPreviewAfterInit) {
            if (selectedPreviewSize == null) {
                Log.w(TAG, ""Preview started without setting preview size"");
            }
            startPreview();
        }
    }

    private void startPreview() {
        if (mCameraInitialized && mCamera != null) {
            mCamera.setDisplayOrientation(mPreviewOrientation);
            mCamera.startPreview();
            mPreviewActive = true;
        }
    }

    private void switchToCamera(SelectableResolution resolution, boolean startPreview) {
        if (mCamera != null) {
            mCamera.stopPreview();
            mCamera.release();
        }

        mSelectedResolution = resolution;
        mCamera = Camera.open(mSelectedResolution.cameraId);

        initializeCamera(startPreview);
    }

    /**
     * Get the best supported focus mode.
     *
     * @param camera - Android camera object.
     * @return the best supported focus mode.
     */
    private static String getFocusMode(Camera camera) {
        List<String> modes = camera.getParameters().getSupportedFocusModes();
        if (modes != null) {
            if (modes.contains(Camera.Parameters.FOCUS_MODE_INFINITY)) {
                Log.v(TAG, ""Using Focus mode infinity"");
                return Camera.Parameters.FOCUS_MODE_INFINITY;
            }
            if (modes.contains(Camera.Parameters.FOCUS_MODE_FIXED)) {
                Log.v(TAG, ""Using Focus mode fixed"");
                return Camera.Parameters.FOCUS_MODE_FIXED;
            }
        }
        Log.v(TAG, ""Using Focus mode auto."");
        return Camera.Parameters.FOCUS_MODE_AUTO;
    }

    /**
     * Set the common camera parameters on the given camera and returns the
     * parameter object for further modification, if needed.
     */
    private Camera.Parameters setCameraParams(Camera camera) {
        // The picture size is taken and set from the spinner selection
        // callback.
        Camera.Parameters params = camera.getParameters();
        params.setJpegThumbnailSize(0, 0);
        params.setJpegQuality(100);
        params.setRotation(mJpegOrientation);
        params.setFocusMode(getFocusMode(camera));
        params.setZoom(0);
        params.setPictureSize(mSelectedResolution.width, mSelectedResolution.height);
        return params;
    }

    private Size getBestPreviewSize(
            int width, int height, Camera.Parameters parameters) {
        Size result = null;

        for (Camera.Size size : parameters.getSupportedPreviewSizes()) {
            if (size.width <= width && size.height <= height) {
                if (result == null) {
                    result = new Size(size.width, size.height);
                } else {
                    int resultArea = result.width * result.height;
                    int newArea = size.width * size.height;

                    if (newArea > resultArea) {
                        result = new Size(size.width, size.height);
                    }
                }
            }
        }
        return result;
    }

    private void calculateOrientations(Activity activity,
            int cameraId, android.hardware.Camera camera) {
        android.hardware.Camera.CameraInfo info =
                new android.hardware.Camera.CameraInfo();
        android.hardware.Camera.getCameraInfo(cameraId, info);
        int rotation = activity.getWindowManager().getDefaultDisplay()
                .getRotation();
        int degrees = 0;
        switch (rotation) {
            case Surface.ROTATION_0: degrees = 0; break;
            case Surface.ROTATION_90: degrees = 90; break;
            case Surface.ROTATION_180: degrees = 180; break;
            case Surface.ROTATION_270: degrees = 270; break;
        }

        if (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {
            mJpegOrientation = (info.orientation + degrees) % 360;
            mPreviewOrientation = (360 - mJpegOrientation) % 360;  // compensate the mirror
        } else {  // back-facing
            mJpegOrientation = (info.orientation - degrees + 360) % 360;
            mPreviewOrientation = mJpegOrientation;
        }
    }

    private boolean isExternalCamera(int cameraId) {
        CameraManager manager = (CameraManager) this.getSystemService(Context.CAMERA_SERVICE);
        try {
            String cameraIdStr = manager.getCameraIdList()[cameraId];
            CameraCharacteristics characteristics =
                    manager.getCameraCharacteristics(cameraIdStr);

            if (characteristics.get(CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL) ==
                            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL) {
                // External camera doesn't support FOV informations
                return true;
            }
        } catch (CameraAccessException e) {
            Toast.makeText(this, ""Could not access camera "" + cameraId +
                    "": "" + e.getMessage(), Toast.LENGTH_LONG).show();
        }
        return false;
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.its.ItsService"	"doCheckSensorExistence"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/its/ItsService.java"	""	"public void test/*
 *.
 */

package com.android.cts.verifier.camera.its;

import android.app.Notification;
import android.app.NotificationChannel;
import android.app.NotificationManager;
import android.app.Service;
import android.content.Context;
import android.content.Intent;
import android.content.pm.ServiceInfo;
import android.graphics.ImageFormat;
import android.graphics.Rect;
import android.hardware.SensorPrivacyManager;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CaptureFailure;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.DngCreator;
import android.hardware.camera2.TotalCaptureResult;
import android.hardware.camera2.cts.PerformanceTest;
import android.hardware.camera2.params.InputConfiguration;
import android.hardware.camera2.params.MeteringRectangle;
import android.hardware.camera2.params.OutputConfiguration;
import android.hardware.camera2.params.SessionConfiguration;
import android.hardware.Sensor;
import android.hardware.SensorEvent;
import android.hardware.SensorEventListener;
import android.hardware.SensorManager;
import android.media.AudioAttributes;
import android.media.Image;
import android.media.ImageReader;
import android.media.ImageWriter;
import android.media.Image.Plane;
import android.net.Uri;
import android.os.Build;
import android.os.Bundle;
import android.os.ConditionVariable;
import android.os.Handler;
import android.os.HandlerThread;
import android.os.IBinder;
import android.os.Message;
import android.os.SystemClock;
import android.os.Vibrator;
import android.util.Log;
import android.util.Rational;
import android.util.Size;
import android.util.SparseArray;
import android.view.Surface;

import androidx.test.InstrumentationRegistry;

import com.android.ex.camera2.blocking.BlockingCameraManager;
import com.android.ex.camera2.blocking.BlockingCameraManager.BlockingOpenException;
import com.android.ex.camera2.blocking.BlockingStateCallback;
import com.android.ex.camera2.blocking.BlockingSessionCallback;

import com.android.compatibility.common.util.ReportLog.Metric;
import com.android.cts.verifier.camera.its.StatsImage;
import com.android.cts.verifier.camera.performance.CameraTestInstrumentation;
import com.android.cts.verifier.camera.performance.CameraTestInstrumentation.MetricListener;
import com.android.cts.verifier.R;

import org.json.JSONArray;
import org.json.JSONObject;
import org.junit.runner.JUnitCore;
import org.junit.runner.Request;
import org.junit.runner.Result;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.io.PrintWriter;
import java.math.BigInteger;
import java.net.ServerSocket;
import java.net.Socket;
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.nio.FloatBuffer;
import java.nio.charset.Charset;
import java.security.MessageDigest;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Executor;
import java.util.concurrent.LinkedBlockingDeque;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.Semaphore;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;

public class ItsService extends Service implements SensorEventListener {
    public static final String TAG = ItsService.class.getSimpleName();

    // Version number to keep host/server communication in sync
    // This string must be in sync with python side device.py
    // Updated when interface between script and ItsService is changed
    private final String ITS_SERVICE_VERSION = ""1.0"";

    private final int SERVICE_NOTIFICATION_ID = 37; // random int that is unique within app
    private NotificationChannel mChannel;

    // Timeouts, in seconds.
    private static final int TIMEOUT_CALLBACK = 20;
    private static final int TIMEOUT_3A = 10;

    // Time given for background requests to warm up pipeline
    private static final long PIPELINE_WARMUP_TIME_MS = 2000;

    // State transition timeouts, in ms.
    private static final long TIMEOUT_IDLE_MS = 2000;
    private static final long TIMEOUT_STATE_MS = 500;
    private static final long TIMEOUT_SESSION_CLOSE = 3000;

    // Timeout to wait for a capture result after the capture buffer has arrived, in ms.
    private static final long TIMEOUT_CAP_RES = 2000;

    private static final int MAX_CONCURRENT_READER_BUFFERS = 10;

    // Supports at most RAW+YUV+JPEG, one surface each, plus optional background stream
    private static final int MAX_NUM_OUTPUT_SURFACES = 4;

    // Performance class R version number
    private static final int PERFORMANCE_CLASS_R = Build.VERSION_CODES.R;
    // Performance class S version number
    private static final int PERFORMANCE_CLASS_S = Build.VERSION_CODES.R + 1;

    public static final int SERVERPORT = 6000;

    public static final String REGION_KEY = ""regions"";
    public static final String REGION_AE_KEY = ""ae"";
    public static final String REGION_AWB_KEY = ""awb"";
    public static final String REGION_AF_KEY = ""af"";
    public static final String LOCK_AE_KEY = ""aeLock"";
    public static final String LOCK_AWB_KEY = ""awbLock"";
    public static final String TRIGGER_KEY = ""triggers"";
    public static final String PHYSICAL_ID_KEY = ""physicalId"";
    public static final String TRIGGER_AE_KEY = ""ae"";
    public static final String TRIGGER_AF_KEY = ""af"";
    public static final String VIB_PATTERN_KEY = ""pattern"";
    public static final String EVCOMP_KEY = ""evComp"";
    public static final String AUDIO_RESTRICTION_MODE_KEY = ""mode"";

    private CameraManager mCameraManager = null;
    private HandlerThread mCameraThread = null;
    private Handler mCameraHandler = null;
    private BlockingCameraManager mBlockingCameraManager = null;
    private BlockingStateCallback mCameraListener = null;
    private CameraDevice mCamera = null;
    private CameraCaptureSession mSession = null;
    private ImageReader[] mOutputImageReaders = null;
    private SparseArray<String> mPhysicalStreamMap = new SparseArray<String>();
    private ImageReader mInputImageReader = null;
    private CameraCharacteristics mCameraCharacteristics = null;
    private HashMap<String, CameraCharacteristics> mPhysicalCameraChars =
            new HashMap<String, CameraCharacteristics>();
    private ItsUtils.ItsCameraIdList mItsCameraIdList = null;

    private Vibrator mVibrator = null;

    private HandlerThread mSaveThreads[] = new HandlerThread[MAX_NUM_OUTPUT_SURFACES];
    private Handler mSaveHandlers[] = new Handler[MAX_NUM_OUTPUT_SURFACES];
    private HandlerThread mResultThread = null;
    private Handler mResultHandler = null;

    private volatile boolean mThreadExitFlag = false;

    private volatile ServerSocket mSocket = null;
    private volatile SocketRunnable mSocketRunnableObj = null;
    private Semaphore mSocketQueueQuota = null;
    private int mMemoryQuota = -1;
    private LinkedList<Integer> mInflightImageSizes = new LinkedList<>();
    private volatile BlockingQueue<ByteBuffer> mSocketWriteQueue =
            new LinkedBlockingDeque<ByteBuffer>();
    private final Object mSocketWriteEnqueueLock = new Object();
    private final Object mSocketWriteDrainLock = new Object();

    private volatile BlockingQueue<Object[]> mSerializerQueue =
            new LinkedBlockingDeque<Object[]>();

    private AtomicInteger mCountCallbacksRemaining = new AtomicInteger();
    private AtomicInteger mCountRawOrDng = new AtomicInteger();
    private AtomicInteger mCountRaw10 = new AtomicInteger();
    private AtomicInteger mCountRaw12 = new AtomicInteger();
    private AtomicInteger mCountJpg = new AtomicInteger();
    private AtomicInteger mCountYuv = new AtomicInteger();
    private AtomicInteger mCountCapRes = new AtomicInteger();
    private boolean mCaptureRawIsDng;
    private boolean mCaptureRawIsStats;
    private int mCaptureStatsGridWidth;
    private int mCaptureStatsGridHeight;
    private CaptureResult mCaptureResults[] = null;

    private volatile ConditionVariable mInterlock3A = new ConditionVariable(true);

    final Object m3AStateLock = new Object();
    private volatile boolean mConvergedAE = false;
    private volatile boolean mConvergedAF = false;
    private volatile boolean mConvergedAWB = false;
    private volatile boolean mLockedAE = false;
    private volatile boolean mLockedAWB = false;
    private volatile boolean mNeedsLockedAE = false;
    private volatile boolean mNeedsLockedAWB = false;

    class MySensorEvent {
        public Sensor sensor;
        public int accuracy;
        public long timestamp;
        public float values[];
    }

    // For capturing motion sensor traces.
    private SensorManager mSensorManager = null;
    private Sensor mAccelSensor = null;
    private Sensor mMagSensor = null;
    private Sensor mGyroSensor = null;
    private volatile LinkedList<MySensorEvent> mEvents = null;
    private volatile Object mEventLock = new Object();
    private volatile boolean mEventsEnabled = false;
    private HandlerThread mSensorThread = null;
    private Handler mSensorHandler = null;

    private SensorPrivacyManager mSensorPrivacyManager;

    // Camera test instrumentation
    private CameraTestInstrumentation mCameraInstrumentation;
    // Camera PerformanceTest metric
    private final ArrayList<Metric> mResults = new ArrayList<Metric>();

    private static final int SERIALIZER_SURFACES_ID = 2;
    private static final int SERIALIZER_PHYSICAL_METADATA_ID = 3;

    public interface CaptureCallback {
        void onCaptureAvailable(Image capture, String physicalCameraId);
    }

    public abstract class CaptureResultListener extends CameraCaptureSession.CaptureCallback {}

    @Override
    public IBinder onBind(Intent intent) {
        return null;
    }

    @Override
    public void onCreate() {
        try {
            mThreadExitFlag = false;

            // Get handle to camera manager.
            mCameraManager = (CameraManager) this.getSystemService(Context.CAMERA_SERVICE);
            if (mCameraManager == null) {
                throw new ItsException(""Failed to connect to camera manager"");
            }
            mBlockingCameraManager = new BlockingCameraManager(mCameraManager);
            mCameraListener = new BlockingStateCallback();

            // Register for motion events.
            mEvents = new LinkedList<MySensorEvent>();
            mSensorManager = (SensorManager)getSystemService(Context.SENSOR_SERVICE);
            mAccelSensor = mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER);
            mMagSensor = mSensorManager.getDefaultSensor(Sensor.TYPE_MAGNETIC_FIELD);
            mGyroSensor = mSensorManager.getDefaultSensor(Sensor.TYPE_GYROSCOPE);
            mSensorThread = new HandlerThread(""SensorThread"");
            mSensorThread.start();
            mSensorHandler = new Handler(mSensorThread.getLooper());
            mSensorManager.registerListener(this, mAccelSensor,
                    /*100hz*/ 10000, mSensorHandler);
            mSensorManager.registerListener(this, mMagSensor,
                    SensorManager.SENSOR_DELAY_NORMAL, mSensorHandler);
            mSensorManager.registerListener(this, mGyroSensor,
                    /*200hz*/5000, mSensorHandler);

            // Get a handle to the system vibrator.
            mVibrator = (Vibrator)getSystemService(Context.VIBRATOR_SERVICE);

            // Create threads to receive images and save them.
            for (int i = 0; i < MAX_NUM_OUTPUT_SURFACES; i++) {
                mSaveThreads[i] = new HandlerThread(""SaveThread"" + i);
                mSaveThreads[i].start();
                mSaveHandlers[i] = new Handler(mSaveThreads[i].getLooper());
            }

            // Create a thread to handle object serialization.
            (new Thread(new SerializerRunnable())).start();;

            // Create a thread to receive capture results and process them.
            mResultThread = new HandlerThread(""ResultThread"");
            mResultThread.start();
            mResultHandler = new Handler(mResultThread.getLooper());

            // Create a thread for the camera device.
            mCameraThread = new HandlerThread(""ItsCameraThread"");
            mCameraThread.start();
            mCameraHandler = new Handler(mCameraThread.getLooper());

            // Create a thread to process commands, listening on a TCP socket.
            mSocketRunnableObj = new SocketRunnable();
            (new Thread(mSocketRunnableObj)).start();
        } catch (ItsException e) {
            Logt.e(TAG, ""Service failed to start: "", e);
        }

        NotificationManager notificationManager =
                (NotificationManager) getSystemService(Context.NOTIFICATION_SERVICE);
        mChannel = new NotificationChannel(
                ""ItsServiceChannel"", ""ItsService"", NotificationManager.IMPORTANCE_LOW);
        // Configure the notification channel.
        mChannel.setDescription(""ItsServiceChannel"");
        mChannel.enableVibration(false);
        notificationManager.createNotificationChannel(mChannel);

        mSensorPrivacyManager = getSystemService(SensorPrivacyManager.class);
    }

    @Override
    public int onStartCommand(Intent intent, int flags, int startId) {
        try {
            // Just log a message indicating that the service is running and is able to accept
            // socket connections.
            while (!mThreadExitFlag && mSocket==null) {
                Thread.sleep(1);
            }
            if (!mThreadExitFlag){
                Logt.i(TAG, ""ItsService ready"");
            } else {
                Logt.e(TAG, ""Starting ItsService in bad state"");
            }

            Notification notification = new Notification.Builder(this, mChannel.getId())
                    .setContentTitle(""CameraITS Service"")
                    .setContentText(""CameraITS Service is running"")
                    .setSmallIcon(R.drawable.icon)
                    .setOngoing(true).build();
            startForeground(SERVICE_NOTIFICATION_ID, notification,
                    ServiceInfo.FOREGROUND_SERVICE_TYPE_CAMERA);
        } catch (java.lang.InterruptedException e) {
            Logt.e(TAG, ""Error starting ItsService (interrupted)"", e);
        }
        return START_STICKY;
    }

    @Override
    public void onDestroy() {
        mThreadExitFlag = true;
        for (int i = 0; i < MAX_NUM_OUTPUT_SURFACES; i++) {
            if (mSaveThreads[i] != null) {
                mSaveThreads[i].quit();
                mSaveThreads[i] = null;
            }
        }
        if (mSensorThread != null) {
            mSensorThread.quitSafely();
            mSensorThread = null;
        }
        if (mResultThread != null) {
            mResultThread.quitSafely();
            mResultThread = null;
        }
        if (mCameraThread != null) {
            mCameraThread.quitSafely();
            mCameraThread = null;
        }
    }

    public void openCameraDevice(String cameraId) throws ItsException {
        Logt.i(TAG, String.format(""Opening camera %s"", cameraId));

        try {
            if (mMemoryQuota == -1) {
                // Initialize memory quota on this device
                if (mItsCameraIdList == null) {
                    mItsCameraIdList = ItsUtils.getItsCompatibleCameraIds(mCameraManager);
                }
                if (mItsCameraIdList.mCameraIds.size() == 0) {
                    throw new ItsException(""No camera devices"");
                }
                for (String camId : mItsCameraIdList.mCameraIds) {
                    CameraCharacteristics chars =  mCameraManager.getCameraCharacteristics(camId);
                    Size maxYuvSize = ItsUtils.getMaxOutputSize(
                            chars, ImageFormat.YUV_420_888);
                    // 4 bytes per pixel for RGBA8888 Bitmap and at least 3 Bitmaps per CDD
                    int quota = maxYuvSize.getWidth() * maxYuvSize.getHeight() * 4 * 3;
                    if (quota > mMemoryQuota) {
                        mMemoryQuota = quota;
                    }
                }
            }
        } catch (CameraAccessException e) {
            throw new ItsException(""Failed to get device ID list"", e);
        }

        try {
            mCamera = mBlockingCameraManager.openCamera(cameraId, mCameraListener, mCameraHandler);
            mCameraCharacteristics = mCameraManager.getCameraCharacteristics(cameraId);

            boolean isLogicalCamera = hasCapability(
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_LOGICAL_MULTI_CAMERA);
            if (isLogicalCamera) {
                Set<String> physicalCameraIds = mCameraCharacteristics.getPhysicalCameraIds();
                for (String id : physicalCameraIds) {
                    mPhysicalCameraChars.put(id, mCameraManager.getCameraCharacteristics(id));
                }
            }
            mSocketQueueQuota = new Semaphore(mMemoryQuota, true);
        } catch (CameraAccessException e) {
            throw new ItsException(""Failed to open camera"", e);
        } catch (BlockingOpenException e) {
            throw new ItsException(""Failed to open camera (after blocking)"", e);
        }
        mSocketRunnableObj.sendResponse(""cameraOpened"", """");
    }

    public void closeCameraDevice() throws ItsException {
        try {
            if (mCamera != null) {
                Logt.i(TAG, ""Closing camera"");
                mCamera.close();
                mCamera = null;
            }
        } catch (Exception e) {
            throw new ItsException(""Failed to close device"");
        }
        mSocketRunnableObj.sendResponse(""cameraClosed"", """");
    }

    class SerializerRunnable implements Runnable {
        // Use a separate thread to perform JSON serialization (since this can be slow due to
        // the reflection).
        @Override
        public void run() {
            Logt.i(TAG, ""Serializer thread starting"");
            while (! mThreadExitFlag) {
                try {
                    Object objs[] = mSerializerQueue.take();
                    JSONObject jsonObj = new JSONObject();
                    String tag = null;
                    for (int i = 0; i < objs.length; i++) {
                        Object obj = objs[i];
                        if (obj instanceof String) {
                            if (tag != null) {
                                throw new ItsException(""Multiple tags for socket response"");
                            }
                            tag = (String)obj;
                        } else if (obj instanceof CameraCharacteristics) {
                            jsonObj.put(""cameraProperties"", ItsSerializer.serialize(
                                    (CameraCharacteristics)obj));
                        } else if (obj instanceof CaptureRequest) {
                            jsonObj.put(""captureRequest"", ItsSerializer.serialize(
                                    (CaptureRequest)obj));
                        } else if (obj instanceof CaptureResult) {
                            jsonObj.put(""captureResult"", ItsSerializer.serialize(
                                    (CaptureResult)obj));
                        } else if (obj instanceof JSONArray) {
                            if (tag == ""captureResults"") {
                                if (i == SERIALIZER_SURFACES_ID) {
                                    jsonObj.put(""outputs"", (JSONArray)obj);
                                } else if (i == SERIALIZER_PHYSICAL_METADATA_ID) {
                                    jsonObj.put(""physicalResults"", (JSONArray)obj);
                                } else {
                                    throw new ItsException(
                                            ""Unsupported JSONArray for captureResults"");
                                }
                            } else {
                                jsonObj.put(""outputs"", (JSONArray)obj);
                            }
                        } else {
                            throw new ItsException(""Invalid object received for serialization"");
                        }
                    }
                    if (tag == null) {
                        throw new ItsException(""No tag provided for socket response"");
                    }
                    mSocketRunnableObj.sendResponse(tag, null, jsonObj, null);
                    Logt.i(TAG, String.format(""Serialized %s"", tag));
                } catch (org.json.JSONException e) {
                    Logt.e(TAG, ""Error serializing object"", e);
                    break;
                } catch (ItsException e) {
                    Logt.e(TAG, ""Error serializing object"", e);
                    break;
                } catch (java.lang.InterruptedException e) {
                    Logt.e(TAG, ""Error serializing object (interrupted)"", e);
                    break;
                }
            }
            Logt.i(TAG, ""Serializer thread terminated"");
        }
    }

    class SocketWriteRunnable implements Runnable {

        // Use a separate thread to service a queue of objects to be written to the socket,
        // writing each sequentially in order. This is needed since different handler functions
        // (called on different threads) will need to send data back to the host script.

        public Socket mOpenSocket = null;
        private Thread mThread = null;

        public SocketWriteRunnable(Socket openSocket) {
            mOpenSocket = openSocket;
        }

        public void setOpenSocket(Socket openSocket) {
            mOpenSocket = openSocket;
        }

        @Override
        public void run() {
            Logt.i(TAG, ""Socket writer thread starting"");
            while (true) {
                try {
                    ByteBuffer b = mSocketWriteQueue.take();
                    synchronized(mSocketWriteDrainLock) {
                        if (mOpenSocket == null) {
                            Logt.e(TAG, ""No open socket connection!"");
                            continue;
                        }
                        if (b.hasArray()) {
                            mOpenSocket.getOutputStream().write(b.array(), 0, b.capacity());
                        } else {
                            byte[] barray = new byte[b.capacity()];
                            b.get(barray);
                            mOpenSocket.getOutputStream().write(barray);
                        }
                        mOpenSocket.getOutputStream().flush();
                        Logt.i(TAG, String.format(""Wrote to socket: %d bytes"", b.capacity()));
                        Integer imgBufSize = mInflightImageSizes.peek();
                        if (imgBufSize != null && imgBufSize == b.capacity()) {
                            mInflightImageSizes.removeFirst();
                            if (mSocketQueueQuota != null) {
                                mSocketQueueQuota.release(imgBufSize);
                            }
                        }
                    }
                } catch (IOException e) {
                    Logt.e(TAG, ""Error writing to socket"", e);
                    mOpenSocket = null;
                    break;
                } catch (java.lang.InterruptedException e) {
                    Logt.e(TAG, ""Error writing to socket (interrupted)"", e);
                    mOpenSocket = null;
                    break;
                }
            }
            Logt.i(TAG, ""Socket writer thread terminated"");
        }

        public synchronized void checkAndStartThread() {
            if (mThread == null || mThread.getState() == Thread.State.TERMINATED) {
                mThread = new Thread(this);
            }
            if (mThread.getState() == Thread.State.NEW) {
                mThread.start();
            }
        }

    }

    class SocketRunnable implements Runnable {

        // Format of sent messages (over the socket):
        // * Serialized JSON object on a single line (newline-terminated)
        // * For byte buffers, the binary data then follows
        //
        // Format of received messages (from the socket):
        // * Serialized JSON object on a single line (newline-terminated)

        private Socket mOpenSocket = null;
        private SocketWriteRunnable mSocketWriteRunnable = null;

        @Override
        public void run() {
            Logt.i(TAG, ""Socket thread starting"");
            try {
                mSocket = new ServerSocket(SERVERPORT);
            } catch (IOException e) {
                Logt.e(TAG, ""Failed to create socket"", e);
            }

            // Create a new thread to handle writes to this socket.
            mSocketWriteRunnable = new SocketWriteRunnable(null);

            while (!mThreadExitFlag) {
                // Receive the socket-open request from the host.
                try {
                    Logt.i(TAG, ""Waiting for client to connect to socket"");
                    mOpenSocket = mSocket.accept();
                    if (mOpenSocket == null) {
                        Logt.e(TAG, ""Socket connection error"");
                        break;
                    }
                    mSocketWriteQueue.clear();
                    mInflightImageSizes.clear();
                    mSocketWriteRunnable.setOpenSocket(mOpenSocket);
                    mSocketWriteRunnable.checkAndStartThread();
                    Logt.i(TAG, ""Socket connected"");
                } catch (IOException e) {
                    Logt.e(TAG, ""Socket open error: "", e);
                    break;
                }

                // Process commands over the open socket.
                while (!mThreadExitFlag) {
                    try {
                        BufferedReader input = new BufferedReader(
                                new InputStreamReader(mOpenSocket.getInputStream()));
                        if (input == null) {
                            Logt.e(TAG, ""Failed to get socket input stream"");
                            break;
                        }
                        String line = input.readLine();
                        if (line == null) {
                            Logt.i(TAG, ""Socket readline returned null (host disconnected)"");
                            break;
                        }
                        processSocketCommand(line);
                    } catch (IOException e) {
                        Logt.e(TAG, ""Socket read error: "", e);
                        break;
                    } catch (ItsException e) {
                        Logt.e(TAG, ""Script error: "", e);
                        break;
                    }
                }

                // Close socket and go back to waiting for a new connection.
                try {
                    synchronized(mSocketWriteDrainLock) {
                        mSocketWriteQueue.clear();
                        mInflightImageSizes.clear();
                        mOpenSocket.close();
                        mOpenSocket = null;
                        mSocketWriteRunnable.setOpenSocket(null);
                        Logt.i(TAG, ""Socket disconnected"");
                    }
                } catch (java.io.IOException e) {
                    Logt.e(TAG, ""Exception closing socket"");
                }
            }

            // It's an overall error state if the code gets here; no recevery.
            // Try to do some cleanup, but the service probably needs to be restarted.
            Logt.i(TAG, ""Socket server loop exited"");
            mThreadExitFlag = true;
            try {
                synchronized(mSocketWriteDrainLock) {
                    if (mOpenSocket != null) {
                        mOpenSocket.close();
                        mOpenSocket = null;
                        mSocketWriteRunnable.setOpenSocket(null);
                    }
                }
            } catch (java.io.IOException e) {
                Logt.w(TAG, ""Exception closing socket"");
            }
            try {
                if (mSocket != null) {
                    mSocket.close();
                    mSocket = null;
                }
            } catch (java.io.IOException e) {
                Logt.w(TAG, ""Exception closing socket"");
            }
        }

        public void processSocketCommand(String cmd)
                throws ItsException {
            // Default locale must be set to ""en-us""
            Locale locale = Locale.getDefault();
            if (!Locale.US.equals(locale)) {
                Logt.e(TAG, ""Default language is not set to "" + Locale.US + ""!"");
                stopSelf();
            }

            // Each command is a serialized JSON object.
            try {
                JSONObject cmdObj = new JSONObject(cmd);
                Logt.i(TAG, ""Start processing command"" + cmdObj.getString(""cmdName""));
                if (""open"".equals(cmdObj.getString(""cmdName""))) {
                    String cameraId = cmdObj.getString(""cameraId"");
                    openCameraDevice(cameraId);
                } else if (""close"".equals(cmdObj.getString(""cmdName""))) {
                    closeCameraDevice();
                } else if (""getCameraProperties"".equals(cmdObj.getString(""cmdName""))) {
                    doGetProps();
                } else if (""getCameraPropertiesById"".equals(cmdObj.getString(""cmdName""))) {
                    doGetPropsById(cmdObj);
                } else if (""startSensorEvents"".equals(cmdObj.getString(""cmdName""))) {
                    doStartSensorEvents();
                } else if (""checkSensorExistence"".equals(cmdObj.getString(""cmdName""))) {
                    doCheckSensorExistence();
                } else if (""getSensorEvents"".equals(cmdObj.getString(""cmdName""))) {
                    doGetSensorEvents();
                } else if (""do3A"".equals(cmdObj.getString(""cmdName""))) {
                    do3A(cmdObj);
                } else if (""doCapture"".equals(cmdObj.getString(""cmdName""))) {
                    doCapture(cmdObj);
                } else if (""doVibrate"".equals(cmdObj.getString(""cmdName""))) {
                    doVibrate(cmdObj);
                } else if (""setAudioRestriction"".equals(cmdObj.getString(""cmdName""))) {
                    doSetAudioRestriction(cmdObj);
                } else if (""getCameraIds"".equals(cmdObj.getString(""cmdName""))) {
                    doGetCameraIds();
                } else if (""doReprocessCapture"".equals(cmdObj.getString(""cmdName""))) {
                    doReprocessCapture(cmdObj);
                } else if (""getItsVersion"".equals(cmdObj.getString(""cmdName""))) {
                    mSocketRunnableObj.sendResponse(""ItsVersion"", ITS_SERVICE_VERSION);
                } else if (""isStreamCombinationSupported"".equals(cmdObj.getString(""cmdName""))) {
                    doCheckStreamCombination(cmdObj);
                } else if (""isCameraPrivacyModeSupported"".equals(cmdObj.getString(""cmdName""))) {
                    doCheckCameraPrivacyModeSupport();
                } else if (""isPerformanceClassPrimaryCamera"".equals(cmdObj.getString(""cmdName""))) {
                    String cameraId = cmdObj.getString(""cameraId"");
                    doCheckPerformanceClassPrimaryCamera(cameraId);
                } else if (""measureCameraLaunchMs"".equals(cmdObj.getString(""cmdName""))) {
                    String cameraId = cmdObj.getString(""cameraId"");
                    doMeasureCameraLaunchMs(cameraId);
                } else if (""measureCamera1080pJpegCaptureMs"".equals(cmdObj.getString(""cmdName""))) {
                    String cameraId = cmdObj.getString(""cameraId"");
                    doMeasureCamera1080pJpegCaptureMs(cameraId);
                } else {
                    throw new ItsException(""Unknown command: "" + cmd);
                }
                Logt.i(TAG, ""Finish processing command"" + cmdObj.getString(""cmdName""));
            } catch (org.json.JSONException e) {
                Logt.e(TAG, ""Invalid command: "", e);
            }
        }

        public void sendResponse(String tag, String str, JSONObject obj, ByteBuffer bbuf)
                throws ItsException {
            try {
                JSONObject jsonObj = new JSONObject();
                jsonObj.put(""tag"", tag);
                if (str != null) {
                    jsonObj.put(""strValue"", str);
                }
                if (obj != null) {
                    jsonObj.put(""objValue"", obj);
                }
                if (bbuf != null) {
                    jsonObj.put(""bufValueSize"", bbuf.capacity());
                }
                ByteBuffer bstr = ByteBuffer.wrap(
                        (jsonObj.toString()+""\n"").getBytes(Charset.defaultCharset()));
                synchronized(mSocketWriteEnqueueLock) {
                    if (bstr != null) {
                        mSocketWriteQueue.put(bstr);
                    }
                    if (bbuf != null) {
                        mInflightImageSizes.add(bbuf.capacity());
                        mSocketWriteQueue.put(bbuf);
                    }
                }
            } catch (org.json.JSONException e) {
                throw new ItsException(""JSON error: "", e);
            } catch (java.lang.InterruptedException e) {
                throw new ItsException(""Socket error: "", e);
            }
        }

        public void sendResponse(String tag, String str)
                throws ItsException {
            sendResponse(tag, str, null, null);
        }

        public void sendResponse(String tag, JSONObject obj)
                throws ItsException {
            sendResponse(tag, null, obj, null);
        }

        public void sendResponseCaptureBuffer(String tag, ByteBuffer bbuf)
                throws ItsException {
            sendResponse(tag, null, null, bbuf);
        }

        public void sendResponse(LinkedList<MySensorEvent> events)
                throws ItsException {
            Logt.i(TAG, ""Sending "" + events.size() + "" sensor events"");
            try {
                JSONArray accels = new JSONArray();
                JSONArray mags = new JSONArray();
                JSONArray gyros = new JSONArray();
                for (MySensorEvent event : events) {
                    JSONObject obj = new JSONObject();
                    obj.put(""time"", event.timestamp);
                    obj.put(""x"", event.values[0]);
                    obj.put(""y"", event.values[1]);
                    obj.put(""z"", event.values[2]);
                    if (event.sensor.getType() == Sensor.TYPE_ACCELEROMETER) {
                        accels.put(obj);
                    } else if (event.sensor.getType() == Sensor.TYPE_MAGNETIC_FIELD) {
                        mags.put(obj);
                    } else if (event.sensor.getType() == Sensor.TYPE_GYROSCOPE) {
                        gyros.put(obj);
                    }
                }
                JSONObject obj = new JSONObject();
                obj.put(""accel"", accels);
                obj.put(""mag"", mags);
                obj.put(""gyro"", gyros);
                sendResponse(""sensorEvents"", null, obj, null);
            } catch (org.json.JSONException e) {
                throw new ItsException(""JSON error: "", e);
            }
            Logt.i(TAG, ""Sent sensor events"");
        }

        public void sendResponse(CameraCharacteristics props)
                throws ItsException {
            try {
                Object objs[] = new Object[2];
                objs[0] = ""cameraProperties"";
                objs[1] = props;
                mSerializerQueue.put(objs);
            } catch (InterruptedException e) {
                throw new ItsException(""Interrupted: "", e);
            }
        }

        public void sendResponseCaptureResult(CameraCharacteristics props,
                                              CaptureRequest request,
                                              TotalCaptureResult result,
                                              ImageReader[] readers)
                throws ItsException {
            try {
                JSONArray jsonSurfaces = new JSONArray();
                for (int i = 0; i < readers.length; i++) {
                    JSONObject jsonSurface = new JSONObject();
                    jsonSurface.put(""width"", readers[i].getWidth());
                    jsonSurface.put(""height"", readers[i].getHeight());
                    int format = readers[i].getImageFormat();
                    if (format == ImageFormat.RAW_SENSOR) {
                        if (mCaptureRawIsStats) {
                            int aaw = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .width();
                            int aah = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .height();
                            jsonSurface.put(""format"", ""rawStats"");
                            jsonSurface.put(""width"", aaw/mCaptureStatsGridWidth);
                            jsonSurface.put(""height"", aah/mCaptureStatsGridHeight);
                        } else if (mCaptureRawIsDng) {
                            jsonSurface.put(""format"", ""dng"");
                        } else {
                            jsonSurface.put(""format"", ""raw"");
                        }
                    } else if (format == ImageFormat.RAW10) {
                        jsonSurface.put(""format"", ""raw10"");
                    } else if (format == ImageFormat.RAW12) {
                        jsonSurface.put(""format"", ""raw12"");
                    } else if (format == ImageFormat.JPEG) {
                        jsonSurface.put(""format"", ""jpeg"");
                    } else if (format == ImageFormat.YUV_420_888) {
                        jsonSurface.put(""format"", ""yuv"");
                    } else if (format == ImageFormat.Y8) {
                        jsonSurface.put(""format"", ""y8"");
                    } else {
                        throw new ItsException(""Invalid format"");
                    }
                    jsonSurfaces.put(jsonSurface);
                }

                Map<String, CaptureResult> physicalMetadata =
                        result.getPhysicalCameraResults();
                JSONArray jsonPhysicalMetadata = new JSONArray();
                for (Map.Entry<String, CaptureResult> pair : physicalMetadata.entrySet()) {
                    JSONObject jsonOneMetadata = new JSONObject();
                    jsonOneMetadata.put(pair.getKey(), ItsSerializer.serialize(pair.getValue()));
                    jsonPhysicalMetadata.put(jsonOneMetadata);
                }
                Object objs[] = new Object[4];
                objs[0] = ""captureResults"";
                objs[1] = result;
                objs[SERIALIZER_SURFACES_ID] = jsonSurfaces;
                objs[SERIALIZER_PHYSICAL_METADATA_ID] = jsonPhysicalMetadata;
                mSerializerQueue.put(objs);
            } catch (org.json.JSONException e) {
                throw new ItsException(""JSON error: "", e);
            } catch (InterruptedException e) {
                throw new ItsException(""Interrupted: "", e);
            }
        }
    }

    public ImageReader.OnImageAvailableListener
            createAvailableListener(final CaptureCallback listener) {
        return new ImageReader.OnImageAvailableListener() {
            @Override
            public void onImageAvailable(ImageReader reader) {
                Image i = null;
                try {
                    i = reader.acquireNextImage();
                    String physicalCameraId = new String();
                    for (int idx = 0; idx < mOutputImageReaders.length; idx++) {
                        if (mOutputImageReaders[idx] == reader) {
                            physicalCameraId = mPhysicalStreamMap.get(idx);
                        }
                    }
                    listener.onCaptureAvailable(i, physicalCameraId);
                } finally {
                    if (i != null) {
                        i.close();
                    }
                }
            }
        };
    }

    private ImageReader.OnImageAvailableListener
            createAvailableListenerDropper() {
        return new ImageReader.OnImageAvailableListener() {
            @Override
            public void onImageAvailable(ImageReader reader) {
                Image i = reader.acquireNextImage();
                i.close();
            }
        };
    }

    private void doStartSensorEvents() throws ItsException {
        synchronized(mEventLock) {
            mEventsEnabled = true;
        }
        mSocketRunnableObj.sendResponse(""sensorEventsStarted"", """");
    }

    private void doCheckSensorExistence() throws ItsException {
        try {
            JSONObject obj = new JSONObject();
            obj.put(""accel"", mAccelSensor != null);
            obj.put(""mag"", mMagSensor != null);
            obj.put(""gyro"", mGyroSensor != null);
            obj.put(""vibrator"", mVibrator.hasVibrator());
            mSocketRunnableObj.sendResponse(""sensorExistence"", null, obj, null);
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        }
    }

    private void doGetSensorEvents() throws ItsException {
        synchronized(mEventLock) {
            mSocketRunnableObj.sendResponse(mEvents);
            mEvents.clear();
            mEventsEnabled = false;
        }
    }

    private void doGetProps() throws ItsException {
        mSocketRunnableObj.sendResponse(mCameraCharacteristics);
    }

    private void doGetPropsById(JSONObject params) throws ItsException {
        String[] devices;
        try {
            // Intentionally not using ItsUtils.getItsCompatibleCameraIds here so it's possible to
            // write some simple script to query camera characteristics even for devices exempted
            // from ITS today.
            devices = mCameraManager.getCameraIdList();
            if (devices == null || devices.length == 0) {
                throw new ItsException(""No camera devices"");
            }
        } catch (CameraAccessException e) {
            throw new ItsException(""Failed to get device ID list"", e);
        }

        try {
            String cameraId = params.getString(""cameraId"");
            CameraCharacteristics characteristics =
                    mCameraManager.getCameraCharacteristics(cameraId);
            mSocketRunnableObj.sendResponse(characteristics);
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        } catch (IllegalArgumentException e) {
            throw new ItsException(""Illegal argument error:"", e);
        } catch (CameraAccessException e) {
            throw new ItsException(""Access error: "", e);
        }
    }

    private void doGetCameraIds() throws ItsException {
        if (mItsCameraIdList == null) {
            mItsCameraIdList = ItsUtils.getItsCompatibleCameraIds(mCameraManager);
        }
        if (mItsCameraIdList.mCameraIdCombos.size() == 0) {
            throw new ItsException(""No camera devices"");
        }

        try {
            JSONObject obj = new JSONObject();
            JSONArray array = new JSONArray();
            for (String id : mItsCameraIdList.mCameraIdCombos) {
                array.put(id);
            }
            obj.put(""cameraIdArray"", array);
            mSocketRunnableObj.sendResponse(""cameraIds"", obj);
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        }
    }

    private static class HandlerExecutor implements Executor {
        private final Handler mHandler;

        public HandlerExecutor(Handler handler) {
            mHandler = handler;
        }

        @Override
        public void execute(Runnable runCmd) {
            mHandler.post(runCmd);
        }
    }

    private void doCheckStreamCombination(JSONObject params) throws ItsException {
        try {
            JSONObject obj = new JSONObject();
            JSONArray jsonOutputSpecs = ItsUtils.getOutputSpecs(params);
            prepareImageReadersWithOutputSpecs(jsonOutputSpecs, /*inputSize*/null,
                    /*inputFormat*/0, /*maxInputBuffers*/0, /*backgroundRequest*/false);
            int numSurfaces = mOutputImageReaders.length;
            List<OutputConfiguration> outputConfigs =
                    new ArrayList<OutputConfiguration>(numSurfaces);
            for (int i = 0; i < numSurfaces; i++) {
                OutputConfiguration config = new OutputConfiguration(
                        mOutputImageReaders[i].getSurface());
                if (mPhysicalStreamMap.get(i) != null) {
                    config.setPhysicalCameraId(mPhysicalStreamMap.get(i));
                }
                outputConfigs.add(config);
            }

            BlockingSessionCallback sessionListener = new BlockingSessionCallback();
            SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_REGULAR, outputConfigs,
                new HandlerExecutor(mCameraHandler), sessionListener);
            boolean supported = mCamera.isSessionConfigurationSupported(sessionConfig);

            String supportString = supported ? ""supportedCombination"" : ""unsupportedCombination"";
            mSocketRunnableObj.sendResponse(""streamCombinationSupport"", supportString);

        } catch (UnsupportedOperationException e) {
            mSocketRunnableObj.sendResponse(""streamCombinationSupport"", ""unsupportedOperation"");
        } catch (IllegalArgumentException e) {
            throw new ItsException(""Error checking stream combination"", e);
        } catch (CameraAccessException e) {
            throw new ItsException(""Error checking stream combination"", e);
        }
    }

    private void doCheckCameraPrivacyModeSupport() throws ItsException {
        boolean hasPrivacySupport = mSensorPrivacyManager
                .supportsSensorToggle(SensorPrivacyManager.Sensors.CAMERA);
        mSocketRunnableObj.sendResponse(""cameraPrivacyModeSupport"",
                hasPrivacySupport ? ""true"" : ""false"");
    }

    private void doCheckPerformanceClassPrimaryCamera(String cameraId) throws ItsException {
        boolean  isPerfClass = (Build.VERSION.MEDIA_PERFORMANCE_CLASS == PERFORMANCE_CLASS_S
                || Build.VERSION.MEDIA_PERFORMANCE_CLASS == PERFORMANCE_CLASS_R);

        if (mItsCameraIdList == null) {
            mItsCameraIdList = ItsUtils.getItsCompatibleCameraIds(mCameraManager);
        }
        if (mItsCameraIdList.mCameraIds.size() == 0) {
            throw new ItsException(""No camera devices"");
        }
        if (!mItsCameraIdList.mCameraIds.contains(cameraId)) {
            throw new ItsException(""Invalid cameraId "" + cameraId);
        }

        boolean isPrimaryCamera = false;
        try {
            CameraCharacteristics c = mCameraManager.getCameraCharacteristics(cameraId);
            Integer cameraFacing = c.get(CameraCharacteristics.LENS_FACING);
            for (String id : mItsCameraIdList.mCameraIds) {
                c = mCameraManager.getCameraCharacteristics(id);
                Integer facing = c.get(CameraCharacteristics.LENS_FACING);
                if (cameraFacing.equals(facing)) {
                    if (cameraId.equals(id)) {
                        isPrimaryCamera = true;
                    } else {
                        isPrimaryCamera = false;
                    }
                    break;
                }
            }
        } catch (CameraAccessException e) {
            throw new ItsException(""Failed to get camera characteristics"", e);
        }

        mSocketRunnableObj.sendResponse(""performanceClassPrimaryCamera"",
                (isPerfClass && isPrimaryCamera) ? ""true"" : ""false"");
    }

    private double invokeCameraPerformanceTest(Class testClass, String testName,
            String cameraId, String metricName) throws ItsException {
        mResults.clear();
        mCameraInstrumentation = new CameraTestInstrumentation();
        MetricListener metricListener = new MetricListener() {
            @Override
            public void onResultMetric(Metric metric) {
                mResults.add(metric);
            }
        };
        mCameraInstrumentation.initialize(this, metricListener);

        Bundle bundle = new Bundle();
        bundle.putString(""camera-id"", cameraId);
        bundle.putString(""perf-measure"", ""on"");
        bundle.putString(""perf-class-test"", ""on"");
        InstrumentationRegistry.registerInstance(mCameraInstrumentation, bundle);

        JUnitCore testRunner = new JUnitCore();
        Log.v(TAG, String.format(""Execute Test: %s#%s"", testClass.getSimpleName(), testName));
        Request request = Request.method(testClass, testName);
        Result runResult = testRunner.run(request);
        if (!runResult.wasSuccessful()) {
            throw new ItsException(""Camera PerformanceTest "" + testClass.getSimpleName() +
                    ""#"" + testName + "" failed"");
        }

        for (Metric m : mResults) {
            if (m.getMessage().equals(metricName) && m.getValues().length == 1) {
                return m.getValues()[0];
            }
        }

        throw new ItsException(""Failed to look up "" + metricName +
                "" in Camera PerformanceTest result!"");
    }

    private void doMeasureCameraLaunchMs(String cameraId) throws ItsException {
        double launchMs = invokeCameraPerformanceTest(PerformanceTest.class,
                ""testCameraLaunch"", cameraId, ""camera_launch_average_time_for_all_cameras"");
        mSocketRunnableObj.sendResponse(""cameraLaunchMs"", Double.toString(launchMs));
    }

    private void doMeasureCamera1080pJpegCaptureMs(String cameraId) throws ItsException {
        double jpegCaptureMs = invokeCameraPerformanceTest(PerformanceTest.class,
                ""testSingleCapture"", cameraId,
                ""camera_capture_average_latency_for_all_cameras_jpeg"");
        mSocketRunnableObj.sendResponse(""camera1080pJpegCaptureMs"", Double.toString(jpegCaptureMs));
    }

    private void prepareImageReaders(Size[] outputSizes, int[] outputFormats, Size inputSize,
            int inputFormat, int maxInputBuffers) {
        closeImageReaders();
        mOutputImageReaders = new ImageReader[outputSizes.length];
        for (int i = 0; i < outputSizes.length; i++) {
            // Check if the output image reader can be shared with the input image reader.
            if (outputSizes[i].equals(inputSize) && outputFormats[i] == inputFormat) {
                mOutputImageReaders[i] = ImageReader.newInstance(outputSizes[i].getWidth(),
                        outputSizes[i].getHeight(), outputFormats[i],
                        MAX_CONCURRENT_READER_BUFFERS + maxInputBuffers);
                mInputImageReader = mOutputImageReaders[i];
            } else {
                mOutputImageReaders[i] = ImageReader.newInstance(outputSizes[i].getWidth(),
                        outputSizes[i].getHeight(), outputFormats[i],
                        MAX_CONCURRENT_READER_BUFFERS);
            }
        }

        if (inputSize != null && mInputImageReader == null) {
            mInputImageReader = ImageReader.newInstance(inputSize.getWidth(), inputSize.getHeight(),
                    inputFormat, maxInputBuffers);
        }
    }

    private void closeImageReaders() {
        if (mOutputImageReaders != null) {
            for (int i = 0; i < mOutputImageReaders.length; i++) {
                if (mOutputImageReaders[i] != null) {
                    mOutputImageReaders[i].close();
                    mOutputImageReaders[i] = null;
                }
            }
        }
        if (mInputImageReader != null) {
            mInputImageReader.close();
            mInputImageReader = null;
        }
    }

    private void do3A(JSONObject params) throws ItsException {
        ThreeAResultListener threeAListener = new ThreeAResultListener();
        try {
            // Start a 3A action, and wait for it to converge.
            // Get the converged values for each ""A"", and package into JSON result for caller.

            // Configure streams on physical sub-camera if PHYSICAL_ID_KEY is specified.
            String physicalId = null;
            CameraCharacteristics c = mCameraCharacteristics;
            if (params.has(PHYSICAL_ID_KEY)) {
                physicalId = params.getString(PHYSICAL_ID_KEY);
                c = mPhysicalCameraChars.get(physicalId);
            }

            // 3A happens on full-res frames.
            Size sizes[] = ItsUtils.getYuvOutputSizes(c);
            int outputFormats[] = new int[1];
            outputFormats[0] = ImageFormat.YUV_420_888;
            Size[] outputSizes = new Size[1];
            outputSizes[0] = sizes[0];
            int width = outputSizes[0].getWidth();
            int height = outputSizes[0].getHeight();

            prepareImageReaders(outputSizes, outputFormats, /*inputSize*/null, /*inputFormat*/0,
                    /*maxInputBuffers*/0);

            List<OutputConfiguration> outputConfigs = new ArrayList<OutputConfiguration>(1);
            OutputConfiguration config =
                    new OutputConfiguration(mOutputImageReaders[0].getSurface());
            if (physicalId != null) {
                config.setPhysicalCameraId(physicalId);
            }
            outputConfigs.add(config);
            BlockingSessionCallback sessionListener = new BlockingSessionCallback();
            mCamera.createCaptureSessionByOutputConfigurations(
                    outputConfigs, sessionListener, mCameraHandler);
            mSession = sessionListener.waitAndGetSession(TIMEOUT_IDLE_MS);

            // Add a listener that just recycles buffers; they aren't saved anywhere.
            ImageReader.OnImageAvailableListener readerListener =
                    createAvailableListenerDropper();
            mOutputImageReaders[0].setOnImageAvailableListener(readerListener, mSaveHandlers[0]);

            // Get the user-specified regions for AE, AWB, AF.
            // Note that the user specifies normalized [x,y,w,h], which is converted below
            // to an [x0,y0,x1,y1] region in sensor coords. The capture request region
            // also has a fifth ""weight"" element: [x0,y0,x1,y1,w].
            // Use logical camera's active array size for 3A regions.
            Rect activeArray = mCameraCharacteristics.get(
                    CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE);
            int aaWidth = activeArray.right - activeArray.left;
            int aaHeight = activeArray.bottom - activeArray.top;
            MeteringRectangle[] regionAE = new MeteringRectangle[]{
                    new MeteringRectangle(0,0,aaWidth,aaHeight,1)};
            MeteringRectangle[] regionAF = new MeteringRectangle[]{
                    new MeteringRectangle(0,0,aaWidth,aaHeight,1)};
            MeteringRectangle[] regionAWB = new MeteringRectangle[]{
                    new MeteringRectangle(0,0,aaWidth,aaHeight,1)};
            if (params.has(REGION_KEY)) {
                JSONObject regions = params.getJSONObject(REGION_KEY);
                if (regions.has(REGION_AE_KEY)) {
                    regionAE = ItsUtils.getJsonWeightedRectsFromArray(
                            regions.getJSONArray(REGION_AE_KEY), true, aaWidth, aaHeight);
                }
                if (regions.has(REGION_AF_KEY)) {
                    regionAF = ItsUtils.getJsonWeightedRectsFromArray(
                            regions.getJSONArray(REGION_AF_KEY), true, aaWidth, aaHeight);
                }
                if (regions.has(REGION_AWB_KEY)) {
                    regionAWB = ItsUtils.getJsonWeightedRectsFromArray(
                            regions.getJSONArray(REGION_AWB_KEY), true, aaWidth, aaHeight);
                }
            }

            // An EV compensation can be specified as part of AE convergence.
            int evComp = params.optInt(EVCOMP_KEY, 0);
            if (evComp != 0) {
                Logt.i(TAG, String.format(""Running 3A with AE exposure compensation value: %d"", evComp));
            }

            // By default, AE and AF both get triggered, but the user can optionally override this.
            // Also, AF won't get triggered if the lens is fixed-focus.
            boolean doAE = true;
            boolean doAF = true;
            if (params.has(TRIGGER_KEY)) {
                JSONObject triggers = params.getJSONObject(TRIGGER_KEY);
                if (triggers.has(TRIGGER_AE_KEY)) {
                    doAE = triggers.getBoolean(TRIGGER_AE_KEY);
                }
                if (triggers.has(TRIGGER_AF_KEY)) {
                    doAF = triggers.getBoolean(TRIGGER_AF_KEY);
                }
            }
            Float minFocusDistance = c.get(
                    CameraCharacteristics.LENS_INFO_MINIMUM_FOCUS_DISTANCE);
            boolean isFixedFocusLens = minFocusDistance != null && minFocusDistance == 0.0;
            if (doAF && isFixedFocusLens) {
                // Send a fake result back for the code that is waiting for this message to see
                // that AF has converged.
                Logt.i(TAG, ""Ignoring request for AF on fixed-focus camera"");
                mSocketRunnableObj.sendResponse(""afResult"", ""0.0"");
                doAF = false;
            }

            mInterlock3A.open();
            synchronized(m3AStateLock) {
                // If AE or AWB lock is specified, then the 3A will converge first and then lock these
                // values, waiting until the HAL has reported that the lock was successful.
                mNeedsLockedAE = params.optBoolean(LOCK_AE_KEY, false);
                mNeedsLockedAWB = params.optBoolean(LOCK_AWB_KEY, false);
                mConvergedAE = false;
                mConvergedAWB = false;
                mConvergedAF = false;
                mLockedAE = false;
                mLockedAWB = false;
            }
            long tstart = System.currentTimeMillis();
            boolean triggeredAE = false;
            boolean triggeredAF = false;

            Logt.i(TAG, String.format(""Initiating 3A: AE:%d, AF:%d, AWB:1, AELOCK:%d, AWBLOCK:%d"",
                    doAE?1:0, doAF?1:0, mNeedsLockedAE?1:0, mNeedsLockedAWB?1:0));

            // Keep issuing capture requests until 3A has converged.
            while (true) {

                // Block until can take the next 3A frame. Only want one outstanding frame
                // at a time, to simplify the logic here.
                if (!mInterlock3A.block(TIMEOUT_3A * 1000) ||
                        System.currentTimeMillis() - tstart > TIMEOUT_3A * 1000) {
                    throw new ItsException(
                            ""3A failed to converge after "" + TIMEOUT_3A + "" seconds.\n"" +
                            ""AE converge state: "" + mConvergedAE + "", \n"" +
                            ""AF convergence state: "" + mConvergedAF + "", \n"" +
                            ""AWB convergence state: "" + mConvergedAWB + ""."");
                }
                mInterlock3A.close();

                synchronized(m3AStateLock) {
                    // If not converged yet, issue another capture request.
                    if (       (doAE && (!triggeredAE || !mConvergedAE))
                            || !mConvergedAWB
                            || (doAF && (!triggeredAF || !mConvergedAF))
                            || (doAE && mNeedsLockedAE && !mLockedAE)
                            || (mNeedsLockedAWB && !mLockedAWB)) {

                        // Baseline capture request for 3A.
                        CaptureRequest.Builder req = mCamera.createCaptureRequest(
                                CameraDevice.TEMPLATE_PREVIEW);
                        req.set(CaptureRequest.FLASH_MODE, CaptureRequest.FLASH_MODE_OFF);
                        req.set(CaptureRequest.CONTROL_MODE, CaptureRequest.CONTROL_MODE_AUTO);
                        req.set(CaptureRequest.CONTROL_CAPTURE_INTENT,
                                CaptureRequest.CONTROL_CAPTURE_INTENT_PREVIEW);
                        req.set(CaptureRequest.CONTROL_AE_MODE,
                                CaptureRequest.CONTROL_AE_MODE_ON);
                        req.set(CaptureRequest.CONTROL_AE_EXPOSURE_COMPENSATION, 0);
                        req.set(CaptureRequest.CONTROL_AE_LOCK, false);
                        req.set(CaptureRequest.CONTROL_AE_REGIONS, regionAE);
                        req.set(CaptureRequest.CONTROL_AF_MODE,
                                CaptureRequest.CONTROL_AF_MODE_AUTO);
                        req.set(CaptureRequest.CONTROL_AF_REGIONS, regionAF);
                        req.set(CaptureRequest.CONTROL_AWB_MODE,
                                CaptureRequest.CONTROL_AWB_MODE_AUTO);
                        req.set(CaptureRequest.CONTROL_AWB_LOCK, false);
                        req.set(CaptureRequest.CONTROL_AWB_REGIONS, regionAWB);
                        // ITS only turns OIS on when it's explicitly requested
                        req.set(CaptureRequest.LENS_OPTICAL_STABILIZATION_MODE,
                                CaptureRequest.LENS_OPTICAL_STABILIZATION_MODE_OFF);

                        if (evComp != 0) {
                            req.set(CaptureRequest.CONTROL_AE_EXPOSURE_COMPENSATION, evComp);
                        }

                        if (mConvergedAE && mNeedsLockedAE) {
                            req.set(CaptureRequest.CONTROL_AE_LOCK, true);
                        }
                        if (mConvergedAWB && mNeedsLockedAWB) {
                            req.set(CaptureRequest.CONTROL_AWB_LOCK, true);
                        }

                        boolean triggering = false;
                        // Trigger AE first.
                        if (doAE && !triggeredAE) {
                            Logt.i(TAG, ""Triggering AE"");
                            req.set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,
                                    CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_START);
                            triggeredAE = true;
                            triggering = true;
                        }

                        // After AE has converged, trigger AF.
                        if (doAF && !triggeredAF && (!doAE || (triggeredAE && mConvergedAE))) {
                            Logt.i(TAG, ""Triggering AF"");
                            req.set(CaptureRequest.CONTROL_AF_TRIGGER,
                                    CaptureRequest.CONTROL_AF_TRIGGER_START);
                            triggeredAF = true;
                            triggering = true;
                        }

                        req.addTarget(mOutputImageReaders[0].getSurface());

                        if (triggering) {
                            // Send single request for AE/AF trigger
                            mSession.capture(req.build(),
                                    threeAListener, mResultHandler);
                        } else {
                            // Use repeating request for non-trigger requests
                            mSession.setRepeatingRequest(req.build(),
                                    threeAListener, mResultHandler);
                        }
                    } else {
                        mSocketRunnableObj.sendResponse(""3aConverged"", """");
                        Logt.i(TAG, ""3A converged"");
                        break;
                    }
                }
            }
        } catch (android.hardware.camera2.CameraAccessException e) {
            throw new ItsException(""Access error: "", e);
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        } finally {
            mSocketRunnableObj.sendResponse(""3aDone"", """");
            // stop listener from updating 3A states
            threeAListener.stop();
            if (mSession != null) {
                mSession.close();
            }
        }
    }

    private void doVibrate(JSONObject params) throws ItsException {
        try {
            if (mVibrator == null) {
                throw new ItsException(""Unable to start vibrator"");
            }
            JSONArray patternArray = params.getJSONArray(VIB_PATTERN_KEY);
            int len = patternArray.length();
            long pattern[] = new long[len];
            for (int i = 0; i < len; i++) {
                pattern[i] = patternArray.getLong(i);
            }
            Logt.i(TAG, String.format(""Starting vibrator, pattern length %d"",len));

            // Mark the vibrator as alarm to test the audio restriction API
            // TODO: consider making this configurable
            AudioAttributes audioAttributes = new AudioAttributes.Builder()
                    .setUsage(AudioAttributes.USAGE_ALARM).build();
            mVibrator.vibrate(pattern, -1, audioAttributes);
            mSocketRunnableObj.sendResponse(""vibrationStarted"", """");
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        }
    }

    private void doSetAudioRestriction(JSONObject params) throws ItsException {
        try {
            if (mCamera == null) {
                throw new ItsException(""Camera is closed"");
            }
            int mode = params.getInt(AUDIO_RESTRICTION_MODE_KEY);
            mCamera.setCameraAudioRestriction(mode);
            Logt.i(TAG, String.format(""Set audio restriction mode to %d"", mode));

            mSocketRunnableObj.sendResponse(""audioRestrictionSet"", """");
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        } catch (android.hardware.camera2.CameraAccessException e) {
            throw new ItsException(""Access error: "", e);
        }
    }

    /**
     * Parse jsonOutputSpecs to get output surface sizes and formats. Create input and output
     * image readers for the parsed output surface sizes, output formats, and the given input
     * size and format.
     */
    private void prepareImageReadersWithOutputSpecs(JSONArray jsonOutputSpecs, Size inputSize,
            int inputFormat, int maxInputBuffers, boolean backgroundRequest) throws ItsException {
        Size outputSizes[];
        int outputFormats[];
        int numSurfaces = 0;
        mPhysicalStreamMap.clear();

        if (jsonOutputSpecs != null) {
            try {
                numSurfaces = jsonOutputSpecs.length();
                if (backgroundRequest) {
                    numSurfaces += 1;
                }
                if (numSurfaces > MAX_NUM_OUTPUT_SURFACES) {
                    throw new ItsException(""Too many output surfaces"");
                }

                outputSizes = new Size[numSurfaces];
                outputFormats = new int[numSurfaces];
                for (int i = 0; i < numSurfaces; i++) {
                    // Append optional background stream at the end
                    if (backgroundRequest && i == numSurfaces - 1) {
                        outputFormats[i] = ImageFormat.YUV_420_888;
                        outputSizes[i] = new Size(640, 480);
                        continue;
                    }
                    // Get the specified surface.
                    JSONObject surfaceObj = jsonOutputSpecs.getJSONObject(i);
                    String physicalCameraId = surfaceObj.optString(""physicalCamera"");
                    CameraCharacteristics cameraCharacteristics =  mCameraCharacteristics;
                    mPhysicalStreamMap.put(i, physicalCameraId);
                    if (!physicalCameraId.isEmpty()) {
                        cameraCharacteristics = mPhysicalCameraChars.get(physicalCameraId);
                    }

                    String sformat = surfaceObj.optString(""format"");
                    Size sizes[];
                    if (""yuv"".equals(sformat) || """".equals(sformat)) {
                        // Default to YUV if no format is specified.
                        outputFormats[i] = ImageFormat.YUV_420_888;
                        sizes = ItsUtils.getYuvOutputSizes(cameraCharacteristics);
                    } else if (""jpg"".equals(sformat) || ""jpeg"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.JPEG;
                        sizes = ItsUtils.getJpegOutputSizes(cameraCharacteristics);
                    } else if (""raw"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.RAW_SENSOR;
                        sizes = ItsUtils.getRaw16OutputSizes(cameraCharacteristics);
                    } else if (""raw10"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.RAW10;
                        sizes = ItsUtils.getRaw10OutputSizes(cameraCharacteristics);
                    } else if (""raw12"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.RAW12;
                        sizes = ItsUtils.getRaw12OutputSizes(cameraCharacteristics);
                    } else if (""dng"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.RAW_SENSOR;
                        sizes = ItsUtils.getRaw16OutputSizes(cameraCharacteristics);
                        mCaptureRawIsDng = true;
                    } else if (""rawStats"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.RAW_SENSOR;
                        sizes = ItsUtils.getRaw16OutputSizes(cameraCharacteristics);
                        mCaptureRawIsStats = true;
                        mCaptureStatsGridWidth = surfaceObj.optInt(""gridWidth"");
                        mCaptureStatsGridHeight = surfaceObj.optInt(""gridHeight"");
                    } else if (""y8"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.Y8;
                        sizes = ItsUtils.getY8OutputSizes(cameraCharacteristics);
                    } else {
                        throw new ItsException(""Unsupported format: "" + sformat);
                    }
                    // If the size is omitted, then default to the largest allowed size for the
                    // format.
                    int width = surfaceObj.optInt(""width"");
                    int height = surfaceObj.optInt(""height"");
                    if (width <= 0) {
                        if (sizes == null || sizes.length == 0) {
                            throw new ItsException(String.format(
                                    ""Zero stream configs available for requested format: %s"",
                                    sformat));
                        }
                        width = ItsUtils.getMaxSize(sizes).getWidth();
                    }
                    if (height <= 0) {
                        height = ItsUtils.getMaxSize(sizes).getHeight();
                    }
                    // The stats computation only applies to the active array region.
                    int aaw = ItsUtils.getActiveArrayCropRegion(cameraCharacteristics).width();
                    int aah = ItsUtils.getActiveArrayCropRegion(cameraCharacteristics).height();
                    if (mCaptureStatsGridWidth <= 0 || mCaptureStatsGridWidth > aaw) {
                        mCaptureStatsGridWidth = aaw;
                    }
                    if (mCaptureStatsGridHeight <= 0 || mCaptureStatsGridHeight > aah) {
                        mCaptureStatsGridHeight = aah;
                    }

                    outputSizes[i] = new Size(width, height);
                }
            } catch (org.json.JSONException e) {
                throw new ItsException(""JSON error"", e);
            }
        } else {
            // No surface(s) specified at all.
            // Default: a single output surface which is full-res YUV.
            Size maxYuvSize = ItsUtils.getMaxOutputSize(
                    mCameraCharacteristics, ImageFormat.YUV_420_888);
            numSurfaces = backgroundRequest ? 2 : 1;

            outputSizes = new Size[numSurfaces];
            outputFormats = new int[numSurfaces];
            outputSizes[0] = maxYuvSize;
            outputFormats[0] = ImageFormat.YUV_420_888;
            if (backgroundRequest) {
                outputSizes[1] = new Size(640, 480);
                outputFormats[1] = ImageFormat.YUV_420_888;
            }
        }

        prepareImageReaders(outputSizes, outputFormats, inputSize, inputFormat, maxInputBuffers);
    }

    /**
     * Wait until mCountCallbacksRemaining is 0 or a specified amount of time has elapsed between
     * each callback.
     */
    private void waitForCallbacks(long timeoutMs) throws ItsException {
        synchronized(mCountCallbacksRemaining) {
            int currentCount = mCountCallbacksRemaining.get();
            while (currentCount > 0) {
                try {
                    mCountCallbacksRemaining.wait(timeoutMs);
                } catch (InterruptedException e) {
                    throw new ItsException(""Waiting for callbacks was interrupted."", e);
                }

                int newCount = mCountCallbacksRemaining.get();
                if (newCount == currentCount) {
                    throw new ItsException(""No callback received within timeout "" +
                            timeoutMs + ""ms"");
                }
                currentCount = newCount;
            }
        }
    }

    private void doCapture(JSONObject params) throws ItsException {
        try {
            // Parse the JSON to get the list of capture requests.
            List<CaptureRequest.Builder> requests = ItsSerializer.deserializeRequestList(
                    mCamera, params, ""captureRequests"");

            // optional background preview requests
            List<CaptureRequest.Builder> backgroundRequests = ItsSerializer.deserializeRequestList(
                    mCamera, params, ""repeatRequests"");
            boolean backgroundRequest = backgroundRequests.size() > 0;

            int numSurfaces = 0;
            int numCaptureSurfaces = 0;
            BlockingSessionCallback sessionListener = new BlockingSessionCallback();
            try {
                mCountRawOrDng.set(0);
                mCountJpg.set(0);
                mCountYuv.set(0);
                mCountRaw10.set(0);
                mCountRaw12.set(0);
                mCountCapRes.set(0);
                mCaptureRawIsDng = false;
                mCaptureRawIsStats = false;
                mCaptureResults = new CaptureResult[requests.size()];

                JSONArray jsonOutputSpecs = ItsUtils.getOutputSpecs(params);

                prepareImageReadersWithOutputSpecs(jsonOutputSpecs, /*inputSize*/null,
                        /*inputFormat*/0, /*maxInputBuffers*/0, backgroundRequest);
                numSurfaces = mOutputImageReaders.length;
                numCaptureSurfaces = numSurfaces - (backgroundRequest ? 1 : 0);

                List<OutputConfiguration> outputConfigs =
                        new ArrayList<OutputConfiguration>(numSurfaces);
                for (int i = 0; i < numSurfaces; i++) {
                    OutputConfiguration config = new OutputConfiguration(
                            mOutputImageReaders[i].getSurface());
                    if (mPhysicalStreamMap.get(i) != null) {
                        config.setPhysicalCameraId(mPhysicalStreamMap.get(i));
                    }
                    outputConfigs.add(config);
                }
                mCamera.createCaptureSessionByOutputConfigurations(outputConfigs,
                        sessionListener, mCameraHandler);
                mSession = sessionListener.waitAndGetSession(TIMEOUT_IDLE_MS);

                for (int i = 0; i < numSurfaces; i++) {
                    ImageReader.OnImageAvailableListener readerListener;
                    if (backgroundRequest && i == numSurfaces - 1) {
                        readerListener = createAvailableListenerDropper();
                    } else {
                        readerListener = createAvailableListener(mCaptureCallback);
                    }
                    mOutputImageReaders[i].setOnImageAvailableListener(readerListener,
                            mSaveHandlers[i]);
                }

                // Plan for how many callbacks need to be received throughout the duration of this
                // sequence of capture requests. There is one callback per image surface, and one
                // callback for the CaptureResult, for each capture.
                int numCaptures = requests.size();
                mCountCallbacksRemaining.set(numCaptures * (numCaptureSurfaces + 1));

            } catch (CameraAccessException e) {
                throw new ItsException(""Error configuring outputs"", e);
            }

            // Start background requests and let it warm up pipeline
            if (backgroundRequest) {
                List<CaptureRequest> bgRequestList =
                        new ArrayList<CaptureRequest>(backgroundRequests.size());
                for (int i = 0; i < backgroundRequests.size(); i++) {
                    CaptureRequest.Builder req = backgroundRequests.get(i);
                    req.addTarget(mOutputImageReaders[numCaptureSurfaces].getSurface());
                    bgRequestList.add(req.build());
                }
                mSession.setRepeatingBurst(bgRequestList, null, null);
                // warm up the pipeline
                Thread.sleep(PIPELINE_WARMUP_TIME_MS);
            }

            // Initiate the captures.
            long maxExpTimeNs = -1;
            List<CaptureRequest> requestList =
                    new ArrayList<>(requests.size());
            for (int i = 0; i < requests.size(); i++) {
                CaptureRequest.Builder req = requests.get(i);
                // For DNG captures, need the LSC map to be available.
                if (mCaptureRawIsDng) {
                    req.set(CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE, 1);
                }
                Long expTimeNs = req.get(CaptureRequest.SENSOR_EXPOSURE_TIME);
                if (expTimeNs != null && expTimeNs > maxExpTimeNs) {
                    maxExpTimeNs = expTimeNs;
                }

                for (int j = 0; j < numCaptureSurfaces; j++) {
                    req.addTarget(mOutputImageReaders[j].getSurface());
                }
                requestList.add(req.build());
            }
            mSession.captureBurst(requestList, mCaptureResultListener, mResultHandler);

            long timeout = TIMEOUT_CALLBACK * 1000;
            if (maxExpTimeNs > 0) {
                timeout += maxExpTimeNs / 1000000; // ns to ms
            }
            // Make sure all callbacks have been hit (wait until captures are done).
            // If no timeouts are received after a timeout, then fail.
            waitForCallbacks(timeout);

            // Close session and wait until session is fully closed
            mSession.close();
            sessionListener.getStateWaiter().waitForState(
                    BlockingSessionCallback.SESSION_CLOSED, TIMEOUT_SESSION_CLOSE);

        } catch (android.hardware.camera2.CameraAccessException e) {
            throw new ItsException(""Access error: "", e);
        } catch (InterruptedException e) {
            throw new ItsException(""Unexpected InterruptedException: "", e);
        }
    }

    /**
     * Perform reprocess captures.
     *
     * It takes captureRequests in a JSON object and perform capture requests in two steps:
     * regular capture request to get reprocess input and reprocess capture request to get
     * reprocess outputs.
     *
     * Regular capture requests:
     *   1. For each capture request in the JSON object, create a full-size capture request with
     *      the settings in the JSON object.
     *   2. Remember and clear noise reduction, edge enhancement, and effective exposure factor
     *      from the regular capture requests. (Those settings will be used for reprocess requests.)
     *   3. Submit the regular capture requests.
     *
     * Reprocess capture requests:
     *   4. Wait for the regular capture results and use them to create reprocess capture requests.
     *   5. Wait for the regular capture output images and queue them to the image writer.
     *   6. Set the noise reduction, edge enhancement, and effective exposure factor from #2.
     *   7. Submit the reprocess capture requests.
     *
     * The output images and results for the regular capture requests won't be written to socket.
     * The output images and results for the reprocess capture requests will be written to socket.
     */
    private void doReprocessCapture(JSONObject params) throws ItsException {
        ImageWriter imageWriter = null;
        ArrayList<Integer> noiseReductionModes = new ArrayList<>();
        ArrayList<Integer> edgeModes = new ArrayList<>();
        ArrayList<Float> effectiveExposureFactors = new ArrayList<>();

        mCountRawOrDng.set(0);
        mCountJpg.set(0);
        mCountYuv.set(0);
        mCountRaw10.set(0);
        mCountRaw12.set(0);
        mCountCapRes.set(0);
        mCaptureRawIsDng = false;
        mCaptureRawIsStats = false;

        try {
            // Parse the JSON to get the list of capture requests.
            List<CaptureRequest.Builder> inputRequests =
                    ItsSerializer.deserializeRequestList(mCamera, params, ""captureRequests"");

            // Prepare the image readers for reprocess input and reprocess outputs.
            int inputFormat = getReprocessInputFormat(params);
            Size inputSize = ItsUtils.getMaxOutputSize(mCameraCharacteristics, inputFormat);
            JSONArray jsonOutputSpecs = ItsUtils.getOutputSpecs(params);
            prepareImageReadersWithOutputSpecs(jsonOutputSpecs, inputSize, inputFormat,
                    inputRequests.size(), /*backgroundRequest*/false);

            // Prepare a reprocessable session.
            int numOutputSurfaces = mOutputImageReaders.length;
            InputConfiguration inputConfig = new InputConfiguration(inputSize.getWidth(),
                    inputSize.getHeight(), inputFormat);
            List<Surface> outputSurfaces = new ArrayList<Surface>();
            boolean addSurfaceForInput = true;
            for (int i = 0; i < numOutputSurfaces; i++) {
                outputSurfaces.add(mOutputImageReaders[i].getSurface());
                if (mOutputImageReaders[i] == mInputImageReader) {
                    // If input and one of the outputs share the same image reader, avoid
                    // adding the same surfaces twice.
                    addSurfaceForInput = false;
                }
            }

            if (addSurfaceForInput) {
                // Besides the output surfaces specified in JSON object, add an additional one
                // for reprocess input.
                outputSurfaces.add(mInputImageReader.getSurface());
            }

            BlockingSessionCallback sessionListener = new BlockingSessionCallback();
            mCamera.createReprocessableCaptureSession(inputConfig, outputSurfaces, sessionListener,
                    mCameraHandler);
            mSession = sessionListener.waitAndGetSession(TIMEOUT_IDLE_MS);

            // Create an image writer for reprocess input.
            Surface inputSurface = mSession.getInputSurface();
            imageWriter = ImageWriter.newInstance(inputSurface, inputRequests.size());

            // Set up input reader listener and capture callback listener to get
            // reprocess input buffers and the results in order to create reprocess capture
            // requests.
            ImageReaderListenerWaiter inputReaderListener = new ImageReaderListenerWaiter();
            mInputImageReader.setOnImageAvailableListener(inputReaderListener, mSaveHandlers[0]);

            CaptureCallbackWaiter captureCallbackWaiter = new CaptureCallbackWaiter();
            // Prepare the reprocess input request
            for (CaptureRequest.Builder inputReqest : inputRequests) {
                // Remember and clear noise reduction, edge enhancement, and effective exposure
                // factors.
                noiseReductionModes.add(inputReqest.get(CaptureRequest.NOISE_REDUCTION_MODE));
                edgeModes.add(inputReqest.get(CaptureRequest.EDGE_MODE));
                effectiveExposureFactors.add(inputReqest.get(
                        CaptureRequest.REPROCESS_EFFECTIVE_EXPOSURE_FACTOR));

                inputReqest.set(CaptureRequest.NOISE_REDUCTION_MODE,
                        CaptureRequest.NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG);
                inputReqest.set(CaptureRequest.EDGE_MODE, CaptureRequest.EDGE_MODE_ZERO_SHUTTER_LAG);
                inputReqest.set(CaptureRequest.REPROCESS_EFFECTIVE_EXPOSURE_FACTOR, null);
                inputReqest.addTarget(mInputImageReader.getSurface());
                mSession.capture(inputReqest.build(), captureCallbackWaiter, mResultHandler);
            }

            // Wait for reprocess input images
            ArrayList<CaptureRequest.Builder> reprocessOutputRequests = new ArrayList<>();
            for (int i = 0; i < inputRequests.size(); i++) {
                TotalCaptureResult result =
                        captureCallbackWaiter.getResult(TIMEOUT_CALLBACK * 1000);
                reprocessOutputRequests.add(mCamera.createReprocessCaptureRequest(result));
                imageWriter.queueInputImage(inputReaderListener.getImage(TIMEOUT_CALLBACK * 1000));
            }

            // Start performing reprocess captures.

            mCaptureResults = new CaptureResult[inputRequests.size()];

            // Prepare reprocess capture requests.
            for (int i = 0; i < numOutputSurfaces; i++) {
                ImageReader.OnImageAvailableListener outputReaderListener =
                        createAvailableListener(mCaptureCallback);
                mOutputImageReaders[i].setOnImageAvailableListener(outputReaderListener,
                        mSaveHandlers[i]);
            }

            // Plan for how many callbacks need to be received throughout the duration of this
            // sequence of capture requests. There is one callback per image surface, and one
            // callback for the CaptureResult, for each capture.
            int numCaptures = reprocessOutputRequests.size();
            mCountCallbacksRemaining.set(numCaptures * (numOutputSurfaces + 1));

            // Initiate the captures.
            for (int i = 0; i < reprocessOutputRequests.size(); i++) {
                CaptureRequest.Builder req = reprocessOutputRequests.get(i);
                for (ImageReader outputImageReader : mOutputImageReaders) {
                    req.addTarget(outputImageReader.getSurface());
                }

                req.set(CaptureRequest.NOISE_REDUCTION_MODE, noiseReductionModes.get(i));
                req.set(CaptureRequest.EDGE_MODE, edgeModes.get(i));
                req.set(CaptureRequest.REPROCESS_EFFECTIVE_EXPOSURE_FACTOR,
                        effectiveExposureFactors.get(i));

                mSession.capture(req.build(), mCaptureResultListener, mResultHandler);
            }

            // Make sure all callbacks have been hit (wait until captures are done).
            // If no timeouts are received after a timeout, then fail.
            waitForCallbacks(TIMEOUT_CALLBACK * 1000);
        } catch (android.hardware.camera2.CameraAccessException e) {
            throw new ItsException(""Access error: "", e);
        } finally {
            closeImageReaders();
            if (mSession != null) {
                mSession.close();
                mSession = null;
            }
            if (imageWriter != null) {
                imageWriter.close();
            }
        }
    }

    @Override
    public final void onAccuracyChanged(Sensor sensor, int accuracy) {
        Logt.i(TAG, ""Sensor "" + sensor.getName() + "" accuracy changed to "" + accuracy);
    }

    @Override
    public final void onSensorChanged(SensorEvent event) {
        synchronized(mEventLock) {
            if (mEventsEnabled) {
                MySensorEvent ev2 = new MySensorEvent();
                ev2.sensor = event.sensor;
                ev2.accuracy = event.accuracy;
                ev2.timestamp = event.timestamp;
                ev2.values = new float[event.values.length];
                System.arraycopy(event.values, 0, ev2.values, 0, event.values.length);
                mEvents.add(ev2);
            }
        }
    }

    private final CaptureCallback mCaptureCallback = new CaptureCallback() {
        @Override
        public void onCaptureAvailable(Image capture, String physicalCameraId) {
            try {
                int format = capture.getFormat();
                if (format == ImageFormat.JPEG) {
                    Logt.i(TAG, ""Received JPEG capture"");
                    byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                    ByteBuffer buf = ByteBuffer.wrap(img);
                    int count = mCountJpg.getAndIncrement();
                    mSocketRunnableObj.sendResponseCaptureBuffer(""jpegImage""+physicalCameraId, buf);
                } else if (format == ImageFormat.YUV_420_888) {
                    Logt.i(TAG, ""Received YUV capture"");
                    byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                    ByteBuffer buf = ByteBuffer.wrap(img);
                    mSocketRunnableObj.sendResponseCaptureBuffer(
                            ""yuvImage""+physicalCameraId, buf);
                } else if (format == ImageFormat.RAW10) {
                    Logt.i(TAG, ""Received RAW10 capture"");
                    byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                    ByteBuffer buf = ByteBuffer.wrap(img);
                    int count = mCountRaw10.getAndIncrement();
                    mSocketRunnableObj.sendResponseCaptureBuffer(
                            ""raw10Image""+physicalCameraId, buf);
                } else if (format == ImageFormat.RAW12) {
                    Logt.i(TAG, ""Received RAW12 capture"");
                    byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                    ByteBuffer buf = ByteBuffer.wrap(img);
                    int count = mCountRaw12.getAndIncrement();
                    mSocketRunnableObj.sendResponseCaptureBuffer(""raw12Image""+physicalCameraId, buf);
                } else if (format == ImageFormat.RAW_SENSOR) {
                    Logt.i(TAG, ""Received RAW16 capture"");
                    int count = mCountRawOrDng.getAndIncrement();
                    if (! mCaptureRawIsDng) {
                        byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                        if (! mCaptureRawIsStats) {
                            ByteBuffer buf = ByteBuffer.wrap(img);
                            mSocketRunnableObj.sendResponseCaptureBuffer(
                                    ""rawImage"" + physicalCameraId, buf);
                        } else {
                            // Compute the requested stats on the raw frame, and return the results
                            // in a new ""stats image"".
                            long startTimeMs = SystemClock.elapsedRealtime();
                            int w = capture.getWidth();
                            int h = capture.getHeight();
                            int aaw = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .width();
                            int aah = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .height();
                            int aax = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .left;
                            int aay = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .top;

                            if (w == aaw) {
                                aax = 0;
                            }
                            if (h == aah) {
                                aay = 0;
                            }

                            int gw = mCaptureStatsGridWidth;
                            int gh = mCaptureStatsGridHeight;
                            float[] stats = StatsImage.computeStatsImage(
                                                             img, w, h, aax, aay, aaw, aah, gw, gh);
                            long endTimeMs = SystemClock.elapsedRealtime();
                            Log.e(TAG, ""Raw stats computation takes "" + (endTimeMs - startTimeMs) + "" ms"");
                            int statsImgSize = stats.length * 4;
                            if (mSocketQueueQuota != null) {
                                mSocketQueueQuota.release(img.length);
                                mSocketQueueQuota.acquire(statsImgSize);
                            }
                            ByteBuffer bBuf = ByteBuffer.allocate(statsImgSize);
                            bBuf.order(ByteOrder.nativeOrder());
                            FloatBuffer fBuf = bBuf.asFloatBuffer();
                            fBuf.put(stats);
                            fBuf.position(0);
                            mSocketRunnableObj.sendResponseCaptureBuffer(
                                    ""rawStatsImage""+physicalCameraId, bBuf);
                        }
                    } else {
                        // Wait until the corresponding capture result is ready, up to a timeout.
                        long t0 = android.os.SystemClock.elapsedRealtime();
                        while (! mThreadExitFlag
                                && android.os.SystemClock.elapsedRealtime()-t0 < TIMEOUT_CAP_RES) {
                            if (mCaptureResults[count] != null) {
                                Logt.i(TAG, ""Writing capture as DNG"");
                                DngCreator dngCreator = new DngCreator(
                                        mCameraCharacteristics, mCaptureResults[count]);
                                ByteArrayOutputStream dngStream = new ByteArrayOutputStream();
                                dngCreator.writeImage(dngStream, capture);
                                byte[] dngArray = dngStream.toByteArray();
                                if (mSocketQueueQuota != null) {
                                    // Ideally we should acquire before allocating memory, but
                                    // here the DNG size is unknown before toByteArray call, so
                                    // we have to register the size afterward. This should still
                                    // works most of the time since all DNG images are handled by
                                    // the same handler thread, so we are at most one buffer over
                                    // the quota.
                                    mSocketQueueQuota.acquire(dngArray.length);
                                }
                                ByteBuffer dngBuf = ByteBuffer.wrap(dngArray);
                                mSocketRunnableObj.sendResponseCaptureBuffer(""dngImage"", dngBuf);
                                break;
                            } else {
                                Thread.sleep(1);
                            }
                        }
                    }
                } else if (format == ImageFormat.Y8) {
                    Logt.i(TAG, ""Received Y8 capture"");
                    byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                    ByteBuffer buf = ByteBuffer.wrap(img);
                    mSocketRunnableObj.sendResponseCaptureBuffer(
                            ""y8Image""+physicalCameraId, buf);
                } else {
                    throw new ItsException(""Unsupported image format: "" + format);
                }

                synchronized(mCountCallbacksRemaining) {
                    mCountCallbacksRemaining.decrementAndGet();
                    mCountCallbacksRemaining.notify();
                }
            } catch (IOException e) {
                Logt.e(TAG, ""Script error: "", e);
            } catch (InterruptedException e) {
                Logt.e(TAG, ""Script error: "", e);
            } catch (ItsException e) {
                Logt.e(TAG, ""Script error: "", e);
            }
        }
    };

    private static float r2f(Rational r) {
        return (float)r.getNumerator() / (float)r.getDenominator();
    }

    private boolean hasCapability(int capability) throws ItsException {
        int[] capabilities = mCameraCharacteristics.get(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
        if (capabilities == null) {
            throw new ItsException(""Failed to get capabilities"");
        }
        for (int c : capabilities) {
            if (c == capability) {
                return true;
            }
        }
        return false;
    }

    private String buildLogString(CaptureResult result) throws ItsException {
        StringBuilder logMsg = new StringBuilder();
        logMsg.append(String.format(
                ""Capt result: AE=%d, AF=%d, AWB=%d, "",
                result.get(CaptureResult.CONTROL_AE_STATE),
                result.get(CaptureResult.CONTROL_AF_STATE),
                result.get(CaptureResult.CONTROL_AWB_STATE)));

        boolean readSensorSettings = hasCapability(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS);

        if (readSensorSettings) {
            logMsg.append(String.format(
                    ""sens=%d, exp=%.1fms, dur=%.1fms, "",
                    result.get(CaptureResult.SENSOR_SENSITIVITY),
                    result.get(CaptureResult.SENSOR_EXPOSURE_TIME).longValue() / 1000000.0f,
                    result.get(CaptureResult.SENSOR_FRAME_DURATION).longValue() /
                                1000000.0f));
        }
        if (result.get(CaptureResult.COLOR_CORRECTION_GAINS) != null) {
            logMsg.append(String.format(
                    ""gains=[%.1f, %.1f, %.1f, %.1f], "",
                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getRed(),
                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getGreenEven(),
                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getGreenOdd(),
                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getBlue()));
        } else {
            logMsg.append(""gains=[], "");
        }
        if (result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM) != null) {
            logMsg.append(String.format(
                    ""xform=[%.1f, %.1f, %.1f, %.1f, %.1f, %.1f, %.1f, %.1f, %.1f], "",
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(0,0)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(1,0)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(2,0)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(0,1)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(1,1)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(2,1)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(0,2)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(1,2)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(2,2))));
        } else {
            logMsg.append(""xform=[], "");
        }
        logMsg.append(String.format(
                ""foc=%.1f"",
                result.get(CaptureResult.LENS_FOCUS_DISTANCE)));
        return logMsg.toString();
    }

    private class ThreeAResultListener extends CaptureResultListener {
        private volatile boolean stopped = false;
        private boolean aeResultSent = false;
        private boolean awbResultSent = false;
        private boolean afResultSent = false;

        @Override
        public void onCaptureStarted(CameraCaptureSession session, CaptureRequest request,
                long timestamp, long frameNumber) {
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                TotalCaptureResult result) {
            try {
                if (stopped) {
                    return;
                }

                if (request == null || result == null) {
                    throw new ItsException(""Request/result is invalid"");
                }

                Logt.i(TAG, buildLogString(result));

                synchronized(m3AStateLock) {
                    if (result.get(CaptureResult.CONTROL_AE_STATE) != null) {
                        mConvergedAE = result.get(CaptureResult.CONTROL_AE_STATE) ==
                                                  CaptureResult.CONTROL_AE_STATE_CONVERGED ||
                                       result.get(CaptureResult.CONTROL_AE_STATE) ==
                                                  CaptureResult.CONTROL_AE_STATE_FLASH_REQUIRED ||
                                       result.get(CaptureResult.CONTROL_AE_STATE) ==
                                                  CaptureResult.CONTROL_AE_STATE_LOCKED;
                        mLockedAE = result.get(CaptureResult.CONTROL_AE_STATE) ==
                                               CaptureResult.CONTROL_AE_STATE_LOCKED;
                    }
                    if (result.get(CaptureResult.CONTROL_AF_STATE) != null) {
                        mConvergedAF = result.get(CaptureResult.CONTROL_AF_STATE) ==
                                                  CaptureResult.CONTROL_AF_STATE_FOCUSED_LOCKED;
                    }
                    if (result.get(CaptureResult.CONTROL_AWB_STATE) != null) {
                        mConvergedAWB = result.get(CaptureResult.CONTROL_AWB_STATE) ==
                                                   CaptureResult.CONTROL_AWB_STATE_CONVERGED ||
                                        result.get(CaptureResult.CONTROL_AWB_STATE) ==
                                                   CaptureResult.CONTROL_AWB_STATE_LOCKED;
                        mLockedAWB = result.get(CaptureResult.CONTROL_AWB_STATE) ==
                                                CaptureResult.CONTROL_AWB_STATE_LOCKED;
                    }

                    if (mConvergedAE && (!mNeedsLockedAE || mLockedAE) && !aeResultSent) {
                        aeResultSent = true;
                        if (result.get(CaptureResult.SENSOR_SENSITIVITY) != null
                                && result.get(CaptureResult.SENSOR_EXPOSURE_TIME) != null) {
                            mSocketRunnableObj.sendResponse(""aeResult"", String.format(""%d %d"",
                                    result.get(CaptureResult.SENSOR_SENSITIVITY).intValue(),
                                    result.get(CaptureResult.SENSOR_EXPOSURE_TIME).intValue()
                                    ));
                        } else {
                            Logt.i(TAG, String.format(
                                    ""AE converged but NULL exposure values, sensitivity:%b, expTime:%b"",
                                    result.get(CaptureResult.SENSOR_SENSITIVITY) == null,
                                    result.get(CaptureResult.SENSOR_EXPOSURE_TIME) == null));
                        }
                    }

                    if (mConvergedAF && !afResultSent) {
                        afResultSent = true;
                        if (result.get(CaptureResult.LENS_FOCUS_DISTANCE) != null) {
                            mSocketRunnableObj.sendResponse(""afResult"", String.format(""%f"",
                                    result.get(CaptureResult.LENS_FOCUS_DISTANCE)
                                    ));
                        } else {
                            Logt.i(TAG, ""AF converged but NULL focus distance values"");
                        }
                    }

                    if (mConvergedAWB && (!mNeedsLockedAWB || mLockedAWB) && !awbResultSent) {
                        awbResultSent = true;
                        if (result.get(CaptureResult.COLOR_CORRECTION_GAINS) != null
                                && result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM) != null) {
                            mSocketRunnableObj.sendResponse(""awbResult"", String.format(
                                    ""%f %f %f %f %f %f %f %f %f %f %f %f %f"",
                                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getRed(),
                                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getGreenEven(),
                                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getGreenOdd(),
                                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getBlue(),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(0,0)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(1,0)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(2,0)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(0,1)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(1,1)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(2,1)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(0,2)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(1,2)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(2,2))));
                        } else {
                            Logt.i(TAG, String.format(
                                    ""AWB converged but NULL color correction values, gains:%b, ccm:%b"",
                                    result.get(CaptureResult.COLOR_CORRECTION_GAINS) == null,
                                    result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM) == null));
                        }
                    }
                }

                mInterlock3A.open();
            } catch (ItsException e) {
                Logt.e(TAG, ""Script error: "", e);
            } catch (Exception e) {
                Logt.e(TAG, ""Script error: "", e);
            }
        }

        @Override
        public void onCaptureFailed(CameraCaptureSession session, CaptureRequest request,
                CaptureFailure failure) {
            Logt.e(TAG, ""Script error: capture failed"");
        }

        public void stop() {
            stopped = true;
        }
    }

    private final CaptureResultListener mCaptureResultListener = new CaptureResultListener() {
        @Override
        public void onCaptureStarted(CameraCaptureSession session, CaptureRequest request,
                long timestamp, long frameNumber) {
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                TotalCaptureResult result) {
            try {
                if (request == null || result == null) {
                    throw new ItsException(""Request/result is invalid"");
                }

                Logt.i(TAG, buildLogString(result));

                int count = mCountCapRes.getAndIncrement();
                mCaptureResults[count] = result;
                mSocketRunnableObj.sendResponseCaptureResult(mCameraCharacteristics,
                        request, result, mOutputImageReaders);
                synchronized(mCountCallbacksRemaining) {
                    mCountCallbacksRemaining.decrementAndGet();
                    mCountCallbacksRemaining.notify();
                }
            } catch (ItsException e) {
                Logt.e(TAG, ""Script error: "", e);
            } catch (Exception e) {
                Logt.e(TAG, ""Script error: "", e);
            }
        }

        @Override
        public void onCaptureFailed(CameraCaptureSession session, CaptureRequest request,
                CaptureFailure failure) {
            Logt.e(TAG, ""Script error: capture failed"");
        }
    };

    private class CaptureCallbackWaiter extends CameraCaptureSession.CaptureCallback {
        private final LinkedBlockingQueue<TotalCaptureResult> mResultQueue =
                new LinkedBlockingQueue<>();

        @Override
        public void onCaptureStarted(CameraCaptureSession session, CaptureRequest request,
                long timestamp, long frameNumber) {
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                TotalCaptureResult result) {
            try {
                mResultQueue.put(result);
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onImageAvailable"");
            }
        }

        @Override
        public void onCaptureFailed(CameraCaptureSession session, CaptureRequest request,
                CaptureFailure failure) {
            Logt.e(TAG, ""Script error: capture failed"");
        }

        public TotalCaptureResult getResult(long timeoutMs) throws ItsException {
            TotalCaptureResult result;
            try {
                result = mResultQueue.poll(timeoutMs, TimeUnit.MILLISECONDS);
            } catch (InterruptedException e) {
                throw new ItsException(e);
            }

            if (result == null) {
                throw new ItsException(""Getting an image timed out after "" + timeoutMs +
                        ""ms"");
            }

            return result;
        }
    }

    private static class ImageReaderListenerWaiter implements ImageReader.OnImageAvailableListener {
        private final LinkedBlockingQueue<Image> mImageQueue = new LinkedBlockingQueue<>();

        @Override
        public void onImageAvailable(ImageReader reader) {
            try {
                mImageQueue.put(reader.acquireNextImage());
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onImageAvailable"");
            }
        }

        public Image getImage(long timeoutMs) throws ItsException {
            Image image;
            try {
                image = mImageQueue.poll(timeoutMs, TimeUnit.MILLISECONDS);
            } catch (InterruptedException e) {
                throw new ItsException(e);
            }

            if (image == null) {
                throw new ItsException(""Getting an image timed out after "" + timeoutMs +
                        ""ms"");
            }
            return image;
        }
    }

    private int getReprocessInputFormat(JSONObject params) throws ItsException {
        String reprocessFormat;
        try {
            reprocessFormat = params.getString(""reprocessFormat"");
        } catch (org.json.JSONException e) {
            throw new ItsException(""Error parsing reprocess format: "" + e);
        }

        if (reprocessFormat.equals(""yuv"")) {
            return ImageFormat.YUV_420_888;
        } else if (reprocessFormat.equals(""private"")) {
            return ImageFormat.PRIVATE;
        }

        throw new ItsException(""Uknown reprocess format: "" + reprocessFormat);
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.app.notification.legacy29.cts.NotificationAssistantServiceTest"	"testAdjustNotification_smartActionKey"	"CtsLegacyNotification29TestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/notificationlegacy/notificationlegacy29/src/android/app/notification/legacy29/cts/NotificationAssistantServiceTest.java"	""	"public void testAdjustNotification_smartActionKey() throws Exception {
        setUpListeners();

        mUi.adoptShellPermissionIdentity(""android.permission.STATUS_BAR_SERVICE"");
        mNotificationManager.allowAssistantAdjustment(Adjustment.KEY_CONTEXTUAL_ACTIONS);
        mUi.dropShellPermissionIdentity();

        PendingIntent sendIntent = PendingIntent.getActivity(mContext, 0,
                new Intent(Intent.ACTION_SEND), PendingIntent.FLAG_MUTABLE_UNAUDITED);
        Notification.Action sendAction = new Notification.Action.Builder(ICON_ID, ""SEND"",
                sendIntent).build();

        sendNotification(1, ICON_ID);
        StatusBarNotification sbn = getFirstNotificationFromPackage(TestNotificationListener.PKG);
        NotificationListenerService.Ranking out = new NotificationListenerService.Ranking();
        mNotificationListenerService.mRankingMap.getRanking(sbn.getKey(), out);

        List<Notification.Action> smartActions = out.getSmartActions();
        if (smartActions != null) {
            for (int i = 0; i < smartActions.size(); i++) {
                Notification.Action action = smartActions.get(i);
                assertNotEquals(sendIntent, action.actionIntent);
            }
        }

        ArrayList<Notification.Action> extraAction = new ArrayList<>();
        extraAction.add(sendAction);
        Bundle signals = new Bundle();
        signals.putParcelableArrayList(Adjustment.KEY_CONTEXTUAL_ACTIONS, extraAction);
        Adjustment adjustment = new Adjustment(sbn.getPackageName(), sbn.getKey(), signals, """",
                sbn.getUser());

        mNotificationAssistantService.adjustNotification(adjustment);
        Thread.sleep(SLEEP_TIME); //wait for adjustment to be processed

        mNotificationListenerService.mRankingMap.getRanking(sbn.getKey(), out);

        boolean actionFound = false;
        smartActions = out.getSmartActions();
        for (int i = 0; i < smartActions.size(); i++) {
            Notification.Action action = smartActions.get(i);
            actionFound = actionFound || action.actionIntent.equals(sendIntent);
        }
        assertTrue(actionFound);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.helpers.CameraMetadataGetter"	"getCameraIdListNoLazy"	""	"/home/gpoor/cts-12-source/cts/tests/camera/utils/src/android/hardware/camera2/cts/helpers/CameraMetadataGetter.java"	""	"public void test/*
 *.
 */

package android.hardware.camera2.cts.helpers;

import static com.android.ex.camera2.blocking.BlockingStateCallback.*;

import android.graphics.Point;
import android.graphics.Rect;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CameraMetadata;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.TotalCaptureResult;
import android.hardware.camera2.params.BlackLevelPattern;
import android.hardware.camera2.params.ColorSpaceTransform;
import android.hardware.camera2.params.Face;
import android.hardware.camera2.params.LensShadingMap;
import android.hardware.camera2.params.MeteringRectangle;
import android.hardware.camera2.params.RggbChannelVector;
import android.hardware.camera2.params.StreamConfigurationMap;
import android.hardware.camera2.params.TonemapCurve;
import android.location.Location;
import android.os.Handler;
import android.os.HandlerThread;
import android.util.Log;
import android.util.Pair;
import android.util.Rational;
import android.util.Size;
import android.util.SizeF;
import android.util.Range;

import com.android.ex.camera2.blocking.BlockingCameraManager;
import com.android.ex.camera2.blocking.BlockingCameraManager.BlockingOpenException;
import com.android.ex.camera2.blocking.BlockingStateCallback;

import org.json.JSONArray;
import org.json.JSONObject;

import java.lang.reflect.Array;
import java.lang.reflect.Field;
import java.lang.reflect.GenericArrayType;
import java.lang.reflect.Modifier;
import java.lang.reflect.ParameterizedType;
import java.lang.reflect.Type;

/**
 * Utility class to dump the camera metadata.
 */
public final class CameraMetadataGetter implements AutoCloseable {
    private static final String TAG = CameraMetadataGetter.class.getSimpleName();
    private static final int CAMERA_CLOSE_TIMEOUT_MS = 5000;
    public static final int[] TEMPLATE_IDS = {
        CameraDevice.TEMPLATE_PREVIEW,
        CameraDevice.TEMPLATE_STILL_CAPTURE,
        CameraDevice.TEMPLATE_RECORD,
        CameraDevice.TEMPLATE_VIDEO_SNAPSHOT,
        CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG,
        CameraDevice.TEMPLATE_MANUAL,
    };
    private CameraManager mCameraManager;
    private BlockingStateCallback mCameraListener;
    private HandlerThread mHandlerThread;
    private Handler mHandler;

    private static class MetadataEntry {
        public MetadataEntry(String k, Object v) {
            key = k;
            value = v;
        }

        public String key;
        public Object value;
    }

    public CameraMetadataGetter(CameraManager cameraManager) {
        if (cameraManager == null) {
            throw new IllegalArgumentException(""can not create an CameraMetadataGetter object""
                    + "" with null CameraManager"");
        }

        mCameraManager = cameraManager;

        mCameraListener = new BlockingStateCallback();
        mHandlerThread = new HandlerThread(TAG);
        mHandlerThread.start();
        mHandler = new Handler(mHandlerThread.getLooper());
    }

    public String getCameraInfo() {
        StringBuffer cameraInfo = new StringBuffer(""{\""CameraStaticMetadata\"":{"");
        CameraCharacteristics staticMetadata;
        String[] cameraIds;
        try {
            cameraIds = mCameraManager.getCameraIdListNoLazy();
        } catch (CameraAccessException e) {
            Log.e(TAG, ""Unable to get camera ids, skip this info, error: "" + e.getMessage());
            return """";
        }
        for (String id : cameraIds) {
            String value = null;
            try {
                staticMetadata = mCameraManager.getCameraCharacteristics(id);
                value = serialize(staticMetadata).toString();
            } catch (CameraAccessException e) {
                Log.e(TAG,
                        ""Unable to get camera camera static info, skip this camera, error: ""
                                + e.getMessage());
            }
            cameraInfo.append(""\""camera"" + id + ""\"":""); // Key
            cameraInfo.append(value); // Value
            // If not last, print "","" // Separator
            if (!id.equals(cameraIds[cameraIds.length - 1])) {
                cameraInfo.append("","");
            }
        }
        cameraInfo.append(""}}"");

        return cameraInfo.toString();
    }

    public JSONObject getCameraInfo(String cameraId) {
        JSONObject staticMetadata = null;
        try {
            staticMetadata = serialize(mCameraManager.getCameraCharacteristics(cameraId));
        } catch (CameraAccessException e) {
            Log.e(TAG,
                    ""Unable to get camera camera static info, skip this camera, error: ""
                            + e.getMessage());
        }
        return staticMetadata;
    }

    public JSONObject[] getCaptureRequestTemplates(String cameraId) {
        JSONObject[] templates = new JSONObject[TEMPLATE_IDS.length];
        CameraDevice camera = null;
        try {
            camera = (new BlockingCameraManager(mCameraManager)).openCamera(cameraId,
                            mCameraListener, mHandler);
            for (int i = 0; i < TEMPLATE_IDS.length; i++) {
                CaptureRequest.Builder request;
                try {
                    request = camera.createCaptureRequest(TEMPLATE_IDS[i]);
                    templates[i] = serialize(request.build());
                } catch (Exception e) {
                    Log.e(TAG, ""Unable to create template "" + TEMPLATE_IDS[i]
                                    + "" because of error "" + e.getMessage());
                    templates[i] = null;
                }
            }
            return templates;
        } catch (CameraAccessException | BlockingOpenException e) {
            Log.e(TAG, ""Unable to open camera "" + cameraId + "" because of error ""
                            + e.getMessage());
            return new JSONObject[0];
        } finally {
            if (camera != null) {
                camera.close();
            }
        }
    }

    public String getCaptureRequestTemplates() {
        StringBuffer templates = new StringBuffer(""{\""CameraRequestTemplates\"":{"");
        String[] cameraIds;
        try {
            cameraIds = mCameraManager.getCameraIdListNoLazy();
        } catch (CameraAccessException e) {
            Log.e(TAG, ""Unable to get camera ids, skip this info, error: "" + e.getMessage());
            return """";
        }
        CameraDevice camera = null;
        for (String id : cameraIds) {
            try {
                try {
                    camera = (new BlockingCameraManager(mCameraManager)).openCamera(id,
                                    mCameraListener, mHandler);
                } catch (CameraAccessException | BlockingOpenException e) {
                    Log.e(TAG, ""Unable to open camera "" + id + "" because of error ""
                                    + e.getMessage());
                    continue;
                }

                for (int i = 0; i < TEMPLATE_IDS.length; i++) {
                    String value = null;
                    CaptureRequest.Builder request;
                    try {
                        request = camera.createCaptureRequest(TEMPLATE_IDS[i]);
                        value = serialize(request.build()).toString();
                    } catch (Exception e) {
                        Log.e(TAG, ""Unable to create template "" + TEMPLATE_IDS[i]
                                        + "" because of error "" + e.getMessage());
                    }
                    templates.append(""\""Camera"" + id + ""CaptureTemplate"" +
                                    TEMPLATE_IDS[i] + ""\"":"");
                    templates.append(value);
                    if (!id.equals(cameraIds[cameraIds.length - 1]) ||
                                    i < (TEMPLATE_IDS.length - 1)) {
                        templates.append("","");
                    }
                }
            } finally {
                if (camera != null) {
                    camera.close();
                    mCameraListener.waitForState(STATE_CLOSED, CAMERA_CLOSE_TIMEOUT_MS);
                }
            }
        }

        templates.append(""}}"");
        return templates.toString();
    }

    /*
     * Cleanup the resources.
     */
    @Override
    public void close() throws Exception {
        mHandlerThread.quitSafely();
    }

    @Override
    protected void finalize() throws Throwable {
        try {
            close();
        } finally {
            super.finalize();
        }
    }

    @SuppressWarnings(""unchecked"")
    private static Object serializeRational(Rational rat) throws org.json.JSONException {
        JSONObject ratObj = new JSONObject();
        ratObj.put(""numerator"", rat.getNumerator());
        ratObj.put(""denominator"", rat.getDenominator());
        return ratObj;
    }

    @SuppressWarnings(""unchecked"")
    private static Object serializeSize(Size size) throws org.json.JSONException {
        JSONObject sizeObj = new JSONObject();
        sizeObj.put(""width"", size.getWidth());
        sizeObj.put(""height"", size.getHeight());
        return sizeObj;
    }

    @SuppressWarnings(""unchecked"")
    private static Object serializeSizeF(SizeF size) throws org.json.JSONException {
        JSONObject sizeObj = new JSONObject();
        sizeObj.put(""width"", size.getWidth());
        sizeObj.put(""height"", size.getHeight());
        return sizeObj;
    }

    @SuppressWarnings(""unchecked"")
    private static Object serializeRect(Rect rect) throws org.json.JSONException {
        JSONObject rectObj = new JSONObject();
        rectObj.put(""left"", rect.left);
        rectObj.put(""right"", rect.right);
        rectObj.put(""top"", rect.top);
        rectObj.put(""bottom"", rect.bottom);
        return rectObj;
    }

    private static Object serializePoint(Point point) throws org.json.JSONException {
        JSONObject pointObj = new JSONObject();
        pointObj.put(""x"", point.x);
        pointObj.put(""y"", point.y);
        return pointObj;
    }

    @SuppressWarnings(""unchecked"")
    private static Object serializeFace(Face face)
                    throws org.json.JSONException {
        JSONObject faceObj = new JSONObject();
        faceObj.put(""bounds"", serializeRect(face.getBounds()));
        faceObj.put(""score"", face.getScore());
        faceObj.put(""id"", face.getId());
        faceObj.put(""leftEye"", serializePoint(face.getLeftEyePosition()));
        faceObj.put(""rightEye"", serializePoint(face.getRightEyePosition()));
        faceObj.put(""mouth"", serializePoint(face.getMouthPosition()));
        return faceObj;
    }

    @SuppressWarnings(""unchecked"")
    private static Object serializeStreamConfigurationMap(
                    StreamConfigurationMap map)
                    throws org.json.JSONException {
        // TODO: Serialize the rest of the StreamConfigurationMap fields.
        JSONObject mapObj = new JSONObject();
        JSONArray cfgArray = new JSONArray();
        int fmts[] = map.getOutputFormats();
        if (fmts != null) {
            for (int fi = 0; fi < Array.getLength(fmts); fi++) {
                Size sizes[] = map.getOutputSizes(fmts[fi]);
                if (sizes != null) {
                    for (int si = 0; si < Array.getLength(sizes); si++) {
                        JSONObject obj = new JSONObject();
                        obj.put(""format"", fmts[fi]);
                        obj.put(""width"", sizes[si].getWidth());
                        obj.put(""height"", sizes[si].getHeight());
                        obj.put(""input"", false);
                        obj.put(""minFrameDuration"",
                                        map.getOutputMinFrameDuration(fmts[fi], sizes[si]));
                        cfgArray.put(obj);
                    }
                }
            }
        }
        mapObj.put(""availableStreamConfigurations"", cfgArray);
        return mapObj;
    }

    @SuppressWarnings(""unchecked"")
    private static Object serializeMeteringRectangle(MeteringRectangle rect)
                    throws org.json.JSONException {
        JSONObject rectObj = new JSONObject();
        rectObj.put(""x"", rect.getX());
        rectObj.put(""y"", rect.getY());
        rectObj.put(""width"", rect.getWidth());
        rectObj.put(""height"", rect.getHeight());
        rectObj.put(""weight"", rect.getMeteringWeight());
        return rectObj;
    }

    @SuppressWarnings(""unchecked"")
    private static Object serializePair(Pair pair)
                    throws org.json.JSONException {
        JSONArray pairObj = new JSONArray();
        pairObj.put(pair.first);
        pairObj.put(pair.second);
        return pairObj;
    }

    @SuppressWarnings(""unchecked"")
    private static Object serializeRange(Range range)
                    throws org.json.JSONException {
        JSONArray rangeObj = new JSONArray();
        rangeObj.put(range.getLower());
        rangeObj.put(range.getUpper());
        return rangeObj;
    }

    @SuppressWarnings(""unchecked"")
    private static Object serializeColorSpaceTransform(ColorSpaceTransform xform)
                    throws org.json.JSONException {
        JSONArray xformObj = new JSONArray();
        for (int row = 0; row < 3; row++) {
            for (int col = 0; col < 3; col++) {
                xformObj.put(serializeRational(xform.getElement(col, row)));
            }
        }
        return xformObj;
    }

    @SuppressWarnings(""unchecked"")
    private static Object serializeTonemapCurve(TonemapCurve curve)
                    throws org.json.JSONException {
        JSONObject curveObj = new JSONObject();
        String names[] = {
                        ""red"", ""green"", ""blue"" };
        for (int ch = 0; ch < 3; ch++) {
            JSONArray curveArr = new JSONArray();
            int len = curve.getPointCount(ch);
            for (int i = 0; i < len; i++) {
                curveArr.put(curve.getPoint(ch, i).x);
                curveArr.put(curve.getPoint(ch, i).y);
            }
            curveObj.put(names[ch], curveArr);
        }
        return curveObj;
    }

    @SuppressWarnings(""unchecked"")
    private static Object serializeRggbChannelVector(RggbChannelVector vec)
                    throws org.json.JSONException {
        JSONArray vecObj = new JSONArray();
        vecObj.put(vec.getRed());
        vecObj.put(vec.getGreenEven());
        vecObj.put(vec.getGreenOdd());
        vecObj.put(vec.getBlue());
        return vecObj;
    }

    @SuppressWarnings(""unchecked"")
    private static Object serializeBlackLevelPattern(BlackLevelPattern pat)
                    throws org.json.JSONException {
        int patVals[] = new int[4];
        pat.copyTo(patVals, 0);
        JSONArray patObj = new JSONArray();
        patObj.put(patVals[0]);
        patObj.put(patVals[1]);
        patObj.put(patVals[2]);
        patObj.put(patVals[3]);
        return patObj;
    }

    @SuppressWarnings(""unchecked"")
    private static Object serializeLocation(Location loc)
                    throws org.json.JSONException {
        return loc.toString();
    }

    @SuppressWarnings(""unchecked"")
    private static Object serializeLensShadingMap(LensShadingMap map)
            throws org.json.JSONException {
        JSONArray mapObj = new JSONArray();
        for (int row = 0; row < map.getRowCount(); row++) {
            for (int col = 0; col < map.getColumnCount(); col++) {
                for (int ch = 0; ch < 4; ch++) {
                    mapObj.put(map.getGainFactor(ch, col, row));
                }
            }
        }
        return mapObj;
    }

    private static String getKeyName(Object keyObj) {
        if (keyObj.getClass() == CaptureResult.Key.class
                || keyObj.getClass() == TotalCaptureResult.class) {
            return ((CaptureResult.Key) keyObj).getName();
        } else if (keyObj.getClass() == CaptureRequest.Key.class) {
            return ((CaptureRequest.Key) keyObj).getName();
        } else if (keyObj.getClass() == CameraCharacteristics.Key.class) {
            return ((CameraCharacteristics.Key) keyObj).getName();
        }

        throw new IllegalArgumentException(""Invalid key object"");
    }

    private static Object getKeyValue(CameraMetadata md, Object keyObj) {
        if (md.getClass() == CaptureResult.class || md.getClass() == TotalCaptureResult.class) {
            return ((CaptureResult) md).get((CaptureResult.Key) keyObj);
        } else if (md.getClass() == CaptureRequest.class) {
            return ((CaptureRequest) md).get((CaptureRequest.Key) keyObj);
        } else if (md.getClass() == CameraCharacteristics.class) {
            return ((CameraCharacteristics) md).get((CameraCharacteristics.Key) keyObj);
        }

        throw new IllegalArgumentException(""Invalid key object"");
    }

    @SuppressWarnings(""unchecked"")
    private static MetadataEntry serializeEntry(Type keyType, Object keyObj, CameraMetadata md) {
        String keyName = getKeyName(keyObj);

        try {
            Object keyValue = getKeyValue(md, keyObj);
            if (keyValue == null) {
                return new MetadataEntry(keyName, JSONObject.NULL);
            } else if (keyType == Float.class) {
                // The JSON serializer doesn't handle floating point NaN or Inf.
                if (((Float) keyValue).isInfinite() || ((Float) keyValue).isNaN()) {
                    Log.w(TAG, ""Inf/NaN floating point value serialized: "" + keyName);
                    return null;
                }
                return new MetadataEntry(keyName, keyValue);
            } else if (keyType == Integer.class || keyType == Long.class || keyType == Byte.class ||
                    keyType == Boolean.class || keyType == String.class) {
                return new MetadataEntry(keyName, keyValue);
            } else if (keyType == Rational.class) {
                return new MetadataEntry(keyName, serializeRational((Rational) keyValue));
            } else if (keyType == Size.class) {
                return new MetadataEntry(keyName, serializeSize((Size) keyValue));
            } else if (keyType == SizeF.class) {
                return new MetadataEntry(keyName, serializeSizeF((SizeF) keyValue));
            } else if (keyType == Rect.class) {
                return new MetadataEntry(keyName, serializeRect((Rect) keyValue));
            } else if (keyType == Face.class) {
                return new MetadataEntry(keyName, serializeFace((Face) keyValue));
            } else if (keyType == StreamConfigurationMap.class) {
                return new MetadataEntry(keyName,
                        serializeStreamConfigurationMap((StreamConfigurationMap) keyValue));
            } else if (keyType instanceof ParameterizedType &&
                    ((ParameterizedType) keyType).getRawType() == Range.class) {
                return new MetadataEntry(keyName, serializeRange((Range) keyValue));
            } else if (keyType == ColorSpaceTransform.class) {
                return new MetadataEntry(keyName,
                        serializeColorSpaceTransform((ColorSpaceTransform) keyValue));
            } else if (keyType == MeteringRectangle.class) {
                return new MetadataEntry(keyName,
                        serializeMeteringRectangle((MeteringRectangle) keyValue));
            } else if (keyType == Location.class) {
                return new MetadataEntry(keyName,
                        serializeLocation((Location) keyValue));
            } else if (keyType == RggbChannelVector.class) {
                return new MetadataEntry(keyName,
                        serializeRggbChannelVector((RggbChannelVector) keyValue));
            } else if (keyType == BlackLevelPattern.class) {
                return new MetadataEntry(keyName,
                        serializeBlackLevelPattern((BlackLevelPattern) keyValue));
            } else if (keyType == TonemapCurve.class) {
                return new MetadataEntry(keyName,
                        serializeTonemapCurve((TonemapCurve) keyValue));
            } else if (keyType == Point.class) {
                return new MetadataEntry(keyName,
                        serializePoint((Point) keyValue));
            } else if (keyType == LensShadingMap.class) {
                return new MetadataEntry(keyName,
                        serializeLensShadingMap((LensShadingMap) keyValue));
            } else {
                Log.w(TAG, String.format(""Serializing unsupported key type: "" + keyType));
                return null;
            }
        } catch (org.json.JSONException e) {
            throw new IllegalStateException(""JSON error for key: "" + keyName + "": "", e);
        }
    }

    @SuppressWarnings(""unchecked"")
    private static MetadataEntry serializeArrayEntry(Type keyType, Object keyObj,
            CameraMetadata md) {
        String keyName = getKeyName(keyObj);
        try {
            Object keyValue = getKeyValue(md, keyObj);
            if (keyValue == null) {
                return new MetadataEntry(keyName, JSONObject.NULL);
            }
            int arrayLen = Array.getLength(keyValue);
            Type elmtType = ((GenericArrayType) keyType).getGenericComponentType();
            if (elmtType == int.class || elmtType == float.class || elmtType == byte.class ||
                    elmtType == long.class || elmtType == double.class
                    || elmtType == boolean.class) {
                return new MetadataEntry(keyName, new JSONArray(keyValue));
            } else if (elmtType == Rational.class) {
                JSONArray jsonArray = new JSONArray();
                for (int i = 0; i < arrayLen; i++) {
                    jsonArray.put(serializeRational((Rational) Array.get(keyValue, i)));
                }
                return new MetadataEntry(keyName, jsonArray);
            } else if (elmtType == Size.class) {
                JSONArray jsonArray = new JSONArray();
                for (int i = 0; i < arrayLen; i++) {
                    jsonArray.put(serializeSize((Size) Array.get(keyValue, i)));
                }
                return new MetadataEntry(keyName, jsonArray);
            } else if (elmtType == Rect.class) {
                JSONArray jsonArray = new JSONArray();
                for (int i = 0; i < arrayLen; i++) {
                    jsonArray.put(serializeRect((Rect) Array.get(keyValue, i)));
                }
                return new MetadataEntry(keyName, jsonArray);
            } else if (elmtType == Face.class) {
                JSONArray jsonArray = new JSONArray();
                for (int i = 0; i < arrayLen; i++) {
                    jsonArray.put(serializeFace((Face) Array.get(keyValue, i)));
                }
                return new MetadataEntry(keyName, jsonArray);
            } else if (elmtType == StreamConfigurationMap.class) {
                JSONArray jsonArray = new JSONArray();
                for (int i = 0; i < arrayLen; i++) {
                    jsonArray.put(serializeStreamConfigurationMap(
                            (StreamConfigurationMap) Array.get(keyValue, i)));
                }
                return new MetadataEntry(keyName, jsonArray);
            } else if (elmtType instanceof ParameterizedType &&
                    ((ParameterizedType) elmtType).getRawType() == Range.class) {
                JSONArray jsonArray = new JSONArray();
                for (int i = 0; i < arrayLen; i++) {
                    jsonArray.put(serializeRange((Range) Array.get(keyValue, i)));
                }
                return new MetadataEntry(keyName, jsonArray);
            } else if (elmtType instanceof ParameterizedType &&
                    ((ParameterizedType) elmtType).getRawType() == Pair.class) {
                JSONArray jsonArray = new JSONArray();
                for (int i = 0; i < arrayLen; i++) {
                    jsonArray.put(serializePair((Pair) Array.get(keyValue, i)));
                }
                return new MetadataEntry(keyName, jsonArray);
            } else if (elmtType == MeteringRectangle.class) {
                JSONArray jsonArray = new JSONArray();
                for (int i = 0; i < arrayLen; i++) {
                    jsonArray.put(serializeMeteringRectangle(
                            (MeteringRectangle) Array.get(keyValue, i)));
                }
                return new MetadataEntry(keyName, jsonArray);
            } else if (elmtType == Location.class) {
                JSONArray jsonArray = new JSONArray();
                for (int i = 0; i < arrayLen; i++) {
                    jsonArray.put(serializeLocation((Location) Array.get(keyValue, i)));
                }
                return new MetadataEntry(keyName, jsonArray);
            } else if (elmtType == RggbChannelVector.class) {
                JSONArray jsonArray = new JSONArray();
                for (int i = 0; i < arrayLen; i++) {
                    jsonArray.put(serializeRggbChannelVector(
                            (RggbChannelVector) Array.get(keyValue, i)));
                }
                return new MetadataEntry(keyName, jsonArray);
            } else if (elmtType == BlackLevelPattern.class) {
                JSONArray jsonArray = new JSONArray();
                for (int i = 0; i < arrayLen; i++) {
                    jsonArray.put(serializeBlackLevelPattern(
                            (BlackLevelPattern) Array.get(keyValue, i)));
                }
                return new MetadataEntry(keyName, jsonArray);
            } else if (elmtType == Point.class) {
                JSONArray jsonArray = new JSONArray();
                for (int i = 0; i < arrayLen; i++) {
                    jsonArray.put(serializePoint((Point) Array.get(keyValue, i)));
                }
                return new MetadataEntry(keyName, jsonArray);
            } else {
                Log.w(TAG, String.format(""Serializing unsupported array type: "" + elmtType));
                return null;
            }
        } catch (org.json.JSONException e) {
            throw new IllegalStateException(""JSON error for key: "" + keyName + "": "", e);
        }
    }

    @SuppressWarnings(""unchecked"")
    private static JSONObject serialize(CameraMetadata md) {
        JSONObject jsonObj = new JSONObject();
        Field[] allFields = md.getClass().getDeclaredFields();
        if (md.getClass() == TotalCaptureResult.class) {
            allFields = CaptureResult.class.getDeclaredFields();
        }
        for (Field field : allFields) {
            if (Modifier.isPublic(field.getModifiers()) &&
                    Modifier.isStatic(field.getModifiers()) &&
                            (field.getType() == CaptureRequest.Key.class
                            || field.getType() == CaptureResult.Key.class
                            || field.getType() == TotalCaptureResult.Key.class
                            || field.getType() == CameraCharacteristics.Key.class)
                    &&
                    field.getGenericType() instanceof ParameterizedType) {
                ParameterizedType paramType = (ParameterizedType) field.getGenericType();
                Type[] argTypes = paramType.getActualTypeArguments();
                if (argTypes.length > 0) {
                    try {
                        Type keyType = argTypes[0];
                        Object keyObj = field.get(md);
                        MetadataEntry entry;
                        if (keyType instanceof GenericArrayType) {
                            entry = serializeArrayEntry(keyType, keyObj, md);
                        } else {
                            entry = serializeEntry(keyType, keyObj, md);
                        }

                        // TODO: Figure this weird case out.
                        // There is a weird case where the entry is non-null but
                        // the toString
                        // of the entry is null, and if this happens, the
                        // null-ness spreads like
                        // a virus and makes the whole JSON object null from the
                        // top level down.
                        // Not sure if it's a bug in the library or I'm just not
                        // using it right.
                        // Workaround by checking for this case explicitly and
                        // not adding the
                        // value to the jsonObj when it is detected.
                        if (entry != null && entry.key != null && entry.value != null
                                && entry.value.toString() == null) {
                            Log.w(TAG, ""Error encountered serializing value for key: ""
                                    + entry.key);
                        } else if (entry != null) {
                            jsonObj.put(entry.key, entry.value);
                        } else {
                            // Ignore.
                        }
                    } catch (IllegalAccessException e) {
                        throw new IllegalStateException(
                                ""Access error for field: "" + field + "": "", e);
                    } catch (org.json.JSONException e) {
                        throw new IllegalStateException(
                                ""JSON error for field: "" + field + "": "", e);
                    }
                }
            }
        }
        return jsonObj;
    }
}"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediaprovidertranscode.cts.TranscodeTest"	"testExtraAcceptOriginalFormatTrue_ContentResolver"	""	"/home/gpoor/cts-12-source/cts/tests/MediaProviderTranscode/src/android/mediaprovidertranscode/cts/TranscodeTest.java"	""	"public void testExtraAcceptOriginalFormatTrue_ContentResolver() throws Exception {
        File modernFile = new File(DIR_CAMERA, HEVC_FILE_NAME);
        try {
            Uri uri = TranscodeTestUtils.stageHEVCVideoFile(modernFile);

            ParcelFileDescriptor pfdOriginal1 = open(uri, false, null /* bundle */);

            TranscodeTestUtils.enableTranscodingForPackage(getContext().getPackageName());

            Bundle bundle = new Bundle();
            bundle.putBoolean(MediaStore.EXTRA_ACCEPT_ORIGINAL_MEDIA_FORMAT, true);
            ParcelFileDescriptor pfdOriginal2 = open(uri, false, bundle);

            assertFileContent(modernFile, modernFile, pfdOriginal1, pfdOriginal2, true);
        } finally {
            modernFile.delete();
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediaprovidertranscode.cts.TranscodeTest"	"testExtraAcceptOriginalFormatFalse_ContentResolver"	""	"/home/gpoor/cts-12-source/cts/tests/MediaProviderTranscode/src/android/mediaprovidertranscode/cts/TranscodeTest.java"	""	"public void testExtraAcceptOriginalFormatFalse_ContentResolver() throws Exception {
        File modernFile = new File(DIR_CAMERA, HEVC_FILE_NAME);
        try {
            Uri uri = TranscodeTestUtils.stageHEVCVideoFile(modernFile);

            ParcelFileDescriptor pfdOriginal = open(uri, false, null /* bundle */);

            TranscodeTestUtils.enableTranscodingForPackage(getContext().getPackageName());

            Bundle bundle = new Bundle();
            bundle.putBoolean(MediaStore.EXTRA_ACCEPT_ORIGINAL_MEDIA_FORMAT, false);
            ParcelFileDescriptor pfdTranscoded = open(uri, false, bundle);

            assertFileContent(modernFile, modernFile, pfdOriginal, pfdTranscoded, false);
        } finally {
            modernFile.delete();
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediaprovidertranscode.cts.TranscodeTest"	"testExtraAcceptOriginalTrueAndMediaCapabilitiesHevcFalse_ContentResolver"	""	"/home/gpoor/cts-12-source/cts/tests/MediaProviderTranscode/src/android/mediaprovidertranscode/cts/TranscodeTest.java"	""	"public void testExtraAcceptOriginalTrueAndMediaCapabilitiesHevcFalse_ContentResolver()
            throws Exception {
        File modernFile = new File(DIR_CAMERA, HEVC_FILE_NAME);
        try {
            Uri uri = TranscodeTestUtils.stageHEVCVideoFile(modernFile);

            ParcelFileDescriptor pfdOriginal1 = open(uri, false, null /* bundle */);

            TranscodeTestUtils.enableTranscodingForPackage(getContext().getPackageName());

            Bundle bundle = new Bundle();
            ApplicationMediaCapabilities capabilities =
                    new ApplicationMediaCapabilities.Builder().build();
            bundle.putParcelable(MediaStore.EXTRA_MEDIA_CAPABILITIES, capabilities);
            bundle.putBoolean(MediaStore.EXTRA_ACCEPT_ORIGINAL_MEDIA_FORMAT, true);
            ParcelFileDescriptor pfdOriginal2 = open(uri, false, bundle);

            assertFileContent(modernFile, modernFile, pfdOriginal1, pfdOriginal2, true);
        } finally {
            modernFile.delete();
        }
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.mediadrm.cts.NativeMediaDrmClearkeyTest"	"isWatchDevice"	"CtsMediaDrmTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/mediadrm/src/android/mediadrm/cts/NativeMediaDrmClearkeyTest.java"	""	"public void test/*
 *.
 */
package android.mediadrm.cts;

import android.content.pm.PackageManager;
import android.media.MediaDrm;
import android.media.cts.ConnectionStatus;
import android.media.cts.IConnectionStatus;
import android.media.cts.MediaCodecBlockModelHelper;
import android.net.Uri;
import android.platform.test.annotations.AppModeFull;
import android.util.Log;
import android.view.Surface;

import com.android.compatibility.common.util.ApiLevelUtil;
import com.android.compatibility.common.util.MediaUtils;
import com.google.android.collect.Lists;

import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.UUID;

import static org.junit.Assert.assertThat;
import static org.junit.matchers.JUnitMatchers.containsString;

/**
 * Tests MediaDrm NDK APIs. ClearKey system uses a subset of NDK APIs,
 * this test only tests the APIs that are supported by ClearKey system.
 */
@AppModeFull(reason = ""TODO: evaluate and port to instant"")
public class NativeMediaDrmClearkeyTest extends MediaPlayerTestBase {
    private static final String TAG = NativeMediaDrmClearkeyTest.class.getSimpleName();

    private static final int CONNECTION_RETRIES = 10;
    private static final int VIDEO_WIDTH_CENC = 1280;
    private static final int VIDEO_HEIGHT_CENC = 720;
    private static final String ISO_BMFF_VIDEO_MIME_TYPE = ""video/avc"";
    private static final String ISO_BMFF_AUDIO_MIME_TYPE = ""audio/avc"";
    private static final String CENC_AUDIO_PATH =
            ""/clear/h264/llama/llama_aac_audio.mp4"";

    private static final String CENC_CLEARKEY_VIDEO_PATH =
            ""/clearkey/llama_h264_main_720p_8000.mp4"";

    private static final int UUID_BYTE_SIZE = 16;
    private static final UUID COMMON_PSSH_SCHEME_UUID =
            new UUID(0x1077efecc0b24d02L, 0xace33c1e52e2fb4bL);
    private static final UUID CLEARKEY_SCHEME_UUID =
            new UUID(0xe2719d58a985b3c9L, 0x781ab030af78d30eL);
    private static final UUID BAD_SCHEME_UUID =
            new UUID(0xffffffffffffffffL, 0xffffffffffffffffL);

    static {
        try {
            System.loadLibrary(""mediadrm_jni"");
        } catch (UnsatisfiedLinkError e) {
            Log.e(TAG, ""NativeMediaDrmClearkeyTest: Error loading JNI library"");
            e.printStackTrace();
        }
        try {
            System.loadLibrary(""mediandk"");
        } catch (UnsatisfiedLinkError e) {
            Log.e(TAG, ""NativeMediaDrmClearkeyTest: Error loading JNI library"");
            e.printStackTrace();
        }
    }

    public static class PlaybackParams {
        public Surface surface;
        public String mimeType;
        public String audioUrl;
        public String videoUrl;
    }

    protected void setUp() throws Exception {
        super.setUp();
        if (false == deviceHasMediaDrm()) {
            tearDown();
        }
    }

    protected void tearDown() throws Exception {
        super.tearDown();
    }

    private boolean watchHasNoClearkeySupport() {
        if (!MediaDrm.isCryptoSchemeSupported(CLEARKEY_SCHEME_UUID)) {
            if (isWatchDevice()) {
                return true;
            } else {
                throw new Error(""Crypto scheme is not supported"");
            }
        }
        return false;
    }

    private boolean isWatchDevice() {
        return mContext.getPackageManager().hasSystemFeature(PackageManager.FEATURE_WATCH);
    }

    private boolean deviceHasMediaDrm() {
        // ClearKey is introduced after KitKat.
        if (ApiLevelUtil.isAtMost(android.os.Build.VERSION_CODES.KITKAT)) {
            return false;
        }
        return true;
    }

    private static final byte[] uuidByteArray(UUID uuid) {
        ByteBuffer buffer = ByteBuffer.wrap(new byte[UUID_BYTE_SIZE]);
        buffer.putLong(uuid.getMostSignificantBits());
        buffer.putLong(uuid.getLeastSignificantBits());
        return buffer.array();
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
"2.2.7.1  . Media"	"5.3"	"H-1-2"	"5.3/H-1-2"	"05030000.720102"	"""[5.3/H-1-2] MUST NOT drop more than 1 frame in 10 seconds during a video resolution change in a 30 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128Kbps AAC audio playback.  | [5.3/H-1-2] MUST NOT drop more than 2 frames in 10 seconds during a video resolution change in a 60 fps video session under load. Load is defined as a concurrent 1080p to 720p video-only transcoding session using hardware video codecs, as well as a 128 kbps AAC audio playback. """	""	""	"AAC"	""	""	""	""	""	""	""	""	"android.renderscript.cts.generated.TestFabs"	"toString"	"CtsRenderscriptTestCases"	"/home/gpoor/cts-12-source/cts/tests/tests/renderscript/src/android/renderscript/cts/generated/TestFabs.java"	""	"public void test/*
 *.
 */

// Don't edit this file!  It is auto-generated by frameworks/rs/api/generate.sh.

package android.renderscript.cts;

import android.renderscript.Allocation;
import android.renderscript.RSRuntimeException;
import android.renderscript.Element;
import android.renderscript.cts.Target;

import java.util.Arrays;

public class TestFabs extends RSBaseCompute {

    private ScriptC_TestFabs script;
    private ScriptC_TestFabsRelaxed scriptRelaxed;

    @Override
    protected void setUp() throws Exception {
        super.setUp();
        script = new ScriptC_TestFabs(mRS);
        scriptRelaxed = new ScriptC_TestFabsRelaxed(mRS);
    }

    @Override
    protected void tearDown() throws Exception {
        script.destroy();
        scriptRelaxed.destroy();
        super.tearDown();
    }

    public class ArgumentsFloatFloat {
        public float inV;
        public Target.Floaty out;
    }

    private void checkFabsFloatFloat() {
        Allocation inV = createRandomAllocation(mRS, Element.DataType.FLOAT_32, 1, 0xd84cf8f27f929ae9l, false);
        try {
            Allocation out = Allocation.createSized(mRS, getElement(mRS, Element.DataType.FLOAT_32, 1), INPUTSIZE);
            script.forEach_testFabsFloatFloat(inV, out);
            verifyResultsFabsFloatFloat(inV, out, false);
            out.destroy();
        } catch (Exception e) {
            throw new RSRuntimeException(""RenderScript. Can't invoke forEach_testFabsFloatFloat: "" + e.toString());
        }
        try {
            Allocation out = Allocation.createSized(mRS, getElement(mRS, Element.DataType.FLOAT_32, 1), INPUTSIZE);
            scriptRelaxed.forEach_testFabsFloatFloat(inV, out);
            verifyResultsFabsFloatFloat(inV, out, true);
            out.destroy();
        } catch (Exception e) {
            throw new RSRuntimeException(""RenderScript. Can't invoke forEach_testFabsFloatFloat: "" + e.toString());
        }
        inV.destroy();
    }

    private void verifyResultsFabsFloatFloat(Allocation inV, Allocation out, boolean relaxed) {
        float[] arrayInV = new float[INPUTSIZE * 1];
        Arrays.fill(arrayInV, (float) 42);
        inV.copyTo(arrayInV);
        float[] arrayOut = new float[INPUTSIZE * 1];
        Arrays.fill(arrayOut, (float) 42);
        out.copyTo(arrayOut);
        StringBuilder message = new StringBuilder();
        boolean errorFound = false;
        for (int i = 0; i < INPUTSIZE; i++) {
            for (int j = 0; j < 1 ; j++) {
                // Extract the inputs.
                ArgumentsFloatFloat args = new ArgumentsFloatFloat();
                args.inV = arrayInV[i];
                // Figure out what the outputs should have been.
                Target target = new Target(Target.FunctionType.NORMAL, Target.ReturnType.FLOAT, relaxed);
                CoreMathVerifier.computeFabs(args, target);
                // Validate the outputs.
                boolean valid = true;
                if (!args.out.couldBe(arrayOut[i * 1 + j])) {
                    valid = false;
                }
                if (!valid) {
                    if (!errorFound) {
                        errorFound = true;
                        message.append(""Input inV: "");
                        appendVariableToMessage(message, args.inV);
                        message.append(""\n"");
                        message.append(""Expected output out: "");
                        appendVariableToMessage(message, args.out);
                        message.append(""\n"");
                        message.append(""Actual   output out: "");
                        appendVariableToMessage(message, arrayOut[i * 1 + j]);
                        if (!args.out.couldBe(arrayOut[i * 1 + j])) {
                            message.append("" FAIL"");
                        }
                        message.append(""\n"");
                        message.append(""Errors at"");
                    }
                    message.append("" ["");
                    message.append(Integer.toString(i));
                    message.append("", "");
                    message.append(Integer.toString(j));
                    message.append(""]"");
                }
            }
        }
        assertFalse(""Incorrect output for checkFabsFloatFloat"" +
                (relaxed ? ""_relaxed"" : """") + "":\n"" + message.toString(), errorFound);
    }

    private void checkFabsFloat2Float2() {
        Allocation inV = createRandomAllocation(mRS, Element.DataType.FLOAT_32, 2, 0x43ccc4c22e5b1dc5l, false);
        try {
            Allocation out = Allocation.createSized(mRS, getElement(mRS, Element.DataType.FLOAT_32, 2), INPUTSIZE);
            script.forEach_testFabsFloat2Float2(inV, out);
            verifyResultsFabsFloat2Float2(inV, out, false);
            out.destroy();
        } catch (Exception e) {
            throw new RSRuntimeException(""RenderScript. Can't invoke forEach_testFabsFloat2Float2: "" + e.toString());
        }
        try {
            Allocation out = Allocation.createSized(mRS, getElement(mRS, Element.DataType.FLOAT_32, 2), INPUTSIZE);
            scriptRelaxed.forEach_testFabsFloat2Float2(inV, out);
            verifyResultsFabsFloat2Float2(inV, out, true);
            out.destroy();
        } catch (Exception e) {
            throw new RSRuntimeException(""RenderScript. Can't invoke forEach_testFabsFloat2Float2: "" + e.toString());
        }
        inV.destroy();
    }

    private void verifyResultsFabsFloat2Float2(Allocation inV, Allocation out, boolean relaxed) {
        float[] arrayInV = new float[INPUTSIZE * 2];
        Arrays.fill(arrayInV, (float) 42);
        inV.copyTo(arrayInV);
        float[] arrayOut = new float[INPUTSIZE * 2];
        Arrays.fill(arrayOut, (float) 42);
        out.copyTo(arrayOut);
        StringBuilder message = new StringBuilder();
        boolean errorFound = false;
        for (int i = 0; i < INPUTSIZE; i++) {
            for (int j = 0; j < 2 ; j++) {
                // Extract the inputs.
                ArgumentsFloatFloat args = new ArgumentsFloatFloat();
                args.inV = arrayInV[i * 2 + j];
                // Figure out what the outputs should have been.
                Target target = new Target(Target.FunctionType.NORMAL, Target.ReturnType.FLOAT, relaxed);
                CoreMathVerifier.computeFabs(args, target);
                // Validate the outputs.
                boolean valid = true;
                if (!args.out.couldBe(arrayOut[i * 2 + j])) {
                    valid = false;
                }
                if (!valid) {
                    if (!errorFound) {
                        errorFound = true;
                        message.append(""Input inV: "");
                        appendVariableToMessage(message, args.inV);
                        message.append(""\n"");
                        message.append(""Expected output out: "");
                        appendVariableToMessage(message, args.out);
                        message.append(""\n"");
                        message.append(""Actual   output out: "");
                        appendVariableToMessage(message, arrayOut[i * 2 + j]);
                        if (!args.out.couldBe(arrayOut[i * 2 + j])) {
                            message.append("" FAIL"");
                        }
                        message.append(""\n"");
                        message.append(""Errors at"");
                    }
                    message.append("" ["");
                    message.append(Integer.toString(i));
                    message.append("", "");
                    message.append(Integer.toString(j));
                    message.append(""]"");
                }
            }
        }
        assertFalse(""Incorrect output for checkFabsFloat2Float2"" +
                (relaxed ? ""_relaxed"" : """") + "":\n"" + message.toString(), errorFound);
    }

    private void checkFabsFloat3Float3() {
        Allocation inV = createRandomAllocation(mRS, Element.DataType.FLOAT_32, 3, 0x43ce8ddd24763ea3l, false);
        try {
            Allocation out = Allocation.createSized(mRS, getElement(mRS, Element.DataType.FLOAT_32, 3), INPUTSIZE);
            script.forEach_testFabsFloat3Float3(inV, out);
            verifyResultsFabsFloat3Float3(inV, out, false);
            out.destroy();
        } catch (Exception e) {
            throw new RSRuntimeException(""RenderScript. Can't invoke forEach_testFabsFloat3Float3: "" + e.toString());
        }
        try {
            Allocation out = Allocation.createSized(mRS, getElement(mRS, Element.DataType.FLOAT_32, 3), INPUTSIZE);
            scriptRelaxed.forEach_testFabsFloat3Float3(inV, out);
            verifyResultsFabsFloat3Float3(inV, out, true);
            out.destroy();
        } catch (Exception e) {
            throw new RSRuntimeException(""RenderScript. Can't invoke forEach_testFabsFloat3Float3: "" + e.toString());
        }
        inV.destroy();
    }

    private void verifyResultsFabsFloat3Float3(Allocation inV, Allocation out, boolean relaxed) {
        float[] arrayInV = new float[INPUTSIZE * 4];
        Arrays.fill(arrayInV, (float) 42);
        inV.copyTo(arrayInV);
        float[] arrayOut = new float[INPUTSIZE * 4];
        Arrays.fill(arrayOut, (float) 42);
        out.copyTo(arrayOut);
        StringBuilder message = new StringBuilder();
        boolean errorFound = false;
        for (int i = 0; i < INPUTSIZE; i++) {
            for (int j = 0; j < 3 ; j++) {
                // Extract the inputs.
                ArgumentsFloatFloat args = new ArgumentsFloatFloat();
                args.inV = arrayInV[i * 4 + j];
                // Figure out what the outputs should have been.
                Target target = new Target(Target.FunctionType.NORMAL, Target.ReturnType.FLOAT, relaxed);
                CoreMathVerifier.computeFabs(args, target);
                // Validate the outputs.
                boolean valid = true;
                if (!args.out.couldBe(arrayOut[i * 4 + j])) {
                    valid = false;
                }
                if (!valid) {
                    if (!errorFound) {
                        errorFound = true;
                        message.append(""Input inV: "");
                        appendVariableToMessage(message, args.inV);
                        message.append(""\n"");
                        message.append(""Expected output out: "");
                        appendVariableToMessage(message, args.out);
                        message.append(""\n"");
                        message.append(""Actual   output out: "");
                        appendVariableToMessage(message, arrayOut[i * 4 + j]);
                        if (!args.out.couldBe(arrayOut[i * 4 + j])) {
                            message.append("" FAIL"");
                        }
                        message.append(""\n"");
                        message.append(""Errors at"");
                    }
                    message.append("" ["");
                    message.append(Integer.toString(i));
                    message.append("", "");
                    message.append(Integer.toString(j));
                    message.append(""]"");
                }
            }
        }
        assertFalse(""Incorrect output for checkFabsFloat3Float3"" +
                (relaxed ? ""_relaxed"" : """") + "":\n"" + message.toString(), errorFound);
    }

    private void checkFabsFloat4Float4() {
        Allocation inV = createRandomAllocation(mRS, Element.DataType.FLOAT_32, 4, 0x43d056f81a915f81l, false);
        try {
            Allocation out = Allocation.createSized(mRS, getElement(mRS, Element.DataType.FLOAT_32, 4), INPUTSIZE);
            script.forEach_testFabsFloat4Float4(inV, out);
            verifyResultsFabsFloat4Float4(inV, out, false);
            out.destroy();
        } catch (Exception e) {
            throw new RSRuntimeException(""RenderScript. Can't invoke forEach_testFabsFloat4Float4: "" + e.toString());
        }
        try {
            Allocation out = Allocation.createSized(mRS, getElement(mRS, Element.DataType.FLOAT_32, 4), INPUTSIZE);
            scriptRelaxed.forEach_testFabsFloat4Float4(inV, out);
            verifyResultsFabsFloat4Float4(inV, out, true);
            out.destroy();
        } catch (Exception e) {
            throw new RSRuntimeException(""RenderScript. Can't invoke forEach_testFabsFloat4Float4: "" + e.toString());
        }
        inV.destroy();
    }

    private void verifyResultsFabsFloat4Float4(Allocation inV, Allocation out, boolean relaxed) {
        float[] arrayInV = new float[INPUTSIZE * 4];
        Arrays.fill(arrayInV, (float) 42);
        inV.copyTo(arrayInV);
        float[] arrayOut = new float[INPUTSIZE * 4];
        Arrays.fill(arrayOut, (float) 42);
        out.copyTo(arrayOut);
        StringBuilder message = new StringBuilder();
        boolean errorFound = false;
        for (int i = 0; i < INPUTSIZE; i++) {
            for (int j = 0; j < 4 ; j++) {
                // Extract the inputs.
                ArgumentsFloatFloat args = new ArgumentsFloatFloat();
                args.inV = arrayInV[i * 4 + j];
                // Figure out what the outputs should have been.
                Target target = new Target(Target.FunctionType.NORMAL, Target.ReturnType.FLOAT, relaxed);
                CoreMathVerifier.computeFabs(args, target);
                // Validate the outputs.
                boolean valid = true;
                if (!args.out.couldBe(arrayOut[i * 4 + j])) {
                    valid = false;
                }
                if (!valid) {
                    if (!errorFound) {
                        errorFound = true;
                        message.append(""Input inV: "");
                        appendVariableToMessage(message, args.inV);
                        message.append(""\n"");
                        message.append(""Expected output out: "");
                        appendVariableToMessage(message, args.out);
                        message.append(""\n"");
                        message.append(""Actual   output out: "");
                        appendVariableToMessage(message, arrayOut[i * 4 + j]);
                        if (!args.out.couldBe(arrayOut[i * 4 + j])) {
                            message.append("" FAIL"");
                        }
                        message.append(""\n"");
                        message.append(""Errors at"");
                    }
                    message.append("" ["");
                    message.append(Integer.toString(i));
                    message.append("", "");
                    message.append(Integer.toString(j));
                    message.append(""]"");
                }
            }
        }
        assertFalse(""Incorrect output for checkFabsFloat4Float4"" +
                (relaxed ? ""_relaxed"" : """") + "":\n"" + message.toString(), errorFound);
    }

    public class ArgumentsHalfHalf {
        public short inV;
        public double inVDouble;
        public Target.Floaty out;
    }

    private void checkFabsHalfHalf() {
        Allocation inV = createRandomAllocation(mRS, Element.DataType.FLOAT_16, 1, 0x8c0fe057e268991bl, false);
        try {
            Allocation out = Allocation.createSized(mRS, getElement(mRS, Element.DataType.FLOAT_16, 1), INPUTSIZE);
            script.forEach_testFabsHalfHalf(inV, out);
            verifyResultsFabsHalfHalf(inV, out, false);
            out.destroy();
        } catch (Exception e) {
            throw new RSRuntimeException(""RenderScript. Can't invoke forEach_testFabsHalfHalf: "" + e.toString());
        }
        try {
            Allocation out = Allocation.createSized(mRS, getElement(mRS, Element.DataType.FLOAT_16, 1), INPUTSIZE);
            scriptRelaxed.forEach_testFabsHalfHalf(inV, out);
            verifyResultsFabsHalfHalf(inV, out, true);
            out.destroy();
        } catch (Exception e) {
            throw new RSRuntimeException(""RenderScript. Can't invoke forEach_testFabsHalfHalf: "" + e.toString());
        }
        inV.destroy();
    }

    private void verifyResultsFabsHalfHalf(Allocation inV, Allocation out, boolean relaxed) {
        short[] arrayInV = new short[INPUTSIZE * 1];
        Arrays.fill(arrayInV, (short) 42);
        inV.copyTo(arrayInV);
        short[] arrayOut = new short[INPUTSIZE * 1];
        Arrays.fill(arrayOut, (short) 42);
        out.copyTo(arrayOut);
        StringBuilder message = new StringBuilder();
        boolean errorFound = false;
        for (int i = 0; i < INPUTSIZE; i++) {
            for (int j = 0; j < 1 ; j++) {
                // Extract the inputs.
                ArgumentsHalfHalf args = new ArgumentsHalfHalf();
                args.inV = arrayInV[i];
                args.inVDouble = Float16Utils.convertFloat16ToDouble(args.inV);
                // Figure out what the outputs should have been.
                Target target = new Target(Target.FunctionType.NORMAL, Target.ReturnType.HALF, relaxed);
                CoreMathVerifier.computeFabs(args, target);
                // Validate the outputs.
                boolean valid = true;
                if (!args.out.couldBe(Float16Utils.convertFloat16ToDouble(arrayOut[i * 1 + j]))) {
                    valid = false;
                }
                if (!valid) {
                    if (!errorFound) {
                        errorFound = true;
                        message.append(""Input inV: "");
                        appendVariableToMessage(message, args.inV);
                        message.append(""\n"");
                        message.append(""Expected output out: "");
                        appendVariableToMessage(message, args.out);
                        message.append(""\n"");
                        message.append(""Actual   output out: "");
                        appendVariableToMessage(message, arrayOut[i * 1 + j]);
                        message.append(""\n"");
                        message.append(""Actual   output out (in double): "");
                        appendVariableToMessage(message, Float16Utils.convertFloat16ToDouble(arrayOut[i * 1 + j]));
                        if (!args.out.couldBe(Float16Utils.convertFloat16ToDouble(arrayOut[i * 1 + j]))) {
                            message.append("" FAIL"");
                        }
                        message.append(""\n"");
                        message.append(""Errors at"");
                    }
                    message.append("" ["");
                    message.append(Integer.toString(i));
                    message.append("", "");
                    message.append(Integer.toString(j));
                    message.append(""]"");
                }
            }
        }
        assertFalse(""Incorrect output for checkFabsHalfHalf"" +
                (relaxed ? ""_relaxed"" : """") + "":\n"" + message.toString(), errorFound);
    }

    private void checkFabsHalf2Half2() {
        Allocation inV = createRandomAllocation(mRS, Element.DataType.FLOAT_16, 2, 0x9eaa6aa5ecb0e225l, false);
        try {
            Allocation out = Allocation.createSized(mRS, getElement(mRS, Element.DataType.FLOAT_16, 2), INPUTSIZE);
            script.forEach_testFabsHalf2Half2(inV, out);
            verifyResultsFabsHalf2Half2(inV, out, false);
            out.destroy();
        } catch (Exception e) {
            throw new RSRuntimeException(""RenderScript. Can't invoke forEach_testFabsHalf2Half2: "" + e.toString());
        }
        try {
            Allocation out = Allocation.createSized(mRS, getElement(mRS, Element.DataType.FLOAT_16, 2), INPUTSIZE);
            scriptRelaxed.forEach_testFabsHalf2Half2(inV, out);
            verifyResultsFabsHalf2Half2(inV, out, true);
            out.destroy();
        } catch (Exception e) {
            throw new RSRuntimeException(""RenderScript. Can't invoke forEach_testFabsHalf2Half2: "" + e.toString());
        }
        inV.destroy();
    }

    private void verifyResultsFabsHalf2Half2(Allocation inV, Allocation out, boolean relaxed) {
        short[] arrayInV = new short[INPUTSIZE * 2];
        Arrays.fill(arrayInV, (short) 42);
        inV.copyTo(arrayInV);
        short[] arrayOut = new short[INPUTSIZE * 2];
        Arrays.fill(arrayOut, (short) 42);
        out.copyTo(arrayOut);
        StringBuilder message = new StringBuilder();
        boolean errorFound = false;
        for (int i = 0; i < INPUTSIZE; i++) {
            for (int j = 0; j < 2 ; j++) {
                // Extract the inputs.
                ArgumentsHalfHalf args = new ArgumentsHalfHalf();
                args.inV = arrayInV[i * 2 + j];
                args.inVDouble = Float16Utils.convertFloat16ToDouble(args.inV);
                // Figure out what the outputs should have been.
                Target target = new Target(Target.FunctionType.NORMAL, Target.ReturnType.HALF, relaxed);
                CoreMathVerifier.computeFabs(args, target);
                // Validate the outputs.
                boolean valid = true;
                if (!args.out.couldBe(Float16Utils.convertFloat16ToDouble(arrayOut[i * 2 + j]))) {
                    valid = false;
                }
                if (!valid) {
                    if (!errorFound) {
                        errorFound = true;
                        message.append(""Input inV: "");
                        appendVariableToMessage(message, args.inV);
                        message.append(""\n"");
                        message.append(""Expected output out: "");
                        appendVariableToMessage(message, args.out);
                        message.append(""\n"");
                        message.append(""Actual   output out: "");
                        appendVariableToMessage(message, arrayOut[i * 2 + j]);
                        message.append(""\n"");
                        message.append(""Actual   output out (in double): "");
                        appendVariableToMessage(message, Float16Utils.convertFloat16ToDouble(arrayOut[i * 2 + j]));
                        if (!args.out.couldBe(Float16Utils.convertFloat16ToDouble(arrayOut[i * 2 + j]))) {
                            message.append("" FAIL"");
                        }
                        message.append(""\n"");
                        message.append(""Errors at"");
                    }
                    message.append("" ["");
                    message.append(Integer.toString(i));
                    message.append("", "");
                    message.append(Integer.toString(j));
                    message.append(""]"");
                }
            }
        }
        assertFalse(""Incorrect output for checkFabsHalf2Half2"" +
                (relaxed ? ""_relaxed"" : """") + "":\n"" + message.toString(), errorFound);
    }

    private void checkFabsHalf3Half3() {
        Allocation inV = createRandomAllocation(mRS, Element.DataType.FLOAT_16, 3, 0x9eaa75474bb8a719l, false);
        try {
            Allocation out = Allocation.createSized(mRS, getElement(mRS, Element.DataType.FLOAT_16, 3), INPUTSIZE);
            script.forEach_testFabsHalf3Half3(inV, out);
            verifyResultsFabsHalf3Half3(inV, out, false);
            out.destroy();
        } catch (Exception e) {
            throw new RSRuntimeException(""RenderScript. Can't invoke forEach_testFabsHalf3Half3: "" + e.toString());
        }
        try {
            Allocation out = Allocation.createSized(mRS, getElement(mRS, Element.DataType.FLOAT_16, 3), INPUTSIZE);
            scriptRelaxed.forEach_testFabsHalf3Half3(inV, out);
            verifyResultsFabsHalf3Half3(inV, out, true);
            out.destroy();
        } catch (Exception e) {
            throw new RSRuntimeException(""RenderScript. Can't invoke forEach_testFabsHalf3Half3: "" + e.toString());
        }
        inV.destroy();
    }

    private void verifyResultsFabsHalf3Half3(Allocation inV, Allocation out, boolean relaxed) {
        short[] arrayInV = new short[INPUTSIZE * 4];
        Arrays.fill(arrayInV, (short) 42);
        inV.copyTo(arrayInV);
        short[] arrayOut = new short[INPUTSIZE * 4];
        Arrays.fill(arrayOut, (short) 42);
        out.copyTo(arrayOut);
        StringBuilder message = new StringBuilder();
        boolean errorFound = false;
        for (int i = 0; i < INPUTSIZE; i++) {
            for (int j = 0; j < 3 ; j++) {
                // Extract the inputs.
                ArgumentsHalfHalf args = new ArgumentsHalfHalf();
                args.inV = arrayInV[i * 4 + j];
                args.inVDouble = Float16Utils.convertFloat16ToDouble(args.inV);
                // Figure out what the outputs should have been.
                Target target = new Target(Target.FunctionType.NORMAL, Target.ReturnType.HALF, relaxed);
                CoreMathVerifier.computeFabs(args, target);
                // Validate the outputs.
                boolean valid = true;
                if (!args.out.couldBe(Float16Utils.convertFloat16ToDouble(arrayOut[i * 4 + j]))) {
                    valid = false;
                }
                if (!valid) {
                    if (!errorFound) {
                        errorFound = true;
                        message.append(""Input inV: "");
                        appendVariableToMessage(message, args.inV);
                        message.append(""\n"");
                        message.append(""Expected output out: "");
                        appendVariableToMessage(message, args.out);
                        message.append(""\n"");
                        message.append(""Actual   output out: "");
                        appendVariableToMessage(message, arrayOut[i * 4 + j]);
                        message.append(""\n"");
                        message.append(""Actual   output out (in double): "");
                        appendVariableToMessage(message, Float16Utils.convertFloat16ToDouble(arrayOut[i * 4 + j]));
                        if (!args.out.couldBe(Float16Utils.convertFloat16ToDouble(arrayOut[i * 4 + j]))) {
                            message.append("" FAIL"");
                        }
                        message.append(""\n"");
                        message.append(""Errors at"");
                    }
                    message.append("" ["");
                    message.append(Integer.toString(i));
                    message.append("", "");
                    message.append(Integer.toString(j));
                    message.append(""]"");
                }
            }
        }
        assertFalse(""Incorrect output for checkFabsHalf3Half3"" +
                (relaxed ? ""_relaxed"" : """") + "":\n"" + message.toString(), errorFound);
    }

    private void checkFabsHalf4Half4() {
        Allocation inV = createRandomAllocation(mRS, Element.DataType.FLOAT_16, 4, 0x9eaa7fe8aac06c0dl, false);
        try {
            Allocation out = Allocation.createSized(mRS, getElement(mRS, Element.DataType.FLOAT_16, 4), INPUTSIZE);
            script.forEach_testFabsHalf4Half4(inV, out);
            verifyResultsFabsHalf4Half4(inV, out, false);
            out.destroy();
        } catch (Exception e) {
            throw new RSRuntimeException(""RenderScript. Can't invoke forEach_testFabsHalf4Half4: "" + e.toString());
        }
        try {
            Allocation out = Allocation.createSized(mRS, getElement(mRS, Element.DataType.FLOAT_16, 4), INPUTSIZE);
            scriptRelaxed.forEach_testFabsHalf4Half4(inV, out);
            verifyResultsFabsHalf4Half4(inV, out, true);
            out.destroy();
        } catch (Exception e) {
            throw new RSRuntimeException(""RenderScript. Can't invoke forEach_testFabsHalf4Half4: "" + e.toString());
        }
        inV.destroy();
    }

    private void verifyResultsFabsHalf4Half4(Allocation inV, Allocation out, boolean relaxed) {
        short[] arrayInV = new short[INPUTSIZE * 4];
        Arrays.fill(arrayInV, (short) 42);
        inV.copyTo(arrayInV);
        short[] arrayOut = new short[INPUTSIZE * 4];
        Arrays.fill(arrayOut, (short) 42);
        out.copyTo(arrayOut);
        StringBuilder message = new StringBuilder();
        boolean errorFound = false;
        for (int i = 0; i < INPUTSIZE; i++) {
            for (int j = 0; j < 4 ; j++) {
                // Extract the inputs.
                ArgumentsHalfHalf args = new ArgumentsHalfHalf();
                args.inV = arrayInV[i * 4 + j];
                args.inVDouble = Float16Utils.convertFloat16ToDouble(args.inV);
                // Figure out what the outputs should have been.
                Target target = new Target(Target.FunctionType.NORMAL, Target.ReturnType.HALF, relaxed);
                CoreMathVerifier.computeFabs(args, target);
                // Validate the outputs.
                boolean valid = true;
                if (!args.out.couldBe(Float16Utils.convertFloat16ToDouble(arrayOut[i * 4 + j]))) {
                    valid = false;
                }
                if (!valid) {
                    if (!errorFound) {
                        errorFound = true;
                        message.append(""Input inV: "");
                        appendVariableToMessage(message, args.inV);
                        message.append(""\n"");
                        message.append(""Expected output out: "");
                        appendVariableToMessage(message, args.out);
                        message.append(""\n"");
                        message.append(""Actual   output out: "");
                        appendVariableToMessage(message, arrayOut[i * 4 + j]);
                        message.append(""\n"");
                        message.append(""Actual   output out (in double): "");
                        appendVariableToMessage(message, Float16Utils.convertFloat16ToDouble(arrayOut[i * 4 + j]));
                        if (!args.out.couldBe(Float16Utils.convertFloat16ToDouble(arrayOut[i * 4 + j]))) {
                            message.append("" FAIL"");
                        }
                        message.append(""\n"");
                        message.append(""Errors at"");
                    }
                    message.append("" ["");
                    message.append(Integer.toString(i));
                    message.append("", "");
                    message.append(Integer.toString(j));
                    message.append(""]"");
                }
            }
        }
        assertFalse(""Incorrect output for checkFabsHalf4Half4"" +
                (relaxed ? ""_relaxed"" : """") + "":\n"" + message.toString(), errorFound);
    }"	""	""	"AAC"	""	""	""	""	""	""	""	""	""	""
