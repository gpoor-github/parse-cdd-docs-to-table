"Section"	"section_id"	"req_id"	"full_key"	"key_as_number"	"requirement"	"Test Availability"	"search_roots"	"search_terms"	"manual_search_terms"	"not_search_terms"	"not_files"	"max_matches"	"class_defs"	"methods"	"modules"	"protected"	"class_def"	"method"	"module"	"file_name"	"matched_files"	"methods_string"	"urls"	"method_text"	"matched_terms"	"qualified_method"	"Annotation?"	"New Req for S?"	"New CTS for S?"	"Comment(internal) e.g. why a test is not possible"	"CTS Bug Id"	"CDD Bug Id"	"Area"	"Shortened"	"Test Level"
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.BurstCaptureTest"	"testYuvBurstWithStillBokeh"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/BurstCaptureTest.java"	""	"public void testYuvBurstWithStillBokeh() throws Exception {
        final int YUV_BURST_SIZE = 100;
        testBurst(ImageFormat.YUV_420_888, YUV_BURST_SIZE, true/*checkFrameRate*/,
                true/*testStillBokeh*/);
    }

    private void testBurst(int fmt, int burstSize, boolean checkFrameRate, boolean testStillBokeh)
            throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                String id = mCameraIdsUnderTest[i];

                StaticMetadata staticInfo = mAllStaticInfo.get(id);
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" does not support color outputs, skipping"");
                }
                if (!staticInfo.isAeLockSupported() || !staticInfo.isAwbLockSupported()) {
                    Log.i(TAG, ""AE/AWB lock is not supported in camera "" + id +
                            "". Skip the test"");
                    continue;
                }

                if (staticInfo.isHardwareLevelLegacy()) {
                    Log.i(TAG, ""Legacy camera doesn't report min frame duration"" +
                            "". Skip the test"");
                    continue;
                }

                Capability[] extendedSceneModeCaps =
                        staticInfo.getAvailableExtendedSceneModeCapsChecked();
                boolean supportStillBokeh = false;
                for (Capability cap : extendedSceneModeCaps) {
                    if (cap.getMode() ==
                            CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE) {
                        supportStillBokeh = true;
                        break;
                    }
                }
                if (testStillBokeh && !supportStillBokeh) {
                    Log.v(TAG, ""Device doesn't support STILL_CAPTURE bokeh. Skip the test"");
                    continue;
                }

                openDevice(id);
                burstTestByCamera(id, fmt, burstSize, checkFrameRate, testStillBokeh);
            } finally {
                closeDevice();
                closeImageReader();
            }
        }
    }

    private void burstTestByCamera(String cameraId, int fmt, int burstSize,
            boolean checkFrameRate, boolean testStillBokeh) throws Exception {
        // Parameters
        final int MAX_CONVERGENCE_FRAMES = 150; // 5 sec at 30fps
        final long MAX_PREVIEW_RESULT_TIMEOUT_MS = 2000;
        final float FRAME_DURATION_MARGIN_FRACTION = 0.1f;

        // Find a good preview size (bound to 1080p)
        final Size previewSize = mOrderedPreviewSizes.get(0);

        // Get maximum size for fmt
        final Size stillSize = getSortedSizesForFormat(
                cameraId, mCameraManager, fmt, /*bound*/null).get(0);

        // Find max pipeline depth and sync latency
        final int maxPipelineDepth = mStaticInfo.getCharacteristics().get(
            CameraCharacteristics.REQUEST_PIPELINE_MAX_DEPTH);
        final int maxSyncLatency = mStaticInfo.getCharacteristics().get(
            CameraCharacteristics.SYNC_MAX_LATENCY);

        // Find minimum frame duration for full-res resolution
        StreamConfigurationMap config = mStaticInfo.getCharacteristics().get(
            CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        final long minStillFrameDuration =
                config.getOutputMinFrameDuration(fmt, stillSize);


        Range<Integer> targetRange = getSuitableFpsRangeForDuration(cameraId, minStillFrameDuration);

        Log.i(TAG, String.format(""Selected frame rate range %d - %d for YUV burst"",
                        targetRange.getLower(), targetRange.getUpper()));

        // Check if READ_SENSOR_SETTINGS is supported
        final boolean checkSensorSettings = mStaticInfo.isCapabilitySupported(
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS);

        // Configure basic preview and burst settings

        CaptureRequest.Builder previewBuilder =
            mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        int burstTemplate = (fmt == ImageFormat.JPEG) ?
                CameraDevice.TEMPLATE_STILL_CAPTURE : CameraDevice.TEMPLATE_PREVIEW;
        CaptureRequest.Builder burstBuilder = mCamera.createCaptureRequest(burstTemplate);
        Boolean enableZsl = burstBuilder.get(CaptureRequest.CONTROL_ENABLE_ZSL);
        boolean zslStillEnabled = enableZsl != null && enableZsl &&
                burstTemplate == CameraDevice.TEMPLATE_STILL_CAPTURE;

        previewBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE,
                targetRange);
        burstBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE,
                targetRange);
        burstBuilder.set(CaptureRequest.CONTROL_AE_LOCK, true);
        burstBuilder.set(CaptureRequest.CONTROL_AWB_LOCK, true);
        if (testStillBokeh) {
            previewBuilder.set(CaptureRequest.CONTROL_EXTENDED_SCENE_MODE,
                    CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE);
            burstBuilder.set(CaptureRequest.CONTROL_EXTENDED_SCENE_MODE,
                    CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE);
        }

        // Create session and start up preview

        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleCaptureCallback burstResultListener = new SimpleCaptureCallback();
        ImageDropperListener imageDropper = new ImageDropperListener();

        prepareCaptureAndStartPreview(
            previewBuilder, burstBuilder,
            previewSize, stillSize,
            fmt, resultListener,
            /*maxNumImages*/ 3, imageDropper);

        // Create burst

        List<CaptureRequest> burst = new ArrayList<>();
        for (int i = 0; i < burstSize; i++) {
            burst.add(burstBuilder.build());
        }

        // Converge AE/AWB

        int frameCount = 0;
        while (true) {
            CaptureResult result = resultListener.getCaptureResult(MAX_PREVIEW_RESULT_TIMEOUT_MS);
            int aeState = result.get(CaptureResult.CONTROL_AE_STATE);
            int awbState = result.get(CaptureResult.CONTROL_AWB_STATE);

            if (DEBUG) {
                Log.d(TAG, ""aeState: "" + aeState + "". awbState: "" + awbState);
            }

            if ((aeState == CaptureResult.CONTROL_AE_STATE_CONVERGED ||
                    aeState == CaptureResult.CONTROL_AE_STATE_FLASH_REQUIRED) &&
                    awbState == CaptureResult.CONTROL_AWB_STATE_CONVERGED) {
                break;
            }
            frameCount++;
            assertTrue(String.format(""Cam %s: Can not converge AE and AWB within %d frames"",
                    cameraId, MAX_CONVERGENCE_FRAMES),
                frameCount < MAX_CONVERGENCE_FRAMES);
        }

        // Lock AF if there's a focuser

        if (mStaticInfo.hasFocuser()) {
            previewBuilder.set(CaptureRequest.CONTROL_AF_TRIGGER,
                CaptureRequest.CONTROL_AF_TRIGGER_START);
            mSession.capture(previewBuilder.build(), resultListener, mHandler);
            previewBuilder.set(CaptureRequest.CONTROL_AF_TRIGGER,
                CaptureRequest.CONTROL_AF_TRIGGER_IDLE);

            frameCount = 0;
            while (true) {
                CaptureResult result = resultListener.getCaptureResult(MAX_PREVIEW_RESULT_TIMEOUT_MS);
                int afState = result.get(CaptureResult.CONTROL_AF_STATE);

                if (DEBUG) {
                    Log.d(TAG, ""afState: "" + afState);
                }

                if (afState == CaptureResult.CONTROL_AF_STATE_FOCUSED_LOCKED ||
                    afState == CaptureResult.CONTROL_AF_STATE_NOT_FOCUSED_LOCKED) {
                    break;
                }
                frameCount++;
                assertTrue(String.format(""Cam %s: Cannot lock AF within %d frames"", cameraId,
                        MAX_CONVERGENCE_FRAMES),
                    frameCount < MAX_CONVERGENCE_FRAMES);
            }
        }

        // Lock AE/AWB

        previewBuilder.set(CaptureRequest.CONTROL_AE_LOCK, true);
        previewBuilder.set(CaptureRequest.CONTROL_AWB_LOCK, true);

        CaptureRequest lockedRequest = previewBuilder.build();
        mSession.setRepeatingRequest(lockedRequest, resultListener, mHandler);

        // Wait for first result with locking
        resultListener.drain();
        CaptureResult lockedResult =
                resultListener.getCaptureResultForRequest(lockedRequest, maxPipelineDepth);

        int pipelineDepth = lockedResult.get(CaptureResult.REQUEST_PIPELINE_DEPTH);

        // Then start waiting on results to get the first result that should be synced
        // up, and also fire the burst as soon as possible

        if (maxSyncLatency == CameraCharacteristics.SYNC_MAX_LATENCY_PER_FRAME_CONTROL) {
            // The locked result we have is already synchronized so start the burst
            mSession.captureBurst(burst, burstResultListener, mHandler);
        } else {
            // Need to get a synchronized result, and may need to start burst later to
            // be synchronized correctly

            boolean burstSent = false;

            // Calculate how many requests we need to still send down to camera before we
            // know the settings have settled for the burst

            int numFramesWaited = maxSyncLatency;
            if (numFramesWaited == CameraCharacteristics.SYNC_MAX_LATENCY_UNKNOWN) {
                numFramesWaited = NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY;
            }

            int requestsNeededToSync = numFramesWaited - pipelineDepth;
            for (int i = 0; i < numFramesWaited; i++) {
                if (!burstSent && requestsNeededToSync <= 0) {
                    mSession.captureBurst(burst, burstResultListener, mHandler);
                    burstSent = true;
                }
                lockedResult = resultListener.getCaptureResult(MAX_PREVIEW_RESULT_TIMEOUT_MS);
                requestsNeededToSync--;
            }

            assertTrue(""Cam "" + cameraId + "": Burst failed to fire!"", burstSent);
        }

        // Read in locked settings if supported

        long burstExposure = 0;
        long burstFrameDuration = 0;
        int burstSensitivity = 0;
        if (checkSensorSettings) {
            burstExposure = lockedResult.get(CaptureResult.SENSOR_EXPOSURE_TIME);
            burstFrameDuration = lockedResult.get(CaptureResult.SENSOR_FRAME_DURATION);
            burstSensitivity = lockedResult.get(CaptureResult.SENSOR_SENSITIVITY);

            assertTrue(String.format(""Cam %s: Frame duration %d ns too short compared to "" +
                    ""exposure time %d ns"", cameraId, burstFrameDuration, burstExposure),
                burstFrameDuration >= burstExposure);

            assertTrue(String.format(""Cam %s: Exposure time is not valid: %d"",
                    cameraId, burstExposure),
                burstExposure > 0);
            assertTrue(String.format(""Cam %s: Frame duration is not valid: %d"",
                    cameraId, burstFrameDuration),
                burstFrameDuration > 0);
            assertTrue(String.format(""Cam %s: Sensitivity is not valid: %d"",
                    cameraId, burstSensitivity),
                burstSensitivity > 0);
        }

        // Process burst results
        int burstIndex = 0;
        CaptureResult burstResult =
                burstResultListener.getCaptureResult(MAX_PREVIEW_RESULT_TIMEOUT_MS);
        long prevTimestamp = -1;
        final long frameDurationBound = (long)
                (minStillFrameDuration * (1 + FRAME_DURATION_MARGIN_FRACTION) );

        long burstStartTimestamp = burstResult.get(CaptureResult.SENSOR_TIMESTAMP);
        long burstEndTimeStamp = 0;

        List<Long> frameDurations = new ArrayList<>();

        while(true) {
            // Verify the result
            assertTrue(""Cam "" + cameraId + "": Result doesn't match expected request"",
                    burstResult.getRequest() == burst.get(burstIndex));

            // Verify locked settings
            if (checkSensorSettings) {
                long exposure = burstResult.get(CaptureResult.SENSOR_EXPOSURE_TIME);
                int sensitivity = burstResult.get(CaptureResult.SENSOR_SENSITIVITY);
                assertTrue(""Cam "" + cameraId + "": Exposure not locked!"",
                    exposure == burstExposure);
                assertTrue(""Cam "" + cameraId + "": Sensitivity not locked!"",
                    sensitivity == burstSensitivity);
            }

            // Collect inter-frame durations
            long timestamp = burstResult.get(CaptureResult.SENSOR_TIMESTAMP);
            if (prevTimestamp != -1) {
                long frameDuration = timestamp - prevTimestamp;
                frameDurations.add(frameDuration);
                if (DEBUG) {
                    Log.i(TAG, String.format(""Frame %03d    Duration %.2f ms"", burstIndex,
                            frameDuration/1e6));
                }
            }
            prevTimestamp = timestamp;

            // Get next result
            burstIndex++;
            if (burstIndex == burstSize) {
                burstEndTimeStamp = burstResult.get(CaptureResult.SENSOR_TIMESTAMP);
                break;
            }
            burstResult = burstResultListener.getCaptureResult(MAX_PREVIEW_RESULT_TIMEOUT_MS);
        }

        // Verify no preview frames interleaved in burst results
        while (true) {
            CaptureResult previewResult =
                    resultListener.getCaptureResult(MAX_PREVIEW_RESULT_TIMEOUT_MS);
            long previewTimestamp = previewResult.get(CaptureResult.SENSOR_TIMESTAMP);
            if (!zslStillEnabled && previewTimestamp >= burstStartTimestamp
                    && previewTimestamp <= burstEndTimeStamp) {
                fail(""Preview frame is interleaved with burst frames! Preview timestamp:"" +
                        previewTimestamp + "", burst ["" + burstStartTimestamp + "", "" +
                        burstEndTimeStamp + ""]"");
            } else if (previewTimestamp > burstEndTimeStamp) {
                break;
            }
        }

        // Verify inter-frame durations
        if (checkFrameRate) {
            long meanFrameSum = 0;
            for (Long duration : frameDurations) {
                meanFrameSum += duration;
            }
            float meanFrameDuration = (float) meanFrameSum / frameDurations.size();

            float stddevSum = 0;
            for (Long duration : frameDurations) {
                stddevSum += (duration - meanFrameDuration) * (duration - meanFrameDuration);
            }
            float stddevFrameDuration = (float)
                    Math.sqrt(1.f / (frameDurations.size() - 1 ) * stddevSum);

            Log.i(TAG, String.format(""Cam %s: Burst frame duration mean: %.1f, stddev: %.1f"",
                    cameraId, meanFrameDuration, stddevFrameDuration));

            assertTrue(
                String.format(""Cam %s: Burst frame duration mean %.1f ns is larger than "" +
                    ""acceptable, expecting below %d ns, allowing below %d"", cameraId,
                    meanFrameDuration, minStillFrameDuration, frameDurationBound),
                meanFrameDuration <= frameDurationBound);

            // Calculate upper 97.5% bound (assuming durations are normally distributed...)
            float limit95FrameDuration = meanFrameDuration + 2 * stddevFrameDuration;

            // Don't enforce this yet, but warn
            if (limit95FrameDuration > frameDurationBound) {
                Log.w(TAG,
                    String.format(""Cam %s: Standard deviation is too large compared to limit: "" +
                        ""mean: %.1f ms, stddev: %.1f ms: 95%% bound: %f ms"", cameraId,
                        meanFrameDuration/1e6, stddevFrameDuration/1e6,
                        limit95FrameDuration/1e6));
            }
        }
    }
}"	""	""	"minimum resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.HeifWriterTest"	"testHeif"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/HeifWriterTest.java"	""	"public void testHeif() throws Exception {
        final int NUM_SINGLE_CAPTURE_TESTED = 3;
        final int NUM_HEIC_CAPTURE_TESTED = 2;
        final int SESSION_WARMUP_MS = 1000;
        final int HEIF_STOP_TIMEOUT = 3000 * NUM_SINGLE_CAPTURE_TESTED;

        if (!canEncodeHeic()) {
            MediaUtils.skipTest(""heic encoding is not supported on this device"");
            return;
        }

        boolean sessionFailure = false;
        Integer[] sessionStates = {BlockingSessionCallback.SESSION_READY,
                BlockingSessionCallback.SESSION_CONFIGURE_FAILED};
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.v(TAG, ""Testing HEIF capture for Camera "" + id);
                openDevice(id);

                Size[] availableSizes = mStaticInfo.getAvailableSizesForFormatChecked(
                        ImageFormat.PRIVATE,
                        StaticMetadata.StreamDirection.Output);

                // for each resolution, test imageReader:
                for (Size sz : availableSizes) {
                    HeifWriter heifWriter = null;
                    OutputConfiguration outConfig = null;
                    Surface latestSurface = null;
                    CaptureRequest.Builder reqStill = null;
                    int width = sz.getWidth();
                    int height = sz.getHeight();
                    for (int cap = 0; cap < NUM_HEIC_CAPTURE_TESTED; cap++) {
                        if (VERBOSE) {
                            Log.v(TAG, ""Testing size "" + sz.toString() + "" format PRIVATE""
                                    + "" for camera "" + mCamera.getId() + "". Iteration:"" + cap);
                        }

                        try {
                            TestConfig.Builder builder = new TestConfig.Builder(/*useGrid*/false);
                            builder.setNumImages(NUM_SINGLE_CAPTURE_TESTED);
                            builder.setSize(sz);
                            String filename = ""Cam"" + id + ""_"" + width + ""x"" + height +
                                    ""_"" + cap + "".heic"";
                            builder.setOutputPath(
                                    new File(mFilePath, filename).getAbsolutePath());
                            TestConfig config = builder.build();

                            try {
                                heifWriter = new HeifWriter.Builder(
                                        config.mOutputPath,
                                        width, height, INPUT_MODE_SURFACE)
                                    .setGridEnabled(config.mUseGrid)
                                    .setMaxImages(config.mMaxNumImages)
                                    .setQuality(config.mQuality)
                                    .setPrimaryIndex(config.mNumImages - 1)
                                    .setHandler(mHandler)
                                    .build();
                            } catch (IOException e) {
                                // Continue in case the size is not supported
                                sessionFailure = true;
                                Log.i(TAG, ""Skip due to heifWriter creation failure: ""
                                        + e.getMessage());
                                continue;
                            }

                            // First capture. Start capture session
                            latestSurface = heifWriter.getInputSurface();
                            outConfig = new OutputConfiguration(latestSurface);
                            List<OutputConfiguration> configs =
                                new ArrayList<OutputConfiguration>();
                            configs.add(outConfig);

                            SurfaceTexture preview = new SurfaceTexture(/*random int*/ 1);
                            Surface previewSurface = new Surface(preview);
                            preview.setDefaultBufferSize(640, 480);
                            configs.add(new OutputConfiguration(previewSurface));

                            CaptureRequest.Builder reqPreview = mCamera.createCaptureRequest(
                                    CameraDevice.TEMPLATE_PREVIEW);
                            reqPreview.addTarget(previewSurface);

                            reqStill = mCamera.createCaptureRequest(
                                    CameraDevice.TEMPLATE_STILL_CAPTURE);
                            reqStill.addTarget(previewSurface);
                            reqStill.addTarget(latestSurface);

                            // Start capture session and preview
                            createSessionByConfigs(configs);
                            int state = mCameraSessionListener.getStateWaiter().waitForAnyOfStates(
                                    Arrays.asList(sessionStates), SESSION_CONFIGURE_TIMEOUT_MS);
                            if (state == BlockingSessionCallback.SESSION_CONFIGURE_FAILED) {
                                // session configuration failure. Bail out due to known issue of
                                // HeifWriter INPUT_SURFACE mode support for camera. b/79699819
                                sessionFailure = true;
                                break;
                            }
                            startCapture(reqPreview.build(), /*repeating*/true, null, null);

                            SystemClock.sleep(SESSION_WARMUP_MS);

                            heifWriter.start();

                            // Start capture.
                            CaptureRequest request = reqStill.build();
                            SimpleCaptureCallback listener = new SimpleCaptureCallback();

                            int numImages = config.mNumImages;

                            for (int i = 0; i < numImages; i++) {
                                startCapture(request, /*repeating*/false, listener, mHandler);
                            }

                            // Validate capture result.
                            CaptureResult result = validateCaptureResult(
                                    ImageFormat.PRIVATE, sz, listener, numImages);

                            // TODO: convert capture results into EXIF and send to heifwriter

                            heifWriter.stop(HEIF_STOP_TIMEOUT);

                            verifyResult(config.mOutputPath, width, height,
                                    config.mRotation, config.mUseGrid,
                                    Math.min(numImages, config.mMaxNumImages));
                        } finally {
                            if (heifWriter != null) {
                                heifWriter.close();
                                heifWriter = null;
                            }
                            if (!sessionFailure) {
                                stopCapture(/*fast*/false);
                            }
                        }
                    }

                    if (sessionFailure) {
                        break;
                    }
                }
            } finally {
                closeDevice(id);
            }
        }
    }

    private static boolean canEncodeHeic() {
        return MediaUtils.hasEncoder(MediaFormat.MIMETYPE_VIDEO_HEVC)
            || MediaUtils.hasEncoder(MediaFormat.MIMETYPE_IMAGE_ANDROID_HEIC);
    }

    private static class TestConfig {
        final boolean mUseGrid;
        final int mMaxNumImages;
        final int mNumImages;
        final int mWidth;
        final int mHeight;
        final int mRotation;
        final int mQuality;
        final String mOutputPath;

        TestConfig(boolean useGrid, int maxNumImages, int numImages,
                   int width, int height, int rotation, int quality,
                   String outputPath) {
            mUseGrid = useGrid;
            mMaxNumImages = maxNumImages;
            mNumImages = numImages;
            mWidth = width;
            mHeight = height;
            mRotation = rotation;
            mQuality = quality;
            mOutputPath = outputPath;
        }

        static class Builder {
            final boolean mUseGrid;
            int mMaxNumImages;
            int mNumImages;
            int mWidth;
            int mHeight;
            int mRotation;
            final int mQuality;
            String mOutputPath;

            Builder(boolean useGrids) {
                mUseGrid = useGrids;
                mMaxNumImages = mNumImages = 4;
                mWidth = 1920;
                mHeight = 1080;
                mRotation = 0;
                mQuality = 100;
                mOutputPath = new File(Environment.getExternalStorageDirectory(),
                        OUTPUT_FILENAME).getAbsolutePath();
            }

            Builder setNumImages(int numImages) {
                mMaxNumImages = mNumImages = numImages;
                return this;
            }

            Builder setRotation(int rotation) {
                mRotation = rotation;
                return this;
            }

            Builder setSize(Size sz) {
                mWidth = sz.getWidth();
                mHeight = sz.getHeight();
                return this;
            }

            Builder setOutputPath(String path) {
                mOutputPath = path;
                return this;
            }

            private void cleanupStaleOutputs() {
                File outputFile = new File(mOutputPath);
                if (outputFile.exists()) {
                    outputFile.delete();
                }
            }

            TestConfig build() {
                cleanupStaleOutputs();
                return new TestConfig(mUseGrid, mMaxNumImages, mNumImages,
                        mWidth, mHeight, mRotation, mQuality, mOutputPath);
            }
        }

        @Override
        public String toString() {
            return ""TestConfig""
                    + "": mUseGrid "" + mUseGrid
                    + "", mMaxNumImages "" + mMaxNumImages
                    + "", mNumImages "" + mNumImages
                    + "", mWidth "" + mWidth
                    + "", mHeight "" + mHeight
                    + "", mRotation "" + mRotation
                    + "", mQuality "" + mQuality
                    + "", mOutputPath "" + mOutputPath;
        }
    }

    private void verifyResult(
            String filename, int width, int height, int rotation, boolean useGrid, int numImages)
            throws Exception {
        MediaMetadataRetriever retriever = new MediaMetadataRetriever();
        retriever.setDataSource(filename);
        String hasImage = retriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_HAS_IMAGE);
        if (!""yes"".equals(hasImage)) {
            throw new Exception(""No images found in file "" + filename);
        }
        assertEquals(""Wrong image count"", numImages,
                Integer.parseInt(retriever.extractMetadata(
                    MediaMetadataRetriever.METADATA_KEY_IMAGE_COUNT)));
        assertEquals(""Wrong width"", width,
                Integer.parseInt(retriever.extractMetadata(
                    MediaMetadataRetriever.METADATA_KEY_IMAGE_WIDTH)));
        assertEquals(""Wrong height"", height,
                Integer.parseInt(retriever.extractMetadata(
                    MediaMetadataRetriever.METADATA_KEY_IMAGE_HEIGHT)));
        assertEquals(""Wrong rotation"", rotation,
                Integer.parseInt(retriever.extractMetadata(
                    MediaMetadataRetriever.METADATA_KEY_IMAGE_ROTATION)));
        retriever.release();

        if (useGrid) {
            MediaExtractor extractor = new MediaExtractor();
            extractor.setDataSource(filename);
            MediaFormat format = extractor.getTrackFormat(0);
            int tileWidth = format.getInteger(MediaFormat.KEY_TILE_WIDTH);
            int tileHeight = format.getInteger(MediaFormat.KEY_TILE_HEIGHT);
            int gridRows = format.getInteger(MediaFormat.KEY_GRID_ROWS);
            int gridCols = format.getInteger(MediaFormat.KEY_GRID_COLUMNS);
            assertTrue(""Wrong tile width or grid cols"",
                    ((width + tileWidth - 1) / tileWidth) == gridCols);
            assertTrue(""Wrong tile height or grid rows"",
                    ((height + tileHeight - 1) / tileHeight) == gridRows);
            extractor.release();
        }
    }

    /**
     * Validate capture results.
     *
     * @param format The format of this capture.
     * @param size The capture size.
     * @param listener The capture listener to get capture result callbacks.
     * @return the last verified CaptureResult
     */
    private CaptureResult validateCaptureResult(
            int format, Size size, SimpleCaptureCallback listener, int numFrameVerified) {
        CaptureResult result = null;
        for (int i = 0; i < numFrameVerified; i++) {
            result = listener.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);
            if (mStaticInfo.isCapabilitySupported(
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS)) {
                Long exposureTime = getValueNotNull(result, CaptureResult.SENSOR_EXPOSURE_TIME);
                Integer sensitivity = getValueNotNull(result, CaptureResult.SENSOR_SENSITIVITY);
                mCollector.expectInRange(
                        String.format(
                                ""Capture for format %d, size %s exposure time is invalid."",
                                format, size.toString()),
                        exposureTime,
                        mStaticInfo.getExposureMinimumOrDefault(),
                        mStaticInfo.getExposureMaximumOrDefault()
                );
                mCollector.expectInRange(
                        String.format(""Capture for format %d, size %s sensitivity is invalid."",
                                format, size.toString()),
                        sensitivity,
                        mStaticInfo.getSensitivityMinimumOrDefault(),
                        mStaticInfo.getSensitivityMaximumOrDefault()
                );
            }
            // TODO: add more key validations.
        }
        return result;
    }
}"	""	""	"minimum resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RobustnessTest"	"testBadSurfaceDimensions"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RobustnessTest.java"	""	"public void testBadSurfaceDimensions() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.i(TAG, ""Testing Camera "" + id);
                openDevice(id);

                List<Size> testSizes = null;
                int format = mStaticInfo.isColorOutputSupported() ?
                    ImageFormat.YUV_420_888 : ImageFormat.DEPTH16;

                testSizes = CameraTestUtils.getSortedSizesForFormat(id, mCameraManager,
                        format, null);

                // Find some size not supported by the camera
                Size weirdSize = new Size(643, 577);
                int count = 0;
                while(testSizes.contains(weirdSize)) {
                    // Really, they can't all be supported...
                    weirdSize = new Size(weirdSize.getWidth() + 1, weirdSize.getHeight() + 1);
                    count++;
                    assertTrue(""Too many exotic YUV_420_888 resolutions supported."", count < 100);
                }

                // Setup imageReader with invalid dimension
                ImageReader imageReader = ImageReader.newInstance(weirdSize.getWidth(),
                        weirdSize.getHeight(), format, 3);

                // Setup ImageReaderListener
                SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
                imageReader.setOnImageAvailableListener(imageListener, mHandler);

                Surface surface = imageReader.getSurface();
                List<Surface> surfaces = new ArrayList<>();
                surfaces.add(surface);

                // Setup a capture request and listener
                CaptureRequest.Builder request =
                        mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
                request.addTarget(surface);

                // Check that correct session callback is hit.
                CameraCaptureSession.StateCallback sessionListener =
                        mock(CameraCaptureSession.StateCallback.class);
                CameraCaptureSession session = CameraTestUtils.configureCameraSession(mCamera,
                        surfaces, sessionListener, mHandler);

                verify(sessionListener, timeout(CONFIGURE_TIMEOUT).atLeastOnce()).
                        onConfigured(any(CameraCaptureSession.class));
                verify(sessionListener, timeout(CONFIGURE_TIMEOUT).atLeastOnce()).
                        onReady(any(CameraCaptureSession.class));
                verify(sessionListener, never()).onConfigureFailed(any(CameraCaptureSession.class));
                verify(sessionListener, never()).onActive(any(CameraCaptureSession.class));
                verify(sessionListener, never()).onClosed(any(CameraCaptureSession.class));

                CameraCaptureSession.CaptureCallback captureListener =
                        mock(CameraCaptureSession.CaptureCallback.class);
                session.capture(request.build(), captureListener, mHandler);

                verify(captureListener, timeout(CAPTURE_TIMEOUT).atLeastOnce()).
                        onCaptureCompleted(any(CameraCaptureSession.class),
                                any(CaptureRequest.class), any(TotalCaptureResult.class));
                verify(captureListener, never()).onCaptureFailed(any(CameraCaptureSession.class),
                        any(CaptureRequest.class), any(CaptureFailure.class));

                Image image = imageListener.getImage(CAPTURE_TIMEOUT);
                int imageWidth = image.getWidth();
                int imageHeight = image.getHeight();
                Size actualSize = new Size(imageWidth, imageHeight);

                assertTrue(""Camera does not contain outputted image resolution "" + actualSize,
                        testSizes.contains(actualSize));
                imageReader.close();
            } finally {
                closeDevice(id);
            }
        }
    }

    /**
     * Test for making sure the mandatory stream combinations work as expected.
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RobustnessTest"	"testMandatoryOutputCombinations"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RobustnessTest.java"	""	"public void testMandatoryOutputCombinations() throws Exception {
        testMandatoryOutputCombinations(/*maxResolution*/false);
    }

    /**
     * Test for making sure the mandatory stream combinations work as expected.
     */
    private void testMandatoryOutputCombinations(boolean maxResolution) throws Exception {
        final int AVAILABILITY_TIMEOUT_MS = 10;
        final LinkedBlockingQueue<Pair<String, String>> unavailablePhysicalCamEventQueue =
                new LinkedBlockingQueue<>();
        CameraManager.AvailabilityCallback ac = new CameraManager.AvailabilityCallback() {
             @Override
            public void onPhysicalCameraUnavailable(String cameraId, String physicalCameraId) {
                unavailablePhysicalCamEventQueue.offer(new Pair<>(cameraId, physicalCameraId));
            }
        };

        mCameraManager.registerAvailabilityCallback(ac, mHandler);
        Set<Pair<String, String>> unavailablePhysicalCameras = new HashSet<Pair<String, String>>();
        Pair<String, String> candidatePhysicalIds =
                unavailablePhysicalCamEventQueue.poll(AVAILABILITY_TIMEOUT_MS,
                java.util.concurrent.TimeUnit.MILLISECONDS);
        while (candidatePhysicalIds != null) {
            unavailablePhysicalCameras.add(candidatePhysicalIds);
            candidatePhysicalIds =
                unavailablePhysicalCamEventQueue.poll(AVAILABILITY_TIMEOUT_MS,
                java.util.concurrent.TimeUnit.MILLISECONDS);
        }
        mCameraManager.unregisterAvailabilityCallback(ac);
        CameraCharacteristics.Key<MandatoryStreamCombination []> ck =
                CameraCharacteristics.SCALER_MANDATORY_STREAM_COMBINATIONS;

        if (maxResolution) {
            ck = CameraCharacteristics.SCALER_MANDATORY_MAXIMUM_RESOLUTION_STREAM_COMBINATIONS;
        }
        for (String id : mCameraIdsUnderTest) {
            openDevice(id);
            MandatoryStreamCombination[] combinations = mStaticInfo.getCharacteristics().get(ck);

            if (combinations == null) {
                String maxResolutionStr = maxResolution ? "" "" : "" maximum resolution "";
                Log.i(TAG, ""No mandatory"" + maxResolutionStr + ""stream combinations for camera: "" +
                        id + "" skip test"");
                closeDevice(id);
                continue;
            }

            try {
                for (MandatoryStreamCombination combination : combinations) {
                    if (!combination.isReprocessable()) {
                        if (maxResolution) {
                            testMandatoryStreamCombination(id, mStaticInfo,
                                    /*physicalCameraId*/ null, combination, /*substituteY8*/false,
                                    /*substituteHeic*/false, /*maxResolution*/true);
                        } else {
                            testMandatoryStreamCombination(id, mStaticInfo,
                                    null/*physicalCameraId*/, combination);
                        }
                    }
                }

                // Make sure mandatory stream combinations for each physical camera work
                // as expected.
                if (mStaticInfo.isLogicalMultiCamera()) {
                    Set<String> physicalCameraIds =
                            mStaticInfo.getCharacteristics().getPhysicalCameraIds();
                    boolean skipTest = false;
                    for (String physicalId : physicalCameraIds) {
                        if (Arrays.asList(mCameraIdsUnderTest).contains(physicalId)) {
                            // If physicalId is advertised in camera ID list, do not need to test
                            // its stream combination through logical camera.
                            skipTest = true;
                        }
                        for (Pair<String, String> unavailPhysicalCam : unavailablePhysicalCameras) {
                            if (unavailPhysicalCam.first.equals(id) ||
                                    unavailPhysicalCam.second.equals(physicalId)) {
                                // This particular physical camera isn't available. Skip.
                                skipTest = true;
                                break;
                            }
                        }
                        if (skipTest) {
                            continue;
                        }
                        StaticMetadata physicalStaticInfo = mAllStaticInfo.get(physicalId);

                        MandatoryStreamCombination[] phyCombinations =
                                physicalStaticInfo.getCharacteristics().get(ck);

                        if (phyCombinations == null) {
                            Log.i(TAG, ""No mandatory stream combinations for physical camera device: "" + id + "" skip test"");
                            continue;
                        }

                        for (MandatoryStreamCombination combination : phyCombinations) {
                            if (!combination.isReprocessable()) {
                                if (maxResolution) {
                                   testMandatoryStreamCombination(id, physicalStaticInfo,
                                           physicalId, combination, /*substituteY8*/false,
                                           /*substituteHeic*/false, /*maxResolution*/true);
                                } else {
                                    testMandatoryStreamCombination(id, physicalStaticInfo,
                                            physicalId, combination);
                                }
                            }
                        }
                    }
                }

            } finally {
                closeDevice(id);
            }
        }
    }


    /**
     * Test for making sure the mandatory stream combinations work as expected.
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RobustnessTest"	"testMandatoryMaximumResolutionOutputCombinations"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RobustnessTest.java"	""	"public void testMandatoryMaximumResolutionOutputCombinations() throws Exception {
        testMandatoryOutputCombinations(/*maxResolution*/ true);
    }

    private void testMandatoryStreamCombination(String cameraId, StaticMetadata staticInfo,
            String physicalCameraId, MandatoryStreamCombination combination) throws Exception {
        // Check whether substituting YUV_888 format with Y8 format
        boolean substituteY8 = false;
        if (staticInfo.isMonochromeWithY8()) {
            List<MandatoryStreamInformation> streamsInfo = combination.getStreamsInformation();
            for (MandatoryStreamInformation streamInfo : streamsInfo) {
                if (streamInfo.getFormat() == ImageFormat.YUV_420_888) {
                    substituteY8 = true;
                    break;
                }
            }
        }

        // Check whether substituting JPEG format with HEIC format
        boolean substituteHeic = false;
        if (staticInfo.isHeicSupported()) {
            List<MandatoryStreamInformation> streamsInfo = combination.getStreamsInformation();
            for (MandatoryStreamInformation streamInfo : streamsInfo) {
                if (streamInfo.getFormat() == ImageFormat.JPEG) {
                    substituteHeic = true;
                    break;
                }
            }
        }

        // Test camera output combination
        String log = ""Testing mandatory stream combination: "" + combination.getDescription() +
                "" on camera: "" + cameraId;
        if (physicalCameraId != null) {
            log += "", physical sub-camera: "" + physicalCameraId;
        }
        Log.i(TAG, log);
        testMandatoryStreamCombination(cameraId, staticInfo, physicalCameraId, combination,
                /*substituteY8*/false, /*substituteHeic*/false, /*maxResolution*/false);

        if (substituteY8) {
            Log.i(TAG, log + "" with Y8"");
            testMandatoryStreamCombination(cameraId, staticInfo, physicalCameraId, combination,
                    /*substituteY8*/true, /*substituteHeic*/false, /*maxResolution*/false);
        }

        if (substituteHeic) {
            Log.i(TAG, log + "" with HEIC"");
            testMandatoryStreamCombination(cameraId, staticInfo, physicalCameraId, combination,
                    /*substituteY8*/false, /*substituteHeic*/true, /**maxResolution*/ false);
        }
    }

    private void testMandatoryStreamCombination(String cameraId,
            StaticMetadata staticInfo, String physicalCameraId,
            MandatoryStreamCombination combination,
            boolean substituteY8, boolean substituteHeic, boolean ultraHighResolution)
            throws Exception {

        // Timeout is relaxed by 1 second for LEGACY devices to reduce false positive rate in CTS
        // TODO: This needs to be adjusted based on feedback
        final int TIMEOUT_MULTIPLIER = ultraHighResolution ? 2 : 1;
        final int TIMEOUT_FOR_RESULT_MS =
                ((staticInfo.isHardwareLevelLegacy()) ? 2000 : 1000) * TIMEOUT_MULTIPLIER;
        final int MIN_RESULT_COUNT = 3;

        // Set up outputs
        List<OutputConfiguration> outputConfigs = new ArrayList<>();
        List<Surface> outputSurfaces = new ArrayList<Surface>();
        List<Surface> uhOutputSurfaces = new ArrayList<Surface>();
        StreamCombinationTargets targets = new StreamCombinationTargets();

        CameraTestUtils.setupConfigurationTargets(combination.getStreamsInformation(),
                targets, outputConfigs, outputSurfaces, uhOutputSurfaces, MIN_RESULT_COUNT,
                substituteY8, substituteHeic, physicalCameraId, /*multiResStreamConfig*/null,
                mHandler);

        boolean haveSession = false;
        try {
            CaptureRequest.Builder requestBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
            CaptureRequest.Builder uhRequestBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);

            for (Surface s : outputSurfaces) {
                requestBuilder.addTarget(s);
            }

            for (Surface s : uhOutputSurfaces) {
                uhRequestBuilder.addTarget(s);
            }
            // We need to explicitly set the sensor pixel mode to default since we're mixing default
            // and max resolution requests in the same capture session.
            requestBuilder.set(CaptureRequest.SENSOR_PIXEL_MODE,
                    CameraMetadata.SENSOR_PIXEL_MODE_DEFAULT);
            if (ultraHighResolution) {
                uhRequestBuilder.set(CaptureRequest.SENSOR_PIXEL_MODE,
                        CameraMetadata.SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION);
            }
            CameraCaptureSession.CaptureCallback mockCaptureCallback =
                    mock(CameraCaptureSession.CaptureCallback.class);

            if (physicalCameraId == null) {
                checkSessionConfigurationSupported(mCamera, mHandler, outputConfigs,
                        /*inputConfig*/ null, SessionConfiguration.SESSION_REGULAR,
                        true/*defaultSupport*/, String.format(
                        ""Session configuration query from combination: %s failed"",
                        combination.getDescription()));
            } else {
                SessionConfigSupport sessionConfigSupport = isSessionConfigSupported(
                        mCamera, mHandler, outputConfigs, /*inputConfig*/ null,
                        SessionConfiguration.SESSION_REGULAR, false/*defaultSupport*/);
                assertTrue(
                        String.format(""Session configuration query from combination: %s failed"",
                        combination.getDescription()), !sessionConfigSupport.error);
                if (!sessionConfigSupport.callSupported) {
                    return;
                }
                assertTrue(
                        String.format(""Session configuration must be supported for combination: "" +
                        ""%s"", combination.getDescription()), sessionConfigSupport.configSupported);
            }

            createSessionByConfigs(outputConfigs);
            haveSession = true;
            CaptureRequest request = requestBuilder.build();
            CaptureRequest uhRequest = uhRequestBuilder.build();
            mCameraSession.setRepeatingRequest(request, mockCaptureCallback, mHandler);
            if (ultraHighResolution) {
                mCameraSession.capture(uhRequest, mockCaptureCallback, mHandler);
            }
            verify(mockCaptureCallback,
                    timeout(TIMEOUT_FOR_RESULT_MS * MIN_RESULT_COUNT).atLeast(MIN_RESULT_COUNT))
                    .onCaptureCompleted(
                        eq(mCameraSession),
                        eq(request),
                        isA(TotalCaptureResult.class));
           if (ultraHighResolution) {
                verify(mockCaptureCallback,
                        timeout(TIMEOUT_FOR_RESULT_MS).atLeast(1))
                        .onCaptureCompleted(
                            eq(mCameraSession),
                            eq(uhRequest),
                            isA(TotalCaptureResult.class));
            }

            verify(mockCaptureCallback, never()).
                    onCaptureFailed(
                        eq(mCameraSession),
                        eq(request),
                        isA(CaptureFailure.class));

        } catch (Throwable e) {
            mCollector.addMessage(String.format(""Mandatory stream combination: %s failed due: %s"",
                    combination.getDescription(), e.getMessage()));
        }
        if (haveSession) {
            try {
                Log.i(TAG, String.format(""Done with camera %s, combination: %s, closing session"",
                                cameraId, combination.getDescription()));
                stopCapture(/*fast*/false);
            } catch (Throwable e) {
                mCollector.addMessage(
                    String.format(""Closing down for combination: %s failed due to: %s"",
                            combination.getDescription(), e.getMessage()));
            }
        }

        targets.close();
    }

    /**
     * Test for making sure the required reprocess input/output combinations for each hardware
     * level and capability work as expected.
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RobustnessTest"	"testMandatoryReprocessConfigurations"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RobustnessTest.java"	""	"public void testMandatoryReprocessConfigurations() throws Exception {
        testMandatoryReprocessConfigurations(/*maxResolution*/false);
    }

    /**
     * Test for making sure the required reprocess input/output combinations for each hardware
     * level and capability work as expected.
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RobustnessTest"	"testMandatoryMaximumResolutionReprocessConfigurations"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RobustnessTest.java"	""	"public void testMandatoryMaximumResolutionReprocessConfigurations() throws Exception {
        testMandatoryReprocessConfigurations(/*maxResolution*/true);
    }

    /**
     * Test for making sure the required reprocess input/output combinations for each hardware
     * level and capability work as expected.
     */
    public void testMandatoryReprocessConfigurations(boolean maxResolution) throws Exception {
        for (String id : mCameraIdsUnderTest) {
            openDevice(id);
            CameraCharacteristics chars = mStaticInfo.getCharacteristics();
            if (maxResolution && !CameraTestUtils.hasCapability(
                  chars, CameraMetadata.REQUEST_AVAILABLE_CAPABILITIES_REMOSAIC_REPROCESSING)) {
                Log.i(TAG, ""Camera id "" + id + ""doesn't support REMOSAIC_REPROCESSING, skip test"");
                closeDevice(id);
                continue;
            }
            CameraCharacteristics.Key<MandatoryStreamCombination []> ck =
                    CameraCharacteristics.SCALER_MANDATORY_STREAM_COMBINATIONS;

            if (maxResolution) {
                ck = CameraCharacteristics.SCALER_MANDATORY_MAXIMUM_RESOLUTION_STREAM_COMBINATIONS;
            }

            MandatoryStreamCombination[] combinations = chars.get(ck);
            if (combinations == null) {
                Log.i(TAG, ""No mandatory stream combinations for camera: "" + id + "" skip test"");
                closeDevice(id);
                continue;
            }

            try {
                for (MandatoryStreamCombination combination : combinations) {
                    if (combination.isReprocessable()) {
                        Log.i(TAG, ""Testing mandatory reprocessable stream combination: "" +
                                combination.getDescription() + "" on camera: "" + id);
                        testMandatoryReprocessableStreamCombination(id, combination, maxResolution);
                    }
                }
            } finally {
                closeDevice(id);
            }
        }
    }

    private void testMandatoryReprocessableStreamCombination(String cameraId,
            MandatoryStreamCombination combination, boolean maxResolution)  throws Exception {
        // Test reprocess stream combination
        testMandatoryReprocessableStreamCombination(cameraId, combination,
                /*substituteY8*/false, /*substituteHeic*/false, maxResolution/*maxResolution*/);
        if (maxResolution) {
            // Maximum resolution mode doesn't guarantee HEIC and Y8 streams.
            return;
        }

        // Test substituting YUV_888 format with Y8 format in reprocess stream combination.
        if (mStaticInfo.isMonochromeWithY8()) {
            List<MandatoryStreamInformation> streamsInfo = combination.getStreamsInformation();
            boolean substituteY8 = false;
            for (MandatoryStreamInformation streamInfo : streamsInfo) {
                if (streamInfo.getFormat() == ImageFormat.YUV_420_888) {
                    substituteY8 = true;
                }
            }
            if (substituteY8) {
                testMandatoryReprocessableStreamCombination(cameraId, combination,
                        /*substituteY8*/true, /*substituteHeic*/false, false/*maxResolution*/);
            }
        }

        if (mStaticInfo.isHeicSupported()) {
            List<MandatoryStreamInformation> streamsInfo = combination.getStreamsInformation();
            boolean substituteHeic = false;
            for (MandatoryStreamInformation streamInfo : streamsInfo) {
                if (streamInfo.getFormat() == ImageFormat.JPEG) {
                    substituteHeic = true;
                }
            }
            if (substituteHeic) {
                testMandatoryReprocessableStreamCombination(cameraId, combination,
                        /*substituteY8*/false, /*substituteHeic*/true, false/*maxResolution*/);
            }
        }
    }

    private void testMandatoryReprocessableStreamCombination(String cameraId,
            MandatoryStreamCombination combination, boolean substituteY8,
            boolean substituteHeic, boolean maxResolution) throws Exception {

        final int TIMEOUT_MULTIPLIER = maxResolution ? 2 : 1;
        final int TIMEOUT_FOR_RESULT_MS = 5000 * TIMEOUT_MULTIPLIER;
        final int NUM_REPROCESS_CAPTURES_PER_CONFIG = 3;

        StreamCombinationTargets targets = new StreamCombinationTargets();
        ArrayList<Surface> defaultOutputSurfaces = new ArrayList<>();
        ArrayList<Surface> allOutputSurfaces = new ArrayList<>();
        List<OutputConfiguration> outputConfigs = new ArrayList<>();
        List<Surface> uhOutputSurfaces = new ArrayList<Surface>();
        ImageReader inputReader = null;
        ImageWriter inputWriter = null;
        SimpleImageReaderListener inputReaderListener = new SimpleImageReaderListener();
        SimpleCaptureCallback inputCaptureListener = new SimpleCaptureCallback();
        SimpleCaptureCallback reprocessOutputCaptureListener = new SimpleCaptureCallback();

        List<MandatoryStreamInformation> streamInfo = combination.getStreamsInformation();
        assertTrue(""Reprocessable stream combinations should have at least 3 or more streams"",
                (streamInfo != null) && (streamInfo.size() >= 3));

        assertTrue(""The first mandatory stream information in a reprocessable combination must "" +
                ""always be input"", streamInfo.get(0).isInput());

        List<Size> inputSizes = streamInfo.get(0).getAvailableSizes();
        int inputFormat = streamInfo.get(0).getFormat();
        if (substituteY8 && (inputFormat == ImageFormat.YUV_420_888)) {
            inputFormat = ImageFormat.Y8;
        }

        Log.i(TAG, ""testMandatoryReprocessableStreamCombination: "" +
                combination.getDescription() + "", substituteY8 = "" + substituteY8 +
                "", substituteHeic = "" + substituteHeic);
        try {
            // The second stream information entry is the ZSL stream, which is configured
            // separately.
            List<MandatoryStreamInformation> mandatoryStreamInfos = null;
            mandatoryStreamInfos = new ArrayList<MandatoryStreamInformation>();
            mandatoryStreamInfos = streamInfo.subList(2, streamInfo.size());
            CameraTestUtils.setupConfigurationTargets(mandatoryStreamInfos, targets,
                    outputConfigs, defaultOutputSurfaces, uhOutputSurfaces,
                    NUM_REPROCESS_CAPTURES_PER_CONFIG,
                    substituteY8, substituteHeic, null/*overridePhysicalCameraId*/,
                    /*multiResStreamConfig*/null, mHandler);
            allOutputSurfaces.addAll(defaultOutputSurfaces);
            allOutputSurfaces.addAll(uhOutputSurfaces);
            InputConfiguration inputConfig = new InputConfiguration(inputSizes.get(0).getWidth(),
                    inputSizes.get(0).getHeight(), inputFormat);

            // For each config, YUV and JPEG outputs will be tested. (For YUV/Y8 reprocessing,
            // the YUV/Y8 ImageReader for input is also used for output.)
            final boolean inputIsYuv = inputConfig.getFormat() == ImageFormat.YUV_420_888;
            final boolean inputIsY8 = inputConfig.getFormat() == ImageFormat.Y8;
            final boolean useYuv = inputIsYuv || targets.mYuvTargets.size() > 0;
            final boolean useY8 = inputIsY8 || targets.mY8Targets.size() > 0;
            final int totalNumReprocessCaptures =  NUM_REPROCESS_CAPTURES_PER_CONFIG *
                    (maxResolution ? 1 : (((inputIsYuv || inputIsY8) ? 1 : 0) +
                    (substituteHeic ? targets.mHeicTargets.size() : targets.mJpegTargets.size()) +
                    (useYuv ? targets.mYuvTargets.size() : targets.mY8Targets.size())));

            // It needs 1 input buffer for each reprocess capture + the number of buffers
            // that will be used as outputs.
            inputReader = ImageReader.newInstance(inputConfig.getWidth(), inputConfig.getHeight(),
                    inputConfig.getFormat(),
                    totalNumReprocessCaptures + NUM_REPROCESS_CAPTURES_PER_CONFIG);
            inputReader.setOnImageAvailableListener(inputReaderListener, mHandler);
            allOutputSurfaces.add(inputReader.getSurface());

            checkSessionConfigurationWithSurfaces(mCamera, mHandler, allOutputSurfaces,
                    inputConfig, SessionConfiguration.SESSION_REGULAR, /*defaultSupport*/ true,
                    String.format(""Session configuration query %s failed"",
                    combination.getDescription()));

            // Verify we can create a reprocessable session with the input and all outputs.
            BlockingSessionCallback sessionListener = new BlockingSessionCallback();
            CameraCaptureSession session = configureReprocessableCameraSession(mCamera,
                    inputConfig, allOutputSurfaces, sessionListener, mHandler);
            inputWriter = ImageWriter.newInstance(session.getInputSurface(),
                    totalNumReprocessCaptures);

            // Prepare a request for reprocess input
            CaptureRequest.Builder builder = mCamera.createCaptureRequest(
                    CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG);
            builder.addTarget(inputReader.getSurface());
            if (maxResolution) {
                builder.set(CaptureRequest.SENSOR_PIXEL_MODE,
                        CameraMetadata.SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION);
            }

            for (int i = 0; i < totalNumReprocessCaptures; i++) {
                session.capture(builder.build(), inputCaptureListener, mHandler);
            }

            List<CaptureRequest> reprocessRequests = new ArrayList<>();
            List<Surface> reprocessOutputs = new ArrayList<>();

            if (maxResolution) {
                if (uhOutputSurfaces.size() == 0) { // RAW -> RAW reprocessing
                    reprocessOutputs.add(inputReader.getSurface());
                } else {
                    for (Surface surface : uhOutputSurfaces) {
                        reprocessOutputs.add(surface);
                    }
                }
            } else {
                if (inputIsYuv || inputIsY8) {
                    reprocessOutputs.add(inputReader.getSurface());
                }

                for (ImageReader reader : targets.mJpegTargets) {
                    reprocessOutputs.add(reader.getSurface());
                }

                for (ImageReader reader : targets.mHeicTargets) {
                    reprocessOutputs.add(reader.getSurface());
                }

                for (ImageReader reader : targets.mYuvTargets) {
                    reprocessOutputs.add(reader.getSurface());
                }

                for (ImageReader reader : targets.mY8Targets) {
                    reprocessOutputs.add(reader.getSurface());
                }
            }

            for (int i = 0; i < NUM_REPROCESS_CAPTURES_PER_CONFIG; i++) {
                for (Surface output : reprocessOutputs) {
                    TotalCaptureResult result = inputCaptureListener.getTotalCaptureResult(
                            TIMEOUT_FOR_RESULT_MS);
                    builder =  mCamera.createReprocessCaptureRequest(result);
                    inputWriter.queueInputImage(
                            inputReaderListener.getImage(TIMEOUT_FOR_RESULT_MS));
                    builder.addTarget(output);
                    reprocessRequests.add(builder.build());
                }
            }

            session.captureBurst(reprocessRequests, reprocessOutputCaptureListener, mHandler);

            for (int i = 0; i < reprocessOutputs.size() * NUM_REPROCESS_CAPTURES_PER_CONFIG; i++) {
                TotalCaptureResult result = reprocessOutputCaptureListener.getTotalCaptureResult(
                        TIMEOUT_FOR_RESULT_MS);
            }
        } catch (Throwable e) {
            mCollector.addMessage(String.format(""Reprocess stream combination %s failed due to: %s"",
                    combination.getDescription(), e.getMessage()));
        } finally {
            inputReaderListener.drain();
            reprocessOutputCaptureListener.drain();
            targets.close();

            if (inputReader != null) {
                inputReader.close();
            }

            if (inputWriter != null) {
                inputWriter.close();
            }
        }
    }"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RobustnessTest"	"testConfigureInvalidSensorPixelModes"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RobustnessTest.java"	""	"public void testConfigureInvalidSensorPixelModes() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            // Go through given, stream configuration map, add the incorrect sensor pixel mode
            // to an OutputConfiguration, make sure the session configuration fails.
            CameraCharacteristics chars = mCameraManager.getCameraCharacteristics(id);
            StreamConfigurationMap defaultStreamConfigMap =
                    chars.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            StreamConfigurationMap maxStreamConfigMap =
                    chars.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP_MAXIMUM_RESOLUTION);
            openDevice(id);
            try {
                verifyBasicSensorPixelModes(id, defaultStreamConfigMap, /*maxResolution*/ false);
                verifyBasicSensorPixelModes(id, maxStreamConfigMap, /*maxResolution*/ true);
            } finally {
                closeDevice(id);
            }
        }
    }"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RobustnessTest"	"testOisDataMode"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RobustnessTest.java"	""	"public void testOisDataMode() throws Exception {
        final int NUM_FRAMES_VERIFIED = 3;

        for (String id : mCameraIdsUnderTest) {
            Log.i(TAG, String.format(""Testing Camera %s for OIS mode"", id));

            StaticMetadata staticInfo =
                    new StaticMetadata(mCameraManager.getCameraCharacteristics(id));
            if (!staticInfo.isOisDataModeSupported()) {
                continue;
            }

            openDevice(id);

            try {
                SurfaceTexture preview = new SurfaceTexture(/*random int*/ 1);
                Surface previewSurface = new Surface(preview);

                CaptureRequest.Builder previewRequest = preparePreviewTestSession(preview);
                SimpleCaptureCallback previewListener = new CameraTestUtils.SimpleCaptureCallback();

                int[] availableOisDataModes = staticInfo.getCharacteristics().get(
                        CameraCharacteristics.STATISTICS_INFO_AVAILABLE_OIS_DATA_MODES);

                // Test each OIS data mode
                for (int oisMode : availableOisDataModes) {
                    previewRequest.set(CaptureRequest.STATISTICS_OIS_DATA_MODE, oisMode);

                    int sequenceId = mCameraSession.setRepeatingRequest(previewRequest.build(),
                            previewListener, mHandler);

                    // Check OIS data in each mode.
                    for (int i = 0; i < NUM_FRAMES_VERIFIED; i++) {
                        TotalCaptureResult result =
                            previewListener.getTotalCaptureResult(CAPTURE_TIMEOUT);

                        OisSample[] oisSamples = result.get(CaptureResult.STATISTICS_OIS_SAMPLES);

                        if (oisMode == CameraCharacteristics.STATISTICS_OIS_DATA_MODE_OFF) {
                            mCollector.expectKeyValueEquals(result,
                                    CaptureResult.STATISTICS_OIS_DATA_MODE,
                                    CaptureResult.STATISTICS_OIS_DATA_MODE_OFF);
                            mCollector.expectTrue(""OIS samples reported in OIS_DATA_MODE_OFF"",
                                    oisSamples == null || oisSamples.length == 0);

                        } else if (oisMode == CameraCharacteristics.STATISTICS_OIS_DATA_MODE_ON) {
                            mCollector.expectKeyValueEquals(result,
                                    CaptureResult.STATISTICS_OIS_DATA_MODE,
                                    CaptureResult.STATISTICS_OIS_DATA_MODE_ON);
                            mCollector.expectTrue(""OIS samples not reported in OIS_DATA_MODE_ON"",
                                    oisSamples != null && oisSamples.length != 0);
                        } else {
                            mCollector.addMessage(String.format(""Invalid OIS mode: %d"", oisMode));
                        }
                    }

                    mCameraSession.stopRepeating();
                    previewListener.getCaptureSequenceLastFrameNumber(sequenceId, CAPTURE_TIMEOUT);
                    previewListener.drain();
                }
            } finally {
                closeDevice(id);
            }
        }
    }

    private CaptureRequest.Builder preparePreviewTestSession(SurfaceTexture preview)
            throws Exception {
        Surface previewSurface = new Surface(preview);

        preview.setDefaultBufferSize(640, 480);

        ArrayList<Surface> sessionOutputs = new ArrayList<>();
        sessionOutputs.add(previewSurface);

        createSession(sessionOutputs);

        CaptureRequest.Builder previewRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);

        previewRequest.addTarget(previewSurface);

        return previewRequest;
    }

    private CaptureRequest.Builder prepareTriggerTestSession(
            SurfaceTexture preview, int aeMode, int afMode) throws Exception {
        Log.i(TAG, String.format(""Testing AE mode %s, AF mode %s"",
                        StaticMetadata.getAeModeName(aeMode),
                        StaticMetadata.getAfModeName(afMode)));

        CaptureRequest.Builder previewRequest = preparePreviewTestSession(preview);
        previewRequest.set(CaptureRequest.CONTROL_AE_MODE, aeMode);
        previewRequest.set(CaptureRequest.CONTROL_AF_MODE, afMode);

        return previewRequest;
    }

    private void cancelTriggersAndWait(CaptureRequest.Builder previewRequest,
            SimpleCaptureCallback captureListener, int afMode) throws Exception {
        previewRequest.set(CaptureRequest.CONTROL_AF_TRIGGER,
                CaptureRequest.CONTROL_AF_TRIGGER_CANCEL);
        previewRequest.set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,
                CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_CANCEL);

        CaptureRequest triggerRequest = previewRequest.build();
        mCameraSession.capture(triggerRequest, captureListener, mHandler);

        // Wait for a few frames to initialize 3A

        CaptureResult previewResult = null;
        int afState;
        int aeState;

        for (int i = 0; i < PREVIEW_WARMUP_FRAMES; i++) {
            previewResult = captureListener.getCaptureResult(
                    CameraTestUtils.CAPTURE_RESULT_TIMEOUT_MS);
            if (VERBOSE) {
                afState = previewResult.get(CaptureResult.CONTROL_AF_STATE);
                aeState = previewResult.get(CaptureResult.CONTROL_AE_STATE);
                Log.v(TAG, String.format(""AF state: %s, AE state: %s"",
                                StaticMetadata.AF_STATE_NAMES[afState],
                                StaticMetadata.AE_STATE_NAMES[aeState]));
            }
        }

        // Verify starting states

        afState = previewResult.get(CaptureResult.CONTROL_AF_STATE);
        aeState = previewResult.get(CaptureResult.CONTROL_AE_STATE);

        verifyStartingAfState(afMode, afState);

        // After several frames, AE must no longer be in INACTIVE state
        assertTrue(String.format(""AE state must be SEARCHING, CONVERGED, "" +
                        ""or FLASH_REQUIRED, is %s"", StaticMetadata.AE_STATE_NAMES[aeState]),
                aeState == CaptureResult.CONTROL_AE_STATE_SEARCHING ||
                aeState == CaptureResult.CONTROL_AE_STATE_CONVERGED ||
                aeState == CaptureResult.CONTROL_AE_STATE_FLASH_REQUIRED);
    }

    private void verifyBasicSensorPixelModes(String id, StreamConfigurationMap configs,
            boolean maxResolution) throws Exception {
        // Go through StreamConfiguration map, set up OutputConfiguration and add the opposite
        // sensorPixelMode.
        final int MIN_RESULT_COUNT = 3;
        if (!maxResolution) {
            assertTrue(""Default stream config map must be present for id: "" + id, configs != null);
        }
        if (configs == null) {
            Log.i(TAG, ""camera id "" + id + "" has no StreamConfigurationMap for max resolution "" +
                "", skipping verifyBasicSensorPixelModes"");
            return;
        }
        OutputConfiguration outputConfig = null;
        for (int format : configs.getOutputFormats()) {
            Size targetSize = CameraTestUtils.getMaxSize(configs.getOutputSizes(format));
            // Create outputConfiguration with this size and format
            SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
            SurfaceTexture textureTarget = null;
            ImageReader readerTarget = null;
            if (format == ImageFormat.PRIVATE) {
                textureTarget = new SurfaceTexture(1);
                textureTarget.setDefaultBufferSize(targetSize.getWidth(), targetSize.getHeight());
                outputConfig = new OutputConfiguration(new Surface(textureTarget));
            } else {
                readerTarget = ImageReader.newInstance(targetSize.getWidth(),
                        targetSize.getHeight(), format, MIN_RESULT_COUNT);
                readerTarget.setOnImageAvailableListener(imageListener, mHandler);
                outputConfig = new OutputConfiguration(readerTarget.getSurface());
            }
            try {
                int invalidSensorPixelMode =
                        maxResolution ? CameraMetadata.SENSOR_PIXEL_MODE_DEFAULT :
                                CameraMetadata.SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION;

                outputConfig.addSensorPixelModeUsed(invalidSensorPixelMode);
                CameraCaptureSession.StateCallback sessionListener =
                        mock(CameraCaptureSession.StateCallback.class);
                List<OutputConfiguration> outputs = new ArrayList<>();
                outputs.add(outputConfig);
                CameraCaptureSession session =
                        CameraTestUtils.configureCameraSessionWithConfig(mCamera, outputs,
                                sessionListener, mHandler);

                verify(sessionListener, timeout(CONFIGURE_TIMEOUT).atLeastOnce()).
                        onConfigureFailed(any(CameraCaptureSession.class));
                verify(sessionListener, never()).onConfigured(any(CameraCaptureSession.class));

                // Remove the invalid sensor pixel mode, session configuration should succeed
                sessionListener = mock(CameraCaptureSession.StateCallback.class);
                outputConfig.removeSensorPixelModeUsed(invalidSensorPixelMode);
                CameraTestUtils.configureCameraSessionWithConfig(mCamera, outputs,
                        sessionListener, mHandler);
                verify(sessionListener, timeout(CONFIGURE_TIMEOUT).atLeastOnce()).
                        onConfigured(any(CameraCaptureSession.class));
                verify(sessionListener, never()).onConfigureFailed(any(CameraCaptureSession.class));
            } finally {
                if (textureTarget != null) {
                    textureTarget.release();
                }

                if (readerTarget != null) {
                    readerTarget.close();
                }
            }
        }
    }

    private void verifyStartingAfState(int afMode, int afState) {
        switch (afMode) {
            case CaptureResult.CONTROL_AF_MODE_AUTO:
            case CaptureResult.CONTROL_AF_MODE_MACRO:
                assertTrue(String.format(""AF state not INACTIVE, is %s"",
                                StaticMetadata.AF_STATE_NAMES[afState]),
                        afState == CaptureResult.CONTROL_AF_STATE_INACTIVE);
                break;
            case CaptureResult.CONTROL_AF_MODE_CONTINUOUS_PICTURE:
            case CaptureResult.CONTROL_AF_MODE_CONTINUOUS_VIDEO:
                // After several frames, AF must no longer be in INACTIVE state
                assertTrue(String.format(""In AF mode %s, AF state not PASSIVE_SCAN"" +
                                "", PASSIVE_FOCUSED, or PASSIVE_UNFOCUSED, is %s"",
                                StaticMetadata.getAfModeName(afMode),
                                StaticMetadata.AF_STATE_NAMES[afState]),
                        afState == CaptureResult.CONTROL_AF_STATE_PASSIVE_SCAN ||
                        afState == CaptureResult.CONTROL_AF_STATE_PASSIVE_FOCUSED ||
                        afState == CaptureResult.CONTROL_AF_STATE_PASSIVE_UNFOCUSED);
                break;
            default:
                fail(""unexpected af mode"");
        }
    }

    private boolean verifyAfSequence(int afMode, int afState, boolean focusComplete) {
        if (focusComplete) {
            assertTrue(String.format(""AF Mode %s: Focus lock lost after convergence: AF state: %s"",
                            StaticMetadata.getAfModeName(afMode),
                            StaticMetadata.AF_STATE_NAMES[afState]),
                    afState == CaptureResult.CONTROL_AF_STATE_FOCUSED_LOCKED ||
                    afState ==CaptureResult.CONTROL_AF_STATE_NOT_FOCUSED_LOCKED);
            return focusComplete;
        }
        if (VERBOSE) {
            Log.v(TAG, String.format(""AF mode: %s, AF state: %s"",
                            StaticMetadata.getAfModeName(afMode),
                            StaticMetadata.AF_STATE_NAMES[afState]));
        }
        switch (afMode) {
            case CaptureResult.CONTROL_AF_MODE_AUTO:
            case CaptureResult.CONTROL_AF_MODE_MACRO:
                assertTrue(String.format(""AF mode %s: Unexpected AF state %s"",
                                StaticMetadata.getAfModeName(afMode),
                                StaticMetadata.AF_STATE_NAMES[afState]),
                        afState == CaptureResult.CONTROL_AF_STATE_ACTIVE_SCAN ||
                        afState == CaptureResult.CONTROL_AF_STATE_FOCUSED_LOCKED ||
                        afState == CaptureResult.CONTROL_AF_STATE_NOT_FOCUSED_LOCKED);
                focusComplete =
                        (afState != CaptureResult.CONTROL_AF_STATE_ACTIVE_SCAN);
                break;
            case CaptureResult.CONTROL_AF_MODE_CONTINUOUS_PICTURE:
                assertTrue(String.format(""AF mode %s: Unexpected AF state %s"",
                                StaticMetadata.getAfModeName(afMode),
                                StaticMetadata.AF_STATE_NAMES[afState]),
                        afState == CaptureResult.CONTROL_AF_STATE_PASSIVE_SCAN ||
                        afState == CaptureResult.CONTROL_AF_STATE_FOCUSED_LOCKED ||
                        afState == CaptureResult.CONTROL_AF_STATE_NOT_FOCUSED_LOCKED);
                focusComplete =
                        (afState != CaptureResult.CONTROL_AF_STATE_PASSIVE_SCAN);
                break;
            case CaptureResult.CONTROL_AF_MODE_CONTINUOUS_VIDEO:
                assertTrue(String.format(""AF mode %s: Unexpected AF state %s"",
                                StaticMetadata.getAfModeName(afMode),
                                StaticMetadata.AF_STATE_NAMES[afState]),
                        afState == CaptureResult.CONTROL_AF_STATE_FOCUSED_LOCKED ||
                        afState == CaptureResult.CONTROL_AF_STATE_NOT_FOCUSED_LOCKED);
                focusComplete = true;
                break;
            default:
                fail(""Unexpected AF mode: "" + StaticMetadata.getAfModeName(afMode));
        }
        return focusComplete;
    }

    private boolean verifyAeSequence(int aeState, boolean precaptureComplete) {
        if (precaptureComplete) {
            assertTrue(""Precapture state seen after convergence"",
                    aeState != CaptureResult.CONTROL_AE_STATE_PRECAPTURE);
            return precaptureComplete;
        }
        if (VERBOSE) {
            Log.v(TAG, String.format(""AE state: %s"", StaticMetadata.AE_STATE_NAMES[aeState]));
        }
        switch (aeState) {
            case CaptureResult.CONTROL_AE_STATE_PRECAPTURE:
                // scan still continuing
                break;
            case CaptureResult.CONTROL_AE_STATE_CONVERGED:
            case CaptureResult.CONTROL_AE_STATE_FLASH_REQUIRED:
                // completed
                precaptureComplete = true;
                break;
            default:
                fail(String.format(""Precapture sequence transitioned to ""
                                + ""state %s incorrectly!"", StaticMetadata.AE_STATE_NAMES[aeState]));
                break;
        }
        return precaptureComplete;
    }

    /**
     * Test for making sure that all expected mandatory stream combinations are present and
     * advertised accordingly.
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RobustnessTest"	"testVerifyMandatoryOutputCombinationTables"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RobustnessTest.java"	""	"public void testVerifyMandatoryOutputCombinationTables() throws Exception {
       final int[][] LEGACY_COMBINATIONS = {
            // Simple preview, GPU video processing, or no-preview video recording
            {PRIV, MAXIMUM},
            // No-viewfinder still image capture
            {JPEG, MAXIMUM},
            // In-application video/image processing
            {YUV,  MAXIMUM},
            // Standard still imaging.
            {PRIV, PREVIEW,  JPEG, MAXIMUM},
            // In-app processing plus still capture.
            {YUV,  PREVIEW,  JPEG, MAXIMUM},
            // Standard recording.
            {PRIV, PREVIEW,  PRIV, PREVIEW},
            // Preview plus in-app processing.
            {PRIV, PREVIEW,  YUV,  PREVIEW},
            // Still capture plus in-app processing.
            {PRIV, PREVIEW,  YUV,  PREVIEW,  JPEG, MAXIMUM}
        };

        final int[][] LIMITED_COMBINATIONS = {
            // High-resolution video recording with preview.
            {PRIV, PREVIEW,  PRIV, RECORD },
            // High-resolution in-app video processing with preview.
            {PRIV, PREVIEW,  YUV , RECORD },
            // Two-input in-app video processing.
            {YUV , PREVIEW,  YUV , RECORD },
            // High-resolution recording with video snapshot.
            {PRIV, PREVIEW,  PRIV, RECORD,   JPEG, RECORD  },
            // High-resolution in-app processing with video snapshot.
            {PRIV, PREVIEW,  YUV,  RECORD,   JPEG, RECORD  },
            // Two-input in-app processing with still capture.
            {YUV , PREVIEW,  YUV,  PREVIEW,  JPEG, MAXIMUM }
        };

        final int[][] BURST_COMBINATIONS = {
            // Maximum-resolution GPU processing with preview.
            {PRIV, PREVIEW,  PRIV, MAXIMUM },
            // Maximum-resolution in-app processing with preview.
            {PRIV, PREVIEW,  YUV,  MAXIMUM },
            // Maximum-resolution two-input in-app processing.
            {YUV,  PREVIEW,  YUV,  MAXIMUM },
        };

        final int[][] FULL_COMBINATIONS = {
            // Video recording with maximum-size video snapshot.
            {PRIV, PREVIEW,  PRIV, PREVIEW,  JPEG, MAXIMUM },
            // Standard video recording plus maximum-resolution in-app processing.
            {YUV,  VGA,      PRIV, PREVIEW,  YUV,  MAXIMUM },
            // Preview plus two-input maximum-resolution in-app processing.
            {YUV,  VGA,      YUV,  PREVIEW,  YUV,  MAXIMUM }
        };

        final int[][] RAW_COMBINATIONS = {
            // No-preview DNG capture.
            {RAW,  MAXIMUM },
            // Standard DNG capture.
            {PRIV, PREVIEW,  RAW,  MAXIMUM },
            // In-app processing plus DNG capture.
            {YUV,  PREVIEW,  RAW,  MAXIMUM },
            // Video recording with DNG capture.
            {PRIV, PREVIEW,  PRIV, PREVIEW,  RAW, MAXIMUM},
            // Preview with in-app processing and DNG capture.
            {PRIV, PREVIEW,  YUV,  PREVIEW,  RAW, MAXIMUM},
            // Two-input in-app processing plus DNG capture.
            {YUV,  PREVIEW,  YUV,  PREVIEW,  RAW, MAXIMUM},
            // Still capture with simultaneous JPEG and DNG.
            {PRIV, PREVIEW,  JPEG, MAXIMUM,  RAW, MAXIMUM},
            // In-app processing with simultaneous JPEG and DNG.
            {YUV,  PREVIEW,  JPEG, MAXIMUM,  RAW, MAXIMUM}
        };

        final int[][] LEVEL_3_COMBINATIONS = {
            // In-app viewfinder analysis with dynamic selection of output format
            {PRIV, PREVIEW, PRIV, VGA, YUV, MAXIMUM, RAW, MAXIMUM},
            // In-app viewfinder analysis with dynamic selection of output format
            {PRIV, PREVIEW, PRIV, VGA, JPEG, MAXIMUM, RAW, MAXIMUM}
        };

        final int[][][] TABLES =
                { LEGACY_COMBINATIONS, LIMITED_COMBINATIONS, BURST_COMBINATIONS, FULL_COMBINATIONS,
                  RAW_COMBINATIONS, LEVEL_3_COMBINATIONS };

        validityCheckConfigurationTables(TABLES);

        for (String id : mCameraIdsUnderTest) {
            openDevice(id);
            MandatoryStreamCombination[] combinations =
                    mStaticInfo.getCharacteristics().get(
                            CameraCharacteristics.SCALER_MANDATORY_STREAM_COMBINATIONS);
            if ((combinations == null) || (combinations.length == 0)) {
                Log.i(TAG, ""No mandatory stream combinations for camera: "" + id + "" skip test"");
                closeDevice(id);
                continue;
            }

            MaxStreamSizes maxSizes = new MaxStreamSizes(mStaticInfo, id, mContext);
            try {
                if (mStaticInfo.isColorOutputSupported()) {
                    for (int[] c : LEGACY_COMBINATIONS) {
                        assertTrue(String.format(""Expected static stream combination: %s not "" +
                                    ""found among the available mandatory combinations"",
                                    maxSizes.combinationToString(c)),
                                isMandatoryCombinationAvailable(c, maxSizes, combinations));
                    }
                }

                if (!mStaticInfo.isHardwareLevelLegacy()) {
                    if (mStaticInfo.isColorOutputSupported()) {
                        for (int[] c : LIMITED_COMBINATIONS) {
                            assertTrue(String.format(""Expected static stream combination: %s not "" +
                                        ""found among the available mandatory combinations"",
                                        maxSizes.combinationToString(c)),
                                    isMandatoryCombinationAvailable(c, maxSizes, combinations));
                        }
                    }

                    if (mStaticInfo.isCapabilitySupported(
                            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_BURST_CAPTURE)) {
                        for (int[] c : BURST_COMBINATIONS) {
                            assertTrue(String.format(""Expected static stream combination: %s not "" +
                                        ""found among the available mandatory combinations"",
                                        maxSizes.combinationToString(c)),
                                    isMandatoryCombinationAvailable(c, maxSizes, combinations));
                        }
                    }

                    if (mStaticInfo.isHardwareLevelAtLeastFull()) {
                        for (int[] c : FULL_COMBINATIONS) {
                            assertTrue(String.format(""Expected static stream combination: %s not "" +
                                        ""found among the available mandatory combinations"",
                                        maxSizes.combinationToString(c)),
                                    isMandatoryCombinationAvailable(c, maxSizes, combinations));
                        }
                    }

                    if (mStaticInfo.isCapabilitySupported(
                            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                        for (int[] c : RAW_COMBINATIONS) {
                            assertTrue(String.format(""Expected static stream combination: %s not "" +
                                        ""found among the available mandatory combinations"",
                                        maxSizes.combinationToString(c)),
                                    isMandatoryCombinationAvailable(c, maxSizes, combinations));
                        }
                    }

                    if (mStaticInfo.isHardwareLevelAtLeast(
                            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_3)) {
                        for (int[] c: LEVEL_3_COMBINATIONS) {
                            assertTrue(String.format(""Expected static stream combination: %s not "" +
                                        ""found among the available mandatory combinations"",
                                        maxSizes.combinationToString(c)),
                                    isMandatoryCombinationAvailable(c, maxSizes, combinations));
                        }
                    }
                }
            } finally {
                closeDevice(id);
            }
        }
    }

    /**
     * Test for making sure that all expected reprocessable mandatory stream combinations are
     * present and advertised accordingly.
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RobustnessTest"	"testVerifyReprocessMandatoryOutputCombinationTables"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RobustnessTest.java"	""	"public void testVerifyReprocessMandatoryOutputCombinationTables() throws Exception {
        final int[][] LIMITED_COMBINATIONS = {
            // Input           Outputs
            {PRIV, MAXIMUM,    JPEG, MAXIMUM},
            {YUV , MAXIMUM,    JPEG, MAXIMUM},
            {PRIV, MAXIMUM,    PRIV, PREVIEW, JPEG, MAXIMUM},
            {YUV , MAXIMUM,    PRIV, PREVIEW, JPEG, MAXIMUM},
            {PRIV, MAXIMUM,    YUV , PREVIEW, JPEG, MAXIMUM},
            {YUV , MAXIMUM,    YUV , PREVIEW, JPEG, MAXIMUM},
            {PRIV, MAXIMUM,    YUV , PREVIEW, YUV , PREVIEW, JPEG, MAXIMUM},
            {YUV,  MAXIMUM,    YUV , PREVIEW, YUV , PREVIEW, JPEG, MAXIMUM},
        };

        final int[][] FULL_COMBINATIONS = {
            // Input           Outputs
            {YUV , MAXIMUM,    PRIV, PREVIEW},
            {YUV , MAXIMUM,    YUV , PREVIEW},
            {PRIV, MAXIMUM,    PRIV, PREVIEW, YUV , RECORD},
            {YUV , MAXIMUM,    PRIV, PREVIEW, YUV , RECORD},
            {PRIV, MAXIMUM,    PRIV, PREVIEW, YUV , MAXIMUM},
            {PRIV, MAXIMUM,    YUV , PREVIEW, YUV , MAXIMUM},
            {PRIV, MAXIMUM,    PRIV, PREVIEW, YUV , PREVIEW, JPEG, MAXIMUM},
            {YUV , MAXIMUM,    PRIV, PREVIEW, YUV , PREVIEW, JPEG, MAXIMUM},
        };

        final int[][] RAW_COMBINATIONS = {
            // Input           Outputs
            {PRIV, MAXIMUM,    YUV , PREVIEW, RAW , MAXIMUM},
            {YUV , MAXIMUM,    YUV , PREVIEW, RAW , MAXIMUM},
            {PRIV, MAXIMUM,    PRIV, PREVIEW, YUV , PREVIEW, RAW , MAXIMUM},
            {YUV , MAXIMUM,    PRIV, PREVIEW, YUV , PREVIEW, RAW , MAXIMUM},
            {PRIV, MAXIMUM,    YUV , PREVIEW, YUV , PREVIEW, RAW , MAXIMUM},
            {YUV , MAXIMUM,    YUV , PREVIEW, YUV , PREVIEW, RAW , MAXIMUM},
            {PRIV, MAXIMUM,    PRIV, PREVIEW, JPEG, MAXIMUM, RAW , MAXIMUM},
            {YUV , MAXIMUM,    PRIV, PREVIEW, JPEG, MAXIMUM, RAW , MAXIMUM},
            {PRIV, MAXIMUM,    YUV , PREVIEW, JPEG, MAXIMUM, RAW , MAXIMUM},
            {YUV , MAXIMUM,    YUV , PREVIEW, JPEG, MAXIMUM, RAW , MAXIMUM},
        };

        final int[][] LEVEL_3_COMBINATIONS = {
            // Input          Outputs
            // In-app viewfinder analysis with YUV->YUV ZSL and RAW
            {YUV , MAXIMUM,   PRIV, PREVIEW, PRIV, VGA, RAW, MAXIMUM},
            // In-app viewfinder analysis with PRIV->JPEG ZSL and RAW
            {PRIV, MAXIMUM,   PRIV, PREVIEW, PRIV, VGA, RAW, MAXIMUM, JPEG, MAXIMUM},
            // In-app viewfinder analysis with YUV->JPEG ZSL and RAW
            {YUV , MAXIMUM,   PRIV, PREVIEW, PRIV, VGA, RAW, MAXIMUM, JPEG, MAXIMUM},
        };

        final int[][][] TABLES =
                { LIMITED_COMBINATIONS, FULL_COMBINATIONS, RAW_COMBINATIONS, LEVEL_3_COMBINATIONS };

        validityCheckConfigurationTables(TABLES);

        for (String id : mCameraIdsUnderTest) {
            openDevice(id);
            MandatoryStreamCombination[] cs = mStaticInfo.getCharacteristics().get(
                    CameraCharacteristics.SCALER_MANDATORY_STREAM_COMBINATIONS);
            if ((cs == null) || (cs.length == 0)) {
                Log.i(TAG, ""No mandatory stream combinations for camera: "" + id + "" skip test"");
                closeDevice(id);
                continue;
            }

            boolean supportYuvReprocess = mStaticInfo.isCapabilitySupported(
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING);
            boolean supportOpaqueReprocess = mStaticInfo.isCapabilitySupported(
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING);
            if (!supportYuvReprocess && !supportOpaqueReprocess) {
                Log.i(TAG, ""No reprocess support for camera: "" + id + "" skip test"");
                closeDevice(id);
                continue;
            }

            MaxStreamSizes maxSizes = new MaxStreamSizes(mStaticInfo, id, mContext);
            try {
                for (int[] c : LIMITED_COMBINATIONS) {
                    assertTrue(String.format(""Expected static reprocessable stream combination:"" +
                                ""%s not found among the available mandatory combinations"",
                                maxSizes.reprocessCombinationToString(c)),
                            isMandatoryCombinationAvailable(c, maxSizes, /*isInput*/ true, cs));
                }

                if (mStaticInfo.isHardwareLevelAtLeastFull()) {
                    for (int[] c : FULL_COMBINATIONS) {
                        assertTrue(String.format(
                                    ""Expected static reprocessable stream combination:"" +
                                    ""%s not found among the available mandatory combinations"",
                                    maxSizes.reprocessCombinationToString(c)),
                                isMandatoryCombinationAvailable(c, maxSizes, /*isInput*/ true, cs));
                    }
                }

                if (mStaticInfo.isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                    for (int[] c : RAW_COMBINATIONS) {
                        assertTrue(String.format(
                                    ""Expected static reprocessable stream combination:"" +
                                    ""%s not found among the available mandatory combinations"",
                                    maxSizes.reprocessCombinationToString(c)),
                                isMandatoryCombinationAvailable(c, maxSizes, /*isInput*/ true, cs));
                    }
                }

                if (mStaticInfo.isHardwareLevelAtLeast(
                            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_3)) {
                    for (int[] c : LEVEL_3_COMBINATIONS) {
                        assertTrue(String.format(
                                    ""Expected static reprocessable stream combination:"" +
                                    ""%s not found among the available mandatory combinations"",
                                    maxSizes.reprocessCombinationToString(c)),
                                isMandatoryCombinationAvailable(c, maxSizes, /*isInput*/ true, cs));
                    }
                }
            } finally {
                closeDevice(id);
            }
        }
    }

    private boolean isMandatoryCombinationAvailable(final int[] combination,
            final MaxStreamSizes maxSizes,
            final MandatoryStreamCombination[] availableCombinations) {
        return isMandatoryCombinationAvailable(combination, maxSizes, /*isInput*/ false,
                availableCombinations);
    }

    private boolean isMandatoryCombinationAvailable(final int[] combination,
            final MaxStreamSizes maxSizes, boolean isInput,
            final MandatoryStreamCombination[] availableCombinations) {
        boolean supportYuvReprocess = mStaticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING);
        boolean supportOpaqueReprocess = mStaticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING);
        // Static combinations to be verified can be composed of multiple entries
        // that have the following layout (format, size). In case ""isInput"" is set,
        // the first stream configuration entry will contain the input format and size
        // as well as the first matching output.
        int streamCount = combination.length / 2;

        List<Pair<Pair<Integer, Boolean>, Size>> currentCombination =
                new ArrayList<Pair<Pair<Integer, Boolean>, Size>>(streamCount);
        for (int i = 0; i < combination.length; i += 2) {
            if (isInput && (i == 0)) {
                // Skip the combination if the format is not supported for reprocessing.
                if ((combination[i] == YUV && !supportYuvReprocess) ||
                        (combination[i] == PRIV && !supportOpaqueReprocess)) {
                    return true;
                }
                Size sz = maxSizes.getMaxInputSizeForFormat(combination[i]);
                currentCombination.add(Pair.create(Pair.create(new Integer(combination[i]),
                            new Boolean(true)), sz));
                currentCombination.add(Pair.create(Pair.create(new Integer(combination[i]),
                            new Boolean(false)), sz));
            } else {
                Size sz = maxSizes.getOutputSizeForFormat(combination[i], combination[i+1]);
                currentCombination.add(Pair.create(Pair.create(new Integer(combination[i]),
                            new Boolean(false)), sz));
            }
        }

        for (MandatoryStreamCombination c : availableCombinations) {
            List<MandatoryStreamInformation> streamInfoList = c.getStreamsInformation();
            if ((streamInfoList.size() == currentCombination.size()) &&
                    (isInput == c.isReprocessable())) {
                ArrayList<Pair<Pair<Integer, Boolean>, Size>> expected =
                        new ArrayList<Pair<Pair<Integer, Boolean>, Size>>(currentCombination);

                for (MandatoryStreamInformation streamInfo : streamInfoList) {
                    Size maxSize = CameraTestUtils.getMaxSize(
                            streamInfo.getAvailableSizes().toArray(new Size[0]));
                    Pair p = Pair.create(Pair.create(new Integer(streamInfo.getFormat()),
                            new Boolean(streamInfo.isInput())), maxSize);
                    if (expected.contains(p)) {
                        expected.remove(p);
                    }
                }

                if (expected.isEmpty()) {
                    return true;
                }
            }
        }

        return false;
    }

    /**
     * Verify correctness of the configuration tables.
     */
    private void validityCheckConfigurationTables(final int[][][] tables) throws Exception {
        int tableIdx = 0;
        for (int[][] table : tables) {
            int rowIdx = 0;
            for (int[] row : table) {
                assertTrue(String.format(""Odd number of entries for table %d row %d: %s "",
                                tableIdx, rowIdx, Arrays.toString(row)),
                        (row.length % 2) == 0);
                for (int i = 0; i < row.length; i += 2) {
                    int format = row[i];
                    int maxSize = row[i + 1];
                    assertTrue(String.format(""table %d row %d index %d format not valid: %d"",
                                    tableIdx, rowIdx, i, format),
                            format == PRIV || format == JPEG || format == YUV || format == RAW);
                    assertTrue(String.format(""table %d row %d index %d max size not valid: %d"",
                                    tableIdx, rowIdx, i + 1, maxSize),
                            maxSize == PREVIEW || maxSize == RECORD ||
                            maxSize == MAXIMUM || maxSize == VGA);
                }
                rowIdx++;
            }
            tableIdx++;
        }
    }

    /**
     * Simple holder for resolutions to use for different camera outputs and size limits.
     */
    static class MaxStreamSizes {
        // Format shorthands
        static final int PRIV = ImageFormat.PRIVATE;
        static final int JPEG = ImageFormat.JPEG;
        static final int YUV  = ImageFormat.YUV_420_888;
        static final int RAW  = ImageFormat.RAW_SENSOR;
        static final int Y8   = ImageFormat.Y8;
        static final int HEIC = ImageFormat.HEIC;

        // Max resolution indices
        static final int PREVIEW = 0;
        static final int RECORD  = 1;
        static final int MAXIMUM = 2;
        static final int VGA = 3;
        static final int VGA_FULL_FOV = 4;
        static final int MAX_30FPS = 5;
        static final int RESOLUTION_COUNT = 6;

        static final long FRAME_DURATION_30FPS_NSEC = (long) 1e9 / 30;

        public MaxStreamSizes(StaticMetadata sm, String cameraId, Context context) {
            Size[] privSizes = sm.getAvailableSizesForFormatChecked(ImageFormat.PRIVATE,
                    StaticMetadata.StreamDirection.Output, /*fastSizes*/true, /*slowSizes*/false);
            Size[] yuvSizes = sm.getAvailableSizesForFormatChecked(ImageFormat.YUV_420_888,
                    StaticMetadata.StreamDirection.Output, /*fastSizes*/true, /*slowSizes*/false);

            Size[] y8Sizes = sm.getAvailableSizesForFormatChecked(ImageFormat.Y8,
                    StaticMetadata.StreamDirection.Output, /*fastSizes*/true, /*slowSizes*/false);
            Size[] jpegSizes = sm.getAvailableSizesForFormatChecked(ImageFormat.JPEG,
                    StaticMetadata.StreamDirection.Output, /*fastSizes*/true, /*slowSizes*/false);
            Size[] rawSizes = sm.getAvailableSizesForFormatChecked(ImageFormat.RAW_SENSOR,
                    StaticMetadata.StreamDirection.Output, /*fastSizes*/true, /*slowSizes*/false);
            Size[] heicSizes = sm.getAvailableSizesForFormatChecked(ImageFormat.HEIC,
                    StaticMetadata.StreamDirection.Output, /*fastSizes*/true, /*slowSizes*/false);

            Size maxPreviewSize = getMaxPreviewSize(context, cameraId);

            maxRawSize = (rawSizes.length != 0) ? CameraTestUtils.getMaxSize(rawSizes) : null;

            StreamConfigurationMap configs = sm.getCharacteristics().get(
                    CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            if (sm.isColorOutputSupported()) {
                maxPrivSizes[PREVIEW] = getMaxSize(privSizes, maxPreviewSize);
                maxYuvSizes[PREVIEW]  = getMaxSize(yuvSizes, maxPreviewSize);
                maxJpegSizes[PREVIEW] = getMaxSize(jpegSizes, maxPreviewSize);

                if (sm.isExternalCamera()) {
                    maxPrivSizes[RECORD] = getMaxExternalRecordingSize(cameraId, configs);
                    maxYuvSizes[RECORD]  = getMaxExternalRecordingSize(cameraId, configs);
                    maxJpegSizes[RECORD] = getMaxExternalRecordingSize(cameraId, configs);
                } else {
                    maxPrivSizes[RECORD] = getMaxRecordingSize(cameraId);
                    maxYuvSizes[RECORD]  = getMaxRecordingSize(cameraId);
                    maxJpegSizes[RECORD] = getMaxRecordingSize(cameraId);
                }

                maxPrivSizes[MAXIMUM] = CameraTestUtils.getMaxSize(privSizes);
                maxYuvSizes[MAXIMUM] = CameraTestUtils.getMaxSize(yuvSizes);
                maxJpegSizes[MAXIMUM] = CameraTestUtils.getMaxSize(jpegSizes);

                // Must always be supported, add unconditionally
                final Size vgaSize = new Size(640, 480);
                maxPrivSizes[VGA] = vgaSize;
                maxYuvSizes[VGA] = vgaSize;
                maxJpegSizes[VGA] = vgaSize;

                if (sm.isMonochromeWithY8()) {
                    maxY8Sizes[PREVIEW]  = getMaxSize(y8Sizes, maxPreviewSize);
                    if (sm.isExternalCamera()) {
                        maxY8Sizes[RECORD]  = getMaxExternalRecordingSize(cameraId, configs);
                    } else {
                        maxY8Sizes[RECORD]  = getMaxRecordingSize(cameraId);
                    }
                    maxY8Sizes[MAXIMUM] = CameraTestUtils.getMaxSize(y8Sizes);
                    maxY8Sizes[VGA] = vgaSize;
                }

                if (sm.isHeicSupported()) {
                    maxHeicSizes[PREVIEW] = getMaxSize(heicSizes, maxPreviewSize);
                    maxHeicSizes[RECORD] = getMaxRecordingSize(cameraId);
                    maxHeicSizes[MAXIMUM] = CameraTestUtils.getMaxSize(heicSizes);
                    maxHeicSizes[VGA] = vgaSize;
                }
            }
            if (sm.isColorOutputSupported() && !sm.isHardwareLevelLegacy()) {
                // VGA resolution, but with aspect ratio matching full res FOV
                float fullFovAspect = maxYuvSizes[MAXIMUM].getWidth() /
                    (float) maxYuvSizes[MAXIMUM].getHeight();
                Size vgaFullFovSize = new Size(640, (int) (640 / fullFovAspect));

                maxPrivSizes[VGA_FULL_FOV] = vgaFullFovSize;
                maxYuvSizes[VGA_FULL_FOV] = vgaFullFovSize;
                maxJpegSizes[VGA_FULL_FOV] = vgaFullFovSize;
                if (sm.isMonochromeWithY8()) {
                    maxY8Sizes[VGA_FULL_FOV] = vgaFullFovSize;
                }

                // Max resolution that runs at 30fps

                Size maxPriv30fpsSize = null;
                Size maxYuv30fpsSize = null;
                Size maxY830fpsSize = null;
                Size maxJpeg30fpsSize = null;
                Comparator<Size> comparator = new SizeComparator();
                for (Map.Entry<Size, Long> e :
                             sm.getAvailableMinFrameDurationsForFormatChecked(ImageFormat.PRIVATE).
                             entrySet()) {
                    Size s = e.getKey();
                    Long minDuration = e.getValue();
                    Log.d(TAG, String.format(""Priv Size: %s, duration %d limit %d"", s, minDuration,
                                FRAME_DURATION_30FPS_NSEC));
                    if (minDuration <= FRAME_DURATION_30FPS_NSEC) {
                        if (maxPriv30fpsSize == null ||
                                comparator.compare(maxPriv30fpsSize, s) < 0) {
                            maxPriv30fpsSize = s;
                        }
                    }
                }
                assertTrue(""No PRIVATE resolution available at 30fps!"", maxPriv30fpsSize != null);

                for (Map.Entry<Size, Long> e :
                             sm.getAvailableMinFrameDurationsForFormatChecked(
                                     ImageFormat.YUV_420_888).
                             entrySet()) {
                    Size s = e.getKey();
                    Long minDuration = e.getValue();
                    Log.d(TAG, String.format(""YUV Size: %s, duration %d limit %d"", s, minDuration,
                                FRAME_DURATION_30FPS_NSEC));
                    if (minDuration <= FRAME_DURATION_30FPS_NSEC) {
                        if (maxYuv30fpsSize == null ||
                                comparator.compare(maxYuv30fpsSize, s) < 0) {
                            maxYuv30fpsSize = s;
                        }
                    }
                }
                assertTrue(""No YUV_420_888 resolution available at 30fps!"",
                        maxYuv30fpsSize != null);

                if (sm.isMonochromeWithY8()) {
                    for (Map.Entry<Size, Long> e :
                                 sm.getAvailableMinFrameDurationsForFormatChecked(
                                         ImageFormat.Y8).
                                 entrySet()) {
                        Size s = e.getKey();
                        Long minDuration = e.getValue();
                        Log.d(TAG, String.format(""Y8 Size: %s, duration %d limit %d"",
                                s, minDuration, FRAME_DURATION_30FPS_NSEC));
                        if (minDuration <= FRAME_DURATION_30FPS_NSEC) {
                            if (maxY830fpsSize == null ||
                                    comparator.compare(maxY830fpsSize, s) < 0) {
                                maxY830fpsSize = s;
                            }
                        }
                    }
                    assertTrue(""No Y8 resolution available at 30fps!"", maxY830fpsSize != null);
                }

                for (Map.Entry<Size, Long> e :
                             sm.getAvailableMinFrameDurationsForFormatChecked(ImageFormat.JPEG).
                             entrySet()) {
                    Size s = e.getKey();
                    Long minDuration = e.getValue();
                    Log.d(TAG, String.format(""JPEG Size: %s, duration %d limit %d"", s, minDuration,
                                FRAME_DURATION_30FPS_NSEC));
                    if (minDuration <= FRAME_DURATION_30FPS_NSEC) {
                        if (maxJpeg30fpsSize == null ||
                                comparator.compare(maxJpeg30fpsSize, s) < 0) {
                            maxJpeg30fpsSize = s;
                        }
                    }
                }
                assertTrue(""No JPEG resolution available at 30fps!"", maxJpeg30fpsSize != null);

                maxPrivSizes[MAX_30FPS] = maxPriv30fpsSize;
                maxYuvSizes[MAX_30FPS] = maxYuv30fpsSize;
                maxY8Sizes[MAX_30FPS] = maxY830fpsSize;
                maxJpegSizes[MAX_30FPS] = maxJpeg30fpsSize;
            }

            Size[] privInputSizes = configs.getInputSizes(ImageFormat.PRIVATE);
            maxInputPrivSize = privInputSizes != null ?
                    CameraTestUtils.getMaxSize(privInputSizes) : null;
            Size[] yuvInputSizes = configs.getInputSizes(ImageFormat.YUV_420_888);
            maxInputYuvSize = yuvInputSizes != null ?
                    CameraTestUtils.getMaxSize(yuvInputSizes) : null;
            Size[] y8InputSizes = configs.getInputSizes(ImageFormat.Y8);
            maxInputY8Size = y8InputSizes != null ?
                    CameraTestUtils.getMaxSize(y8InputSizes) : null;
        }

        private final Size[] maxPrivSizes = new Size[RESOLUTION_COUNT];
        private final Size[] maxJpegSizes = new Size[RESOLUTION_COUNT];
        private final Size[] maxYuvSizes = new Size[RESOLUTION_COUNT];
        private final Size[] maxY8Sizes = new Size[RESOLUTION_COUNT];
        private final Size[] maxHeicSizes = new Size[RESOLUTION_COUNT];
        private final Size maxRawSize;
        // TODO: support non maximum reprocess input.
        private final Size maxInputPrivSize;
        private final Size maxInputYuvSize;
        private final Size maxInputY8Size;

        public final Size getOutputSizeForFormat(int format, int resolutionIndex) {
            if (resolutionIndex >= RESOLUTION_COUNT) {
                return new Size(0, 0);
            }

            switch (format) {
                case PRIV:
                    return maxPrivSizes[resolutionIndex];
                case YUV:
                    return maxYuvSizes[resolutionIndex];
                case JPEG:
                    return maxJpegSizes[resolutionIndex];
                case Y8:
                    return maxY8Sizes[resolutionIndex];
                case HEIC:
                    return maxHeicSizes[resolutionIndex];
                case RAW:
                    return maxRawSize;
                default:
                    return new Size(0, 0);
            }
        }

        public final Size getMaxInputSizeForFormat(int format) {
            switch (format) {
                case PRIV:
                    return maxInputPrivSize;
                case YUV:
                    return maxInputYuvSize;
                case Y8:
                    return maxInputY8Size;
                default:
                    return new Size(0, 0);
            }
        }

        static public String combinationToString(int[] combination) {
            StringBuilder b = new StringBuilder(""{ "");
            for (int i = 0; i < combination.length; i += 2) {
                int format = combination[i];
                int sizeLimit = combination[i + 1];

                appendFormatSize(b, format, sizeLimit);
                b.append("" "");
            }
            b.append(""}"");
            return b.toString();
        }

        static public String reprocessCombinationToString(int[] reprocessCombination) {
            // reprocessConfig[0..1] is the input configuration
            StringBuilder b = new StringBuilder(""Input: "");
            appendFormatSize(b, reprocessCombination[0], reprocessCombination[1]);

            // reprocessCombnation[0..1] is also output combination to be captured as reprocess
            // input.
            b.append("", Outputs: { "");
            for (int i = 0; i < reprocessCombination.length; i += 2) {
                int format = reprocessCombination[i];
                int sizeLimit = reprocessCombination[i + 1];

                appendFormatSize(b, format, sizeLimit);
                b.append("" "");
            }
            b.append(""}"");
            return b.toString();
        }

        static private void appendFormatSize(StringBuilder b, int format, int Size) {
            switch (format) {
                case PRIV:
                    b.append(""[PRIV, "");
                    break;
                case JPEG:
                    b.append(""[JPEG, "");
                    break;
                case YUV:
                    b.append(""[YUV, "");
                    break;
                case Y8:
                    b.append(""[Y8, "");
                    break;
                case RAW:
                    b.append(""[RAW, "");
                    break;
                default:
                    b.append(""[UNK, "");
                    break;
            }

            switch (Size) {
                case PREVIEW:
                    b.append(""PREVIEW]"");
                    break;
                case RECORD:
                    b.append(""RECORD]"");
                    break;
                case MAXIMUM:
                    b.append(""MAXIMUM]"");
                    break;
                case VGA:
                    b.append(""VGA]"");
                    break;
                case VGA_FULL_FOV:
                    b.append(""VGA_FULL_FOV]"");
                    break;
                case MAX_30FPS:
                    b.append(""MAX_30FPS]"");
                    break;
                default:
                    b.append(""UNK]"");
                    break;
            }
        }
    }

    private static Size getMaxRecordingSize(String cameraId) {
        int id = Integer.valueOf(cameraId);

        int quality =
                CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_2160P) ?
                    CamcorderProfile.QUALITY_2160P :
                CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_1080P) ?
                    CamcorderProfile.QUALITY_1080P :
                CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_720P) ?
                    CamcorderProfile.QUALITY_720P :
                CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_480P) ?
                    CamcorderProfile.QUALITY_480P :
                CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_QVGA) ?
                    CamcorderProfile.QUALITY_QVGA :
                CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_CIF) ?
                    CamcorderProfile.QUALITY_CIF :
                CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_QCIF) ?
                    CamcorderProfile.QUALITY_QCIF :
                    -1;

        assertTrue(""No recording supported for camera id "" + cameraId, quality != -1);

        CamcorderProfile maxProfile = CamcorderProfile.get(id, quality);
        return new Size(maxProfile.videoFrameWidth, maxProfile.videoFrameHeight);
    }

    private static Size getMaxExternalRecordingSize(
            String cameraId, StreamConfigurationMap config) {
        final Size FULLHD = new Size(1920, 1080);

        Size[] videoSizeArr = config.getOutputSizes(android.media.MediaRecorder.class);
        List<Size> sizes = new ArrayList<Size>();
        for (Size sz: videoSizeArr) {
            if (sz.getWidth() <= FULLHD.getWidth() && sz.getHeight() <= FULLHD.getHeight()) {
                sizes.add(sz);
            }
        }
        List<Size> videoSizes = getAscendingOrderSizes(sizes, /*ascending*/false);
        for (Size sz : videoSizes) {
            long minFrameDuration = config.getOutputMinFrameDuration(
                    android.media.MediaRecorder.class, sz);
            // Give some margin for rounding error
            if (minFrameDuration > (1e9 / 30.1)) {
                Log.i(TAG, ""External camera "" + cameraId + "" has max video size:"" + sz);
                return sz;
            }
        }
        fail(""Camera "" + cameraId + "" does not support any 30fps video output"");
        return FULLHD; // doesn't matter what size is returned here
    }

    /**
     * Get maximum size in list that's equal or smaller to than the bound.
     * Returns null if no size is smaller than or equal to the bound.
     */
    private static Size getMaxSize(Size[] sizes, Size bound) {
        if (sizes == null || sizes.length == 0) {
            throw new IllegalArgumentException(""sizes was empty"");
        }

        Size sz = null;
        for (Size size : sizes) {
            if (size.getWidth() <= bound.getWidth() && size.getHeight() <= bound.getHeight()) {

                if (sz == null) {
                    sz = size;
                } else {
                    long curArea = sz.getWidth() * (long) sz.getHeight();
                    long newArea = size.getWidth() * (long) size.getHeight();
                    if ( newArea > curArea ) {
                        sz = size;
                    }
                }
            }
        }

        assertTrue(""No size under bound found: "" + Arrays.toString(sizes) + "" bound "" + bound,
                sz != null);

        return sz;
    }

    private static Size getMaxPreviewSize(Context context, String cameraId) {
        try {
            WindowManager windowManager =
                (WindowManager) context.getSystemService(Context.WINDOW_SERVICE);
            Display display = windowManager.getDefaultDisplay();

            int width = display.getWidth();
            int height = display.getHeight();

            if (height > width) {
                height = width;
                width = display.getHeight();
            }

            CameraManager camMgr =
                (CameraManager) context.getSystemService(Context.CAMERA_SERVICE);
            List<Size> orderedPreviewSizes = CameraTestUtils.getSupportedPreviewSizes(
                cameraId, camMgr, PREVIEW_SIZE_BOUND);

            if (orderedPreviewSizes != null) {
                for (Size size : orderedPreviewSizes) {
                    if (width >= size.getWidth() &&
                        height >= size.getHeight())
                        return size;
                }
            }
        } catch (Exception e) {
            Log.e(TAG, ""getMaxPreviewSize Failed. ""+e.toString());
        }
        return PREVIEW_SIZE_BOUND;
    }
}"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.helpers.StaticMetadat"	"getCharacteristics"	""	"/home/gpoor/cts-12-source/cts/tests/camera/utils/src/android/hardware/camera2/cts/helpers/StaticMetadata.java"	""	"public void test/*
 *.
 */

package android.hardware.camera2.cts.helpers;

import android.graphics.Rect;
import android.graphics.ImageFormat;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraCharacteristics.Key;
import android.hardware.camera2.CameraMetadata;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.cts.CameraTestUtils;
import android.hardware.camera2.params.StreamConfigurationMap;
import android.hardware.camera2.params.Capability;
import android.util.Range;
import android.util.Size;
import android.util.Log;
import android.util.Rational;

import junit.framework.Assert;

import java.lang.reflect.Array;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import static android.hardware.camera2.cts.helpers.AssertHelpers.*;
import static android.hardware.camera2.CameraCharacteristics.*;

/**
 * Helpers to get common static info out of the camera.
 *
 * <p>Avoid boiler plate by putting repetitive get/set patterns in this class.</p>
 *
 * <p>Attempt to be durable against the camera device having bad or missing metadata
 * by providing reasonable defaults and logging warnings when that happens.</p>
 */
public class StaticMetadata {

    private static final String TAG = ""StaticMetadata"";
    private static final int IGNORE_SIZE_CHECK = -1;

    private static final long SENSOR_INFO_EXPOSURE_TIME_RANGE_MIN_AT_MOST = 100000L; // 100us
    private static final long SENSOR_INFO_EXPOSURE_TIME_RANGE_MAX_AT_LEAST = 100000000; // 100ms
    private static final int SENSOR_INFO_SENSITIVITY_RANGE_MIN_AT_MOST = 100;
    private static final int SENSOR_INFO_SENSITIVITY_RANGE_MAX_AT_LEAST = 800;
    private static final int STATISTICS_INFO_MAX_FACE_COUNT_MIN_AT_LEAST = 4;
    private static final int TONEMAP_MAX_CURVE_POINTS_AT_LEAST = 64;
    private static final int CONTROL_AE_COMPENSATION_RANGE_DEFAULT_MIN = -2;
    private static final int CONTROL_AE_COMPENSATION_RANGE_DEFAULT_MAX = 2;
    private static final Rational CONTROL_AE_COMPENSATION_STEP_DEFAULT = new Rational(1, 2);
    private static final byte REQUEST_PIPELINE_MAX_DEPTH_MAX = 8;
    private static final int MAX_REPROCESS_MAX_CAPTURE_STALL = 4;

    // TODO: Consider making this work across any metadata object, not just camera characteristics
    private final CameraCharacteristics mCharacteristics;
    private final CheckLevel mLevel;
    private final CameraErrorCollector mCollector;

    // Last defined capability enum, for iterating over all of them
    public static final int LAST_CAPABILITY_ENUM =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_REMOSAIC_REPROCESSING;

    // Access via getAeModeName() to account for vendor extensions
    public static final String[] AE_MODE_NAMES = new String[] {
        ""AE_MODE_OFF"",
        ""AE_MODE_ON"",
        ""AE_MODE_ON_AUTO_FLASH"",
        ""AE_MODE_ON_ALWAYS_FLASH"",
        ""AE_MODE_ON_AUTO_FLASH_REDEYE""
    };

    // Access via getAfModeName() to account for vendor extensions
    public static final String[] AF_MODE_NAMES = new String[] {
        ""AF_MODE_OFF"",
        ""AF_MODE_AUTO"",
        ""AF_MODE_MACRO"",
        ""AF_MODE_CONTINUOUS_VIDEO"",
        ""AF_MODE_CONTINUOUS_PICTURE"",
        ""AF_MODE_EDOF""
    };

    // Index with android.control.aeState
    public static final String[] AE_STATE_NAMES = new String[] {
        ""AE_STATE_INACTIVE"",
        ""AE_STATE_SEARCHING"",
        ""AE_STATE_CONVERGED"",
        ""AE_STATE_LOCKED"",
        ""AE_STATE_FLASH_REQUIRED"",
        ""AE_STATE_PRECAPTURE""
    };

    // Index with android.control.afState
    public static final String[] AF_STATE_NAMES = new String[] {
        ""AF_STATE_INACTIVE"",
        ""AF_STATE_PASSIVE_SCAN"",
        ""AF_STATE_PASSIVE_FOCUSED"",
        ""AF_STATE_ACTIVE_SCAN"",
        ""AF_STATE_FOCUSED_LOCKED"",
        ""AF_STATE_NOT_FOCUSED_LOCKED"",
        ""AF_STATE_PASSIVE_UNFOCUSED""
    };

    // Index with android.control.aePrecaptureTrigger
    public static final String[] AE_TRIGGER_NAMES = new String[] {
        ""AE_TRIGGER_IDLE"",
        ""AE_TRIGGER_START"",
        ""AE_TRIGGER_CANCEL""
    };

    // Index with android.control.afTrigger
    public static final String[] AF_TRIGGER_NAMES = new String[] {
        ""AF_TRIGGER_IDLE"",
        ""AF_TRIGGER_START"",
        ""AF_TRIGGER_CANCEL""
    };

    public enum CheckLevel {
        /** Only log warnings for metadata check failures. Execution continues. */
        WARN,
        /**
         * Use ErrorCollector to collect the metadata check failures, Execution
         * continues.
         */
        COLLECT,
        /** Assert the metadata check failures. Execution aborts. */
        ASSERT
    }

    /**
     * Construct a new StaticMetadata object.
     *
     *<p> Default constructor, only log warnings for the static metadata check failures</p>
     *
     * @param characteristics static info for a camera
     * @throws IllegalArgumentException if characteristics was null
     */
    public StaticMetadata(CameraCharacteristics characteristics) {
        this(characteristics, CheckLevel.WARN, /*collector*/null);
    }

    /**
     * Construct a new StaticMetadata object with {@link CameraErrorCollector}.
     * <p>
     * When level is not {@link CheckLevel.COLLECT}, the {@link CameraErrorCollector} will be
     * ignored, otherwise, it will be used to log the check failures.
     * </p>
     *
     * @param characteristics static info for a camera
     * @param collector The {@link CameraErrorCollector} used by this StaticMetadata
     * @throws IllegalArgumentException if characteristics or collector was null.
     */
    public StaticMetadata(CameraCharacteristics characteristics, CameraErrorCollector collector) {
        this(characteristics, CheckLevel.COLLECT, collector);
    }

    /**
     * Construct a new StaticMetadata object with {@link CheckLevel} and
     * {@link CameraErrorCollector}.
     * <p>
     * When level is not {@link CheckLevel.COLLECT}, the {@link CameraErrorCollector} will be
     * ignored, otherwise, it will be used to log the check failures.
     * </p>
     *
     * @param characteristics static info for a camera
     * @param level The {@link CheckLevel} of this StaticMetadata
     * @param collector The {@link CameraErrorCollector} used by this StaticMetadata
     * @throws IllegalArgumentException if characteristics was null or level was
     *         {@link CheckLevel.COLLECT} but collector was null.
     */
    public StaticMetadata(CameraCharacteristics characteristics, CheckLevel level,
            CameraErrorCollector collector) {
        if (characteristics == null) {
            throw new IllegalArgumentException(""characteristics was null"");
        }
        if (level == CheckLevel.COLLECT && collector == null) {
            throw new IllegalArgumentException(""collector must valid when COLLECT level is set"");
        }

        mCharacteristics = characteristics;
        mLevel = level;
        mCollector = collector;
    }

    /**
     * Get the CameraCharacteristics associated with this StaticMetadata.
     *
     * @return A non-null CameraCharacteristics object
     */
    public CameraCharacteristics getCharacteristics() {
        return mCharacteristics;
    }

    /**
     * Whether or not the hardware level reported by android.info.supportedHardwareLevel
     * is at least {@value CameraMetadata#INFO_SUPPORTED_HARDWARE_LEVEL_FULL}.
     *
     * <p>If the camera device is not reporting the hardwareLevel, this
     * will cause the test to fail.</p>
     *
     * @return {@code true} if the device is {@code FULL}, {@code false} otherwise.
     */
    public boolean isHardwareLevelAtLeastFull() {
        return isHardwareLevelAtLeast(CameraMetadata.INFO_SUPPORTED_HARDWARE_LEVEL_FULL);
    }

    /**
     * Whether or not the hardware level reported by android.info.supportedHardwareLevel is
     * at least the desired one (but could be higher)
     */
    public boolean isHardwareLevelAtLeast(int level) {
        final int[] sortedHwLevels = {
            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY,
            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL,
            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED,
            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_FULL,
            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_3
        };
        int deviceLevel = getHardwareLevelChecked();
        if (level == deviceLevel) {
            return true;
        }

        for (int sortedlevel : sortedHwLevels) {
            if (sortedlevel == level) {
                return true;
            } else if (sortedlevel == deviceLevel) {
                return false;
            }
        }
        Assert.fail(""Unknown hardwareLevel "" + level + "" and device hardware level "" + deviceLevel);
        return false;
    }

    /**
     * Whether or not the camera is an external camera. If so the hardware level
     * reported by android.info.supportedHardwareLevel is
     * {@value CameraMetadata#INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL}.
     *
     * <p>If the camera device is not reporting the hardwareLevel, this
     * will cause the test to fail.</p>
     *
     * @return {@code true} if the device is external, {@code false} otherwise.
     */
    public boolean isExternalCamera() {
        int deviceLevel = getHardwareLevelChecked();
        return deviceLevel == CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL;
    }

    /**
     * Whether or not the hardware level reported by android.info.supportedHardwareLevel
     * Return the supported hardware level of the device, or fail if no value is reported.
     *
     * @return the supported hardware level as a constant defined for
     *      {@link CameraCharacteristics#INFO_SUPPORTED_HARDWARE_LEVEL}.
     */
    public int getHardwareLevelChecked() {
        Integer hwLevel = getValueFromKeyNonNull(
                CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL);
        if (hwLevel == null) {
            Assert.fail(""No supported hardware level reported."");
        }
        return hwLevel;
    }

    /**
     * Whether or not the hardware level reported by android.info.supportedHardwareLevel
     * is {@value CameraMetadata#INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY}.
     *
     * <p>If the camera device is not reporting the hardwareLevel, this
     * will cause the test to fail.</p>
     *
     * @return {@code true} if the device is {@code LEGACY}, {@code false} otherwise.
     */
    public boolean isHardwareLevelLegacy() {
        return getHardwareLevelChecked() == CameraMetadata.INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY;
    }

    /**
     * Whether or not the per frame control is supported by the camera device.
     *
     * @return {@code true} if per frame control is supported, {@code false} otherwise.
     */
    public boolean isPerFrameControlSupported() {
        return getSyncMaxLatency() == CameraMetadata.SYNC_MAX_LATENCY_PER_FRAME_CONTROL;
    }

    /**
     * Get the maximum number of frames to wait for a request settings being applied
     *
     * @return CameraMetadata.SYNC_MAX_LATENCY_UNKNOWN for unknown latency
     *         CameraMetadata.SYNC_MAX_LATENCY_PER_FRAME_CONTROL for per frame control
     *         a positive int otherwise
     */
    public int getSyncMaxLatency() {
        Integer value = getValueFromKeyNonNull(CameraCharacteristics.SYNC_MAX_LATENCY);
        if (value == null) {
            return CameraMetadata.SYNC_MAX_LATENCY_UNKNOWN;
        }
        return value;
    }

    /**
     * Get the color filter arrangement for this camera device.
     *
     * @return Color Filter arrangement of this camera device
     */
    public int getCFAChecked() {
        Integer cfa = getValueFromKeyNonNull(
                CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT);
        if (cfa == null) {
            Assert.fail(""No color filter array (CFA) reported."");
        }
        return cfa;
    }

    public boolean isNIRColorFilter() {
        Integer cfa = mCharacteristics.get(
                CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT);
        if (cfa == null) {
            return false;
        }
        return cfa == CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_NIR;
    }

    /**
     * Whether or not the hardware level reported by android.info.supportedHardwareLevel
     * is {@value CameraMetadata#INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED}.
     *
     * <p>If the camera device is incorrectly reporting the hardwareLevel, this
     * will always return {@code true}.</p>
     *
     * @return {@code true} if the device is {@code LIMITED}, {@code false} otherwise.
     */
    public boolean isHardwareLevelLimited() {
        return getHardwareLevelChecked() == CameraMetadata.INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED;
    }

    /**
     * Whether or not the hardware level reported by {@code android.info.supportedHardwareLevel}
     * is at least {@link CameraMetadata#INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED}.
     *
     * <p>If the camera device is incorrectly reporting the hardwareLevel, this
     * will always return {@code false}.</p>
     *
     * @return
     *          {@code true} if the device is {@code LIMITED} or {@code FULL},
     *          {@code false} otherwise (i.e. LEGACY).
     */
    public boolean isHardwareLevelAtLeastLimited() {
        return isHardwareLevelAtLeast(CameraMetadata.INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED);
    }

    /**
     * Get the maximum number of partial result a request can expect
     *
     * @return 1 if partial result is not supported.
     *         a integer value larger than 1 if partial result is supported.
     */
    public int getPartialResultCount() {
        Integer value = mCharacteristics.get(CameraCharacteristics.REQUEST_PARTIAL_RESULT_COUNT);
        if (value == null) {
            // Optional key. Default value is 1 if key is missing.
            return 1;
        }
        return value;
    }

    /**
     * Get the exposure time value and clamp to the range if needed.
     *
     * @param exposure Input exposure time value to check.
     * @return Exposure value in the legal range.
     */
    public long getExposureClampToRange(long exposure) {
        long minExposure = getExposureMinimumOrDefault(Long.MAX_VALUE);
        long maxExposure = getExposureMaximumOrDefault(Long.MIN_VALUE);
        if (minExposure > SENSOR_INFO_EXPOSURE_TIME_RANGE_MIN_AT_MOST) {
            failKeyCheck(CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE,
                    String.format(
                    ""Min value %d is too large, set to maximal legal value %d"",
                    minExposure, SENSOR_INFO_EXPOSURE_TIME_RANGE_MIN_AT_MOST));
            minExposure = SENSOR_INFO_EXPOSURE_TIME_RANGE_MIN_AT_MOST;
        }
        if (isHardwareLevelAtLeastFull() &&
                maxExposure < SENSOR_INFO_EXPOSURE_TIME_RANGE_MAX_AT_LEAST) {
            failKeyCheck(CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE,
                    String.format(
                    ""Max value %d is too small, set to minimal legal value %d"",
                    maxExposure, SENSOR_INFO_EXPOSURE_TIME_RANGE_MAX_AT_LEAST));
            maxExposure = SENSOR_INFO_EXPOSURE_TIME_RANGE_MAX_AT_LEAST;
        }

        return Math.max(minExposure, Math.min(maxExposure, exposure));
    }

    /**
     * Check if the camera device support focuser.
     *
     * @return true if camera device support focuser, false otherwise.
     */
    public boolean hasFocuser() {
        if (areKeysAvailable(CameraCharacteristics.LENS_INFO_MINIMUM_FOCUS_DISTANCE)) {
            // LEGACY devices don't have lens.info.minimumFocusDistance, so guard this query
            return (getMinimumFocusDistanceChecked() > 0);
        } else {
            // Check available AF modes
            int[] availableAfModes = mCharacteristics.get(
                    CameraCharacteristics.CONTROL_AF_AVAILABLE_MODES);

            if (availableAfModes == null) {
                return false;
            }

            // Assume that if we have an AF mode which doesn't ignore AF trigger, we have a focuser
            boolean hasFocuser = false;
            loop: for (int mode : availableAfModes) {
                switch (mode) {
                    case CameraMetadata.CONTROL_AF_MODE_AUTO:
                    case CameraMetadata.CONTROL_AF_MODE_CONTINUOUS_PICTURE:
                    case CameraMetadata.CONTROL_AF_MODE_CONTINUOUS_VIDEO:
                    case CameraMetadata.CONTROL_AF_MODE_MACRO:
                        hasFocuser = true;
                        break loop;
                }
            }

            return hasFocuser;
        }
    }

    /**
     * Check if the camera device has flash unit.
     * @return true if flash unit is available, false otherwise.
     */
    public boolean hasFlash() {
        return getFlashInfoChecked();
    }

    /**
     * Get minimum focus distance.
     *
     * @return minimum focus distance, 0 if minimum focus distance is invalid.
     */
    public float getMinimumFocusDistanceChecked() {
        Key<Float> key = CameraCharacteristics.LENS_INFO_MINIMUM_FOCUS_DISTANCE;
        Float minFocusDistance;

        /**
         * android.lens.info.minimumFocusDistance - required for FULL and MANUAL_SENSOR-capable
         *   devices; optional for all other devices.
         */
        if (isHardwareLevelAtLeastFull() || isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
            minFocusDistance = getValueFromKeyNonNull(key);
        } else {
            minFocusDistance = mCharacteristics.get(key);
        }

        if (minFocusDistance == null) {
            return 0.0f;
        }

        checkTrueForKey(key, "" minFocusDistance value shouldn't be negative"",
                minFocusDistance >= 0);
        if (minFocusDistance < 0) {
            minFocusDistance = 0.0f;
        }

        return minFocusDistance;
    }

    /**
     * Get focusDistanceCalibration.
     *
     * @return focusDistanceCalibration, UNCALIBRATED if value is invalid.
     */
    public int getFocusDistanceCalibrationChecked() {
        Key<Integer> key = CameraCharacteristics.LENS_INFO_FOCUS_DISTANCE_CALIBRATION;
        Integer calibration = getValueFromKeyNonNull(key);

        if (calibration == null) {
            return CameraMetadata.LENS_INFO_FOCUS_DISTANCE_CALIBRATION_UNCALIBRATED;
        }

        checkTrueForKey(key, "" value is out of range"" ,
                calibration >= CameraMetadata.LENS_INFO_FOCUS_DISTANCE_CALIBRATION_UNCALIBRATED &&
                calibration <= CameraMetadata.LENS_INFO_FOCUS_DISTANCE_CALIBRATION_CALIBRATED);

        return calibration;
    }

    public static String getAeModeName(int aeMode) {
        return (aeMode >= AE_MODE_NAMES.length) ? String.format(""VENDOR_AE_MODE_%d"", aeMode) :
                AE_MODE_NAMES[aeMode];
    }

    public static String getAfModeName(int afMode) {
        return (afMode >= AF_MODE_NAMES.length) ? String.format(""VENDOR_AF_MODE_%d"", afMode) :
                AF_MODE_NAMES[afMode];
    }

    /**
     * Get max AE regions and do validity check.
     *
     * @return AE max regions supported by the camera device
     */
    public int getAeMaxRegionsChecked() {
        Integer regionCount = mCharacteristics.get(CameraCharacteristics.CONTROL_MAX_REGIONS_AE);
        if (regionCount == null) {
            return 0;
        }
        return regionCount;
    }

    /**
     * Get max AWB regions and do validity check.
     *
     * @return AWB max regions supported by the camera device
     */
    public int getAwbMaxRegionsChecked() {
        Integer regionCount = mCharacteristics.get(CameraCharacteristics.CONTROL_MAX_REGIONS_AWB);
        if (regionCount == null) {
            return 0;
        }
        return regionCount;
    }

    /**
     * Get max AF regions and do validity check.
     *
     * @return AF max regions supported by the camera device
     */
    public int getAfMaxRegionsChecked() {
        Integer regionCount = mCharacteristics.get(CameraCharacteristics.CONTROL_MAX_REGIONS_AF);
        if (regionCount == null) {
            return 0;
        }
        return regionCount;
    }
    /**
     * Get the available anti-banding modes.
     *
     * @return The array contains available anti-banding modes.
     */
    public int[] getAeAvailableAntiBandingModesChecked() {
        Key<int[]> key = CameraCharacteristics.CONTROL_AE_AVAILABLE_ANTIBANDING_MODES;
        int[] modes = getValueFromKeyNonNull(key);

        boolean foundAuto = false;
        boolean found50Hz = false;
        boolean found60Hz = false;
        for (int mode : modes) {
            checkTrueForKey(key, ""mode value "" + mode + "" is out if range"",
                    mode >= CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_OFF ||
                    mode <= CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_AUTO);
            if (mode == CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_AUTO) {
                foundAuto = true;
            } else if (mode == CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_50HZ) {
                found50Hz = true;
            } else if (mode == CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_60HZ) {
                found60Hz = true;
            }
        }
        // Must contain AUTO mode or one of 50/60Hz mode.
        checkTrueForKey(key, ""Either AUTO mode or both 50HZ/60HZ mode should present"",
                foundAuto || (found50Hz && found60Hz));

        return modes;
    }

    /**
     * Check if the antibanding OFF mode is supported.
     *
     * @return true if antibanding OFF mode is supported, false otherwise.
     */
    public boolean isAntiBandingOffModeSupported() {
        List<Integer> antiBandingModes =
                Arrays.asList(CameraTestUtils.toObject(getAeAvailableAntiBandingModesChecked()));

        return antiBandingModes.contains(CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_OFF);
    }

    public Boolean getFlashInfoChecked() {
        Key<Boolean> key = CameraCharacteristics.FLASH_INFO_AVAILABLE;
        Boolean hasFlash = getValueFromKeyNonNull(key);

        // In case the failOnKey only gives warning.
        if (hasFlash == null) {
            return false;
        }

        return hasFlash;
    }

    public int[] getAvailableTestPatternModesChecked() {
        Key<int[]> key =
                CameraCharacteristics.SENSOR_AVAILABLE_TEST_PATTERN_MODES;
        int[] modes = getValueFromKeyNonNull(key);

        if (modes == null) {
            return new int[0];
        }

        int expectValue = CameraCharacteristics.SENSOR_TEST_PATTERN_MODE_OFF;
        Integer[] boxedModes = CameraTestUtils.toObject(modes);
        checkTrueForKey(key, "" value must contain OFF mode"",
                Arrays.asList(boxedModes).contains(expectValue));

        return modes;
    }

    /**
     * Get available thumbnail sizes and do the validity check.
     *
     * @return The array of available thumbnail sizes
     */
    public Size[] getAvailableThumbnailSizesChecked() {
        Key<Size[]> key = CameraCharacteristics.JPEG_AVAILABLE_THUMBNAIL_SIZES;
        Size[] sizes = getValueFromKeyNonNull(key);
        final List<Size> sizeList = Arrays.asList(sizes);

        // Size must contain (0, 0).
        checkTrueForKey(key, ""size should contain (0, 0)"", sizeList.contains(new Size(0, 0)));

        // Each size must be distinct.
        checkElementDistinct(key, sizeList);

        // Must be sorted in ascending order by area, by width if areas are same.
        List<Size> orderedSizes =
                CameraTestUtils.getAscendingOrderSizes(sizeList, /*ascending*/true);
        checkTrueForKey(key, ""Sizes should be in ascending order: Original "" + sizeList.toString()
                + "", Expected "" + orderedSizes.toString(), orderedSizes.equals(sizeList));

        // TODO: Aspect ratio match, need wait for android.scaler.availableStreamConfigurations
        // implementation see b/12958122.

        return sizes;
    }

    /**
     * Get available focal lengths and do the validity check.
     *
     * @return The array of available focal lengths
     */
    public float[] getAvailableFocalLengthsChecked() {
        Key<float[]> key = CameraCharacteristics.LENS_INFO_AVAILABLE_FOCAL_LENGTHS;
        float[] focalLengths = getValueFromKeyNonNull(key);

        checkTrueForKey(key, ""Array should contain at least one element"", focalLengths.length >= 1);

        for (int i = 0; i < focalLengths.length; i++) {
            checkTrueForKey(key,
                    String.format(""focalLength[%d] %f should be positive."", i, focalLengths[i]),
                    focalLengths[i] > 0);
        }
        checkElementDistinct(key, Arrays.asList(CameraTestUtils.toObject(focalLengths)));

        return focalLengths;
    }

    /**
     * Get available apertures and do the validity check.
     *
     * @return The non-null array of available apertures
     */
    public float[] getAvailableAperturesChecked() {
        Key<float[]> key = CameraCharacteristics.LENS_INFO_AVAILABLE_APERTURES;
        float[] apertures = getValueFromKeyNonNull(key);

        checkTrueForKey(key, ""Array should contain at least one element"", apertures.length >= 1);

        for (int i = 0; i < apertures.length; i++) {
            checkTrueForKey(key,
                    String.format(""apertures[%d] %f should be positive."", i, apertures[i]),
                    apertures[i] > 0);
        }
        checkElementDistinct(key, Arrays.asList(CameraTestUtils.toObject(apertures)));

        return apertures;
    }

    /**
     * Get and check the available hot pixel map modes.
     *
     * @return the available hot pixel map modes
     */
    public int[] getAvailableHotPixelModesChecked() {
        Key<int[]> key = CameraCharacteristics.HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES;
        int[] modes = getValueFromKeyNonNull(key);

        if (modes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
        if (isHardwareLevelAtLeastFull()) {
            checkTrueForKey(key, ""Full-capability camera devices must support FAST mode"",
                    modeList.contains(CameraMetadata.HOT_PIXEL_MODE_FAST));
        }

        if (isHardwareLevelAtLeastLimited()) {
            // FAST and HIGH_QUALITY mode must be both present or both not present
            List<Integer> coupledModes = Arrays.asList(new Integer[] {
                    CameraMetadata.HOT_PIXEL_MODE_FAST,
                    CameraMetadata.HOT_PIXEL_MODE_HIGH_QUALITY
            });
            checkTrueForKey(
                    key, "" FAST and HIGH_QUALITY mode must both present or both not present"",
                    containsAllOrNone(modeList, coupledModes));
        }
        checkElementDistinct(key, modeList);
        checkArrayValuesInRange(key, modes, CameraMetadata.HOT_PIXEL_MODE_OFF,
                CameraMetadata.HOT_PIXEL_MODE_HIGH_QUALITY);

        return modes;
    }

    /**
     * Get and check available face detection modes.
     *
     * @return The non-null array of available face detection modes
     */
    public int[] getAvailableFaceDetectModesChecked() {
        Key<int[]> key = CameraCharacteristics.STATISTICS_INFO_AVAILABLE_FACE_DETECT_MODES;
        int[] modes = getValueFromKeyNonNull(key);

        if (modes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
        checkTrueForKey(key, ""Array should contain OFF mode"",
                modeList.contains(CameraMetadata.STATISTICS_FACE_DETECT_MODE_OFF));
        checkElementDistinct(key, modeList);
        checkArrayValuesInRange(key, modes, CameraMetadata.STATISTICS_FACE_DETECT_MODE_OFF,
                CameraMetadata.STATISTICS_FACE_DETECT_MODE_FULL);

        return modes;
    }

    /**
     * Get and check max face detected count.
     *
     * @return max number of faces that can be detected
     */
    public int getMaxFaceCountChecked() {
        Key<Integer> key = CameraCharacteristics.STATISTICS_INFO_MAX_FACE_COUNT;
        Integer count = getValueFromKeyNonNull(key);

        if (count == null) {
            return 0;
        }

        List<Integer> faceDetectModes =
                Arrays.asList(CameraTestUtils.toObject(getAvailableFaceDetectModesChecked()));
        if (faceDetectModes.contains(CameraMetadata.STATISTICS_FACE_DETECT_MODE_OFF) &&
                faceDetectModes.size() == 1) {
            checkTrueForKey(key, "" value must be 0 if only OFF mode is supported in ""
                    + ""availableFaceDetectionModes"", count == 0);
        } else {
            int maxFaceCountAtLeast = STATISTICS_INFO_MAX_FACE_COUNT_MIN_AT_LEAST;

            // Legacy mode may support fewer than STATISTICS_INFO_MAX_FACE_COUNT_MIN_AT_LEAST faces.
            if (isHardwareLevelLegacy()) {
                maxFaceCountAtLeast = 1;
            }
            checkTrueForKey(key, "" value must be no less than "" + maxFaceCountAtLeast + "" if SIMPLE""
                    + ""or FULL is also supported in availableFaceDetectionModes"",
                    count >= maxFaceCountAtLeast);
        }

        return count;
    }

    /**
     * Get and check the available tone map modes.
     *
     * @return the available tone map modes
     */
    public int[] getAvailableToneMapModesChecked() {
        Key<int[]> key = CameraCharacteristics.TONEMAP_AVAILABLE_TONE_MAP_MODES;
        int[] modes = mCharacteristics.get(key);

        if (modes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
        checkTrueForKey(key, "" Camera devices must always support FAST mode"",
                modeList.contains(CameraMetadata.TONEMAP_MODE_FAST));
        // Qualification check for MANUAL_POSTPROCESSING capability is in
        // StaticMetadataTest#testCapabilities

        if (isHardwareLevelAtLeastLimited()) {
            // FAST and HIGH_QUALITY mode must be both present or both not present
            List<Integer> coupledModes = Arrays.asList(new Integer[] {
                    CameraMetadata.TONEMAP_MODE_FAST,
                    CameraMetadata.TONEMAP_MODE_HIGH_QUALITY
            });
            checkTrueForKey(
                    key, "" FAST and HIGH_QUALITY mode must both present or both not present"",
                    containsAllOrNone(modeList, coupledModes));
        }
        checkElementDistinct(key, modeList);
        checkArrayValuesInRange(key, modes, CameraMetadata.TONEMAP_MODE_CONTRAST_CURVE,
                CameraMetadata.TONEMAP_MODE_PRESET_CURVE);

        return modes;
    }

    /**
     * Get and check max tonemap curve point.
     *
     * @return Max tonemap curve points.
     */
    public int getMaxTonemapCurvePointChecked() {
        Key<Integer> key = CameraCharacteristics.TONEMAP_MAX_CURVE_POINTS;
        Integer count = getValueFromKeyNonNull(key);
        List<Integer> modeList =
                Arrays.asList(CameraTestUtils.toObject(getAvailableToneMapModesChecked()));
        boolean tonemapCurveOutputSupported =
                modeList.contains(CameraMetadata.TONEMAP_MODE_CONTRAST_CURVE) ||
                modeList.contains(CameraMetadata.TONEMAP_MODE_GAMMA_VALUE) ||
                modeList.contains(CameraMetadata.TONEMAP_MODE_PRESET_CURVE);

        if (count == null) {
            if (tonemapCurveOutputSupported) {
                Assert.fail(""Tonemap curve output is supported but MAX_CURVE_POINTS is null"");
            }
            return 0;
        }

        if (tonemapCurveOutputSupported) {
            checkTrueForKey(key, ""Tonemap curve output supported camera device must support ""
                    + ""maxCurvePoints >= "" + TONEMAP_MAX_CURVE_POINTS_AT_LEAST,
                    count >= TONEMAP_MAX_CURVE_POINTS_AT_LEAST);
        }

        return count;
    }

    /**
     * Get and check pixel array size.
     */
    public Size getPixelArraySizeChecked() {
        return getPixelArraySizeChecked(/*maxResolution*/ false);
    }

    /**
     * Get and check pixel array size.
     */
    public Size getPixelArraySizeChecked(boolean maxResolution) {
        Key<Size> key = maxResolution ?
                CameraCharacteristics.SENSOR_INFO_PIXEL_ARRAY_SIZE_MAXIMUM_RESOLUTION :
                CameraCharacteristics.SENSOR_INFO_PIXEL_ARRAY_SIZE;
        Size pixelArray = getValueFromKeyNonNull(key);
        if (pixelArray == null) {
            return new Size(0, 0);
        }

        return pixelArray;
    }

    /**
     * Get and check pre-correction active array size.
     */
    public Rect getPreCorrectedActiveArraySizeChecked() {
        return getPreCorrectedActiveArraySizeChecked(/*maxResolution*/ false);
    }

    /**
     * Get and check pre-correction active array size.
     */
    public Rect getPreCorrectedActiveArraySizeChecked(boolean maxResolution) {
        Key<Rect> key = maxResolution ?
                CameraCharacteristics.SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION :
                        CameraCharacteristics.SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE;
        Rect activeArray = getValueFromKeyNonNull(key);

        if (activeArray == null) {
            return new Rect(0, 0, 0, 0);
        }

        Size pixelArraySize = getPixelArraySizeChecked(maxResolution);
        checkTrueForKey(key, ""values left/top are invalid"", activeArray.left >= 0 && activeArray.top >= 0);
        checkTrueForKey(key, ""values width/height are invalid"",
                activeArray.width() <= pixelArraySize.getWidth() &&
                activeArray.height() <= pixelArraySize.getHeight());

        return activeArray;
    }

    /**
     * Get and check active array size.
     */
    public Rect getActiveArraySizeChecked() {
        return getActiveArraySizeChecked(/*maxResolution*/ false);
    }

    /**
     * Get and check active array size.
     */
    public Rect getActiveArraySizeChecked(boolean maxResolution) {
        Key<Rect> key = maxResolution ?
                CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION :
                        CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE;
        Rect activeArray = getValueFromKeyNonNull(key);

        if (activeArray == null) {
            return new Rect(0, 0, 0, 0);
        }

        Size pixelArraySize = getPixelArraySizeChecked(maxResolution);
        checkTrueForKey(key, ""values left/top are invalid"", activeArray.left >= 0 && activeArray.top >= 0);
        checkTrueForKey(key, ""values width/height are invalid"",
                activeArray.width() <= pixelArraySize.getWidth() &&
                activeArray.height() <= pixelArraySize.getHeight());

        return activeArray;
    }

    /**
     * Get the dimensions to use for RAW16 buffers.
     */
    public Size getRawDimensChecked() throws Exception {
        return getRawDimensChecked(/*maxResolution*/ false);
    }

    /**
     * Get the dimensions to use for RAW16 buffers.
     */
    public Size getRawDimensChecked(boolean maxResolution) throws Exception {
        Size[] targetCaptureSizes = getAvailableSizesForFormatChecked(ImageFormat.RAW_SENSOR,
                        StaticMetadata.StreamDirection.Output, /*fastSizes*/true, /*slowSizes*/true,
                        maxResolution);
        Assert.assertTrue(""No capture sizes available for RAW format!"",
                targetCaptureSizes.length != 0);
        Rect activeArray = getPreCorrectedActiveArraySizeChecked(maxResolution);
        Size preCorrectionActiveArraySize =
                new Size(activeArray.width(), activeArray.height());
        Size pixelArraySize = getPixelArraySizeChecked(maxResolution);
        Assert.assertTrue(""Missing pre-correction active array size"", activeArray.width() > 0 &&
                activeArray.height() > 0);
        Assert.assertTrue(""Missing pixel array size"", pixelArraySize.getWidth() > 0 &&
                pixelArraySize.getHeight() > 0);
        Size[] allowedArraySizes = new Size[] { preCorrectionActiveArraySize,
                pixelArraySize };
        return assertArrayContainsAnyOf(""Available sizes for RAW format"" +
                "" must include either the pre-corrected active array size, or the full "" +
                ""pixel array size"", targetCaptureSizes, allowedArraySizes);
    }

    /**
     * Get the sensitivity value and clamp to the range if needed.
     *
     * @param sensitivity Input sensitivity value to check.
     * @return Sensitivity value in legal range.
     */
    public int getSensitivityClampToRange(int sensitivity) {
        int minSensitivity = getSensitivityMinimumOrDefault();
        int maxSensitivity = getSensitivityMaximumOrDefault();
        if (minSensitivity > SENSOR_INFO_SENSITIVITY_RANGE_MIN_AT_MOST) {
            failKeyCheck(CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE,
                    String.format(
                    ""Min value %d is too large, set to maximal legal value %d"",
                    minSensitivity, SENSOR_INFO_SENSITIVITY_RANGE_MIN_AT_MOST));
            minSensitivity = SENSOR_INFO_SENSITIVITY_RANGE_MIN_AT_MOST;
        }
        if (maxSensitivity < SENSOR_INFO_SENSITIVITY_RANGE_MAX_AT_LEAST) {
            failKeyCheck(CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE,
                    String.format(
                    ""Max value %d is too small, set to minimal legal value %d"",
                    maxSensitivity, SENSOR_INFO_SENSITIVITY_RANGE_MAX_AT_LEAST));
            maxSensitivity = SENSOR_INFO_SENSITIVITY_RANGE_MAX_AT_LEAST;
        }

        return Math.max(minSensitivity, Math.min(maxSensitivity, sensitivity));
    }

    /**
     * Get maxAnalogSensitivity for a camera device.
     * <p>
     * This is only available for FULL capability device, return 0 if it is unavailable.
     * </p>
     *
     * @return maxAnalogSensitivity, 0 if it is not available.
     */
    public int getMaxAnalogSensitivityChecked() {

        Key<Integer> key = CameraCharacteristics.SENSOR_MAX_ANALOG_SENSITIVITY;
        Integer maxAnalogsensitivity = mCharacteristics.get(key);
        if (maxAnalogsensitivity == null) {
            if (isHardwareLevelAtLeastFull()) {
                Assert.fail(""Full device should report max analog sensitivity"");
            }
            return 0;
        }

        int minSensitivity = getSensitivityMinimumOrDefault();
        int maxSensitivity = getSensitivityMaximumOrDefault();
        checkTrueForKey(key, "" Max analog sensitivity "" + maxAnalogsensitivity
                + "" should be no larger than max sensitivity "" + maxSensitivity,
                maxAnalogsensitivity <= maxSensitivity);
        checkTrueForKey(key, "" Max analog sensitivity "" + maxAnalogsensitivity
                + "" should be larger than min sensitivity "" + maxSensitivity,
                maxAnalogsensitivity > minSensitivity);

        return maxAnalogsensitivity;
    }

    /**
     * Get hyperfocalDistance and do the validity check.
     * <p>
     * Note that, this tag is optional, will return -1 if this tag is not
     * available.
     * </p>
     *
     * @return hyperfocalDistance of this device, -1 if this tag is not available.
     */
    public float getHyperfocalDistanceChecked() {
        Key<Float> key = CameraCharacteristics.LENS_INFO_HYPERFOCAL_DISTANCE;
        Float hyperfocalDistance = getValueFromKeyNonNull(key);
        if (hyperfocalDistance == null) {
            return -1;
        }

        if (hasFocuser()) {
            float minFocusDistance = getMinimumFocusDistanceChecked();
            checkTrueForKey(key, String.format("" hyperfocal distance %f should be in the range of""
                    + "" should be in the range of (%f, %f]"", hyperfocalDistance, 0.0f,
                    minFocusDistance),
                    hyperfocalDistance > 0 && hyperfocalDistance <= minFocusDistance);
        }

        return hyperfocalDistance;
    }

    /**
     * Get the minimum value for a sensitivity range from android.sensor.info.sensitivityRange.
     *
     * <p>If the camera is incorrectly reporting values, log a warning and return
     * the default value instead, which is the largest minimum value required to be supported
     * by all camera devices.</p>
     *
     * @return The value reported by the camera device or the defaultValue otherwise.
     */
    public int getSensitivityMinimumOrDefault() {
        return getSensitivityMinimumOrDefault(SENSOR_INFO_SENSITIVITY_RANGE_MIN_AT_MOST);
    }

    /**
     * Get the minimum value for a sensitivity range from android.sensor.info.sensitivityRange.
     *
     * <p>If the camera is incorrectly reporting values, log a warning and return
     * the default value instead.</p>
     *
     * @param defaultValue Value to return if no legal value is available
     * @return The value reported by the camera device or the defaultValue otherwise.
     */
    public int getSensitivityMinimumOrDefault(int defaultValue) {
        Range<Integer> range = mCharacteristics.get(
                CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE);
        if (range == null) {
            if (isHardwareLevelAtLeastFull()) {
                failKeyCheck(CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE,
                        ""had no valid minimum value; using default of "" + defaultValue);
            }
            return defaultValue;
        }
        return range.getLower();
    }

    /**
     * Get the maximum value for a sensitivity range from android.sensor.info.sensitivityRange.
     *
     * <p>If the camera is incorrectly reporting values, log a warning and return
     * the default value instead, which is the smallest maximum value required to be supported
     * by all camera devices.</p>
     *
     * @return The value reported by the camera device or the defaultValue otherwise.
     */
    public int getSensitivityMaximumOrDefault() {
        return getSensitivityMaximumOrDefault(SENSOR_INFO_SENSITIVITY_RANGE_MAX_AT_LEAST);
    }

    /**
     * Get the maximum value for a sensitivity range from android.sensor.info.sensitivityRange.
     *
     * <p>If the camera is incorrectly reporting values, log a warning and return
     * the default value instead.</p>
     *
     * @param defaultValue Value to return if no legal value is available
     * @return The value reported by the camera device or the defaultValue otherwise.
     */
    public int getSensitivityMaximumOrDefault(int defaultValue) {
        Range<Integer> range = mCharacteristics.get(
                CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE);
        if (range == null) {
            if (isHardwareLevelAtLeastFull()) {
                failKeyCheck(CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE,
                        ""had no valid maximum value; using default of "" + defaultValue);
            }
            return defaultValue;
        }
        return range.getUpper();
    }

    /**
     * Get the minimum value for an exposure range from android.sensor.info.exposureTimeRange.
     *
     * <p>If the camera is incorrectly reporting values, log a warning and return
     * the default value instead.</p>
     *
     * @param defaultValue Value to return if no legal value is available
     * @return The value reported by the camera device or the defaultValue otherwise.
     */
    public long getExposureMinimumOrDefault(long defaultValue) {
        Range<Long> range = getValueFromKeyNonNull(
                CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE);
        if (range == null) {
            failKeyCheck(CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE,
                    ""had no valid minimum value; using default of "" + defaultValue);
            return defaultValue;
        }
        return range.getLower();
    }

    /**
     * Get the minimum value for an exposure range from android.sensor.info.exposureTimeRange.
     *
     * <p>If the camera is incorrectly reporting values, log a warning and return
     * the default value instead, which is the largest minimum value required to be supported
     * by all camera devices.</p>
     *
     * @return The value reported by the camera device or the defaultValue otherwise.
     */
    public long getExposureMinimumOrDefault() {
        return getExposureMinimumOrDefault(SENSOR_INFO_EXPOSURE_TIME_RANGE_MIN_AT_MOST);
    }

    /**
     * Get the maximum value for an exposure range from android.sensor.info.exposureTimeRange.
     *
     * <p>If the camera is incorrectly reporting values, log a warning and return
     * the default value instead.</p>
     *
     * @param defaultValue Value to return if no legal value is available
     * @return The value reported by the camera device or the defaultValue otherwise.
     */
    public long getExposureMaximumOrDefault(long defaultValue) {
        Range<Long> range = getValueFromKeyNonNull(
                CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE);
        if (range == null) {
            failKeyCheck(CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE,
                    ""had no valid maximum value; using default of "" + defaultValue);
            return defaultValue;
        }
        return range.getUpper();
    }

    /**
     * Get the maximum value for an exposure range from android.sensor.info.exposureTimeRange.
     *
     * <p>If the camera is incorrectly reporting values, log a warning and return
     * the default value instead, which is the smallest maximum value required to be supported
     * by all camera devices.</p>
     *
     * @return The value reported by the camera device or the defaultValue otherwise.
     */
    public long getExposureMaximumOrDefault() {
        return getExposureMaximumOrDefault(SENSOR_INFO_EXPOSURE_TIME_RANGE_MAX_AT_LEAST);
    }

    /**
     * get android.control.availableModes and do the validity check.
     *
     * @return available control modes.
     */
    public int[] getAvailableControlModesChecked() {
        Key<int[]> modesKey = CameraCharacteristics.CONTROL_AVAILABLE_MODES;
        int[] modes = getValueFromKeyNonNull(modesKey);
        if (modes == null) {
            modes = new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
        checkTrueForKey(modesKey, ""value is empty"", !modeList.isEmpty());

        // All camera device must support AUTO
        checkTrueForKey(modesKey, ""values "" + modeList.toString() + "" must contain AUTO mode"",
                modeList.contains(CameraMetadata.CONTROL_MODE_AUTO));

        boolean isAeOffSupported =  Arrays.asList(
                CameraTestUtils.toObject(getAeAvailableModesChecked())).contains(
                        CameraMetadata.CONTROL_AE_MODE_OFF);
        boolean isAfOffSupported =  Arrays.asList(
                CameraTestUtils.toObject(getAfAvailableModesChecked())).contains(
                        CameraMetadata.CONTROL_AF_MODE_OFF);
        boolean isAwbOffSupported =  Arrays.asList(
                CameraTestUtils.toObject(getAwbAvailableModesChecked())).contains(
                        CameraMetadata.CONTROL_AWB_MODE_OFF);
        if (isAeOffSupported && isAfOffSupported && isAwbOffSupported) {
            // 3A OFF controls are supported, OFF mode must be supported here.
            checkTrueForKey(modesKey, ""values "" + modeList.toString() + "" must contain OFF mode"",
                    modeList.contains(CameraMetadata.CONTROL_MODE_OFF));
        }

        if (isSceneModeSupported()) {
            checkTrueForKey(modesKey, ""values "" + modeList.toString() + "" must contain""
                    + "" USE_SCENE_MODE"",
                    modeList.contains(CameraMetadata.CONTROL_MODE_USE_SCENE_MODE));
        }

        return modes;
    }

    public boolean isSceneModeSupported() {
        List<Integer> availableSceneModes = Arrays.asList(
                CameraTestUtils.toObject(getAvailableSceneModesChecked()));

        if (availableSceneModes.isEmpty()) {
            return false;
        }

        // If sceneMode is not supported, camera device will contain single entry: DISABLED.
        return availableSceneModes.size() > 1 ||
                !availableSceneModes.contains(CameraMetadata.CONTROL_SCENE_MODE_DISABLED);
    }

    /**
     * Get aeAvailableModes and do the validity check.
     *
     * <p>Depending on the check level this class has, for WAR or COLLECT levels,
     * If the aeMode list is invalid, return an empty mode array. The the caller doesn't
     * have to abort the execution even the aeMode list is invalid.</p>
     * @return AE available modes
     */
    public int[] getAeAvailableModesChecked() {
        Key<int[]> modesKey = CameraCharacteristics.CONTROL_AE_AVAILABLE_MODES;
        int[] modes = getValueFromKeyNonNull(modesKey);
        if (modes == null) {
            modes = new int[0];
        }
        List<Integer> modeList = new ArrayList<Integer>();
        for (int mode : modes) {
            // Skip vendor-added modes
            if (mode <= CameraMetadata.CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE) {
                modeList.add(mode);
            }
        }
        checkTrueForKey(modesKey, ""value is empty"", !modeList.isEmpty());
        modes = new int[modeList.size()];
        for (int i = 0; i < modeList.size(); i++) {
            modes[i] = modeList.get(i);
        }

        // All camera device must support ON
        checkTrueForKey(modesKey, ""values "" + modeList.toString() + "" must contain ON mode"",
                modeList.contains(CameraMetadata.CONTROL_AE_MODE_ON));

        // All camera devices with flash units support ON_AUTO_FLASH and ON_ALWAYS_FLASH
        Key<Boolean> flashKey= CameraCharacteristics.FLASH_INFO_AVAILABLE;
        Boolean hasFlash = getValueFromKeyNonNull(flashKey);
        if (hasFlash == null) {
            hasFlash = false;
        }
        if (hasFlash) {
            boolean flashModeConsistentWithFlash =
                    modeList.contains(CameraMetadata.CONTROL_AE_MODE_ON_AUTO_FLASH) &&
                    modeList.contains(CameraMetadata.CONTROL_AE_MODE_ON_ALWAYS_FLASH);
            checkTrueForKey(modesKey,
                    ""value must contain ON_AUTO_FLASH and ON_ALWAYS_FLASH and  when flash is"" +
                    ""available"", flashModeConsistentWithFlash);
        } else {
            boolean flashModeConsistentWithoutFlash =
                    !(modeList.contains(CameraMetadata.CONTROL_AE_MODE_ON_AUTO_FLASH) ||
                    modeList.contains(CameraMetadata.CONTROL_AE_MODE_ON_ALWAYS_FLASH) ||
                    modeList.contains(CameraMetadata.CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE));
            checkTrueForKey(modesKey,
                    ""value must not contain ON_AUTO_FLASH, ON_ALWAYS_FLASH and"" +
                    ""ON_AUTO_FLASH_REDEYE when flash is unavailable"",
                    flashModeConsistentWithoutFlash);
        }

        // FULL mode camera devices always support OFF mode.
        boolean condition =
                !isHardwareLevelAtLeastFull() || modeList.contains(CameraMetadata.CONTROL_AE_MODE_OFF);
        checkTrueForKey(modesKey, ""Full capability device must have OFF mode"", condition);

        // Boundary check.
        for (int mode : modes) {
            checkTrueForKey(modesKey, ""Value "" + mode + "" is out of bound"",
                    mode >= CameraMetadata.CONTROL_AE_MODE_OFF
                    && mode <= CameraMetadata.CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE);
        }

        return modes;
    }

    /**
     * Get available AWB modes and do the validity check.
     *
     * @return array that contains available AWB modes, empty array if awbAvailableModes is
     * unavailable.
     */
    public int[] getAwbAvailableModesChecked() {
        Key<int[]> key =
                CameraCharacteristics.CONTROL_AWB_AVAILABLE_MODES;
        int[] awbModes = getValueFromKeyNonNull(key);

        if (awbModes == null) {
            return new int[0];
        }

        List<Integer> modesList = Arrays.asList(CameraTestUtils.toObject(awbModes));
        checkTrueForKey(key, "" All camera devices must support AUTO mode"",
                modesList.contains(CameraMetadata.CONTROL_AWB_MODE_AUTO));
        if (isHardwareLevelAtLeastFull()) {
            checkTrueForKey(key, "" Full capability camera devices must support OFF mode"",
                    modesList.contains(CameraMetadata.CONTROL_AWB_MODE_OFF));
        }

        return awbModes;
    }

    /**
     * Get available AF modes and do the validity check.
     *
     * @return array that contains available AF modes, empty array if afAvailableModes is
     * unavailable.
     */
    public int[] getAfAvailableModesChecked() {
        Key<int[]> key =
                CameraCharacteristics.CONTROL_AF_AVAILABLE_MODES;
        int[] afModes = getValueFromKeyNonNull(key);

        if (afModes == null) {
            return new int[0];
        }

        List<Integer> modesList = new ArrayList<Integer>();
        for (int afMode : afModes) {
            // Skip vendor-added AF modes
            if (afMode > CameraCharacteristics.CONTROL_AF_MODE_EDOF) continue;
            modesList.add(afMode);
        }
        afModes = new int[modesList.size()];
        for (int i = 0; i < modesList.size(); i++) {
            afModes[i] = modesList.get(i);
        }

        if (isHardwareLevelAtLeastLimited()) {
            // Some LEGACY mode devices do not support AF OFF
            checkTrueForKey(key, "" All camera devices must support OFF mode"",
                    modesList.contains(CameraMetadata.CONTROL_AF_MODE_OFF));
        }
        if (hasFocuser()) {
            checkTrueForKey(key, "" Camera devices that have focuser units must support AUTO mode"",
                    modesList.contains(CameraMetadata.CONTROL_AF_MODE_AUTO));
        }

        return afModes;
    }

    /**
     * Get supported raw output sizes and do the check.
     *
     * @return Empty size array if raw output is not supported
     */
    public Size[] getRawOutputSizesChecked() {
        return getAvailableSizesForFormatChecked(ImageFormat.RAW_SENSOR,
                StreamDirection.Output);
    }

    /**
     * Get supported jpeg output sizes and do the check.
     *
     * @return Empty size array if jpeg output is not supported
     */
    public Size[] getJpegOutputSizesChecked() {
        return getAvailableSizesForFormatChecked(ImageFormat.JPEG,
                StreamDirection.Output);
    }

    /**
     * Get supported heic output sizes and do the check.
     *
     * @return Empty size array if heic output is not supported
     */
    public Size[] getHeicOutputSizesChecked() {
        return getAvailableSizesForFormatChecked(ImageFormat.HEIC,
                StreamDirection.Output);
    }

    /**
     * Used to determine the stream direction for various helpers that look up
     * format or size information.
     */
    public enum StreamDirection {
        /** Stream is used with {@link android.hardware.camera2.CameraDevice#configureOutputs} */
        Output,
        /** Stream is used with {@code CameraDevice#configureInputs} -- NOT YET PUBLIC */
        Input
    }

    /**
     * Get available formats for a given direction.
     *
     * @param direction The stream direction, input or output.
     * @return The formats of the given direction, empty array if no available format is found.
     */
    public int[] getAvailableFormats(StreamDirection direction) {
        Key<StreamConfigurationMap> key =
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP;
        StreamConfigurationMap config = getValueFromKeyNonNull(key);

        if (config == null) {
            return new int[0];
        }

        switch (direction) {
            case Output:
                return config.getOutputFormats();
            case Input:
                return config.getInputFormats();
            default:
                throw new IllegalArgumentException(""direction must be output or input"");
        }
    }

    /**
     * Get valid output formats for a given input format.
     *
     * @param inputFormat The input format used to produce the output images.
     * @return The output formats for the given input format, empty array if
     *         no available format is found.
     */
    public int[] getValidOutputFormatsForInput(int inputFormat) {
        Key<StreamConfigurationMap> key =
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP;
        StreamConfigurationMap config = getValueFromKeyNonNull(key);

        if (config == null) {
            return new int[0];
        }

        return config.getValidOutputFormatsForInput(inputFormat);
    }

    /**
     * Get available sizes for given format and direction.
     *
     * @param format The format for the requested size array.
     * @param direction The stream direction, input or output.
     * @return The sizes of the given format, empty array if no available size is found.
     */
    public Size[] getAvailableSizesForFormatChecked(int format, StreamDirection direction) {
        return getAvailableSizesForFormatChecked(format, direction,
                /*fastSizes*/true, /*slowSizes*/true, /*maxResolution*/false);
    }

    /**
     * Get available sizes for given format and direction, and whether to limit to slow or fast
     * resolutions.
     *
     * @param format The format for the requested size array.
     * @param direction The stream direction, input or output.
     * @param fastSizes whether to include getOutputSizes() sizes (generally faster)
     * @param slowSizes whether to include getHighResolutionOutputSizes() sizes (generally slower)
     * @return The sizes of the given format, empty array if no available size is found.
     */
    public Size[] getAvailableSizesForFormatChecked(int format, StreamDirection direction,
            boolean fastSizes, boolean slowSizes) {
        return  getAvailableSizesForFormatChecked(format, direction, fastSizes, slowSizes,
                /*maxResolution*/ false);
    }

    /**
     * Get available sizes for given format and direction, and whether to limit to slow or fast
     * resolutions.
     *
     * @param format The format for the requested size array.
     * @param direction The stream direction, input or output.
     * @param fastSizes whether to include getOutputSizes() sizes (generally faster)
     * @param slowSizes whether to include getHighResolutionOutputSizes() sizes (generally slower)
     * @return The sizes of the given format, empty array if no available size is found.
     */
    public Size[] getAvailableSizesForFormatChecked(int format, StreamDirection direction,
            boolean fastSizes, boolean slowSizes, boolean maxResolution) {
        Key<StreamConfigurationMap> key = maxResolution ?
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP_MAXIMUM_RESOLUTION :
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP;
        StreamConfigurationMap config = getValueFromKeyNonNull(key);

        if (config == null) {
            return new Size[0];
        }

        Size[] sizes = null;

        switch (direction) {
            case Output:
                Size[] fastSizeList = null;
                Size[] slowSizeList = null;
                if (fastSizes) {
                    fastSizeList = config.getOutputSizes(format);
                }
                if (slowSizes) {
                    slowSizeList = config.getHighResolutionOutputSizes(format);
                }
                if (fastSizeList != null && slowSizeList != null) {
                    sizes = new Size[slowSizeList.length + fastSizeList.length];
                    System.arraycopy(fastSizeList, 0, sizes, 0, fastSizeList.length);
                    System.arraycopy(slowSizeList, 0, sizes, fastSizeList.length, slowSizeList.length);
                } else if (fastSizeList != null) {
                    sizes = fastSizeList;
                } else if (slowSizeList != null) {
                    sizes = slowSizeList;
                }
                break;
            case Input:
                sizes = config.getInputSizes(format);
                break;
            default:
                throw new IllegalArgumentException(""direction must be output or input"");
        }

        if (sizes == null) {
            sizes = new Size[0];
        }

        return sizes;
    }

    /**
     * Get available AE target fps ranges.
     *
     * @return Empty int array if aeAvailableTargetFpsRanges is invalid.
     */
    @SuppressWarnings(""raw"")
    public Range<Integer>[] getAeAvailableTargetFpsRangesChecked() {
        Key<Range<Integer>[]> key =
                CameraCharacteristics.CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES;
        Range<Integer>[] fpsRanges = getValueFromKeyNonNull(key);

        if (fpsRanges == null) {
            return new Range[0];
        }

        // Round down to 2 boundary if it is not integer times of 2, to avoid array out of bound
        // in case the above check fails.
        int fpsRangeLength = fpsRanges.length;
        int minFps, maxFps;
        long maxFrameDuration = getMaxFrameDurationChecked();
        for (int i = 0; i < fpsRangeLength; i += 1) {
            minFps = fpsRanges[i].getLower();
            maxFps = fpsRanges[i].getUpper();
            checkTrueForKey(key, "" min fps must be no larger than max fps!"",
                    minFps > 0 && maxFps >= minFps);
            long maxDuration = (long) (1e9 / minFps);
            checkTrueForKey(key, String.format(
                    "" the frame duration %d for min fps %d must smaller than maxFrameDuration %d"",
                    maxDuration, minFps, maxFrameDuration), maxDuration <= maxFrameDuration);
        }
        return fpsRanges;
    }

    /**
     * Get the highest supported target FPS range.
     * Prioritizes maximizing the min FPS, then the max FPS without lowering min FPS.
     */
    public Range<Integer> getAeMaxTargetFpsRange() {
        Range<Integer>[] fpsRanges = getAeAvailableTargetFpsRangesChecked();

        Range<Integer> targetRange = fpsRanges[0];
        // Assume unsorted list of target FPS ranges, so use two passes, first maximize min FPS
        for (Range<Integer> candidateRange : fpsRanges) {
            if (candidateRange.getLower() > targetRange.getLower()) {
                targetRange = candidateRange;
            }
        }
        // Then maximize max FPS while not lowering min FPS
        for (Range<Integer> candidateRange : fpsRanges) {
            if (candidateRange.getLower() >= targetRange.getLower() &&
                    candidateRange.getUpper() > targetRange.getUpper()) {
                targetRange = candidateRange;
            }
        }
        return targetRange;
    }

    /**
     * Get max frame duration.
     *
     * @return 0 if maxFrameDuration is null
     */
    public long getMaxFrameDurationChecked() {
        Key<Long> key =
                CameraCharacteristics.SENSOR_INFO_MAX_FRAME_DURATION;
        Long maxDuration = getValueFromKeyNonNull(key);

        if (maxDuration == null) {
            return 0;
        }

        return maxDuration;
    }

    /**
     * Get available minimal frame durations for a given format.
     *
     * @param format One of the format from {@link ImageFormat}.
     * @return HashMap of minimal frame durations for different sizes, empty HashMap
     *         if availableMinFrameDurations is null.
     */
    public HashMap<Size, Long> getAvailableMinFrameDurationsForFormatChecked(int format) {

        HashMap<Size, Long> minDurationMap = new HashMap<Size, Long>();

        Key<StreamConfigurationMap> key =
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP;
        StreamConfigurationMap config = getValueFromKeyNonNull(key);

        if (config == null) {
            return minDurationMap;
        }

        for (android.util.Size size : getAvailableSizesForFormatChecked(format,
                StreamDirection.Output)) {
            long minFrameDuration = config.getOutputMinFrameDuration(format, size);

            if (minFrameDuration != 0) {
                minDurationMap.put(new Size(size.getWidth(), size.getHeight()), minFrameDuration);
            }
        }

        return minDurationMap;
    }

    public int[] getAvailableEdgeModesChecked() {
        Key<int[]> key = CameraCharacteristics.EDGE_AVAILABLE_EDGE_MODES;
        int[] edgeModes = getValueFromKeyNonNull(key);

        if (edgeModes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(edgeModes));
        // Full device should always include OFF and FAST
        if (isHardwareLevelAtLeastFull()) {
            checkTrueForKey(key, ""Full device must contain OFF and FAST edge modes"",
                    modeList.contains(CameraMetadata.EDGE_MODE_OFF) &&
                    modeList.contains(CameraMetadata.EDGE_MODE_FAST));
        }

        if (isHardwareLevelAtLeastLimited()) {
            // FAST and HIGH_QUALITY mode must be both present or both not present
            List<Integer> coupledModes = Arrays.asList(new Integer[] {
                    CameraMetadata.EDGE_MODE_FAST,
                    CameraMetadata.EDGE_MODE_HIGH_QUALITY
            });
            checkTrueForKey(
                    key, "" FAST and HIGH_QUALITY mode must both present or both not present"",
                    containsAllOrNone(modeList, coupledModes));
        }

        return edgeModes;
    }

      public int[] getAvailableShadingModesChecked() {
        Key<int[]> key = CameraCharacteristics.SHADING_AVAILABLE_MODES;
        int[] shadingModes = getValueFromKeyNonNull(key);

        if (shadingModes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(shadingModes));
        // Full device should always include OFF and FAST
        if (isHardwareLevelAtLeastFull()) {
            checkTrueForKey(key, ""Full device must contain OFF and FAST shading modes"",
                    modeList.contains(CameraMetadata.SHADING_MODE_OFF) &&
                    modeList.contains(CameraMetadata.SHADING_MODE_FAST));
        }

        if (isHardwareLevelAtLeastLimited()) {
            // FAST and HIGH_QUALITY mode must be both present or both not present
            List<Integer> coupledModes = Arrays.asList(new Integer[] {
                    CameraMetadata.SHADING_MODE_FAST,
                    CameraMetadata.SHADING_MODE_HIGH_QUALITY
            });
            checkTrueForKey(
                    key, "" FAST and HIGH_QUALITY mode must both present or both not present"",
                    containsAllOrNone(modeList, coupledModes));
        }

        return shadingModes;
    }

    public int[] getAvailableNoiseReductionModesChecked() {
        Key<int[]> key =
                CameraCharacteristics.NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES;
        int[] noiseReductionModes = getValueFromKeyNonNull(key);

        if (noiseReductionModes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(noiseReductionModes));
        // Full device should always include OFF and FAST
        if (isHardwareLevelAtLeastFull()) {

            checkTrueForKey(key, ""Full device must contain OFF and FAST noise reduction modes"",
                    modeList.contains(CameraMetadata.NOISE_REDUCTION_MODE_OFF) &&
                    modeList.contains(CameraMetadata.NOISE_REDUCTION_MODE_FAST));
        }

        if (isHardwareLevelAtLeastLimited()) {
            // FAST and HIGH_QUALITY mode must be both present or both not present
            List<Integer> coupledModes = Arrays.asList(new Integer[] {
                    CameraMetadata.NOISE_REDUCTION_MODE_FAST,
                    CameraMetadata.NOISE_REDUCTION_MODE_HIGH_QUALITY
            });
            checkTrueForKey(
                    key, "" FAST and HIGH_QUALITY mode must both present or both not present"",
                    containsAllOrNone(modeList, coupledModes));
        }
        return noiseReductionModes;
    }

    /**
     * Get value of key android.control.aeCompensationStep and do the validity check.
     *
     * @return default value if the value is null.
     */
    public Rational getAeCompensationStepChecked() {
        Key<Rational> key =
                CameraCharacteristics.CONTROL_AE_COMPENSATION_STEP;
        Rational compensationStep = getValueFromKeyNonNull(key);

        if (compensationStep == null) {
            // Return default step.
            return CONTROL_AE_COMPENSATION_STEP_DEFAULT;
        }

        // Legacy devices don't have a minimum step requirement
        if (isHardwareLevelAtLeastLimited()) {
            float compensationStepF =
                    (float) compensationStep.getNumerator() / compensationStep.getDenominator();
            checkTrueForKey(key, "" value must be no more than 1/2"", compensationStepF <= 0.5f);
        }

        return compensationStep;
    }

    /**
     * Get value of key android.control.aeCompensationRange and do the validity check.
     *
     * @return default value if the value is null or malformed.
     */
    public Range<Integer> getAeCompensationRangeChecked() {
        Key<Range<Integer>> key =
                CameraCharacteristics.CONTROL_AE_COMPENSATION_RANGE;
        Range<Integer> compensationRange = getValueFromKeyNonNull(key);
        Rational compensationStep = getAeCompensationStepChecked();
        float compensationStepF = compensationStep.floatValue();
        final Range<Integer> DEFAULT_RANGE = Range.create(
                (int)(CONTROL_AE_COMPENSATION_RANGE_DEFAULT_MIN / compensationStepF),
                (int)(CONTROL_AE_COMPENSATION_RANGE_DEFAULT_MAX / compensationStepF));
        final Range<Integer> ZERO_RANGE = Range.create(0, 0);
        if (compensationRange == null) {
            return ZERO_RANGE;
        }

        // Legacy devices don't have a minimum range requirement
        if (isHardwareLevelAtLeastLimited() && !compensationRange.equals(ZERO_RANGE)) {
            checkTrueForKey(key, "" range value must be at least "" + DEFAULT_RANGE
                    + "", actual "" + compensationRange + "", compensation step "" + compensationStep,
                   compensationRange.getLower() <= DEFAULT_RANGE.getLower() &&
                   compensationRange.getUpper() >= DEFAULT_RANGE.getUpper());
        }

        return compensationRange;
    }

    /**
     * Get availableVideoStabilizationModes and do the validity check.
     *
     * @return available video stabilization modes, empty array if it is unavailable.
     */
    public int[] getAvailableVideoStabilizationModesChecked() {
        Key<int[]> key =
                CameraCharacteristics.CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES;
        int[] modes = getValueFromKeyNonNull(key);

        if (modes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
        checkTrueForKey(key, "" All device should support OFF mode"",
                modeList.contains(CameraMetadata.CONTROL_VIDEO_STABILIZATION_MODE_OFF));
        checkArrayValuesInRange(key, modes,
                CameraMetadata.CONTROL_VIDEO_STABILIZATION_MODE_OFF,
                CameraMetadata.CONTROL_VIDEO_STABILIZATION_MODE_ON);

        return modes;
    }

    public boolean isVideoStabilizationSupported() {
        Integer[] videoStabModes =
                CameraTestUtils.toObject(getAvailableVideoStabilizationModesChecked());
        return Arrays.asList(videoStabModes).contains(
                CameraMetadata.CONTROL_VIDEO_STABILIZATION_MODE_ON);
    }

    /**
     * Get availableOpticalStabilization and do the validity check.
     *
     * @return available optical stabilization modes, empty array if it is unavailable.
     */
    public int[] getAvailableOpticalStabilizationChecked() {
        Key<int[]> key =
                CameraCharacteristics.LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION;
        int[] modes = getValueFromKeyNonNull(key);

        if (modes == null) {
            return new int[0];
        }

        checkArrayValuesInRange(key, modes,
                CameraMetadata.LENS_OPTICAL_STABILIZATION_MODE_OFF,
                CameraMetadata.LENS_OPTICAL_STABILIZATION_MODE_ON);

        return modes;
    }

    /**
     * Get the scaler's max digital zoom ({@code >= 1.0f}) ratio between crop and active array
     * @return the max zoom ratio, or {@code 1.0f} if the value is unavailable
     */
    public float getAvailableMaxDigitalZoomChecked() {
        Key<Float> key =
                CameraCharacteristics.SCALER_AVAILABLE_MAX_DIGITAL_ZOOM;

        Float maxZoom = getValueFromKeyNonNull(key);
        if (maxZoom == null) {
            return 1.0f;
        }

        checkTrueForKey(key, "" max digital zoom should be no less than 1"",
                maxZoom >= 1.0f && !Float.isNaN(maxZoom) && !Float.isInfinite(maxZoom));

        return maxZoom;
    }

    public Range<Float> getZoomRatioRangeChecked() {
        Key<Range<Float>> key =
                CameraCharacteristics.CONTROL_ZOOM_RATIO_RANGE;

        Range<Float> zoomRatioRange = getValueFromKeyNonNull(key);
        if (zoomRatioRange == null) {
            return new Range<Float>(1.0f, 1.0f);
        }

        checkTrueForKey(key, String.format("" min zoom ratio %f should be no more than 1"",
                zoomRatioRange.getLower()), zoomRatioRange.getLower() <= 1.0);
        checkTrueForKey(key, String.format("" max zoom ratio %f should be no less than 1"",
                zoomRatioRange.getUpper()), zoomRatioRange.getUpper() >= 1.0);
        final float ZOOM_MIN_RANGE = 0.01f;
        checkTrueForKey(key, "" zoom ratio range should be reasonably large"",
                zoomRatioRange.getUpper().equals(zoomRatioRange.getLower()) ||
                zoomRatioRange.getUpper() - zoomRatioRange.getLower() > ZOOM_MIN_RANGE);
        return zoomRatioRange;
    }

    public int[] getAvailableSceneModesChecked() {
        Key<int[]> key =
                CameraCharacteristics.CONTROL_AVAILABLE_SCENE_MODES;
        int[] modes = getValueFromKeyNonNull(key);

        if (modes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
        // FACE_PRIORITY must be included if face detection is supported.
        if (areKeysAvailable(CameraCharacteristics.STATISTICS_INFO_MAX_FACE_COUNT) &&
                getMaxFaceCountChecked() > 0) {
            checkTrueForKey(key, "" FACE_PRIORITY must be included if face detection is supported"",
                    modeList.contains(CameraMetadata.CONTROL_SCENE_MODE_FACE_PRIORITY));
        }

        return modes;
    }

    public int[] getAvailableEffectModesChecked() {
        Key<int[]> key =
                CameraCharacteristics.CONTROL_AVAILABLE_EFFECTS;
        int[] modes = getValueFromKeyNonNull(key);

        if (modes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
        // OFF must be included.
        checkTrueForKey(key, "" OFF must be included"",
                modeList.contains(CameraMetadata.CONTROL_EFFECT_MODE_OFF));

        return modes;
    }

    public Capability[] getAvailableExtendedSceneModeCapsChecked() {
        final Size FULL_HD = new Size(1920, 1080);
        Rect activeRect = getValueFromKeyNonNull(
                CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE);
        Key<Capability[]> key =
                CameraCharacteristics.CONTROL_AVAILABLE_EXTENDED_SCENE_MODE_CAPABILITIES;
        Capability[] caps = mCharacteristics.get(key);
        if (caps == null) {
            return new Capability[0];
        }

        Size[] yuvSizes = getAvailableSizesForFormatChecked(ImageFormat.YUV_420_888,
                StaticMetadata.StreamDirection.Output);
        List<Size> yuvSizesList = Arrays.asList(yuvSizes);
        for (Capability cap : caps) {
            int extendedSceneMode = cap.getMode();
            Size maxStreamingSize = cap.getMaxStreamingSize();
            boolean maxStreamingSizeIsZero =
                    maxStreamingSize.getWidth() == 0 && maxStreamingSize.getHeight() == 0;
            switch (extendedSceneMode) {
                case CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE:
                    // STILL_CAPTURE: Must either be (0, 0), or one of supported yuv/private sizes.
                    // Because spec requires yuv and private sizes match, only check YUV sizes here.
                    checkTrueForKey(key,
                            String.format("" maxStreamingSize [%d, %d] for extended scene mode "" +
                            ""%d must be a supported YCBCR_420_888 size, or (0, 0)"",
                            maxStreamingSize.getWidth(), maxStreamingSize.getHeight(),
                            extendedSceneMode),
                            yuvSizesList.contains(maxStreamingSize) || maxStreamingSizeIsZero);
                    break;
                case CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_CONTINUOUS:
                    // CONTINUOUS: Must be one of supported yuv/private stream sizes.
                    checkTrueForKey(key,
                            String.format("" maxStreamingSize [%d, %d] for extended scene mode "" +
                            ""%d must be a supported YCBCR_420_888 size."",
                            maxStreamingSize.getWidth(), maxStreamingSize.getHeight(),
                            extendedSceneMode), yuvSizesList.contains(maxStreamingSize));
                    // Must be at least 1080p if sensor is at least 1080p.
                    if (activeRect.width() >= FULL_HD.getWidth() &&
                            activeRect.height() >= FULL_HD.getHeight()) {
                        checkTrueForKey(key,
                                String.format("" maxStreamingSize [%d, %d] for extended scene "" +
                                ""mode %d must be at least 1080p"", maxStreamingSize.getWidth(),
                                maxStreamingSize.getHeight(), extendedSceneMode),
                                maxStreamingSize.getWidth() >= FULL_HD.getWidth() &&
                                maxStreamingSize.getHeight() >= FULL_HD.getHeight());
                    }
                    break;
                default:
                    break;
            }
        }

        return caps;
    }

    /**
     * Get and check the available color aberration modes
     *
     * @return the available color aberration modes
     */
    public int[] getAvailableColorAberrationModesChecked() {
        Key<int[]> key =
                CameraCharacteristics.COLOR_CORRECTION_AVAILABLE_ABERRATION_MODES;
        int[] modes = getValueFromKeyNonNull(key);

        if (modes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
        checkTrueForKey(key, "" Camera devices must always support either OFF or FAST mode"",
                modeList.contains(CameraMetadata.COLOR_CORRECTION_ABERRATION_MODE_OFF) ||
                modeList.contains(CameraMetadata.COLOR_CORRECTION_ABERRATION_MODE_FAST));

        if (isHardwareLevelAtLeastLimited()) {
            // FAST and HIGH_QUALITY mode must be both present or both not present
            List<Integer> coupledModes = Arrays.asList(new Integer[] {
                    CameraMetadata.COLOR_CORRECTION_ABERRATION_MODE_FAST,
                    CameraMetadata.COLOR_CORRECTION_ABERRATION_MODE_HIGH_QUALITY
            });
            checkTrueForKey(
                    key, "" FAST and HIGH_QUALITY mode must both present or both not present"",
                    containsAllOrNone(modeList, coupledModes));
        }
        checkElementDistinct(key, modeList);
        checkArrayValuesInRange(key, modes,
                CameraMetadata.COLOR_CORRECTION_ABERRATION_MODE_OFF,
                CameraMetadata.COLOR_CORRECTION_ABERRATION_MODE_HIGH_QUALITY);

        return modes;
    }

    /**
     * Get max pipeline depth and do the validity check.
     *
     * @return max pipeline depth, default value if it is not available.
     */
    public byte getPipelineMaxDepthChecked() {
        Key<Byte> key =
                CameraCharacteristics.REQUEST_PIPELINE_MAX_DEPTH;
        Byte maxDepth = getValueFromKeyNonNull(key);

        if (maxDepth == null) {
            return REQUEST_PIPELINE_MAX_DEPTH_MAX;
        }

        checkTrueForKey(key, "" max pipeline depth should be no larger than ""
                + REQUEST_PIPELINE_MAX_DEPTH_MAX, maxDepth <= REQUEST_PIPELINE_MAX_DEPTH_MAX);

        return maxDepth;
    }

    /**
     * Get available lens shading modes.
     */
     public int[] getAvailableLensShadingModesChecked() {
         Key<int[]> key =
                 CameraCharacteristics.SHADING_AVAILABLE_MODES;
         int[] modes = getValueFromKeyNonNull(key);
         if (modes == null) {
             return new int[0];
         }

         List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
         // FAST must be included.
         checkTrueForKey(key, "" FAST must be included"",
                 modeList.contains(CameraMetadata.SHADING_MODE_FAST));

         if (isCapabilitySupported(
                 CameraMetadata.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_POST_PROCESSING)) {
             checkTrueForKey(key, "" OFF must be included for MANUAL_POST_PROCESSING devices"",
                     modeList.contains(CameraMetadata.SHADING_MODE_OFF));
         }
         return modes;
     }

     /**
      * Get available lens shading map modes.
      */
      public int[] getAvailableLensShadingMapModesChecked() {
          Key<int[]> key =
                  CameraCharacteristics.STATISTICS_INFO_AVAILABLE_LENS_SHADING_MAP_MODES;
          int[] modes = getValueFromKeyNonNull(key);
          if (modes == null) {
              return new int[0];
          }

          List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));

          if (isCapabilitySupported(
                  CameraMetadata.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
              checkTrueForKey(key, "" ON must be included for RAW capability devices"",
                      modeList.contains(CameraMetadata.STATISTICS_LENS_SHADING_MAP_MODE_ON));
          }
          return modes;
      }


    /**
     * Get available capabilities and do the validity check.
     *
     * @return reported available capabilities list, empty list if the value is unavailable.
     */
    public List<Integer> getAvailableCapabilitiesChecked() {
        Key<int[]> key =
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES;
        int[] availableCaps = getValueFromKeyNonNull(key);
        List<Integer> capList;

        if (availableCaps == null) {
            return new ArrayList<Integer>();
        }

        checkArrayValuesInRange(key, availableCaps,
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE,
                LAST_CAPABILITY_ENUM);
        capList = Arrays.asList(CameraTestUtils.toObject(availableCaps));
        return capList;
    }

    /**
     * Determine whether the current device supports a capability or not.
     *
     * @param capability (non-negative)
     *
     * @return {@code true} if the capability is supported, {@code false} otherwise.
     *
     * @throws IllegalArgumentException if {@code capability} was negative
     *
     * @see CameraCharacteristics#REQUEST_AVAILABLE_CAPABILITIES
     */
    public boolean isCapabilitySupported(int capability) {
        if (capability < 0) {
            throw new IllegalArgumentException(""capability must be non-negative"");
        }

        List<Integer> availableCapabilities = getAvailableCapabilitiesChecked();

        return availableCapabilities.contains(capability);
    }

    /**
     * Determine whether the current device supports a private reprocessing capability or not.
     *
     * @return {@code true} if the capability is supported, {@code false} otherwise.
     *
     * @throws IllegalArgumentException if {@code capability} was negative
     */
    public boolean isPrivateReprocessingSupported() {
        return isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING);
    }

    /**
     * Get sorted (descending order) size list for given input format. Remove the sizes larger than
     * the bound. If the bound is null, don't do the size bound filtering.
     *
     * @param format input format
     * @param bound maximum allowed size bound
     *
     * @return Sorted input size list (descending order)
     */
    public List<Size> getSortedSizesForInputFormat(int format, Size bound) {
        Size[] availableSizes = getAvailableSizesForFormatChecked(format, StreamDirection.Input);
        if (bound == null) {
            return CameraTestUtils.getAscendingOrderSizes(Arrays.asList(availableSizes),
                    /*ascending*/false);
        }

        List<Size> sizes = new ArrayList<Size>();
        for (Size sz: availableSizes) {
            if (sz.getWidth() <= bound.getWidth() && sz.getHeight() <= bound.getHeight()) {
                sizes.add(sz);
            }
        }

        return CameraTestUtils.getAscendingOrderSizes(sizes, /*ascending*/false);
    }


    /**
     * Determine whether or not all the {@code keys} are available characteristics keys
     * (as in {@link CameraCharacteristics#getKeys}.
     *
     * <p>If this returns {@code true}, then querying for this key from a characteristics
     * object will always return a non-{@code null} value.</p>
     *
     * @param keys collection of camera characteristics keys
     * @return whether or not all characteristics keys are available
     */
    public final boolean areCharacteristicsKeysAvailable(
            Collection<CameraCharacteristics.Key<?>> keys) {
        return mCharacteristics.getKeys().containsAll(keys);
    }

    /**
     * Determine whether or not all the {@code keys} are available result keys
     * (as in {@link CameraCharacteristics#getAvailableCaptureResultKeys}.
     *
     * <p>If this returns {@code true}, then querying for this key from a result
     * object will almost always return a non-{@code null} value.</p>
     *
     * <p>In some cases (e.g. lens shading map), the request must have additional settings
     * configured in order for the key to correspond to a value.</p>
     *
     * @param keys collection of capture result keys
     * @return whether or not all result keys are available
     */
    public final boolean areResultKeysAvailable(Collection<CaptureResult.Key<?>> keys) {
        return mCharacteristics.getAvailableCaptureResultKeys().containsAll(keys);
    }

    /**
     * Determine whether or not all the {@code keys} are available request keys
     * (as in {@link CameraCharacteristics#getAvailableCaptureRequestKeys}.
     *
     * <p>If this returns {@code true}, then setting this key in the request builder
     * may have some effect (and if it's {@code false}, then the camera device will
     * definitely ignore it).</p>
     *
     * <p>In some cases (e.g. manual control of exposure), other keys must be also be set
     * in order for a key to take effect (e.g. control.mode set to OFF).</p>
     *
     * @param keys collection of capture request keys
     * @return whether or not all result keys are available
     */
    public final boolean areRequestKeysAvailable(Collection<CaptureRequest.Key<?>> keys) {
        return mCharacteristics.getAvailableCaptureRequestKeys().containsAll(keys);
    }

    /**
     * Determine whether or not all the {@code keys} are available characteristics keys
     * (as in {@link CameraCharacteristics#getKeys}.
     *
     * <p>If this returns {@code true}, then querying for this key from a characteristics
     * object will always return a non-{@code null} value.</p>
     *
     * @param keys one or more camera characteristic keys
     * @return whether or not all characteristics keys are available
     */
    @SafeVarargs
    public final boolean areKeysAvailable(CameraCharacteristics.Key<?>... keys) {
        return areCharacteristicsKeysAvailable(Arrays.asList(keys));
    }

    /**
     * Determine whether or not all the {@code keys} are available result keys
     * (as in {@link CameraCharacteristics#getAvailableCaptureResultKeys}.
     *
     * <p>If this returns {@code true}, then querying for this key from a result
     * object will almost always return a non-{@code null} value.</p>
     *
     * <p>In some cases (e.g. lens shading map), the request must have additional settings
     * configured in order for the key to correspond to a value.</p>
     *
     * @param keys one or more capture result keys
     * @return whether or not all result keys are available
     */
    @SafeVarargs
    public final boolean areKeysAvailable(CaptureResult.Key<?>... keys) {
        return areResultKeysAvailable(Arrays.asList(keys));
    }

    /**
     * Determine whether or not all the {@code keys} are available request keys
     * (as in {@link CameraCharacteristics#getAvailableCaptureRequestKeys}.
     *
     * <p>If this returns {@code true}, then setting this key in the request builder
     * may have some effect (and if it's {@code false}, then the camera device will
     * definitely ignore it).</p>
     *
     * <p>In some cases (e.g. manual control of exposure), other keys must be also be set
     * in order for a key to take effect (e.g. control.mode set to OFF).</p>
     *
     * @param keys one or more capture request keys
     * @return whether or not all result keys are available
     */
    @SafeVarargs
    public final boolean areKeysAvailable(CaptureRequest.Key<?>... keys) {
        return areRequestKeysAvailable(Arrays.asList(keys));
    }

    /*
     * Determine if camera device support AE lock control
     *
     * @return {@code true} if AE lock control is supported
     */
    public boolean isAeLockSupported() {
        return getValueFromKeyNonNull(CameraCharacteristics.CONTROL_AE_LOCK_AVAILABLE);
    }

    /*
     * Determine if camera device supports keys that must be supported by
     * ULTRA_HIGH_RESOLUTION_SENSORs
     *
     * @return {@code true} if minimum set of keys are supported
     */
    public boolean areMaximumResolutionKeysSupported() {
        return mCharacteristics.get(
                CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION) != null &&
                mCharacteristics.get(
                        SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION) != null &&
                mCharacteristics.get(
                        SENSOR_INFO_PIXEL_ARRAY_SIZE_MAXIMUM_RESOLUTION) != null &&
                mCharacteristics.get(
                        SCALER_STREAM_CONFIGURATION_MAP_MAXIMUM_RESOLUTION) != null;
    }

    /*
     * Determine if camera device support AWB lock control
     *
     * @return {@code true} if AWB lock control is supported
     */
    public boolean isAwbLockSupported() {
        return getValueFromKeyNonNull(CameraCharacteristics.CONTROL_AWB_LOCK_AVAILABLE);
    }


    /*
     * Determine if camera device support manual lens shading map control
     *
     * @return {@code true} if manual lens shading map control is supported
     */
    public boolean isManualLensShadingMapSupported() {
        return areKeysAvailable(CaptureRequest.SHADING_MODE);
    }

    /**
     * Determine if camera device support manual color correction control
     *
     * @return {@code true} if manual color correction control is supported
     */
    public boolean isColorCorrectionSupported() {
        return areKeysAvailable(CaptureRequest.COLOR_CORRECTION_MODE);
    }

    /**
     * Determine if camera device support manual tone mapping control
     *
     * @return {@code true} if manual tone mapping control is supported
     */
    public boolean isManualToneMapSupported() {
        return areKeysAvailable(CaptureRequest.TONEMAP_MODE);
    }

    /**
     * Determine if camera device support manual color aberration control
     *
     * @return {@code true} if manual color aberration control is supported
     */
    public boolean isManualColorAberrationControlSupported() {
        return areKeysAvailable(CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE);
    }

    /**
     * Determine if camera device support edge mode control
     *
     * @return {@code true} if edge mode control is supported
     */
    public boolean isEdgeModeControlSupported() {
        return areKeysAvailable(CaptureRequest.EDGE_MODE);
    }

    /**
     * Determine if camera device support hot pixel mode control
     *
     * @return {@code true} if hot pixel mode control is supported
     */
    public boolean isHotPixelMapModeControlSupported() {
        return areKeysAvailable(CaptureRequest.HOT_PIXEL_MODE);
    }

    /**
     * Determine if camera device support noise reduction mode control
     *
     * @return {@code true} if noise reduction mode control is supported
     */
    public boolean isNoiseReductionModeControlSupported() {
        return areKeysAvailable(CaptureRequest.NOISE_REDUCTION_MODE);
    }

    /**
     * Get max number of output raw streams and do the basic validity check.
     *
     * @return reported max number of raw output stream
     */
    public int getMaxNumOutputStreamsRawChecked() {
        Integer maxNumStreams =
                getValueFromKeyNonNull(CameraCharacteristics.REQUEST_MAX_NUM_OUTPUT_RAW);
        if (maxNumStreams == null)
            return 0;
        return maxNumStreams;
    }

    /**
     * Get max number of output processed streams and do the basic validity check.
     *
     * @return reported max number of processed output stream
     */
    public int getMaxNumOutputStreamsProcessedChecked() {
        Integer maxNumStreams =
                getValueFromKeyNonNull(CameraCharacteristics.REQUEST_MAX_NUM_OUTPUT_PROC);
        if (maxNumStreams == null)
            return 0;
        return maxNumStreams;
    }

    /**
     * Get max number of output stalling processed streams and do the basic validity check.
     *
     * @return reported max number of stalling processed output stream
     */
    public int getMaxNumOutputStreamsProcessedStallChecked() {
        Integer maxNumStreams =
                getValueFromKeyNonNull(CameraCharacteristics.REQUEST_MAX_NUM_OUTPUT_PROC_STALLING);
        if (maxNumStreams == null)
            return 0;
        return maxNumStreams;
    }

    /**
     * Get lens facing and do the validity check
     * @return lens facing, return default value (BACK) if value is unavailable.
     */
    public int getLensFacingChecked() {
        Key<Integer> key =
                CameraCharacteristics.LENS_FACING;
        Integer facing = getValueFromKeyNonNull(key);

        if (facing == null) {
            return CameraCharacteristics.LENS_FACING_BACK;
        }

        checkTrueForKey(key, "" value is out of range "",
                facing >= CameraCharacteristics.LENS_FACING_FRONT &&
                facing <= CameraCharacteristics.LENS_FACING_EXTERNAL);
        return facing;
    }

    /**
     * Get maxCaptureStall frames or default value (if value doesn't exist)
     * @return maxCaptureStall frames or default value.
     */
    public int getMaxCaptureStallOrDefault() {
        Key<Integer> key =
                CameraCharacteristics.REPROCESS_MAX_CAPTURE_STALL;
        Integer value = getValueFromKeyNonNull(key);

        if (value == null) {
            return MAX_REPROCESS_MAX_CAPTURE_STALL;
        }

        checkTrueForKey(key, "" value is out of range "",
                value >= 0 &&
                value <= MAX_REPROCESS_MAX_CAPTURE_STALL);

        return value;
    }

    /**
     * Get the scaler's cropping type (center only or freeform)
     * @return cropping type, return default value (CENTER_ONLY) if value is unavailable
     */
    public int getScalerCroppingTypeChecked() {
        Key<Integer> key =
                CameraCharacteristics.SCALER_CROPPING_TYPE;
        Integer value = getValueFromKeyNonNull(key);

        if (value == null) {
            return CameraCharacteristics.SCALER_CROPPING_TYPE_CENTER_ONLY;
        }

        checkTrueForKey(key, "" value is out of range "",
                value >= CameraCharacteristics.SCALER_CROPPING_TYPE_CENTER_ONLY &&
                value <= CameraCharacteristics.SCALER_CROPPING_TYPE_FREEFORM);

        return value;
    }

    /**
     * Check if the constrained high speed video is supported by the camera device.
     * The high speed FPS ranges and sizes are sanitized in
     * ExtendedCameraCharacteristicsTest#testConstrainedHighSpeedCapability.
     *
     * @return true if the constrained high speed video is supported, false otherwise.
     */
    public boolean isConstrainedHighSpeedVideoSupported() {
        List<Integer> availableCapabilities = getAvailableCapabilitiesChecked();
        return (availableCapabilities.contains(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_CONSTRAINED_HIGH_SPEED_VIDEO));
    }

    /**
     * Check if this camera device is a logical multi-camera backed by multiple
     * physical cameras.
     *
     * @return true if this is a logical multi-camera.
     */
    public boolean isLogicalMultiCamera() {
        List<Integer> availableCapabilities = getAvailableCapabilitiesChecked();
        return (availableCapabilities.contains(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_LOGICAL_MULTI_CAMERA));
    }

    /**
     * Check if this camera device is an ULTRA_HIGH_RESOLUTION_SENSOR
     *
     * @return true if this is an ultra high resolution sensor
     */
    public boolean isUltraHighResolutionSensor() {
        List<Integer> availableCapabilities = getAvailableCapabilitiesChecked();
        return (availableCapabilities.contains(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR));
    }
    /**
     * Check if this camera device is a monochrome camera with Y8 support.
     *
     * @return true if this is a monochrome camera with Y8 support.
     */
    public boolean isMonochromeWithY8() {
        int[] supportedFormats = getAvailableFormats(
                StaticMetadata.StreamDirection.Output);
        return (isColorOutputSupported()
                && isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MONOCHROME)
                && CameraTestUtils.contains(supportedFormats, ImageFormat.Y8));
    }

    /**
     * Check if high speed video is supported (HIGH_SPEED_VIDEO scene mode is
     * supported, supported high speed fps ranges and sizes are valid).
     *
     * @return true if high speed video is supported.
     */
    public boolean isHighSpeedVideoSupported() {
        List<Integer> sceneModes =
                Arrays.asList(CameraTestUtils.toObject(getAvailableSceneModesChecked()));
        if (sceneModes.contains(CameraCharacteristics.CONTROL_SCENE_MODE_HIGH_SPEED_VIDEO)) {
            StreamConfigurationMap config =
                    getValueFromKeyNonNull(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            if (config == null) {
                return false;
            }
            Size[] availableSizes = config.getHighSpeedVideoSizes();
            if (availableSizes.length == 0) {
                return false;
            }

            for (Size size : availableSizes) {
                Range<Integer>[] availableFpsRanges = config.getHighSpeedVideoFpsRangesFor(size);
                if (availableFpsRanges.length == 0) {
                    return false;
                }
            }

            return true;
        } else {
            return false;
        }
    }

    /**
     * Check if depth output is supported, based on the depth capability
     */
    public boolean isDepthOutputSupported() {
        return isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_DEPTH_OUTPUT);
    }

    /**
     * Check if offline processing is supported, based on the respective capability
     */
    public boolean isOfflineProcessingSupported() {
        return isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_OFFLINE_PROCESSING);
    }

    /**
     * Check if standard outputs (PRIVATE, YUV, JPEG) outputs are supported, based on the
     * backwards-compatible capability
     */
    public boolean isColorOutputSupported() {
        return isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE);
    }

    /**
     * Check if this camera is a MONOCHROME camera.
     */
    public boolean isMonochromeCamera() {
        return isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MONOCHROME);
    }

    /**
     * Check if optical black regions key is supported.
     */
    public boolean isOpticalBlackRegionSupported() {
        return areKeysAvailable(CameraCharacteristics.SENSOR_OPTICAL_BLACK_REGIONS);
    }

    /**
     * Check if HEIC format is supported
     */
    public boolean isHeicSupported() {
        int[] formats = getAvailableFormats(StaticMetadata.StreamDirection.Output);
        return CameraTestUtils.contains(formats, ImageFormat.HEIC);
    }

    /**
     * Check if Depth Jpeg format is supported
     */
    public boolean isDepthJpegSupported() {
        int[] formats = getAvailableFormats(StaticMetadata.StreamDirection.Output);
        return CameraTestUtils.contains(formats, ImageFormat.DEPTH_JPEG);
    }

    /**
     * Check if the dynamic black level is supported.
     *
     * <p>
     * Note that: This also indicates if the white level is supported, as dynamic black and white
     * level must be all supported or none of them is supported.
     * </p>
     */
    public boolean isDynamicBlackLevelSupported() {
        return areKeysAvailable(CaptureResult.SENSOR_DYNAMIC_BLACK_LEVEL);
    }

    /**
     * Check if the enable ZSL key is supported.
     */
    public boolean isEnableZslSupported() {
        return areKeysAvailable(CaptureRequest.CONTROL_ENABLE_ZSL);
    }

    /**
     * Check if AF scene change key is supported.
     */
    public boolean isAfSceneChangeSupported() {
        return areKeysAvailable(CaptureResult.CONTROL_AF_SCENE_CHANGE);
    }

    /**
     * Check if OIS data mode is supported.
     */
    public boolean isOisDataModeSupported() {
        int[] availableOisDataModes = mCharacteristics.get(
                CameraCharacteristics.STATISTICS_INFO_AVAILABLE_OIS_DATA_MODES);

        if (availableOisDataModes == null) {
            return false;
        }

        for (int mode : availableOisDataModes) {
            if (mode == CameraMetadata.STATISTICS_OIS_DATA_MODE_ON) {
                return true;
            }
        }

        return false;
    }

    /**
     * Check if rotate and crop is supported
     */
    public boolean isRotateAndCropSupported() {
        int[] availableRotateAndCropModes = mCharacteristics.get(
                CameraCharacteristics.SCALER_AVAILABLE_ROTATE_AND_CROP_MODES);

        if (availableRotateAndCropModes == null) {
            return false;
        }

        for (int mode : availableRotateAndCropModes) {
            if (mode != CameraMetadata.SCALER_ROTATE_AND_CROP_NONE) {
                return true;
            }
        }

        return false;
    }

    /**
     * Check if distortion correction is supported.
     */
    public boolean isDistortionCorrectionSupported() {
        boolean distortionCorrectionSupported = false;
        int[] distortionModes = mCharacteristics.get(
                CameraCharacteristics.DISTORTION_CORRECTION_AVAILABLE_MODES);
        if (distortionModes == null) {
            return false;
        }

        for (int mode : distortionModes) {
            if (mode != CaptureRequest.DISTORTION_CORRECTION_MODE_OFF) {
                return true;
            }
        }

        return false;
    }

    /**
     * Check if active physical camera Id metadata is supported.
     */
    public boolean isActivePhysicalCameraIdSupported() {
        return areKeysAvailable(CaptureResult.LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID);
    }

    /**
     * Get the value in index for a fixed-size array from a given key.
     *
     * <p>If the camera device is incorrectly reporting values, log a warning and return
     * the default value instead.</p>
     *
     * @param key Key to fetch
     * @param defaultValue Default value to return if camera device uses invalid values
     * @param name Human-readable name for the array index (logging only)
     * @param index Array index of the subelement
     * @param size Expected fixed size of the array
     *
     * @return The value reported by the camera device, or the defaultValue otherwise.
     */
    private <T> T getArrayElementOrDefault(Key<?> key, T defaultValue, String name, int index,
            int size) {
        T elementValue = getArrayElementCheckRangeNonNull(
                key,
                index,
                size);

        if (elementValue == null) {
            failKeyCheck(key,
                    ""had no valid "" + name + "" value; using default of "" + defaultValue);
            elementValue = defaultValue;
        }

        return elementValue;
    }

    /**
     * Fetch an array sub-element from an array value given by a key.
     *
     * <p>
     * Prints a warning if the sub-element was null.
     * </p>
     *
     * <p>Use for variable-size arrays since this does not check the array size.</p>
     *
     * @param key Metadata key to look up
     * @param element A non-negative index value.
     * @return The array sub-element, or null if the checking failed.
     */
    private <T> T getArrayElementNonNull(Key<?> key, int element) {
        return getArrayElementCheckRangeNonNull(key, element, IGNORE_SIZE_CHECK);
    }

    /**
     * Fetch an array sub-element from an array value given by a key.
     *
     * <p>
     * Prints a warning if the array size does not match the size, or if the sub-element was null.
     * </p>
     *
     * @param key Metadata key to look up
     * @param element The index in [0,size)
     * @param size A positive size value or otherwise {@value #IGNORE_SIZE_CHECK}
     * @return The array sub-element, or null if the checking failed.
     */
    private <T> T getArrayElementCheckRangeNonNull(Key<?> key, int element, int size) {
        Object array = getValueFromKeyNonNull(key);

        if (array == null) {
            // Warning already printed
            return null;
        }

        if (size != IGNORE_SIZE_CHECK) {
            int actualLength = Array.getLength(array);
            if (actualLength != size) {
                failKeyCheck(key,
                        String.format(""had the wrong number of elements (%d), expected (%d)"",
                                actualLength, size));
                return null;
            }
        }

        @SuppressWarnings(""unchecked"")
        T val = (T) Array.get(array, element);

        if (val == null) {
            failKeyCheck(key, ""had a null element at index"" + element);
            return null;
        }

        return val;
    }

    /**
     * Gets the key, logging warnings for null values.
     */
    public <T> T getValueFromKeyNonNull(Key<T> key) {
        if (key == null) {
            throw new IllegalArgumentException(""key was null"");
        }

        T value = mCharacteristics.get(key);

        if (value == null) {
            failKeyCheck(key, ""was null"");
        }

        return value;
    }

    private void checkArrayValuesInRange(Key<int[]> key, int[] array, int min, int max) {
        for (int value : array) {
            checkTrueForKey(key, String.format("" value is out of range [%d, %d]"", min, max),
                    value <= max && value >= min);
        }
    }

    private void checkArrayValuesInRange(Key<byte[]> key, byte[] array, byte min, byte max) {
        for (byte value : array) {
            checkTrueForKey(key, String.format("" value is out of range [%d, %d]"", min, max),
                    value <= max && value >= min);
        }
    }

    /**
     * Check the uniqueness of the values in a list.
     *
     * @param key The key to be checked
     * @param list The list contains the value of the key
     */
    private <U, T> void checkElementDistinct(Key<U> key, List<T> list) {
        // Each size must be distinct.
        Set<T> sizeSet = new HashSet<T>(list);
        checkTrueForKey(key, ""Each size must be distinct"", sizeSet.size() == list.size());
    }

    private <T> void checkTrueForKey(Key<T> key, String message, boolean condition) {
        if (!condition) {
            failKeyCheck(key, message);
        }
    }

    /* Helper function to check if the coupled modes are either all present or all non-present */
    private <T> boolean containsAllOrNone(Collection<T> observedModes, Collection<T> coupledModes) {
        if (observedModes.containsAll(coupledModes)) {
            return true;
        }
        for (T mode : coupledModes) {
            if (observedModes.contains(mode)) {
                return false;
            }
        }
        return true;
    }

    private <T> void failKeyCheck(Key<T> key, String message) {
        // TODO: Consider only warning once per key/message combination if it's too spammy.
        // TODO: Consider offering other options such as throwing an assertion exception
        String failureCause = String.format(""The static info key '%s' %s"", key.getName(), message);
        switch (mLevel) {
            case WARN:
                Log.w(TAG, failureCause);
                break;
            case COLLECT:
                mCollector.addMessage(failureCause);
                break;
            case ASSERT:
                Assert.fail(failureCause);
            default:
                throw new UnsupportedOperationException(""Unhandled level "" + mLevel);
        }
    }
}"	""	""	"minimum 12 resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.fov.DetermineFovActivity"	"OnSeekBarChangeListener"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/fov/DetermineFovActivity.java"	""	"public void test/*
 *.
 */

package com.android.cts.verifier.camera.fov;

import com.android.cts.verifier.R;

import android.app.Activity;
import android.content.SharedPreferences;
import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.graphics.Canvas;
import android.graphics.Color;
import android.graphics.Paint;
import android.graphics.RectF;
import android.os.Bundle;
import android.preference.PreferenceManager;
import android.view.SurfaceHolder;
import android.view.SurfaceView;
import android.view.View;
import android.widget.Button;
import android.widget.SeekBar;
import android.widget.SeekBar.OnSeekBarChangeListener;

import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;

/**
 * Shows the picture taken and lets the user specify the field of view (FOV).
 */
public class DetermineFovActivity extends Activity {

    private static final float FOV_ADJUSTMENT_RANGE = 20;
    private static final int SEEKBAR_MAX_VALUE = 100;
    private static final float TEXT_SIZE = 16;
    private static final float TEXT_PADDING = 0.2f;
    private static final String DEFAULT_MARKER_DISTANCE = ""36.8"";
    private static final String DEFAULT_TARGET_DISTANCE = ""99.7"";

    private float mMarkerDistanceCm;
    private SurfaceView mSurfaceView;
    private SurfaceHolder mSurfaceHolder;
    private Bitmap mPhotoBitmap;
    private float mFovMinDegrees;
    private float mFovMaxDegrees;
    private float mFovDegrees;
    private float mReportedFovDegrees;
    private SeekBar mSeekBar;
    private Button mDoneButton;
    private float mTargetDistanceCm;
    private String mMeasuredText;
    private String mReportedText;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.camera_fov_calibration_determine_fov);
        File pictureFile = PhotoCaptureActivity.getPictureFile(this);
        try {
            mPhotoBitmap =
                    BitmapFactory.decodeStream(new FileInputStream(pictureFile));
        } catch (IOException e) {
            e.printStackTrace();
        }

        mSurfaceView = (SurfaceView) findViewById(R.id.camera_fov_photo_surface);
        mSurfaceHolder = mSurfaceView.getHolder();
        mSurfaceHolder.addCallback(new SurfaceHolder.Callback() {
            @Override
            public void surfaceDestroyed(SurfaceHolder holder) {}

        @Override
        public void surfaceCreated(SurfaceHolder holder) {
            drawContents();
        }

        @Override
        public void surfaceChanged(
                SurfaceHolder holder, int format, int width, int height) {
            drawContents();
        }
        });

        mSeekBar = (SeekBar) findViewById(R.id.camera_fov_seekBar);
        mSeekBar.setMax(SEEKBAR_MAX_VALUE);
        mSeekBar.setOnSeekBarChangeListener(new OnSeekBarChangeListener() {
            @Override
            public void onStopTrackingTouch(SeekBar seekBar) {}

            @Override
            public void onStartTrackingTouch(SeekBar seekBar) {}

            @Override
            public void onProgressChanged(
                    SeekBar seekBar, int progress, boolean fromUser) {
                mFovDegrees = seekBarProgressToFovDegrees(progress);
                drawContents();
            }
        });

        mDoneButton = (Button) findViewById(R.id.camera_fov_fov_done);
        mDoneButton.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                setResult(RESULT_OK);
                CtsTestHelper.storeCtsTestResult(DetermineFovActivity.this,
                        mReportedFovDegrees, mFovDegrees);
                finish();
            }
        });
    }

    private int fovToSeekBarProgress(float fovDegrees) {
        return Math.round((fovDegrees - mFovMinDegrees)
                / (mFovMaxDegrees - mFovMinDegrees) * SEEKBAR_MAX_VALUE);
    }

    private float seekBarProgressToFovDegrees(int progress) {
        float degrees = mFovMinDegrees + (float) progress / SEEKBAR_MAX_VALUE
                * (mFovMaxDegrees - mFovMinDegrees);
        // keep only 2 decimal places.
        return (int) (degrees * 100) / 100.0f;
    }

    @Override
    protected void onResume() {
        super.onResume();

        setResult(RESULT_CANCELED);
        mMarkerDistanceCm = getMarkerDistance();
        mTargetDistanceCm = getTargetDistance();
        mReportedFovDegrees = PhotoCaptureActivity.getReportedFovDegrees();

        mFovDegrees = mReportedFovDegrees > 120 ? 60 : mReportedFovDegrees;
        mFovMaxDegrees = mFovDegrees + FOV_ADJUSTMENT_RANGE / 2;
        mFovMinDegrees = mFovDegrees - FOV_ADJUSTMENT_RANGE / 2;

        mMeasuredText = getResources().getString(R.string.camera_fov_displayed_fov_label);
        mReportedText = getResources().getString(R.string.camera_fov_reported_fov_label);

        mSeekBar.setProgress(fovToSeekBarProgress(mFovDegrees));
        drawContents();
    }

    private float getMarkerDistance() {
        // Get the marker distance from the preferences.
        SharedPreferences prefs =
                PreferenceManager.getDefaultSharedPreferences(this);
        return Float.parseFloat(prefs.getString(
                CalibrationPreferenceActivity.OPTION_MARKER_DISTANCE,
                DEFAULT_MARKER_DISTANCE));
    }

    private float getTargetDistance() {
        // Get the marker distance from the preferences.
        SharedPreferences prefs =
                PreferenceManager.getDefaultSharedPreferences(this);
        return Float.parseFloat(prefs.getString(
                CalibrationPreferenceActivity.OPTION_TARGET_DISTANCE,
                DEFAULT_TARGET_DISTANCE));
    }

    private float focalLengthPixels(float fovDegrees, float imageWidth) {
        return (float) (imageWidth
                / (2 * Math.tan(fovDegrees / 2 * Math.PI / 180.0f)));
    }

    private void drawContents() {
        SurfaceHolder holder = mSurfaceView.getHolder();
        Canvas canvas = holder.lockCanvas();
        if (canvas == null || mPhotoBitmap == null) {
            return;
        }

        int canvasWidth = canvas.getWidth();
        int canvasHeight = canvas.getHeight();
        int photoWidth = mPhotoBitmap.getWidth();
        int photoHeight = mPhotoBitmap.getHeight();
        RectF drawRect = new RectF();

        // Determine if the canvas aspect ratio is larger than that of the photo.
        float scale = (float) canvasWidth / photoWidth;
        int scaledHeight = (int) (scale * photoHeight);
        if (scaledHeight < canvasHeight) {
            // If the aspect ratio is smaller, set the destination rectangle to pad
            // vertically.
            int pad = (canvasHeight - scaledHeight) / 2;
            drawRect.set(0, pad, canvasWidth, pad + scaledHeight - 1);
        } else {
            // Set the destination rectangle to pad horizontally.
            scale = (float) canvasHeight / photoHeight;
            float scaledWidth = scale * photoWidth;
            float pad = (canvasWidth - scaledWidth) / 2;
            drawRect.set(pad, 0, pad + scaledWidth - 1, canvasHeight);
        }

        // Draw the photo.
        canvas.drawColor(Color.BLACK);
        canvas.drawBitmap(mPhotoBitmap, null, drawRect, null);

        // Draw the fov indicator text.
        Paint paint = new Paint();
        paint.setColor(0xffffffff);
        float textSize = TEXT_SIZE * DetermineFovActivity.this.getResources()
                .getDisplayMetrics().scaledDensity;
        paint.setTextSize(textSize);
        canvas.drawText(mMeasuredText + "" "" + mFovDegrees + "" degrees."", textSize,
                2 * textSize * (1.0f + TEXT_PADDING), paint);
        canvas.drawText(mReportedText + "" "" + mReportedFovDegrees + "" degrees."",
                textSize, textSize * (1.0f + TEXT_PADDING), paint);

        // Draw the image center circle.
        paint.setColor(Color.BLACK);
        paint.setStyle(Paint.Style.STROKE);
        paint.setStrokeWidth(3);
        float dstWidth = drawRect.right - drawRect.left + 1;
        float dstHeight = drawRect.bottom - drawRect.top + 1;
        float centerX = drawRect.left + dstWidth / 2;
        canvas.drawLine(centerX, drawRect.top, centerX, drawRect.bottom, paint);

        // Project the markers into the scaled image with the given field of view.
        float markerX = mMarkerDistanceCm / 2;
        float markerZ = mTargetDistanceCm;
        float focalLength = focalLengthPixels(mFovDegrees, dstWidth);
        float dx = markerX / markerZ * focalLength;
        float projectedMarkerLeft = dstWidth / 2 - dx;
        float projectedMarkerRight = dstWidth / 2 + dx;

        // Draw the marker lines over the image.
        paint.setColor(Color.GREEN);
        paint.setStrokeWidth(2);
        float markerImageLeft = projectedMarkerLeft + drawRect.left;
        canvas.drawLine(
                markerImageLeft, drawRect.top, markerImageLeft, drawRect.bottom, paint);
        float markerImageRight = projectedMarkerRight + drawRect.left;
        canvas.drawLine(markerImageRight, drawRect.top, markerImageRight,
                        drawRect.bottom, paint);

        holder.unlockCanvasAndPost(canvas);
    }
}"	""	""	"12"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraDeviceTest"	"testCreateSessionWithParameters"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CameraDeviceTest.java"	""	"public void testCreateSessionWithParameters() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                openDevice(mCameraIdsUnderTest[i], mCameraMockListener);
                waitForDeviceState(STATE_OPENED, CAMERA_OPEN_TIMEOUT_MS);

                testCreateSessionWithParametersByCamera(mCameraIdsUnderTest[i], /*reprocessable*/false);
                testCreateSessionWithParametersByCamera(mCameraIdsUnderTest[i], /*reprocessable*/true);
            }
            finally {
                closeDevice(mCameraIdsUnderTest[i], mCameraMockListener);
            }
        }
    }

    /**
     * Verify creating a session with additional parameters works
     */
    private void testCreateSessionWithParametersByCamera(String cameraId, boolean reprocessable)
            throws Exception {
        final int SESSION_TIMEOUT_MS = 1000;
        final int CAPTURE_TIMEOUT_MS = 3000;
        int inputFormat = ImageFormat.YUV_420_888;
        int outputFormat = inputFormat;
        Size outputSize = mOrderedPreviewSizes.get(0);
        Size inputSize = outputSize;
        InputConfiguration inputConfig = null;

        if (VERBOSE) {
            Log.v(TAG, ""Testing creating session with parameters for camera "" + cameraId);
        }

        CameraCharacteristics characteristics = mCameraManager.getCameraCharacteristics(cameraId);
        StreamConfigurationMap config = characteristics.get(
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);

        if (reprocessable) {
            //Pick a supported i/o format and size combination.
            //Ideally the input format should match the output.
            boolean found = false;
            int inputFormats [] = config.getInputFormats();
            if (inputFormats.length == 0) {
                return;
            }

            for (int inFormat : inputFormats) {
                int outputFormats [] = config.getValidOutputFormatsForInput(inFormat);
                for (int outFormat : outputFormats) {
                    if (inFormat == outFormat) {
                        inputFormat = inFormat;
                        outputFormat = outFormat;
                        found = true;
                        break;
                    }
                }
                if (found) {
                    break;
                }
            }

            //In case the above combination doesn't exist, pick the first first supported
            //pair.
            if (!found) {
                inputFormat = inputFormats[0];
                int outputFormats [] = config.getValidOutputFormatsForInput(inputFormat);
                assertTrue(""No output formats supported for input format: "" + inputFormat,
                        (outputFormats.length > 0));
                outputFormat = outputFormats[0];
            }

            Size inputSizes[] = config.getInputSizes(inputFormat);
            Size outputSizes[] = config.getOutputSizes(outputFormat);
            assertTrue(""No valid sizes supported for input format: "" + inputFormat,
                    (inputSizes.length > 0));
            assertTrue(""No valid sizes supported for output format: "" + outputFormat,
                    (outputSizes.length > 0));

            inputSize = inputSizes[0];
            outputSize = outputSizes[0];
            inputConfig = new InputConfiguration(inputSize.getWidth(),
                    inputSize.getHeight(), inputFormat);
        } else {
            if (config.isOutputSupportedFor(outputFormat)) {
                outputSize = config.getOutputSizes(outputFormat)[0];
            } else {
                return;
            }
        }

        ImageReader imageReader = ImageReader.newInstance(outputSize.getWidth(),
                outputSize.getHeight(), outputFormat, /*maxImages*/1);

        try {
            mSessionMockListener = spy(new BlockingSessionCallback());
            mSessionWaiter = mSessionMockListener.getStateWaiter();
            List<OutputConfiguration> outputs = new ArrayList<>();
            outputs.add(new OutputConfiguration(imageReader.getSurface()));
            SessionConfiguration sessionConfig = new SessionConfiguration(
                    SessionConfiguration.SESSION_REGULAR, outputs,
                    new HandlerExecutor(mHandler), mSessionMockListener);

            CaptureRequest.Builder builder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
            builder.addTarget(imageReader.getSurface());
            CaptureRequest request = builder.build();

            sessionConfig.setInputConfiguration(inputConfig);
            sessionConfig.setSessionParameters(request);
            mCamera.createCaptureSession(sessionConfig);

            mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);

            // Verify we can capture a frame with the session.
            SimpleCaptureCallback captureListener = new SimpleCaptureCallback();
            SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
            imageReader.setOnImageAvailableListener(imageListener, mHandler);

            mSession.capture(request, captureListener, mHandler);
            captureListener.getCaptureResultForRequest(request, CAPTURE_TIMEOUT_MS);
            imageListener.getImage(CAPTURE_TIMEOUT_MS).close();
        } finally {
            imageReader.close();
            mSession.close();
        }
    }

    /**
     * Verify creating sessions back to back and only the last one is valid for
     * submitting requests.
     */
    private void testCreateSessionsByCamera(String cameraId) throws Exception {
        final int NUM_SESSIONS = 3;
        final int SESSION_TIMEOUT_MS = 1000;
        final int CAPTURE_TIMEOUT_MS = 3000;

        if (VERBOSE) {
            Log.v(TAG, ""Testing creating sessions for camera "" + cameraId);
        }

        Size yuvSize = getSortedSizesForFormat(cameraId, mCameraManager, ImageFormat.YUV_420_888,
                /*bound*/null).get(0);
        Size jpegSize = getSortedSizesForFormat(cameraId, mCameraManager, ImageFormat.JPEG,
                /*bound*/null).get(0);

        // Create a list of image readers. JPEG for last one and YUV for the rest.
        List<ImageReader> imageReaders = new ArrayList<>();
        List<CameraCaptureSession> allSessions = new ArrayList<>();

        try {
            for (int i = 0; i < NUM_SESSIONS - 1; i++) {
                imageReaders.add(ImageReader.newInstance(yuvSize.getWidth(), yuvSize.getHeight(),
                        ImageFormat.YUV_420_888, /*maxImages*/1));
            }
            imageReaders.add(ImageReader.newInstance(jpegSize.getWidth(), jpegSize.getHeight(),
                    ImageFormat.JPEG, /*maxImages*/1));

            // Create multiple sessions back to back.
            MultipleSessionCallback sessionListener =
                    new MultipleSessionCallback(/*failOnConfigureFailed*/true);
            for (int i = 0; i < NUM_SESSIONS; i++) {
                List<Surface> outputs = new ArrayList<>();
                outputs.add(imageReaders.get(i).getSurface());
                mCamera.createCaptureSession(outputs, sessionListener, mHandler);
            }

            // Verify we get onConfigured() for all sessions.
            allSessions = sessionListener.getAllSessions(NUM_SESSIONS,
                    SESSION_TIMEOUT_MS * NUM_SESSIONS);
            assertEquals(String.format(""Got %d sessions but configured %d sessions"",
                    allSessions.size(), NUM_SESSIONS), allSessions.size(), NUM_SESSIONS);

            // Verify all sessions except the last one are closed.
            for (int i = 0; i < NUM_SESSIONS - 1; i++) {
                sessionListener.waitForSessionClose(allSessions.get(i), SESSION_TIMEOUT_MS);
            }

            // Verify we can capture a frame with the last session.
            CameraCaptureSession session = allSessions.get(allSessions.size() - 1);
            SimpleCaptureCallback captureListener = new SimpleCaptureCallback();
            ImageReader reader = imageReaders.get(imageReaders.size() - 1);
            SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
            reader.setOnImageAvailableListener(imageListener, mHandler);

            CaptureRequest.Builder builder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
            builder.addTarget(reader.getSurface());
            CaptureRequest request = builder.build();

            session.capture(request, captureListener, mHandler);
            captureListener.getCaptureResultForRequest(request, CAPTURE_TIMEOUT_MS);
            imageListener.getImage(CAPTURE_TIMEOUT_MS).close();
        } finally {
            for (ImageReader reader : imageReaders) {
                reader.close();
            }
            for (CameraCaptureSession session : allSessions) {
                session.close();
            }
        }
    }

    private void prepareTestByCamera() throws Exception {
        final int PREPARE_TIMEOUT_MS = 10000;

        mSessionMockListener = spy(new BlockingSessionCallback());

        SurfaceTexture output1 = new SurfaceTexture(1);
        Surface output1Surface = new Surface(output1);
        SurfaceTexture output2 = new SurfaceTexture(2);
        Surface output2Surface = new Surface(output2);

        ArrayList<OutputConfiguration> outConfigs = new ArrayList<OutputConfiguration> ();
        outConfigs.add(new OutputConfiguration(output1Surface));
        outConfigs.add(new OutputConfiguration(output2Surface));
        SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_REGULAR, outConfigs,
                new HandlerExecutor(mHandler), mSessionMockListener);
        CaptureRequest.Builder r = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        sessionConfig.setSessionParameters(r.build());
        mCamera.createCaptureSession(sessionConfig);

        mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);

        // Try basic prepare

        mSession.prepare(output1Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(1))
                .onSurfacePrepared(eq(mSession), eq(output1Surface));

        // Should not complain if preparing already prepared stream

        mSession.prepare(output1Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(2))
                .onSurfacePrepared(eq(mSession), eq(output1Surface));

        // Check surface not included in session

        SurfaceTexture output3 = new SurfaceTexture(3);
        Surface output3Surface = new Surface(output3);
        try {
            mSession.prepare(output3Surface);
            // Legacy camera prepare always succeed
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                fail(""Preparing surface not part of session must throw IllegalArgumentException"");
            }
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Ensure second prepare also works

        mSession.prepare(output2Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(1))
                .onSurfacePrepared(eq(mSession), eq(output2Surface));

        // Use output1

        r = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        r.addTarget(output1Surface);

        mSession.capture(r.build(), null, null);

        try {
            mSession.prepare(output1Surface);
            // Legacy camera prepare always succeed
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                fail(""Preparing already-used surface must throw IllegalArgumentException"");
            }
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Create new session with outputs 1 and 3, ensure output1Surface still can't be prepared
        // again

        mSessionMockListener = spy(new BlockingSessionCallback());

        ArrayList<Surface> outputSurfaces = new ArrayList<Surface>(
            Arrays.asList(output1Surface, output3Surface));
        mCamera.createCaptureSession(outputSurfaces, mSessionMockListener, mHandler);

        mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);

        try {
            mSession.prepare(output1Surface);
            // Legacy camera prepare always succeed
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                fail(""Preparing surface used in previous session must throw "" +
                        ""IllegalArgumentException"");
            }
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Use output3, wait for result, then make sure prepare still doesn't work

        r = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        r.addTarget(output3Surface);

        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        mSession.capture(r.build(), resultListener, mHandler);

        resultListener.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);

        try {
            mSession.prepare(output3Surface);
            // Legacy camera prepare always succeed
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                fail(""Preparing already-used surface must throw IllegalArgumentException"");
            }
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Create new session with outputs 1 and 2, ensure output2Surface can be prepared again

        mSessionMockListener = spy(new BlockingSessionCallback());

        outputSurfaces = new ArrayList<>(
            Arrays.asList(output1Surface, output2Surface));
        mCamera.createCaptureSession(outputSurfaces, mSessionMockListener, mHandler);

        mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);

        mSession.prepare(output2Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(1))
                .onSurfacePrepared(eq(mSession), eq(output2Surface));

        try {
            mSession.prepare(output1Surface);
            // Legacy camera prepare always succeed
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                fail(""Preparing surface used in previous session must throw "" +
                        ""IllegalArgumentException"");
            }
        } catch (IllegalArgumentException e) {
            // expected
        }

        output1.release();
        output2.release();
        output3.release();
    }

    private void prepareTestForSharedSurfacesByCamera() throws Exception {
        final int PREPARE_TIMEOUT_MS = 10000;

        mSessionMockListener = spy(new BlockingSessionCallback());

        SurfaceTexture output1 = new SurfaceTexture(1);
        Surface output1Surface = new Surface(output1);
        SurfaceTexture output2 = new SurfaceTexture(2);
        Surface output2Surface = new Surface(output2);

        List<Surface> outputSurfaces = new ArrayList<>(
            Arrays.asList(output1Surface, output2Surface));
        OutputConfiguration surfaceSharedConfig = new OutputConfiguration(
            OutputConfiguration.SURFACE_GROUP_ID_NONE, output1Surface);
        surfaceSharedConfig.enableSurfaceSharing();
        surfaceSharedConfig.addSurface(output2Surface);

        List<OutputConfiguration> outputConfigurations = new ArrayList<>();
        outputConfigurations.add(surfaceSharedConfig);
        mCamera.createCaptureSessionByOutputConfigurations(
                outputConfigurations, mSessionMockListener, mHandler);

        mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);

        // Try prepare on output1Surface
        mSession.prepare(output1Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(1))
                .onSurfacePrepared(eq(mSession), eq(output1Surface));
        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(1))
                .onSurfacePrepared(eq(mSession), eq(output2Surface));

        // Try prepare on output2Surface
        mSession.prepare(output2Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(2))
                .onSurfacePrepared(eq(mSession), eq(output1Surface));
        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(2))
                .onSurfacePrepared(eq(mSession), eq(output2Surface));

        // Try prepare on output1Surface again
        mSession.prepare(output1Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(3))
                .onSurfacePrepared(eq(mSession), eq(output1Surface));
        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(3))
                .onSurfacePrepared(eq(mSession), eq(output2Surface));
    }

    private void invalidRequestCaptureTestByCamera() throws Exception {
        if (VERBOSE) Log.v(TAG, ""invalidRequestCaptureTestByCamera"");

        List<CaptureRequest> emptyRequests = new ArrayList<CaptureRequest>();
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest unConfiguredRequest = requestBuilder.build();
        List<CaptureRequest> unConfiguredRequests = new ArrayList<CaptureRequest>();
        unConfiguredRequests.add(unConfiguredRequest);

        try {
            // Test: CameraCaptureSession capture should throw IAE for null request.
            mSession.capture(/*request*/null, /*listener*/null, mHandler);
            mCollector.addMessage(
                    ""Session capture should throw IllegalArgumentException for null request"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession capture should throw IAE for request
            // without surface configured.
            mSession.capture(unConfiguredRequest, /*listener*/null, mHandler);
            mCollector.addMessage(""Session capture should throw "" +
                    ""IllegalArgumentException for request without surface configured"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession setRepeatingRequest should throw IAE for null request.
            mSession.setRepeatingRequest(/*request*/null, /*listener*/null, mHandler);
            mCollector.addMessage(""Session setRepeatingRequest should throw "" +
                    ""IllegalArgumentException for null request"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession setRepeatingRequest should throw IAE for for request
            // without surface configured.
            mSession.setRepeatingRequest(unConfiguredRequest, /*listener*/null, mHandler);
            mCollector.addMessage(""Capture zero burst should throw IllegalArgumentException "" +
                    ""for request without surface configured"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession captureBurst should throw IAE for null request list.
            mSession.captureBurst(/*requests*/null, /*listener*/null, mHandler);
            mCollector.addMessage(""Session captureBurst should throw "" +
                    ""IllegalArgumentException for null request list"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession captureBurst should throw IAE for empty request list.
            mSession.captureBurst(emptyRequests, /*listener*/null, mHandler);
            mCollector.addMessage(""Session captureBurst should throw "" +
                    "" IllegalArgumentException for empty request list"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession captureBurst should throw IAE for request
            // without surface configured.
            mSession.captureBurst(unConfiguredRequests, /*listener*/null, mHandler);
            fail(""Session captureBurst should throw IllegalArgumentException "" +
                    ""for null request list"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession setRepeatingBurst should throw IAE for null request list.
            mSession.setRepeatingBurst(/*requests*/null, /*listener*/null, mHandler);
            mCollector.addMessage(""Session setRepeatingBurst should throw "" +
                    ""IllegalArgumentException for null request list"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession setRepeatingBurst should throw IAE for empty request list.
            mSession.setRepeatingBurst(emptyRequests, /*listener*/null, mHandler);
            mCollector.addMessage(""Session setRepeatingBurst should throw "" +
                    ""IllegalArgumentException for empty request list"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession setRepeatingBurst should throw IAE for request
            // without surface configured.
            mSession.setRepeatingBurst(unConfiguredRequests, /*listener*/null, mHandler);
            mCollector.addMessage(""Session setRepeatingBurst should throw "" +
                    ""IllegalArgumentException for request without surface configured"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }
    }

    private class IsCaptureResultNotEmpty
            implements ArgumentMatcher<TotalCaptureResult> {
        @Override
        public boolean matches(TotalCaptureResult result) {
            /**
             * Do the simple verification here. Only verify the timestamp for now.
             * TODO: verify more required capture result metadata fields.
             */
            Long timeStamp = result.get(CaptureResult.SENSOR_TIMESTAMP);
            if (timeStamp != null && timeStamp.longValue() > 0L) {
                return true;
            }
            return false;
        }
    }

    /**
     * Run capture test with different test configurations.
     *
     * @param burst If the test uses {@link CameraCaptureSession#captureBurst} or
     * {@link CameraCaptureSession#setRepeatingBurst} to capture the burst.
     * @param repeating If the test uses {@link CameraCaptureSession#setRepeatingBurst} or
     * {@link CameraCaptureSession#setRepeatingRequest} for repeating capture.
     * @param abort If the test uses {@link CameraCaptureSession#abortCaptures} to stop the
     * repeating capture.  It has no effect if repeating is false.
     * @param useExecutor If the test uses {@link java.util.concurrent.Executor} instead of
     * {@link android.os.Handler} for callback invocation.
     */
    private void runCaptureTest(boolean burst, boolean repeating, boolean abort,
            boolean useExecutor) throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                openDevice(mCameraIdsUnderTest[i], mCameraMockListener);
                waitForDeviceState(STATE_OPENED, CAMERA_OPEN_TIMEOUT_MS);

                prepareCapture();

                if (!burst) {
                    // Test: that a single capture of each template type succeeds.
                    for (int j = 0; j < sTemplates.length; j++) {
                        // Skip video snapshots for LEGACY mode
                        if (mStaticInfo.isHardwareLevelLegacy() &&
                                sTemplates[j] == CameraDevice.TEMPLATE_VIDEO_SNAPSHOT) {
                            continue;
                        }
                        // Skip non-PREVIEW templates for non-color output
                        if (!mStaticInfo.isColorOutputSupported() &&
                                sTemplates[j] != CameraDevice.TEMPLATE_PREVIEW) {
                            continue;
                        }

                        captureSingleShot(mCameraIdsUnderTest[i], sTemplates[j], repeating, abort,
                                useExecutor);
                    }
                }
                else {
                    // Test: burst of one shot
                    captureBurstShot(mCameraIdsUnderTest[i], sTemplates, 1, repeating, abort, useExecutor);

                    int template = mStaticInfo.isColorOutputSupported() ?
                        CameraDevice.TEMPLATE_STILL_CAPTURE :
                        CameraDevice.TEMPLATE_PREVIEW;
                    int[] templates = new int[] {
                        template,
                        template,
                        template,
                        template,
                        template
                    };

                    // Test: burst of 5 shots of the same template type
                    captureBurstShot(mCameraIdsUnderTest[i], templates, templates.length, repeating, abort,
                            useExecutor);

                    if (mStaticInfo.isColorOutputSupported()) {
                        // Test: burst of 6 shots of different template types
                        captureBurstShot(mCameraIdsUnderTest[i], sTemplates, sTemplates.length, repeating,
                                abort, useExecutor);
                    }
                }
                verify(mCameraMockListener, never())
                        .onError(
                                any(CameraDevice.class),
                                anyInt());
            } catch (Exception e) {
                mCollector.addError(e);
            } finally {
                try {
                    closeSession();
                } catch (Exception e) {
                    mCollector.addError(e);
                }finally {
                    closeDevice(mCameraIdsUnderTest[i], mCameraMockListener);
                }
            }
        }
    }

    private void captureSingleShot(
            String id,
            int template,
            boolean repeating, boolean abort, boolean useExecutor) throws Exception {

        assertEquals(""Bad initial state for preparing to capture"",
                mLatestSessionState, SESSION_READY);

        final Executor executor = useExecutor ? new HandlerExecutor(mHandler) : null;
        CaptureRequest.Builder requestBuilder = mCamera.createCaptureRequest(template);
        assertNotNull(""Failed to create capture request"", requestBuilder);
        requestBuilder.addTarget(mReaderSurface);
        CameraCaptureSession.CaptureCallback mockCaptureCallback =
                mock(CameraCaptureSession.CaptureCallback.class);

        if (VERBOSE) {
            Log.v(TAG, String.format(""Capturing shot for device %s, template %d"",
                    id, template));
        }

        if (executor != null) {
            startCapture(requestBuilder.build(), repeating, mockCaptureCallback, executor);
        } else {
            startCapture(requestBuilder.build(), repeating, mockCaptureCallback, mHandler);
        }
        waitForSessionState(SESSION_ACTIVE, SESSION_ACTIVE_TIMEOUT_MS);

        int expectedCaptureResultCount = repeating ? REPEATING_CAPTURE_EXPECTED_RESULT_COUNT : 1;
        verifyCaptureResults(mockCaptureCallback, expectedCaptureResultCount);

        if (repeating) {
            if (abort) {
                mSession.abortCaptures();
                // Have to make sure abort and new requests aren't interleave together.
                waitForSessionState(SESSION_READY, SESSION_READY_TIMEOUT_MS);

                // Capture a single capture, and verify the result.
                SimpleCaptureCallback resultCallback = new SimpleCaptureCallback();
                CaptureRequest singleRequest = requestBuilder.build();
                if (executor != null) {
                    mSession.captureSingleRequest(singleRequest, executor, resultCallback);
                } else {
                    mSession.capture(singleRequest, resultCallback, mHandler);
                }
                resultCallback.getCaptureResultForRequest(singleRequest, CAPTURE_RESULT_TIMEOUT_MS);

                // Resume the repeating, and verify that results are returned.
                if (executor != null) {
                    mSession.setSingleRepeatingRequest(singleRequest, executor, resultCallback);
                } else {
                    mSession.setRepeatingRequest(singleRequest, resultCallback, mHandler);
                }
                for (int i = 0; i < REPEATING_CAPTURE_EXPECTED_RESULT_COUNT; i++) {
                    resultCallback.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);
                }
            }
            mSession.stopRepeating();
        }
        waitForSessionState(SESSION_READY, SESSION_READY_TIMEOUT_MS);
    }

    private void captureBurstShot(
            String id,
            int[] templates,
            int len,
            boolean repeating,
            boolean abort, boolean useExecutor) throws Exception {

        assertEquals(""Bad initial state for preparing to capture"",
                mLatestSessionState, SESSION_READY);

        assertTrue(""Invalid args to capture function"", len <= templates.length);
        List<CaptureRequest> requests = new ArrayList<CaptureRequest>();
        List<CaptureRequest> postAbortRequests = new ArrayList<CaptureRequest>();
        final Executor executor = useExecutor ? new HandlerExecutor(mHandler) : null;
        for (int i = 0; i < len; i++) {
            // Skip video snapshots for LEGACY mode
            if (mStaticInfo.isHardwareLevelLegacy() &&
                    templates[i] == CameraDevice.TEMPLATE_VIDEO_SNAPSHOT) {
                continue;
            }
            // Skip non-PREVIEW templates for non-color outpu
            if (!mStaticInfo.isColorOutputSupported() &&
                    templates[i] != CameraDevice.TEMPLATE_PREVIEW) {
                continue;
            }

            CaptureRequest.Builder requestBuilder = mCamera.createCaptureRequest(templates[i]);
            assertNotNull(""Failed to create capture request"", requestBuilder);
            requestBuilder.addTarget(mReaderSurface);
            requests.add(requestBuilder.build());
            if (abort) {
                postAbortRequests.add(requestBuilder.build());
            }
        }
        CameraCaptureSession.CaptureCallback mockCaptureCallback =
                mock(CameraCaptureSession.CaptureCallback.class);

        if (VERBOSE) {
            Log.v(TAG, String.format(""Capturing burst shot for device %s"", id));
        }

        if (!repeating) {
            if (executor != null) {
                mSession.captureBurstRequests(requests, executor, mockCaptureCallback);
            } else {
                mSession.captureBurst(requests, mockCaptureCallback, mHandler);
            }
        }
        else {
            if (executor != null) {
                mSession.setRepeatingBurstRequests(requests, executor, mockCaptureCallback);
            } else {
                mSession.setRepeatingBurst(requests, mockCaptureCallback, mHandler);
            }
        }
        waitForSessionState(SESSION_ACTIVE, SESSION_READY_TIMEOUT_MS);

        int expectedResultCount = requests.size();
        if (repeating) {
            expectedResultCount *= REPEATING_CAPTURE_EXPECTED_RESULT_COUNT;
        }

        verifyCaptureResults(mockCaptureCallback, expectedResultCount);

        if (repeating) {
            if (abort) {
                mSession.abortCaptures();
                // Have to make sure abort and new requests aren't interleave together.
                waitForSessionState(SESSION_READY, SESSION_READY_TIMEOUT_MS);

                // Capture a burst of captures, and verify the results.
                SimpleCaptureCallback resultCallback = new SimpleCaptureCallback();
                if (executor != null) {
                    mSession.captureBurstRequests(postAbortRequests, executor, resultCallback);
                } else {
                    mSession.captureBurst(postAbortRequests, resultCallback, mHandler);
                }
                // Verify that the results are returned.
                for (int i = 0; i < postAbortRequests.size(); i++) {
                    resultCallback.getCaptureResultForRequest(
                            postAbortRequests.get(i), CAPTURE_RESULT_TIMEOUT_MS);
                }

                // Resume the repeating, and verify that results are returned.
                if (executor != null) {
                    mSession.setRepeatingBurstRequests(requests, executor, resultCallback);
                } else {
                    mSession.setRepeatingBurst(requests, resultCallback, mHandler);
                }
                for (int i = 0; i < REPEATING_CAPTURE_EXPECTED_RESULT_COUNT; i++) {
                    resultCallback.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);
                }
            }
            mSession.stopRepeating();
        }
        waitForSessionState(SESSION_READY, SESSION_READY_TIMEOUT_MS);
    }

    /**
     * Precondition: Device must be in known OPENED state (has been waited for).
     *
     * <p>Creates a new capture session and waits until it is in the {@code SESSION_READY} state.
     * </p>
     *
     * <p>Any existing capture session will be closed as a result of calling this.</p>
     * */
    private void prepareCapture() throws Exception {
        if (VERBOSE) Log.v(TAG, ""prepareCapture"");

        assertTrue(""Bad initial state for preparing to capture"",
                mLatestDeviceState == STATE_OPENED);

        if (mSession != null) {
            if (VERBOSE) Log.v(TAG, ""prepareCapture - closing existing session"");
            closeSession();
        }

        // Create a new session listener each time, it's not reusable across cameras
        mSessionMockListener = spy(new BlockingSessionCallback());
        mSessionWaiter = mSessionMockListener.getStateWaiter();

        if (!mStaticInfo.isColorOutputSupported()) {
            createDefaultImageReader(getMaxDepthSize(mCamera.getId(), mCameraManager),
                    ImageFormat.DEPTH16, MAX_NUM_IMAGES, new ImageDropperListener());
        } else {
            createDefaultImageReader(DEFAULT_CAPTURE_SIZE, ImageFormat.YUV_420_888, MAX_NUM_IMAGES,
                    new ImageDropperListener());
        }

        List<Surface> outputSurfaces = new ArrayList<>(Arrays.asList(mReaderSurface));
        mCamera.createCaptureSession(outputSurfaces, mSessionMockListener, mHandler);

        mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        waitForSessionState(SESSION_CONFIGURED, SESSION_CONFIGURE_TIMEOUT_MS);
        waitForSessionState(SESSION_READY, SESSION_READY_TIMEOUT_MS);
    }

    private void waitForDeviceState(int state, long timeoutMs) {
        mCameraMockListener.waitForState(state, timeoutMs);
        mLatestDeviceState = state;
    }

    private void waitForSessionState(int state, long timeoutMs) {
        mSessionWaiter.waitForState(state, timeoutMs);
        mLatestSessionState = state;
    }

    private void verifyCaptureResults(
            CameraCaptureSession.CaptureCallback mockListener,
            int expectResultCount) {
        final int TIMEOUT_PER_RESULT_MS = 2000;
        // Should receive expected number of capture results.
        verify(mockListener,
                timeout(TIMEOUT_PER_RESULT_MS * expectResultCount).atLeast(expectResultCount))
                        .onCaptureCompleted(
                                eq(mSession),
                                isA(CaptureRequest.class),
                                argThat(new IsCaptureResultNotEmpty()));
        // Should not receive any capture failed callbacks.
        verify(mockListener, never())
                        .onCaptureFailed(
                                eq(mSession),
                                isA(CaptureRequest.class),
                                isA(CaptureFailure.class));
        // Should receive expected number of capture shutter calls
        verify(mockListener,
                atLeast(expectResultCount))
                        .onCaptureStarted(
                               eq(mSession),
                               isA(CaptureRequest.class),
                               anyLong(),
                               anyLong());
    }

    private void checkFpsRange(CaptureRequest.Builder request, int template,
            CameraCharacteristics props) {
        CaptureRequest.Key<Range<Integer>> fpsRangeKey = CONTROL_AE_TARGET_FPS_RANGE;
        Range<Integer> fpsRange;
        if ((fpsRange = mCollector.expectKeyValueNotNull(request, fpsRangeKey)) == null) {
            return;
        }

        int minFps = fpsRange.getLower();
        int maxFps = fpsRange.getUpper();
        Range<Integer>[] availableFpsRange = props
                .get(CameraCharacteristics.CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES);
        boolean foundRange = false;
        for (int i = 0; i < availableFpsRange.length; i += 1) {
            if (minFps == availableFpsRange[i].getLower()
                    && maxFps == availableFpsRange[i].getUpper()) {
                foundRange = true;
                break;
            }
        }
        if (!foundRange) {
            mCollector.addMessage(String.format(""Unable to find the fps range (%d, %d)"",
                    minFps, maxFps));
            return;
        }


        if (template != CameraDevice.TEMPLATE_MANUAL &&
                template != CameraDevice.TEMPLATE_STILL_CAPTURE) {
            if (maxFps < MIN_FPS_REQUIRED_FOR_STREAMING) {
                mCollector.addMessage(""Max fps should be at least ""
                        + MIN_FPS_REQUIRED_FOR_STREAMING);
                return;
            }

            // Relax framerate constraints on legacy mode
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                // Need give fixed frame rate for video recording template.
                if (template == CameraDevice.TEMPLATE_RECORD) {
                    if (maxFps != minFps) {
                        mCollector.addMessage(""Video recording frame rate should be fixed"");
                    }
                }
            }
        }
    }

    private void checkAfMode(CaptureRequest.Builder request, int template,
            CameraCharacteristics props) {
        boolean hasFocuser = props.getKeys().contains(CameraCharacteristics.
                LENS_INFO_MINIMUM_FOCUS_DISTANCE) &&
                (props.get(CameraCharacteristics.LENS_INFO_MINIMUM_FOCUS_DISTANCE) > 0f);

        if (!hasFocuser) {
            return;
        }

        int targetAfMode = CaptureRequest.CONTROL_AF_MODE_AUTO;
        int[] availableAfMode = props.get(CameraCharacteristics.CONTROL_AF_AVAILABLE_MODES);
        if (template == CameraDevice.TEMPLATE_PREVIEW ||
                template == CameraDevice.TEMPLATE_STILL_CAPTURE ||
                template == CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG) {
            // Default to CONTINUOUS_PICTURE if it is available, otherwise AUTO.
            for (int i = 0; i < availableAfMode.length; i++) {
                if (availableAfMode[i] == CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_PICTURE) {
                    targetAfMode = CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_PICTURE;
                    break;
                }
            }
        } else if (template == CameraDevice.TEMPLATE_RECORD ||
                template == CameraDevice.TEMPLATE_VIDEO_SNAPSHOT) {
            // Default to CONTINUOUS_VIDEO if it is available, otherwise AUTO.
            for (int i = 0; i < availableAfMode.length; i++) {
                if (availableAfMode[i] == CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_VIDEO) {
                    targetAfMode = CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_VIDEO;
                    break;
                }
            }
        } else if (template == CameraDevice.TEMPLATE_MANUAL) {
            targetAfMode = CaptureRequest.CONTROL_AF_MODE_OFF;
        }

        mCollector.expectKeyValueEquals(request, CONTROL_AF_MODE, targetAfMode);
        if (mStaticInfo.areKeysAvailable(CaptureRequest.LENS_FOCUS_DISTANCE)) {
            mCollector.expectKeyValueNotNull(request, LENS_FOCUS_DISTANCE);
        }
    }

    private void checkAntiBandingMode(CaptureRequest.Builder request, int template) {
        if (template == CameraDevice.TEMPLATE_MANUAL) {
            return;
        }

        if (!mStaticInfo.isColorOutputSupported()) return;

        List<Integer> availableAntiBandingModes =
                Arrays.asList(toObject(mStaticInfo.getAeAvailableAntiBandingModesChecked()));

        if (availableAntiBandingModes.contains(CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_AUTO)) {
            mCollector.expectKeyValueEquals(request, CONTROL_AE_ANTIBANDING_MODE,
                    CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_AUTO);
        } else {
            mCollector.expectKeyValueIsIn(request, CONTROL_AE_ANTIBANDING_MODE,
                    CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_50HZ,
                    CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_60HZ);
        }
    }

    /**
     * <p>Check if 3A metering settings are ""up to HAL"" in request template</p>
     *
     * <p>This function doesn't fail the test immediately, it updates the
     * test pass/fail status and appends the failure message to the error collector each key.</p>
     *
     * @param regions The metering rectangles to be checked
     */
    private void checkMeteringRect(MeteringRectangle[] regions) {
        if (regions == null) {
            return;
        }
        mCollector.expectNotEquals(""Number of metering region should not be 0"", 0, regions.length);
        for (int i = 0; i < regions.length; i++) {
            mCollector.expectEquals(""Default metering regions should have all zero weight"",
                    0, regions[i].getMeteringWeight());
        }
    }

    /**
     * <p>Check if the request settings are suitable for a given request template.</p>
     *
     * <p>This function doesn't fail the test immediately, it updates the
     * test pass/fail status and appends the failure message to the error collector each key.</p>
     *
     * @param request The request to be checked.
     * @param template The capture template targeted by this request.
     * @param props The CameraCharacteristics this request is checked against with.
     */
    private void checkRequestForTemplate(CaptureRequest.Builder request, int template,
            CameraCharacteristics props) {
        Integer hwLevel = props.get(CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL);
        boolean isExternalCamera = (hwLevel ==
                CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL);

        // 3A settings--AE/AWB/AF.
        Integer maxRegionsAeVal = props.get(CameraCharacteristics.CONTROL_MAX_REGIONS_AE);
        int maxRegionsAe = maxRegionsAeVal != null ? maxRegionsAeVal : 0;
        Integer maxRegionsAwbVal = props.get(CameraCharacteristics.CONTROL_MAX_REGIONS_AWB);
        int maxRegionsAwb = maxRegionsAwbVal != null ? maxRegionsAwbVal : 0;
        Integer maxRegionsAfVal = props.get(CameraCharacteristics.CONTROL_MAX_REGIONS_AF);
        int maxRegionsAf = maxRegionsAfVal != null ? maxRegionsAfVal : 0;

        checkFpsRange(request, template, props);

        checkAfMode(request, template, props);
        checkAntiBandingMode(request, template);

        if (template == CameraDevice.TEMPLATE_MANUAL) {
            mCollector.expectKeyValueEquals(request, CONTROL_MODE, CaptureRequest.CONTROL_MODE_OFF);
            mCollector.expectKeyValueEquals(request, CONTROL_AE_MODE,
                    CaptureRequest.CONTROL_AE_MODE_OFF);
            mCollector.expectKeyValueEquals(request, CONTROL_AWB_MODE,
                    CaptureRequest.CONTROL_AWB_MODE_OFF);
        } else {
            mCollector.expectKeyValueEquals(request, CONTROL_MODE,
                    CaptureRequest.CONTROL_MODE_AUTO);
            if (mStaticInfo.isColorOutputSupported()) {
                mCollector.expectKeyValueEquals(request, CONTROL_AE_MODE,
                        CaptureRequest.CONTROL_AE_MODE_ON);
                mCollector.expectKeyValueEquals(request, CONTROL_AE_EXPOSURE_COMPENSATION, 0);
                mCollector.expectKeyValueEquals(request, CONTROL_AE_PRECAPTURE_TRIGGER,
                        CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_IDLE);
                // if AE lock is not supported, expect the control key to be non-exist or false
                if (mStaticInfo.isAeLockSupported() || request.get(CONTROL_AE_LOCK) != null) {
                    mCollector.expectKeyValueEquals(request, CONTROL_AE_LOCK, false);
                }

                mCollector.expectKeyValueEquals(request, CONTROL_AF_TRIGGER,
                        CaptureRequest.CONTROL_AF_TRIGGER_IDLE);

                mCollector.expectKeyValueEquals(request, CONTROL_AWB_MODE,
                        CaptureRequest.CONTROL_AWB_MODE_AUTO);
                // if AWB lock is not supported, expect the control key to be non-exist or false
                if (mStaticInfo.isAwbLockSupported() || request.get(CONTROL_AWB_LOCK) != null) {
                    mCollector.expectKeyValueEquals(request, CONTROL_AWB_LOCK, false);
                }

                // Check 3A regions.
                if (VERBOSE) {
                    Log.v(TAG, String.format(""maxRegions is: {AE: %s, AWB: %s, AF: %s}"",
                                    maxRegionsAe, maxRegionsAwb, maxRegionsAf));
                }
                if (maxRegionsAe > 0) {
                    mCollector.expectKeyValueNotNull(request, CONTROL_AE_REGIONS);
                    MeteringRectangle[] aeRegions = request.get(CONTROL_AE_REGIONS);
                    checkMeteringRect(aeRegions);
                }
                if (maxRegionsAwb > 0) {
                    mCollector.expectKeyValueNotNull(request, CONTROL_AWB_REGIONS);
                    MeteringRectangle[] awbRegions = request.get(CONTROL_AWB_REGIONS);
                    checkMeteringRect(awbRegions);
                }
                if (maxRegionsAf > 0) {
                    mCollector.expectKeyValueNotNull(request, CONTROL_AF_REGIONS);
                    MeteringRectangle[] afRegions = request.get(CONTROL_AF_REGIONS);
                    checkMeteringRect(afRegions);
                }
            }
        }

        // Sensor settings.

        mCollector.expectEquals(""Lens aperture must be present in request if available apertures "" +
                        ""are present in metadata, and vice-versa."",
                mStaticInfo.areKeysAvailable(CameraCharacteristics.LENS_INFO_AVAILABLE_APERTURES),
                mStaticInfo.areKeysAvailable(CaptureRequest.LENS_APERTURE));
        if (mStaticInfo.areKeysAvailable(CameraCharacteristics.LENS_INFO_AVAILABLE_APERTURES)) {
            float[] availableApertures =
                    props.get(CameraCharacteristics.LENS_INFO_AVAILABLE_APERTURES);
            if (availableApertures.length > 1) {
                mCollector.expectKeyValueNotNull(request, LENS_APERTURE);
            }
        }

        mCollector.expectEquals(""Lens filter density must be present in request if available "" +
                        ""filter densities are present in metadata, and vice-versa."",
                mStaticInfo.areKeysAvailable(CameraCharacteristics.
                        LENS_INFO_AVAILABLE_FILTER_DENSITIES),
                mStaticInfo.areKeysAvailable(CaptureRequest.LENS_FILTER_DENSITY));
        if (mStaticInfo.areKeysAvailable(CameraCharacteristics.
                LENS_INFO_AVAILABLE_FILTER_DENSITIES)) {
            float[] availableFilters =
                    props.get(CameraCharacteristics.LENS_INFO_AVAILABLE_FILTER_DENSITIES);
            if (availableFilters.length > 1) {
                mCollector.expectKeyValueNotNull(request, LENS_FILTER_DENSITY);
            }
        }


        if (!isExternalCamera) {
            float[] availableFocalLen =
                    props.get(CameraCharacteristics.LENS_INFO_AVAILABLE_FOCAL_LENGTHS);
            if (availableFocalLen.length > 1) {
                mCollector.expectKeyValueNotNull(request, LENS_FOCAL_LENGTH);
            }
        }


        mCollector.expectEquals(""Lens optical stabilization must be present in request if "" +
                        ""available optical stabilizations are present in metadata, and vice-versa."",
                mStaticInfo.areKeysAvailable(CameraCharacteristics.
                        LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION),
                mStaticInfo.areKeysAvailable(CaptureRequest.LENS_OPTICAL_STABILIZATION_MODE));
        if (mStaticInfo.areKeysAvailable(CameraCharacteristics.
                LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION)) {
            int[] availableOIS =
                    props.get(CameraCharacteristics.LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION);
            if (availableOIS.length > 1) {
                mCollector.expectKeyValueNotNull(request, LENS_OPTICAL_STABILIZATION_MODE);
            }
        }

        if (mStaticInfo.areKeysAvailable(SENSOR_TEST_PATTERN_MODE)) {
            mCollector.expectKeyValueEquals(request, SENSOR_TEST_PATTERN_MODE,
                    CaptureRequest.SENSOR_TEST_PATTERN_MODE_OFF);
        }

        if (mStaticInfo.areKeysAvailable(BLACK_LEVEL_LOCK)) {
            mCollector.expectKeyValueEquals(request, BLACK_LEVEL_LOCK, false);
        }

        if (mStaticInfo.areKeysAvailable(SENSOR_FRAME_DURATION)) {
            mCollector.expectKeyValueNotNull(request, SENSOR_FRAME_DURATION);
        }

        if (mStaticInfo.areKeysAvailable(SENSOR_EXPOSURE_TIME)) {
            mCollector.expectKeyValueNotNull(request, SENSOR_EXPOSURE_TIME);
        }

        if (mStaticInfo.areKeysAvailable(SENSOR_SENSITIVITY)) {
            mCollector.expectKeyValueNotNull(request, SENSOR_SENSITIVITY);
        }

        // ISP-processing settings.
        if (mStaticInfo.isColorOutputSupported()) {
            mCollector.expectKeyValueEquals(
                    request, STATISTICS_FACE_DETECT_MODE,
                    CaptureRequest.STATISTICS_FACE_DETECT_MODE_OFF);
            mCollector.expectKeyValueEquals(request, FLASH_MODE, CaptureRequest.FLASH_MODE_OFF);
        }

        List<Integer> availableCaps = mStaticInfo.getAvailableCapabilitiesChecked();
        if (mStaticInfo.areKeysAvailable(STATISTICS_LENS_SHADING_MAP_MODE)) {
            // If the device doesn't support RAW, all template should have OFF as default.
            if (!availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                mCollector.expectKeyValueEquals(
                        request, STATISTICS_LENS_SHADING_MAP_MODE,
                        CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE_OFF);
            }
        }

        boolean supportReprocessing =
                availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING) ||
                availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING);


        if (template == CameraDevice.TEMPLATE_STILL_CAPTURE) {

            // Ok with either FAST or HIGH_QUALITY
            if (mStaticInfo.areKeysAvailable(COLOR_CORRECTION_MODE)) {
                mCollector.expectKeyValueNotEquals(
                        request, COLOR_CORRECTION_MODE,
                        CaptureRequest.COLOR_CORRECTION_MODE_TRANSFORM_MATRIX);
            }

            // Edge enhancement, noise reduction and aberration correction modes.
            mCollector.expectEquals(""Edge mode must be present in request if "" +
                            ""available edge modes are present in metadata, and vice-versa."",
                    mStaticInfo.areKeysAvailable(CameraCharacteristics.
                            EDGE_AVAILABLE_EDGE_MODES),
                    mStaticInfo.areKeysAvailable(CaptureRequest.EDGE_MODE));
            if (mStaticInfo.areKeysAvailable(EDGE_MODE)) {
                List<Integer> availableEdgeModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableEdgeModesChecked()));
                // Don't need check fast as fast or high quality must be both present or both not.
                if (availableEdgeModes.contains(CaptureRequest.EDGE_MODE_HIGH_QUALITY)) {
                    mCollector.expectKeyValueEquals(request, EDGE_MODE,
                            CaptureRequest.EDGE_MODE_HIGH_QUALITY);
                } else {
                    mCollector.expectKeyValueEquals(request, EDGE_MODE,
                            CaptureRequest.EDGE_MODE_OFF);
                }
            }
            if (mStaticInfo.areKeysAvailable(SHADING_MODE)) {
                List<Integer> availableShadingModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableShadingModesChecked()));
                mCollector.expectKeyValueEquals(request, SHADING_MODE,
                        CaptureRequest.SHADING_MODE_HIGH_QUALITY);
            }

            mCollector.expectEquals(""Noise reduction mode must be present in request if "" +
                            ""available noise reductions are present in metadata, and vice-versa."",
                    mStaticInfo.areKeysAvailable(CameraCharacteristics.
                            NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES),
                    mStaticInfo.areKeysAvailable(CaptureRequest.NOISE_REDUCTION_MODE));
            if (mStaticInfo.areKeysAvailable(
                    CameraCharacteristics.NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES)) {
                List<Integer> availableNoiseReductionModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableNoiseReductionModesChecked()));
                // Don't need check fast as fast or high quality must be both present or both not.
                if (availableNoiseReductionModes
                        .contains(CaptureRequest.NOISE_REDUCTION_MODE_HIGH_QUALITY)) {
                    mCollector.expectKeyValueEquals(
                            request, NOISE_REDUCTION_MODE,
                            CaptureRequest.NOISE_REDUCTION_MODE_HIGH_QUALITY);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, NOISE_REDUCTION_MODE, CaptureRequest.NOISE_REDUCTION_MODE_OFF);
                }
            }

            mCollector.expectEquals(""Hot pixel mode must be present in request if "" +
                            ""available hot pixel modes are present in metadata, and vice-versa."",
                    mStaticInfo.areKeysAvailable(CameraCharacteristics.
                            HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES),
                    mStaticInfo.areKeysAvailable(CaptureRequest.HOT_PIXEL_MODE));

            if (mStaticInfo.areKeysAvailable(HOT_PIXEL_MODE)) {
                List<Integer> availableHotPixelModes =
                        Arrays.asList(toObject(
                                mStaticInfo.getAvailableHotPixelModesChecked()));
                if (availableHotPixelModes
                        .contains(CaptureRequest.HOT_PIXEL_MODE_HIGH_QUALITY)) {
                    mCollector.expectKeyValueEquals(
                            request, HOT_PIXEL_MODE,
                            CaptureRequest.HOT_PIXEL_MODE_HIGH_QUALITY);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, HOT_PIXEL_MODE, CaptureRequest.HOT_PIXEL_MODE_OFF);
                }
            }

            boolean supportAvailableAberrationModes = mStaticInfo.areKeysAvailable(
                    CameraCharacteristics.COLOR_CORRECTION_AVAILABLE_ABERRATION_MODES);
            boolean supportAberrationRequestKey = mStaticInfo.areKeysAvailable(
                    CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE);
            mCollector.expectEquals(""Aberration correction mode must be present in request if "" +
                    ""available aberration correction reductions are present in metadata, and ""
                    + ""vice-versa."", supportAvailableAberrationModes, supportAberrationRequestKey);
            if (supportAberrationRequestKey) {
                List<Integer> availableAberrationModes = Arrays.asList(
                        toObject(mStaticInfo.getAvailableColorAberrationModesChecked()));
                // Don't need check fast as fast or high quality must be both present or both not.
                if (availableAberrationModes
                        .contains(CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_HIGH_QUALITY)) {
                    mCollector.expectKeyValueEquals(
                            request, COLOR_CORRECTION_ABERRATION_MODE,
                            CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_HIGH_QUALITY);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, COLOR_CORRECTION_ABERRATION_MODE,
                            CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_OFF);
                }
            }
        } else if (template == CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG && supportReprocessing) {
            mCollector.expectKeyValueEquals(request, EDGE_MODE,
                    CaptureRequest.EDGE_MODE_ZERO_SHUTTER_LAG);
            mCollector.expectKeyValueEquals(request, NOISE_REDUCTION_MODE,
                    CaptureRequest.NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG);
        } else if (template == CameraDevice.TEMPLATE_PREVIEW ||
                template == CameraDevice.TEMPLATE_RECORD) {

            // Ok with either FAST or HIGH_QUALITY
            if (mStaticInfo.areKeysAvailable(COLOR_CORRECTION_MODE)) {
                mCollector.expectKeyValueNotEquals(
                        request, COLOR_CORRECTION_MODE,
                        CaptureRequest.COLOR_CORRECTION_MODE_TRANSFORM_MATRIX);
            }

            if (mStaticInfo.areKeysAvailable(EDGE_MODE)) {
                List<Integer> availableEdgeModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableEdgeModesChecked()));
                if (availableEdgeModes.contains(CaptureRequest.EDGE_MODE_FAST)) {
                    mCollector.expectKeyValueEquals(request, EDGE_MODE,
                            CaptureRequest.EDGE_MODE_FAST);
                } else {
                    mCollector.expectKeyValueEquals(request, EDGE_MODE,
                            CaptureRequest.EDGE_MODE_OFF);
                }
            }

            if (mStaticInfo.areKeysAvailable(SHADING_MODE)) {
                List<Integer> availableShadingModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableShadingModesChecked()));
                mCollector.expectKeyValueEquals(request, SHADING_MODE,
                        CaptureRequest.SHADING_MODE_FAST);
            }

            if (mStaticInfo.areKeysAvailable(NOISE_REDUCTION_MODE)) {
                List<Integer> availableNoiseReductionModes =
                        Arrays.asList(toObject(
                                mStaticInfo.getAvailableNoiseReductionModesChecked()));
                if (availableNoiseReductionModes
                        .contains(CaptureRequest.NOISE_REDUCTION_MODE_FAST)) {
                    mCollector.expectKeyValueEquals(
                            request, NOISE_REDUCTION_MODE,
                            CaptureRequest.NOISE_REDUCTION_MODE_FAST);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, NOISE_REDUCTION_MODE, CaptureRequest.NOISE_REDUCTION_MODE_OFF);
                }
            }

            if (mStaticInfo.areKeysAvailable(HOT_PIXEL_MODE)) {
                List<Integer> availableHotPixelModes =
                        Arrays.asList(toObject(
                                mStaticInfo.getAvailableHotPixelModesChecked()));
                if (availableHotPixelModes
                        .contains(CaptureRequest.HOT_PIXEL_MODE_FAST)) {
                    mCollector.expectKeyValueEquals(
                            request, HOT_PIXEL_MODE,
                            CaptureRequest.HOT_PIXEL_MODE_FAST);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, HOT_PIXEL_MODE, CaptureRequest.HOT_PIXEL_MODE_OFF);
                }
            }

            if (mStaticInfo.areKeysAvailable(COLOR_CORRECTION_ABERRATION_MODE)) {
                List<Integer> availableAberrationModes = Arrays.asList(
                        toObject(mStaticInfo.getAvailableColorAberrationModesChecked()));
                if (availableAberrationModes
                        .contains(CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_FAST)) {
                    mCollector.expectKeyValueEquals(
                            request, COLOR_CORRECTION_ABERRATION_MODE,
                            CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_FAST);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, COLOR_CORRECTION_ABERRATION_MODE,
                            CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_OFF);
                }
            }
        } else {
            if (mStaticInfo.areKeysAvailable(EDGE_MODE)) {
                mCollector.expectKeyValueNotNull(request, EDGE_MODE);
            }

            if (mStaticInfo.areKeysAvailable(NOISE_REDUCTION_MODE)) {
                mCollector.expectKeyValueNotNull(request, NOISE_REDUCTION_MODE);
            }

            if (mStaticInfo.areKeysAvailable(COLOR_CORRECTION_ABERRATION_MODE)) {
                mCollector.expectKeyValueNotNull(request, COLOR_CORRECTION_ABERRATION_MODE);
            }
        }

        // Tone map and lens shading modes.
        if (template == CameraDevice.TEMPLATE_STILL_CAPTURE) {
            mCollector.expectEquals(""Tonemap mode must be present in request if "" +
                            ""available tonemap modes are present in metadata, and vice-versa."",
                    mStaticInfo.areKeysAvailable(CameraCharacteristics.
                            TONEMAP_AVAILABLE_TONE_MAP_MODES),
                    mStaticInfo.areKeysAvailable(CaptureRequest.TONEMAP_MODE));
            if (mStaticInfo.areKeysAvailable(
                    CameraCharacteristics.TONEMAP_AVAILABLE_TONE_MAP_MODES)) {
                List<Integer> availableToneMapModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableToneMapModesChecked()));
                if (availableToneMapModes.contains(CaptureRequest.TONEMAP_MODE_HIGH_QUALITY)) {
                    mCollector.expectKeyValueEquals(request, TONEMAP_MODE,
                            CaptureRequest.TONEMAP_MODE_HIGH_QUALITY);
                } else {
                    mCollector.expectKeyValueEquals(request, TONEMAP_MODE,
                            CaptureRequest.TONEMAP_MODE_FAST);
                }
            }

            // Still capture template should have android.statistics.lensShadingMapMode ON when
            // RAW capability is supported.
            if (mStaticInfo.areKeysAvailable(STATISTICS_LENS_SHADING_MAP_MODE) &&
                    availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                    mCollector.expectKeyValueEquals(request, STATISTICS_LENS_SHADING_MAP_MODE,
                            STATISTICS_LENS_SHADING_MAP_MODE_ON);
            }
        } else {
            if (mStaticInfo.areKeysAvailable(TONEMAP_MODE)) {
                mCollector.expectKeyValueNotEquals(request, TONEMAP_MODE,
                        CaptureRequest.TONEMAP_MODE_CONTRAST_CURVE);
                mCollector.expectKeyValueNotEquals(request, TONEMAP_MODE,
                        CaptureRequest.TONEMAP_MODE_GAMMA_VALUE);
                mCollector.expectKeyValueNotEquals(request, TONEMAP_MODE,
                        CaptureRequest.TONEMAP_MODE_PRESET_CURVE);
            }
            if (mStaticInfo.areKeysAvailable(STATISTICS_LENS_SHADING_MAP_MODE)) {
                mCollector.expectKeyValueEquals(request, STATISTICS_LENS_SHADING_MAP_MODE,
                        CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE_OFF);
            }
            if (mStaticInfo.areKeysAvailable(STATISTICS_HOT_PIXEL_MAP_MODE)) {
                mCollector.expectKeyValueEquals(request, STATISTICS_HOT_PIXEL_MAP_MODE,
                        false);
            }
        }

        // Enable ZSL
        if (template != CameraDevice.TEMPLATE_STILL_CAPTURE) {
            if (mStaticInfo.areKeysAvailable(CONTROL_ENABLE_ZSL)) {
                    mCollector.expectKeyValueEquals(request, CONTROL_ENABLE_ZSL, false);
            }
        }

        int[] outputFormats = mStaticInfo.getAvailableFormats(
                StaticMetadata.StreamDirection.Output);
        boolean supportRaw = false;
        for (int format : outputFormats) {
            if (format == ImageFormat.RAW_SENSOR || format == ImageFormat.RAW10 ||
                    format == ImageFormat.RAW12 || format == ImageFormat.RAW_PRIVATE) {
                supportRaw = true;
                break;
            }
        }
        if (supportRaw) {
            mCollector.expectKeyValueEquals(request,
                    CONTROL_POST_RAW_SENSITIVITY_BOOST,
                    DEFAULT_POST_RAW_SENSITIVITY_BOOST);
        }

        switch(template) {
            case CameraDevice.TEMPLATE_PREVIEW:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_PREVIEW);
                break;
            case CameraDevice.TEMPLATE_STILL_CAPTURE:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_STILL_CAPTURE);
                break;
            case CameraDevice.TEMPLATE_RECORD:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_VIDEO_RECORD);
                break;
            case CameraDevice.TEMPLATE_VIDEO_SNAPSHOT:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_VIDEO_SNAPSHOT);
                break;
            case CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_ZERO_SHUTTER_LAG);
                break;
            case CameraDevice.TEMPLATE_MANUAL:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_MANUAL);
                break;
            default:
                // Skip unknown templates here
        }

        // Check distortion correction mode
        if (mStaticInfo.isDistortionCorrectionSupported()) {
            mCollector.expectKeyValueNotEquals(request, DISTORTION_CORRECTION_MODE,
                    CaptureRequest.DISTORTION_CORRECTION_MODE_OFF);
        }

        // Scaler settings
        if (mStaticInfo.areKeysAvailable(
                CameraCharacteristics.SCALER_AVAILABLE_ROTATE_AND_CROP_MODES)) {
            List<Integer> rotateAndCropModes = Arrays.asList(toObject(
                props.get(CameraCharacteristics.SCALER_AVAILABLE_ROTATE_AND_CROP_MODES)));
            if (rotateAndCropModes.contains(SCALER_ROTATE_AND_CROP_AUTO)) {
                mCollector.expectKeyValueEquals(request, SCALER_ROTATE_AND_CROP,
                        CaptureRequest.SCALER_ROTATE_AND_CROP_AUTO);
            }
        }

        // Check JPEG quality
        if (mStaticInfo.isColorOutputSupported()) {
            mCollector.expectKeyValueNotNull(request, JPEG_QUALITY);
        }

        // TODO: use the list of keys from CameraCharacteristics to avoid expecting
        //       keys which are not available by this CameraDevice.
    }

    private void captureTemplateTestByCamera(String cameraId, int template) throws Exception {
        try {
            openDevice(cameraId, mCameraMockListener);

            assertTrue(""Camera template "" + template + "" is out of range!"",
                    template >= CameraDevice.TEMPLATE_PREVIEW
                            && template <= CameraDevice.TEMPLATE_MANUAL);

            mCollector.setCameraId(cameraId);

            try {
                CaptureRequest.Builder request = mCamera.createCaptureRequest(template);
                assertNotNull(""Failed to create capture request for template "" + template, request);

                CameraCharacteristics props = mStaticInfo.getCharacteristics();
                checkRequestForTemplate(request, template, props);
            } catch (IllegalArgumentException e) {
                if (template == CameraDevice.TEMPLATE_MANUAL &&
                        !mStaticInfo.isCapabilitySupported(CameraCharacteristics.
                                REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
                    // OK
                } else if (template == CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG &&
                        !mStaticInfo.isCapabilitySupported(CameraCharacteristics.
                                REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING)) {
                    // OK.
                } else if (sLegacySkipTemplates.contains(template) &&
                        mStaticInfo.isHardwareLevelLegacy()) {
                    // OK
                } else if (template != CameraDevice.TEMPLATE_PREVIEW &&
                        mStaticInfo.isDepthOutputSupported() &&
                        !mStaticInfo.isColorOutputSupported()) {
                    // OK, depth-only devices need only support PREVIEW template
                } else {
                    throw e; // rethrow
                }
            }
        }
        finally {
            try {
                closeSession();
            } finally {
                closeDevice(cameraId, mCameraMockListener);
            }
        }
    }

    /**
     * Start capture with given {@link #CaptureRequest}.
     *
     * @param request The {@link #CaptureRequest} to be captured.
     * @param repeating If the capture is single capture or repeating.
     * @param listener The {@link #CaptureCallback} camera device used to notify callbacks.
     * @param handler The handler camera device used to post callbacks.
     */
    @Override
    protected void startCapture(CaptureRequest request, boolean repeating,
            CameraCaptureSession.CaptureCallback listener, Handler handler)
                    throws CameraAccessException {
        if (VERBOSE) Log.v(TAG, ""Starting capture from session"");

        if (repeating) {
            mSession.setRepeatingRequest(request, listener, handler);
        } else {
            mSession.capture(request, listener, handler);
        }
    }

    /**
     * Start capture with given {@link #CaptureRequest}.
     *
     * @param request The {@link #CaptureRequest} to be captured.
     * @param repeating If the capture is single capture or repeating.
     * @param listener The {@link #CaptureCallback} camera device used to notify callbacks.
     * @param executor The executor used to invoke callbacks.
     */
    protected void startCapture(CaptureRequest request, boolean repeating,
            CameraCaptureSession.CaptureCallback listener, Executor executor)
                    throws CameraAccessException {
        if (VERBOSE) Log.v(TAG, ""Starting capture from session"");

        if (repeating) {
            mSession.setSingleRepeatingRequest(request, executor, listener);
        } else {
            mSession.captureSingleRequest(request, executor, listener);
        }
    }

    /**
     * Close a {@link #CameraCaptureSession capture session}; blocking until
     * the close finishes with a transition to {@link CameraCaptureSession.StateCallback#onClosed}.
     */
    protected void closeSession() {
        if (mSession == null) {
            return;
        }

        mSession.close();
        waitForSessionState(SESSION_CLOSED, SESSION_CLOSE_TIMEOUT_MS);
        mSession = null;

        mSessionMockListener = null;
        mSessionWaiter = null;
    }

    /**
     * A camera capture session listener that keeps all the configured and closed sessions.
     */
    private class MultipleSessionCallback extends CameraCaptureSession.StateCallback {
        public static final int SESSION_CONFIGURED = 0;
        public static final int SESSION_CLOSED = 1;

        final List<CameraCaptureSession> mSessions = new ArrayList<>();
        final Map<CameraCaptureSession, Integer> mSessionStates = new HashMap<>();
        CameraCaptureSession mCurrentConfiguredSession = null;

        final ReentrantLock mLock = new ReentrantLock();
        final Condition mNewStateCond = mLock.newCondition();

        final boolean mFailOnConfigureFailed;

        /**
         * If failOnConfigureFailed is true, it calls fail() when onConfigureFailed() is invoked
         * for any session.
         */
        public MultipleSessionCallback(boolean failOnConfigureFailed) {
            mFailOnConfigureFailed = failOnConfigureFailed;
        }

        @Override
        public void onClosed(CameraCaptureSession session) {
            mLock.lock();
            mSessionStates.put(session, SESSION_CLOSED);
            mNewStateCond.signal();
            mLock.unlock();
        }

        @Override
        public void onConfigured(CameraCaptureSession session) {
            mLock.lock();
            mSessions.add(session);
            mSessionStates.put(session, SESSION_CONFIGURED);
            mNewStateCond.signal();
            mLock.unlock();
        }

        @Override
        public void onConfigureFailed(CameraCaptureSession session) {
            if (mFailOnConfigureFailed) {
                fail(""Configuring a session failed"");
            }
        }

        /**
         * Get a number of sessions that have been configured.
         */
        public List<CameraCaptureSession> getAllSessions(int numSessions, int timeoutMs)
                throws Exception {
            long remainingTime = timeoutMs;
            mLock.lock();
            try {
                while (mSessions.size() < numSessions) {
                    long startTime = SystemClock.elapsedRealtime();
                    boolean ret = mNewStateCond.await(remainingTime, TimeUnit.MILLISECONDS);
                    remainingTime -= (SystemClock.elapsedRealtime() - startTime);
                    ret &= remainingTime > 0;

                    assertTrue(""Get "" + numSessions + "" sessions timed out after "" + timeoutMs +
                            ""ms"", ret);
                }

                return mSessions;
            } finally {
                mLock.unlock();
            }
        }

        /**
         * Wait until a previously-configured sessoin is closed or it times out.
         */
        public void waitForSessionClose(CameraCaptureSession session, int timeoutMs) throws Exception {
            long remainingTime = timeoutMs;
            mLock.lock();
            try {
                while (mSessionStates.get(session).equals(SESSION_CLOSED) == false) {
                    long startTime = SystemClock.elapsedRealtime();
                    boolean ret = mNewStateCond.await(remainingTime, TimeUnit.MILLISECONDS);
                    remainingTime -= (SystemClock.elapsedRealtime() - startTime);
                    ret &= remainingTime > 0;

                    assertTrue(""Wait for session close timed out after "" + timeoutMs + ""ms"", ret);
                }
            } finally {
                mLock.unlock();
            }
        }
    }

    /**
     * Verify audio restrictions are set properly for single CameraDevice usage
     */"	""	""	"minimum 12"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RecordingTest"	"testRecordingWithDifferentPreviewSizes"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RecordingTest.java"	""	"public void testRecordingWithDifferentPreviewSizes() throws Exception {
        if (!MediaUtils.checkCodecForDomain(true /* encoder */, ""video"")) {
            return; // skipped
        }
        mPersistentSurface = MediaCodec.createPersistentInputSurface();
        assertNotNull(""Failed to create persistent input surface!"", mPersistentSurface);

        try {
            doRecordingWithDifferentPreviewSizes();
        } finally {
            mPersistentSurface.release();
            mPersistentSurface = null;
        }
    }

    public void doRecordingWithDifferentPreviewSizes() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing recording with different preview sizes for camera "" +
                        mCameraIdsUnderTest[i]);
                StaticMetadata staticInfo = mAllStaticInfo.get(mCameraIdsUnderTest[i]);
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                if (staticInfo.isExternalCamera()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support CamcorderProfile, skipping"");
                    continue;
                }
                // Re-use the MediaRecorder object for the same camera device.
                mMediaRecorder = new MediaRecorder();
                openDevice(mCameraIdsUnderTest[i]);

                initSupportedVideoSize(mCameraIdsUnderTest[i]);

                Size maxPreviewSize = mOrderedPreviewSizes.get(0);
                List<Range<Integer> > fpsRanges = Arrays.asList(
                        mStaticInfo.getAeAvailableTargetFpsRangesChecked());
                int cameraId = Integer.valueOf(mCamera.getId());
                int maxVideoFrameRate = -1;
                for (int profileId : mCamcorderProfileList) {
                    if (!CamcorderProfile.hasProfile(cameraId, profileId)) {
                        continue;
                    }
                    CamcorderProfile profile = CamcorderProfile.get(cameraId, profileId);

                    Size videoSz = new Size(profile.videoFrameWidth, profile.videoFrameHeight);
                    Range<Integer> fpsRange = new Range(
                            profile.videoFrameRate, profile.videoFrameRate);
                    if (maxVideoFrameRate < profile.videoFrameRate) {
                        maxVideoFrameRate = profile.videoFrameRate;
                    }

                    if (allowedUnsupported(cameraId, profileId)) {
                        continue;
                    }

                    if (mStaticInfo.isHardwareLevelLegacy() &&
                            (videoSz.getWidth() > maxPreviewSize.getWidth() ||
                             videoSz.getHeight() > maxPreviewSize.getHeight())) {
                        // Skip. Legacy mode can only do recording up to max preview size
                        continue;
                    }
                    assertTrue(""Video size "" + videoSz.toString() + "" for profile ID "" + profileId +
                                    "" must be one of the camera device supported video size!"",
                                    mSupportedVideoSizes.contains(videoSz));
                    assertTrue(""Frame rate range "" + fpsRange + "" (for profile ID "" + profileId +
                            "") must be one of the camera device available FPS range!"",
                            fpsRanges.contains(fpsRange));

                    // Configure preview and recording surfaces.
                    mOutMediaFileName = mDebugFileNameBase + ""/test_video_surface_reconfig.mp4"";

                    // prepare preview surface by using video size.
                    List<Size> previewSizes = getPreviewSizesForVideo(videoSz,
                            profile.videoFrameRate);
                    if (previewSizes.size() <= 1) {
                        continue;
                    }

                    // 1. Do video recording using largest compatbile preview sizes
                    prepareRecordingWithProfile(profile);
                    updatePreviewSurface(previewSizes.get(0));
                    SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
                    startRecording(
                            /* useMediaRecorder */true, resultListener,
                            /*useVideoStab*/false, fpsRange, false);
                    SystemClock.sleep(RECORDING_DURATION_MS);
                    stopRecording(/* useMediaRecorder */true, /* useIntermediateSurface */false,
                            /* stopStreaming */false);

                    // 2. Reconfigure with the same recording surface, but switch to a smaller
                    // preview size.
                    prepareRecordingWithProfile(profile);
                    updatePreviewSurface(previewSizes.get(1));
                    SimpleCaptureCallback resultListener2 = new SimpleCaptureCallback();
                    startRecording(
                            /* useMediaRecorder */true, resultListener2,
                            /*useVideoStab*/false, fpsRange, false);
                    SystemClock.sleep(RECORDING_DURATION_MS);
                    stopRecording(/* useMediaRecorder */true);
                    break;
                }
            } finally {
                closeDevice();
                releaseRecorder();
            }
        }
    }

    /**
     * Test camera preview and video surface sharing for maximum supported size.
     */
    private void videoPreviewSurfaceSharingTestByCamera() throws Exception {
        for (Size sz : mOrderedPreviewSizes) {
            if (!isSupported(sz, VIDEO_FRAME_RATE, VIDEO_FRAME_RATE)) {
                continue;
            }

            if (VERBOSE) {
                Log.v(TAG, ""Testing camera recording with video size "" + sz.toString());
            }

            // Configure preview and recording surfaces.
            mOutMediaFileName = mDebugFileNameBase + ""/test_video_share.mp4"";
            if (DEBUG_DUMP) {
                mOutMediaFileName = mDebugFileNameBase + ""/test_video_share_"" + mCamera.getId() +
                    ""_"" + sz.toString() + "".mp4"";
            }

            // Allow external camera to use variable fps range
            Range<Integer> fpsRange = null;
            if (mStaticInfo.isExternalCamera()) {
                Range<Integer>[] availableFpsRange =
                        mStaticInfo.getAeAvailableTargetFpsRangesChecked();

                boolean foundRange = false;
                int minFps = 0;
                for (int i = 0; i < availableFpsRange.length; i += 1) {
                    if (minFps < availableFpsRange[i].getLower()
                            && VIDEO_FRAME_RATE == availableFpsRange[i].getUpper()) {
                        minFps = availableFpsRange[i].getLower();
                        foundRange = true;
                    }
                }
                assertTrue(""Cannot find FPS range for maxFps "" + VIDEO_FRAME_RATE, foundRange);
                fpsRange = Range.create(minFps, VIDEO_FRAME_RATE);
            }

            // Use AVC and AAC a/v compression format.
            prepareRecording(sz, VIDEO_FRAME_RATE, VIDEO_FRAME_RATE);

            // prepare preview surface by using video size.
            updatePreviewSurfaceWithVideo(sz, VIDEO_FRAME_RATE);

            // Start recording
            SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
            if (!startSharedRecording(/* useMediaRecorder */true, resultListener,
                    /*useVideoStab*/false, fpsRange)) {
                mMediaRecorder.reset();
                continue;
            }

            // Record certain duration.
            SystemClock.sleep(RECORDING_DURATION_MS);

            // Stop recording and preview
            stopRecording(/* useMediaRecorder */true);
            // Convert number of frames camera produced into the duration in unit of ms.
            float frameDurationMinMs = 1000.0f / VIDEO_FRAME_RATE;
            float durationMinMs = resultListener.getTotalNumFrames() * frameDurationMinMs;
            float durationMaxMs = durationMinMs;
            float frameDurationMaxMs = 0.f;
            if (fpsRange != null) {
                frameDurationMaxMs = 1000.0f / fpsRange.getLower();
                durationMaxMs = resultListener.getTotalNumFrames() * frameDurationMaxMs;
            }

            // Validation.
            validateRecording(sz, durationMinMs, durationMaxMs,
                    frameDurationMinMs, frameDurationMaxMs,
                    FRMDRP_RATE_TOLERANCE);

            break;
        }
    }

    /**
     * Test slow motion recording where capture rate (camera output) is different with
     * video (playback) frame rate for each camera if high speed recording is supported
     * by both camera and encoder.
     *
     * <p>
     * Normal recording use cases make the capture rate (camera output frame
     * rate) the same as the video (playback) frame rate. This guarantees that
     * the motions in the scene play at the normal speed. If the capture rate is
     * faster than video frame rate, for a given time duration, more number of
     * frames are captured than it can be played in the same time duration. This
     * generates ""slow motion"" effect during playback.
     * </p>
     */
    private void slowMotionRecording() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.i(TAG, ""Testing slow motion recording for camera "" + id);
                StaticMetadata staticInfo = mAllStaticInfo.get(id);
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                if (!staticInfo.isHighSpeedVideoSupported()) {
                    continue;
                }

                // Re-use the MediaRecorder object for the same camera device.
                mMediaRecorder = new MediaRecorder();
                openDevice(id);

                StreamConfigurationMap config =
                        mStaticInfo.getValueFromKeyNonNull(
                                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
                Size[] highSpeedVideoSizes = config.getHighSpeedVideoSizes();
                for (Size size : highSpeedVideoSizes) {
                    Range<Integer> fpsRange = getHighestHighSpeedFixedFpsRangeForSize(config, size);
                    mCollector.expectNotNull(""Unable to find the fixed frame rate fps range for "" +
                            ""size "" + size, fpsRange);
                    if (fpsRange == null) {
                        continue;
                    }

                    int captureRate = fpsRange.getLower();
                    int videoFramerate = captureRate / SLOWMO_SLOW_FACTOR;
                    // Skip the test if the highest recording FPS supported by CamcorderProfile
                    if (fpsRange.getUpper() > getFpsFromHighSpeedProfileForSize(size)) {
                        Log.w(TAG, ""high speed recording "" + size + ""@"" + captureRate + ""fps""
                                + "" is not supported by CamcorderProfile"");
                        continue;
                    }

                    mOutMediaFileName = mDebugFileNameBase + ""/test_slowMo_video.mp4"";
                    if (DEBUG_DUMP) {
                        mOutMediaFileName = mDebugFileNameBase + ""/test_slowMo_video_"" + id + ""_""
                                + size.toString() + "".mp4"";
                    }

                    prepareRecording(size, videoFramerate, captureRate);

                    // prepare preview surface by using video size.
                    updatePreviewSurfaceWithVideo(size, captureRate);

                    // Start recording
                    SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
                    startSlowMotionRecording(/*useMediaRecorder*/true, videoFramerate, captureRate,
                            fpsRange, resultListener, /*useHighSpeedSession*/false);

                    // Record certain duration.
                    SystemClock.sleep(RECORDING_DURATION_MS);

                    // Stop recording and preview
                    stopRecording(/*useMediaRecorder*/true);
                    // Convert number of frames camera produced into the duration in unit of ms.
                    float frameDurationMs = 1000.0f / videoFramerate;
                    float durationMs = resultListener.getTotalNumFrames() * frameDurationMs;

                    // Validation.
                    validateRecording(size, durationMs, frameDurationMs, FRMDRP_RATE_TOLERANCE);
                }

            } finally {
                closeDevice();
                releaseRecorder();
            }
        }
    }

    private void constrainedHighSpeedRecording() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.i(TAG, ""Testing constrained high speed recording for camera "" + id);

                if (!mAllStaticInfo.get(id).isConstrainedHighSpeedVideoSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" doesn't support high speed recording, skipping."");
                    continue;
                }

                // Re-use the MediaRecorder object for the same camera device.
                mMediaRecorder = new MediaRecorder();
                openDevice(id);

                StreamConfigurationMap config =
                        mStaticInfo.getValueFromKeyNonNull(
                                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
                Size[] highSpeedVideoSizes = config.getHighSpeedVideoSizes();
                for (Size size : highSpeedVideoSizes) {
                    List<Range<Integer>> fixedFpsRanges =
                            getHighSpeedFixedFpsRangeForSize(config, size);
                    mCollector.expectTrue(""Unable to find the fixed frame rate fps range for "" +
                            ""size "" + size, fixedFpsRanges.size() > 0);
                    // Test recording for each FPS range
                    for (Range<Integer> fpsRange : fixedFpsRanges) {
                        int captureRate = fpsRange.getLower();
                        final int VIDEO_FRAME_RATE = 30;
                        // Skip the test if the highest recording FPS supported by CamcorderProfile
                        if (fpsRange.getUpper() > getFpsFromHighSpeedProfileForSize(size)) {
                            Log.w(TAG, ""high speed recording "" + size + ""@"" + captureRate + ""fps""
                                    + "" is not supported by CamcorderProfile"");
                            continue;
                        }

                        SimpleCaptureCallback previewResultListener = new SimpleCaptureCallback();

                        // prepare preview surface by using video size.
                        updatePreviewSurfaceWithVideo(size, captureRate);

                        startConstrainedPreview(fpsRange, previewResultListener);

                        mOutMediaFileName = mDebugFileNameBase + ""/test_cslowMo_video_"" +
                            captureRate + ""fps_"" + id + ""_"" + size.toString() + "".mp4"";

                        prepareRecording(size, VIDEO_FRAME_RATE, captureRate);

                        SystemClock.sleep(PREVIEW_DURATION_MS);

                        stopCameraStreaming();

                        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
                        // Start recording
                        startSlowMotionRecording(/*useMediaRecorder*/true, VIDEO_FRAME_RATE,
                                captureRate, fpsRange, resultListener,
                                /*useHighSpeedSession*/true);

                        // Record certain duration.
                        SystemClock.sleep(RECORDING_DURATION_MS);

                        // Stop recording and preview
                        stopRecording(/*useMediaRecorder*/true);

                        startConstrainedPreview(fpsRange, previewResultListener);

                        // Convert number of frames camera produced into the duration in unit of ms.
                        float frameDurationMs = 1000.0f / VIDEO_FRAME_RATE;
                        float durationMs = resultListener.getTotalNumFrames() * frameDurationMs;

                        // Validation.
                        validateRecording(size, durationMs, frameDurationMs, FRMDRP_RATE_TOLERANCE);

                        SystemClock.sleep(PREVIEW_DURATION_MS);

                        stopCameraStreaming();
                    }
                }

            } finally {
                closeDevice();
                releaseRecorder();
            }
        }
    }

    /**
     * Get high speed FPS from CamcorderProfiles for a given size.
     *
     * @param size The size used to search the CamcorderProfiles for the FPS.
     * @return high speed video FPS, 0 if the given size is not supported by the CamcorderProfiles.
     */
    private int getFpsFromHighSpeedProfileForSize(Size size) {
        for (int quality = CamcorderProfile.QUALITY_HIGH_SPEED_480P;
                quality <= CamcorderProfile.QUALITY_HIGH_SPEED_2160P; quality++) {
            if (CamcorderProfile.hasProfile(quality)) {
                CamcorderProfile profile = CamcorderProfile.get(quality);
                if (size.equals(new Size(profile.videoFrameWidth, profile.videoFrameHeight))){
                    return profile.videoFrameRate;
                }
            }
        }

        return 0;
    }

    private Range<Integer> getHighestHighSpeedFixedFpsRangeForSize(StreamConfigurationMap config,
            Size size) {
        Range<Integer>[] availableFpsRanges = config.getHighSpeedVideoFpsRangesFor(size);
        Range<Integer> maxRange = availableFpsRanges[0];
        boolean foundRange = false;
        for (Range<Integer> range : availableFpsRanges) {
            if (range.getLower().equals(range.getUpper()) && range.getLower() >= maxRange.getLower()) {
                foundRange = true;
                maxRange = range;
            }
        }

        if (!foundRange) {
            return null;
        }
        return maxRange;
    }

    private List<Range<Integer>> getHighSpeedFixedFpsRangeForSize(StreamConfigurationMap config,
            Size size) {
        Range<Integer>[] availableFpsRanges = config.getHighSpeedVideoFpsRangesFor(size);
        List<Range<Integer>> fixedRanges = new ArrayList<Range<Integer>>();
        for (Range<Integer> range : availableFpsRanges) {
            if (range.getLower().equals(range.getUpper())) {
                fixedRanges.add(range);
            }
        }
        return fixedRanges;
    }

    private void startConstrainedPreview(Range<Integer> fpsRange,
            CameraCaptureSession.CaptureCallback listener) throws Exception {
        List<Surface> outputSurfaces = new ArrayList<Surface>(1);
        assertTrue(""Preview surface should be valid"", mPreviewSurface.isValid());
        outputSurfaces.add(mPreviewSurface);
        mSessionListener = new BlockingSessionCallback();

        List<CaptureRequest> slowMoRequests = null;
        CaptureRequest.Builder requestBuilder =
            mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
        requestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
        requestBuilder.addTarget(mPreviewSurface);
        CaptureRequest initialRequest = requestBuilder.build();
        CameraTestUtils.checkSessionConfigurationWithSurfaces(mCamera, mHandler,
                outputSurfaces, /*inputConfig*/ null, SessionConfiguration.SESSION_HIGH_SPEED,
                /*defaultSupport*/ true, ""Constrained session configuration query failed"");
        mSession = buildConstrainedCameraSession(mCamera, outputSurfaces, mSessionListener,
                mHandler, initialRequest);
        slowMoRequests = ((CameraConstrainedHighSpeedCaptureSession) mSession).
            createHighSpeedRequestList(initialRequest);

        mSession.setRepeatingBurst(slowMoRequests, listener, mHandler);
    }

    private void startSlowMotionRecording(boolean useMediaRecorder, int videoFrameRate,
            int captureRate, Range<Integer> fpsRange,
            CameraCaptureSession.CaptureCallback listener, boolean useHighSpeedSession)
            throws Exception {
        List<Surface> outputSurfaces = new ArrayList<Surface>(2);
        assertTrue(""Both preview and recording surfaces should be valid"",
                mPreviewSurface.isValid() && mRecordingSurface.isValid());
        outputSurfaces.add(mPreviewSurface);
        outputSurfaces.add(mRecordingSurface);
        // Video snapshot surface
        if (mReaderSurface != null) {
            outputSurfaces.add(mReaderSurface);
        }
        mSessionListener = new BlockingSessionCallback();

        // Create slow motion request list
        List<CaptureRequest> slowMoRequests = null;
        if (useHighSpeedSession) {
            CaptureRequest.Builder requestBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
            requestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
            requestBuilder.addTarget(mPreviewSurface);
            requestBuilder.addTarget(mRecordingSurface);
            CaptureRequest initialRequest = requestBuilder.build();
            mSession = buildConstrainedCameraSession(mCamera, outputSurfaces, mSessionListener,
                    mHandler, initialRequest);
            slowMoRequests = ((CameraConstrainedHighSpeedCaptureSession) mSession).
                    createHighSpeedRequestList(initialRequest);
        } else {
            CaptureRequest.Builder recordingRequestBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
            recordingRequestBuilder.set(CaptureRequest.CONTROL_MODE,
                    CaptureRequest.CONTROL_MODE_USE_SCENE_MODE);
            recordingRequestBuilder.set(CaptureRequest.CONTROL_SCENE_MODE,
                    CaptureRequest.CONTROL_SCENE_MODE_HIGH_SPEED_VIDEO);

            CaptureRequest.Builder recordingOnlyBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
            recordingOnlyBuilder.set(CaptureRequest.CONTROL_MODE,
                    CaptureRequest.CONTROL_MODE_USE_SCENE_MODE);
            recordingOnlyBuilder.set(CaptureRequest.CONTROL_SCENE_MODE,
                    CaptureRequest.CONTROL_SCENE_MODE_HIGH_SPEED_VIDEO);
            int slowMotionFactor = captureRate / videoFrameRate;

            // Make sure camera output frame rate is set to correct value.
            recordingRequestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
            recordingRequestBuilder.addTarget(mRecordingSurface);
            recordingRequestBuilder.addTarget(mPreviewSurface);
            recordingOnlyBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
            recordingOnlyBuilder.addTarget(mRecordingSurface);

            CaptureRequest initialRequest = recordingRequestBuilder.build();
            mSession = configureCameraSessionWithParameters(mCamera, outputSurfaces,
                    mSessionListener, mHandler, initialRequest);

            slowMoRequests = new ArrayList<CaptureRequest>();
            slowMoRequests.add(initialRequest);// Preview + recording.

            for (int i = 0; i < slowMotionFactor - 1; i++) {
                slowMoRequests.add(recordingOnlyBuilder.build()); // Recording only.
            }
        }

        mSession.setRepeatingBurst(slowMoRequests, listener, mHandler);

        if (useMediaRecorder) {
            mMediaRecorder.start();
        } else {
            // TODO: need implement MediaCodec path.
        }

    }

    private void basicRecordingTestByCamera(int[] camcorderProfileList, boolean useVideoStab)
            throws Exception {
        basicRecordingTestByCamera(camcorderProfileList, useVideoStab, false);
    }

    private void basicRecordingTestByCamera(int[] camcorderProfileList, boolean useVideoStab,
            boolean useIntermediateSurface) throws Exception {
        basicRecordingTestByCamera(camcorderProfileList, useVideoStab,
                useIntermediateSurface, false);
    }

    /**
     * Test camera recording by using each available CamcorderProfile for a
     * given camera. preview size is set to the video size.
     */
    private void basicRecordingTestByCamera(int[] camcorderProfileList, boolean useVideoStab,
            boolean useIntermediateSurface, boolean useEncoderProfiles) throws Exception {
        Size maxPreviewSize = mOrderedPreviewSizes.get(0);
        List<Range<Integer> > fpsRanges = Arrays.asList(
                mStaticInfo.getAeAvailableTargetFpsRangesChecked());
        int cameraId = Integer.valueOf(mCamera.getId());
        int maxVideoFrameRate = -1;

        // only validate recording for non-perf measurement runs
        boolean validateRecording = !isPerfMeasure();
        for (int profileId : camcorderProfileList) {
            if (!CamcorderProfile.hasProfile(cameraId, profileId)) {
                continue;
            }

            CamcorderProfile profile = CamcorderProfile.get(cameraId, profileId);
            Size videoSz = new Size(profile.videoFrameWidth, profile.videoFrameHeight);

            Range<Integer> fpsRange = new Range(profile.videoFrameRate, profile.videoFrameRate);
            if (maxVideoFrameRate < profile.videoFrameRate) {
                maxVideoFrameRate = profile.videoFrameRate;
            }

            if (allowedUnsupported(cameraId, profileId)) {
                continue;
            }

            if (mStaticInfo.isHardwareLevelLegacy() &&
                    (videoSz.getWidth() > maxPreviewSize.getWidth() ||
                     videoSz.getHeight() > maxPreviewSize.getHeight())) {
                // Skip. Legacy mode can only do recording up to max preview size
                continue;
            }
            assertTrue(""Video size "" + videoSz.toString() + "" for profile ID "" + profileId +
                            "" must be one of the camera device supported video size!"",
                            mSupportedVideoSizes.contains(videoSz));
            assertTrue(""Frame rate range "" + fpsRange + "" (for profile ID "" + profileId +
                    "") must be one of the camera device available FPS range!"",
                    fpsRanges.contains(fpsRange));


            if (useEncoderProfiles) {
                // Iterate through all video-audio codec combination
                EncoderProfiles profiles = CamcorderProfile.getAll(mCamera.getId(), profileId);
                for (EncoderProfiles.VideoProfile videoProfile : profiles.getVideoProfiles()) {
                    boolean hasAudioProfile = false;
                    for (EncoderProfiles.AudioProfile audioProfile : profiles.getAudioProfiles()) {
                        hasAudioProfile = true;
                        doBasicRecordingByProfile(profiles, videoProfile, audioProfile,
                                useVideoStab, useIntermediateSurface, validateRecording);
                        // Only measure the default video profile of the largest video
                        // recording size when measuring perf
                        if (isPerfMeasure()) {
                            break;
                        }
                    }
                    // Timelapse profiles do not have audio track
                    if (!hasAudioProfile) {
                        doBasicRecordingByProfile(profiles, videoProfile, /* audioProfile */null,
                                useVideoStab, useIntermediateSurface, validateRecording);
                    }
                }
            } else {
                doBasicRecordingByProfile(
                        profile, useVideoStab, useIntermediateSurface, validateRecording);
            }

            if (isPerfMeasure()) {
                // Only measure the largest video recording size when measuring perf
                break;
            }
        }
        if (maxVideoFrameRate != -1) {
            // At least one CamcorderProfile is present, check FPS
            assertTrue(""At least one CamcorderProfile must support >= 24 FPS"",
                    maxVideoFrameRate >= 24);
        }
    }

    private void doBasicRecordingByProfile(
            CamcorderProfile profile, boolean userVideoStab,
            boolean useIntermediateSurface, boolean validate) throws Exception {
        Size videoSz = new Size(profile.videoFrameWidth, profile.videoFrameHeight);
        int frameRate = profile.videoFrameRate;

        if (VERBOSE) {
            Log.v(TAG, ""Testing camera recording with video size "" + videoSz.toString());
        }

        // Configure preview and recording surfaces.
        mOutMediaFileName = mDebugFileNameBase + ""/test_video.mp4"";
        if (DEBUG_DUMP) {
            mOutMediaFileName = mDebugFileNameBase + ""/test_video_"" + mCamera.getId() + ""_""
                    + videoSz.toString() + "".mp4"";
        }

        setupMediaRecorder(profile);
        completeBasicRecording(videoSz, frameRate, userVideoStab, useIntermediateSurface, validate);
    }

    private void doBasicRecordingByProfile(
            EncoderProfiles profiles,
            EncoderProfiles.VideoProfile videoProfile, EncoderProfiles.AudioProfile audioProfile,
            boolean userVideoStab, boolean useIntermediateSurface, boolean validate)
                    throws Exception {
        Size videoSz = new Size(videoProfile.getWidth(), videoProfile.getHeight());
        int frameRate = videoProfile.getFrameRate();

        if (VERBOSE) {
            Log.v(TAG, ""Testing camera recording with video size "" + videoSz.toString() +
                  "", video codec "" + videoProfile.getMediaType() + "", and audio codec "" +
                  (audioProfile == null ? ""(null)"" : audioProfile.getMediaType()));
        }

        // Configure preview and recording surfaces.
        mOutMediaFileName = mDebugFileNameBase + ""/test_video.mp4"";
        if (DEBUG_DUMP) {
            mOutMediaFileName = mDebugFileNameBase + ""/test_video_"" + mCamera.getId() + ""_""
                    + videoSz.toString() + ""_"" + videoProfile.getCodec();
            if (audioProfile != null) {
                mOutMediaFileName += ""_"" + audioProfile.getCodec();
            }
            mOutMediaFileName += "".mp4"";
        }

        setupMediaRecorder(profiles, videoProfile, audioProfile);
        completeBasicRecording(videoSz, frameRate, userVideoStab, useIntermediateSurface, validate);
    }

    private void completeBasicRecording(
            Size videoSz, int frameRate, boolean useVideoStab,
            boolean useIntermediateSurface, boolean validate) throws Exception {
        prepareRecording(useIntermediateSurface);

        // prepare preview surface by using video size.
        updatePreviewSurfaceWithVideo(videoSz, frameRate);

        // Start recording
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        startRecording(/* useMediaRecorder */true, resultListener, useVideoStab,
                useIntermediateSurface);

        // Record certain duration.
        SystemClock.sleep(RECORDING_DURATION_MS);

        // Stop recording and preview
        stopRecording(/* useMediaRecorder */true, useIntermediateSurface,
                /* stopCameraStreaming */true);
        // Convert number of frames camera produced into the duration in unit of ms.
        float frameDurationMs = 1000.0f / frameRate;
        float durationMs = 0.f;
        if (useIntermediateSurface) {
            durationMs = mQueuer.getQueuedCount() * frameDurationMs;
        } else {
            durationMs = resultListener.getTotalNumFrames() * frameDurationMs;
        }

        if (VERBOSE) {
            Log.v(TAG, ""video frame rate: "" + frameRate +
                            "", num of frames produced: "" + resultListener.getTotalNumFrames());
        }

        if (validate) {
            validateRecording(videoSz, durationMs, frameDurationMs, FRMDRP_RATE_TOLERANCE);
        }
    }

    /**
     * Test camera recording for each supported video size by camera, preview
     * size is set to the video size.
     */
    private void recordingSizeTestByCamera() throws Exception {
        for (Size sz : mSupportedVideoSizes) {
            if (!isSupported(sz, VIDEO_FRAME_RATE, VIDEO_FRAME_RATE)) {
                continue;
            }

            if (VERBOSE) {
                Log.v(TAG, ""Testing camera recording with video size "" + sz.toString());
            }

            // Configure preview and recording surfaces.
            mOutMediaFileName = mDebugFileNameBase + ""/test_video.mp4"";
            if (DEBUG_DUMP) {
                mOutMediaFileName = mDebugFileNameBase + ""/test_video_"" + mCamera.getId() + ""_""
                        + sz.toString() + "".mp4"";
            }

            // Allow external camera to use variable fps range
            Range<Integer> fpsRange = null;
            if (mStaticInfo.isExternalCamera()) {
                Range<Integer>[] availableFpsRange =
                        mStaticInfo.getAeAvailableTargetFpsRangesChecked();

                boolean foundRange = false;
                int minFps = 0;
                for (int i = 0; i < availableFpsRange.length; i += 1) {
                    if (minFps < availableFpsRange[i].getLower()
                            && VIDEO_FRAME_RATE == availableFpsRange[i].getUpper()) {
                        minFps = availableFpsRange[i].getLower();
                        foundRange = true;
                    }
                }
                assertTrue(""Cannot find FPS range for maxFps "" + VIDEO_FRAME_RATE, foundRange);
                fpsRange = Range.create(minFps, VIDEO_FRAME_RATE);
            }

            // Use AVC and AAC a/v compression format.
            prepareRecording(sz, VIDEO_FRAME_RATE, VIDEO_FRAME_RATE);

            // prepare preview surface by using video size.
            updatePreviewSurfaceWithVideo(sz, VIDEO_FRAME_RATE);

            // Start recording
            SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
            startRecording(
                    /* useMediaRecorder */true, resultListener,
                    /*useVideoStab*/false, fpsRange, false);

            // Record certain duration.
            SystemClock.sleep(RECORDING_DURATION_MS);

            // Stop recording and preview
            stopRecording(/* useMediaRecorder */true);
            // Convert number of frames camera produced into the duration in unit of ms.
            float frameDurationMinMs = 1000.0f / VIDEO_FRAME_RATE;
            float durationMinMs = resultListener.getTotalNumFrames() * frameDurationMinMs;
            float durationMaxMs = durationMinMs;
            float frameDurationMaxMs = 0.f;
            if (fpsRange != null) {
                frameDurationMaxMs = 1000.0f / fpsRange.getLower();
                durationMaxMs = resultListener.getTotalNumFrames() * frameDurationMaxMs;
            }

            // Validation.
            validateRecording(sz, durationMinMs, durationMaxMs,
                    frameDurationMinMs, frameDurationMaxMs,
                    FRMDRP_RATE_TOLERANCE);
        }
    }

    /**
     * Initialize the supported video sizes.
     */
    private void initSupportedVideoSize(String cameraId)  throws Exception {
        int id = Integer.valueOf(cameraId);
        Size maxVideoSize = SIZE_BOUND_720P;
        if (CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_2160P)) {
            maxVideoSize = SIZE_BOUND_2160P;
        } else if (CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_QHD)) {
            maxVideoSize = SIZE_BOUND_QHD;
        } else if (CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_2K)) {
            maxVideoSize = SIZE_BOUND_2K;
        } else if (CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_1080P)) {
            maxVideoSize = SIZE_BOUND_1080P;
        }

        mSupportedVideoSizes =
                getSupportedVideoSizes(cameraId, mCameraManager, maxVideoSize);
    }

    /**
     * Simple wrapper to wrap normal/burst video snapshot tests
     */
    private void videoSnapshotHelper(boolean burstTest) throws Exception {
            for (String id : mCameraIdsUnderTest) {
                try {
                    Log.i(TAG, ""Testing video snapshot for camera "" + id);

                    StaticMetadata staticInfo = mAllStaticInfo.get(id);
                    if (!staticInfo.isColorOutputSupported()) {
                        Log.i(TAG, ""Camera "" + id +
                                "" does not support color outputs, skipping"");
                        continue;
                    }

                    if (staticInfo.isExternalCamera()) {
                        Log.i(TAG, ""Camera "" + id +
                                "" does not support CamcorderProfile, skipping"");
                        continue;
                    }

                    // Re-use the MediaRecorder object for the same camera device.
                    mMediaRecorder = new MediaRecorder();

                    openDevice(id);

                    initSupportedVideoSize(id);

                    videoSnapshotTestByCamera(burstTest);
                } finally {
                    closeDevice();
                    releaseRecorder();
                }
            }
    }

    /**
     * Returns {@code true} if the {@link CamcorderProfile} ID is allowed to be unsupported.
     *
     * <p>This only allows unsupported profiles when using the LEGACY mode of the Camera API.</p>
     *
     * @param profileId a {@link CamcorderProfile} ID to check.
     * @return {@code true} if supported.
     */
    private boolean allowedUnsupported(int cameraId, int profileId) {
        if (!mStaticInfo.isHardwareLevelLegacy()) {
            return false;
        }

        switch(profileId) {
            case CamcorderProfile.QUALITY_2160P:
            case CamcorderProfile.QUALITY_1080P:
            case CamcorderProfile.QUALITY_HIGH:
                return !CamcorderProfile.hasProfile(cameraId, profileId) ||
                        CamcorderProfile.get(cameraId, profileId).videoFrameWidth >= 1080;
        }
        return false;
    }

    /**
     * Test video snapshot for each  available CamcorderProfile for a given camera.
     *
     * <p>
     * Preview size is set to the video size. For the burst test, frame drop and jittering
     * is not checked.
     * </p>
     *
     * @param burstTest Perform burst capture or single capture. For burst capture
     *                  {@value #BURST_VIDEO_SNAPSHOT_NUM} capture requests will be sent.
     */
    private void videoSnapshotTestByCamera(boolean burstTest)
            throws Exception {
        final int NUM_SINGLE_SHOT_TEST = 5;
        final int FRAMEDROP_TOLERANCE = 8;
        final int FRAME_SIZE_15M = 15000000;
        final float FRAME_DROP_TOLERENCE_FACTOR = 1.5f;
        int kFrameDrop_Tolerence = FRAMEDROP_TOLERANCE;

        for (int profileId : mCamcorderProfileList) {
            int cameraId = Integer.valueOf(mCamera.getId());
            if (!CamcorderProfile.hasProfile(cameraId, profileId) ||
                    allowedUnsupported(cameraId, profileId)) {
                continue;
            }

            CamcorderProfile profile = CamcorderProfile.get(cameraId, profileId);
            Size QCIF = new Size(176, 144);
            Size FULL_HD = new Size(1920, 1080);
            Size videoSz = new Size(profile.videoFrameWidth, profile.videoFrameHeight);
            Size maxPreviewSize = mOrderedPreviewSizes.get(0);

            if (mStaticInfo.isHardwareLevelLegacy() &&
                    (videoSz.getWidth() > maxPreviewSize.getWidth() ||
                     videoSz.getHeight() > maxPreviewSize.getHeight())) {
                // Skip. Legacy mode can only do recording up to max preview size
                continue;
            }

            if (!mSupportedVideoSizes.contains(videoSz)) {
                mCollector.addMessage(""Video size "" + videoSz.toString() + "" for profile ID "" +
                        profileId + "" must be one of the camera device supported video size!"");
                continue;
            }

            // For LEGACY, find closest supported smaller or equal JPEG size to the current video
            // size; if no size is smaller than the video, pick the smallest JPEG size.  The assert
            // for video size above guarantees that for LIMITED or FULL, we select videoSz here.
            // Also check for minFrameDuration here to make sure jpeg stream won't slow down
            // video capture
            Size videoSnapshotSz = mOrderedStillSizes.get(mOrderedStillSizes.size() - 1);
            // Allow a bit tolerance so we don't fail for a few nano seconds of difference
            final float FRAME_DURATION_TOLERANCE = 0.01f;
            long videoFrameDuration = (long) (1e9 / profile.videoFrameRate *
                    (1.0 + FRAME_DURATION_TOLERANCE));
            HashMap<Size, Long> minFrameDurationMap = mStaticInfo.
                    getAvailableMinFrameDurationsForFormatChecked(ImageFormat.JPEG);
            for (int i = mOrderedStillSizes.size() - 2; i >= 0; i--) {
                Size candidateSize = mOrderedStillSizes.get(i);
                if (mStaticInfo.isHardwareLevelLegacy()) {
                    // Legacy level doesn't report min frame duration
                    if (candidateSize.getWidth() <= videoSz.getWidth() &&
                            candidateSize.getHeight() <= videoSz.getHeight()) {
                        videoSnapshotSz = candidateSize;
                    }
                } else {
                    Long jpegFrameDuration = minFrameDurationMap.get(candidateSize);
                    assertTrue(""Cannot find minimum frame duration for jpeg size "" + candidateSize,
                            jpegFrameDuration != null);
                    if (candidateSize.getWidth() <= videoSz.getWidth() &&
                            candidateSize.getHeight() <= videoSz.getHeight() &&
                            jpegFrameDuration <= videoFrameDuration) {
                        videoSnapshotSz = candidateSize;
                    }
                }
            }
            Size defaultvideoSnapshotSz = videoSnapshotSz;

            /**
             * Only test full res snapshot when below conditions are all true.
             * 1. Camera is at least a LIMITED device.
             * 2. video size is up to max preview size, which will be bounded by 1080p.
             * 3. Full resolution jpeg stream can keep up to video stream speed.
             *    When full res jpeg stream cannot keep up to video stream speed, search
             *    the largest jpeg size that can susptain video speed instead.
             */
            if (mStaticInfo.isHardwareLevelAtLeastLimited() &&
                    videoSz.getWidth() <= maxPreviewSize.getWidth() &&
                    videoSz.getHeight() <= maxPreviewSize.getHeight()) {
                for (Size jpegSize : mOrderedStillSizes) {
                    Long jpegFrameDuration = minFrameDurationMap.get(jpegSize);
                    assertTrue(""Cannot find minimum frame duration for jpeg size "" + jpegSize,
                            jpegFrameDuration != null);
                    if (jpegFrameDuration <= videoFrameDuration) {
                        videoSnapshotSz = jpegSize;
                        break;
                    }
                    if (jpegSize.equals(videoSz)) {
                        throw new AssertionFailedError(
                                ""Cannot find adequate video snapshot size for video size"" +
                                        videoSz);
                    }
                }
            }

            if (videoSnapshotSz.getWidth() * videoSnapshotSz.getHeight() > FRAME_SIZE_15M)
                kFrameDrop_Tolerence = (int)(FRAMEDROP_TOLERANCE * FRAME_DROP_TOLERENCE_FACTOR);

            createImageReader(
                    videoSnapshotSz, ImageFormat.JPEG,
                    MAX_VIDEO_SNAPSHOT_IMAGES, /*listener*/null);

            // Full or better devices should support whatever video snapshot size calculated above.
            // Limited devices may only be able to support the default one.
            if (mStaticInfo.isHardwareLevelLimited()) {
                List<Surface> outputs = new ArrayList<Surface>();
                outputs.add(mPreviewSurface);
                outputs.add(mRecordingSurface);
                outputs.add(mReaderSurface);
                boolean isSupported = isStreamConfigurationSupported(
                        mCamera, outputs, mSessionListener, mHandler);
                if (!isSupported) {
                    videoSnapshotSz = defaultvideoSnapshotSz;
                    createImageReader(
                            videoSnapshotSz, ImageFormat.JPEG,
                            MAX_VIDEO_SNAPSHOT_IMAGES, /*listener*/null);
                }
            }

            if (videoSz.equals(QCIF) &&
                    ((videoSnapshotSz.getWidth() > FULL_HD.getWidth()) ||
                     (videoSnapshotSz.getHeight() > FULL_HD.getHeight()))) {
                List<Surface> outputs = new ArrayList<Surface>();
                outputs.add(mPreviewSurface);
                outputs.add(mRecordingSurface);
                outputs.add(mReaderSurface);
                boolean isSupported = isStreamConfigurationSupported(
                        mCamera, outputs, mSessionListener, mHandler);
                if (!isSupported) {
                    videoSnapshotSz = defaultvideoSnapshotSz;
                    createImageReader(
                            videoSnapshotSz, ImageFormat.JPEG,
                            MAX_VIDEO_SNAPSHOT_IMAGES, /*listener*/null);
                }
            }

            Log.i(TAG, ""Testing video snapshot size "" + videoSnapshotSz +
                    "" for video size "" + videoSz);

            if (VERBOSE) {
                Log.v(TAG, ""Testing camera recording with video size "" + videoSz.toString());
            }

            // Configure preview and recording surfaces.
            mOutMediaFileName = mDebugFileNameBase + ""/test_video.mp4"";
            if (DEBUG_DUMP) {
                mOutMediaFileName = mDebugFileNameBase + ""/test_video_"" + cameraId + ""_""
                        + videoSz.toString() + "".mp4"";
            }

            int numTestIterations = burstTest ? 1 : NUM_SINGLE_SHOT_TEST;
            int totalDroppedFrames = 0;

            for (int numTested = 0; numTested < numTestIterations; numTested++) {
                prepareRecordingWithProfile(profile);

                // prepare video snapshot
                SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
                SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
                CaptureRequest.Builder videoSnapshotRequestBuilder =
                        mCamera.createCaptureRequest((mStaticInfo.isHardwareLevelLegacy()) ?
                                CameraDevice.TEMPLATE_RECORD :
                                CameraDevice.TEMPLATE_VIDEO_SNAPSHOT);

                // prepare preview surface by using video size.
                updatePreviewSurfaceWithVideo(videoSz, profile.videoFrameRate);

                prepareVideoSnapshot(videoSnapshotRequestBuilder, imageListener);
                Range<Integer> fpsRange = Range.create(profile.videoFrameRate,
                        profile.videoFrameRate);
                videoSnapshotRequestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE,
                        fpsRange);
                if (mStaticInfo.isVideoStabilizationSupported()) {
                    videoSnapshotRequestBuilder.set(CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE,
                            CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE_ON);
                }
                CaptureRequest request = videoSnapshotRequestBuilder.build();

                // Start recording
                startRecording(/* useMediaRecorder */true, resultListener,
                        /*useVideoStab*/mStaticInfo.isVideoStabilizationSupported());
                long startTime = SystemClock.elapsedRealtime();

                // Record certain duration.
                SystemClock.sleep(RECORDING_DURATION_MS / 2);

                // take video snapshot
                if (burstTest) {
                    List<CaptureRequest> requests =
                            new ArrayList<CaptureRequest>(BURST_VIDEO_SNAPSHOT_NUM);
                    for (int i = 0; i < BURST_VIDEO_SNAPSHOT_NUM; i++) {
                        requests.add(request);
                    }
                    mSession.captureBurst(requests, resultListener, mHandler);
                } else {
                    mSession.capture(request, resultListener, mHandler);
                }

                // make sure recording is still going after video snapshot
                SystemClock.sleep(RECORDING_DURATION_MS / 2);

                // Stop recording and preview
                float durationMs = (float) stopRecording(/* useMediaRecorder */true);
                // For non-burst test, use number of frames to also double check video frame rate.
                // Burst video snapshot is allowed to cause frame rate drop, so do not use number
                // of frames to estimate duration
                if (!burstTest) {
                    durationMs = resultListener.getTotalNumFrames() * 1000.0f /
                        profile.videoFrameRate;
                }

                float frameDurationMs = 1000.0f / profile.videoFrameRate;
                // Validation recorded video
                validateRecording(videoSz, durationMs,
                        frameDurationMs, VID_SNPSHT_FRMDRP_RATE_TOLERANCE);

                if (burstTest) {
                    for (int i = 0; i < BURST_VIDEO_SNAPSHOT_NUM; i++) {
                        Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                        validateVideoSnapshotCapture(image, videoSnapshotSz);
                        image.close();
                    }
                } else {
                    // validate video snapshot image
                    Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                    validateVideoSnapshotCapture(image, videoSnapshotSz);

                    // validate if there is framedrop around video snapshot
                    totalDroppedFrames +=  validateFrameDropAroundVideoSnapshot(
                            resultListener, image.getTimestamp());

                    //TODO: validate jittering. Should move to PTS
                    //validateJittering(resultListener);

                    image.close();
                }
            }

            if (!burstTest) {
                Log.w(TAG, String.format(""Camera %d Video size %s: Number of dropped frames "" +
                        ""detected in %d trials is %d frames."", cameraId, videoSz.toString(),
                        numTestIterations, totalDroppedFrames));
                mCollector.expectLessOrEqual(
                        String.format(
                                ""Camera %d Video size %s: Number of dropped frames %d must not""
                                + "" be larger than %d"",
                                cameraId, videoSz.toString(), totalDroppedFrames,
                                kFrameDrop_Tolerence),
                        kFrameDrop_Tolerence, totalDroppedFrames);
            }
            closeImageReader();
        }
    }

    /**
     * Configure video snapshot request according to the still capture size
     */
    private void prepareVideoSnapshot(
            CaptureRequest.Builder requestBuilder,
            ImageReader.OnImageAvailableListener imageListener)
            throws Exception {
        mReader.setOnImageAvailableListener(imageListener, mHandler);
        assertNotNull(""Recording surface must be non-null!"", mRecordingSurface);
        requestBuilder.addTarget(mRecordingSurface);
        assertNotNull(""Preview surface must be non-null!"", mPreviewSurface);
        requestBuilder.addTarget(mPreviewSurface);
        assertNotNull(""Reader surface must be non-null!"", mReaderSurface);
        requestBuilder.addTarget(mReaderSurface);
    }

    /**
     * Find compatible preview sizes for video size and framerate.
     *
     * <p>Preview size will be capped with max preview size.</p>
     *
     * @param videoSize The video size used for preview.
     * @param videoFrameRate The video frame rate
     */
    private List<Size> getPreviewSizesForVideo(Size videoSize, int videoFrameRate) {
        if (mOrderedPreviewSizes == null) {
            throw new IllegalStateException(""supported preview size list is not initialized yet"");
        }
        final float FRAME_DURATION_TOLERANCE = 0.01f;
        long videoFrameDuration = (long) (1e9 / videoFrameRate *
                (1.0 + FRAME_DURATION_TOLERANCE));
        HashMap<Size, Long> minFrameDurationMap = mStaticInfo.
                getAvailableMinFrameDurationsForFormatChecked(ImageFormat.PRIVATE);
        Size maxPreviewSize = mOrderedPreviewSizes.get(0);
        ArrayList<Size> previewSizes = new ArrayList<>();
        if (videoSize.getWidth() > maxPreviewSize.getWidth() ||
                videoSize.getHeight() > maxPreviewSize.getHeight()) {
            for (Size s : mOrderedPreviewSizes) {
                Long frameDuration = minFrameDurationMap.get(s);
                if (mStaticInfo.isHardwareLevelLegacy()) {
                    // Legacy doesn't report min frame duration
                    frameDuration = new Long(0);
                }
                assertTrue(""Cannot find minimum frame duration for private size"" + s,
                        frameDuration != null);
                if (frameDuration <= videoFrameDuration &&
                        s.getWidth() <= videoSize.getWidth() &&
                        s.getHeight() <= videoSize.getHeight()) {
                    Log.v(TAG, ""Add preview size "" + s.toString() + "" for video size "" +
                            videoSize.toString());
                    previewSizes.add(s);
                }
            }
        }

        if (previewSizes.isEmpty()) {
            previewSizes.add(videoSize);
        }

        return previewSizes;
    }

    /**
     * Update preview size with video size.
     *
     * <p>Preview size will be capped with max preview size.</p>
     *
     * @param videoSize The video size used for preview.
     * @param videoFrameRate The video frame rate
     *
     */
    private void updatePreviewSurfaceWithVideo(Size videoSize, int videoFrameRate) {
        List<Size> previewSizes = getPreviewSizesForVideo(videoSize, videoFrameRate);
        updatePreviewSurface(previewSizes.get(0));
    }

    private void prepareRecordingWithProfile(CamcorderProfile profile) throws Exception {
        prepareRecordingWithProfile(profile, false);
    }

    /**
     * Configure MediaRecorder recording session with CamcorderProfile, prepare
     * the recording surface.
     */
    private void prepareRecordingWithProfile(CamcorderProfile profile,
            boolean useIntermediateSurface) throws Exception {
        // Prepare MediaRecorder.
        setupMediaRecorder(profile);
        prepareRecording(useIntermediateSurface);
    }

    private void setupMediaRecorder(CamcorderProfile profile) throws Exception {
        // Set-up MediaRecorder.
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);
        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.SURFACE);
        mMediaRecorder.setProfile(profile);

        mVideoFrameRate = profile.videoFrameRate;
        mVideoSize = new Size(profile.videoFrameWidth, profile.videoFrameHeight);
    }

    private void setupMediaRecorder(
            EncoderProfiles profiles,
            EncoderProfiles.VideoProfile videoProfile,
            EncoderProfiles.AudioProfile audioProfile) throws Exception {
        // Set-up MediaRecorder.
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);
        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.SURFACE);
        mMediaRecorder.setOutputFormat(profiles.getRecommendedFileFormat());
        mMediaRecorder.setVideoProfile(videoProfile);
        if (audioProfile != null) {
            mMediaRecorder.setAudioProfile(audioProfile);
        }

        mVideoFrameRate = videoProfile.getFrameRate();
        mVideoSize = new Size(videoProfile.getWidth(), videoProfile.getHeight());
    }

    private void prepareRecording(boolean useIntermediateSurface) throws Exception {
        // Continue preparing MediaRecorder
        mMediaRecorder.setOutputFile(mOutMediaFileName);
        if (mPersistentSurface != null) {
            mMediaRecorder.setInputSurface(mPersistentSurface);
            mRecordingSurface = mPersistentSurface;
        }
        mMediaRecorder.prepare();
        if (mPersistentSurface == null) {
            mRecordingSurface = mMediaRecorder.getSurface();
        }
        assertNotNull(""Recording surface must be non-null!"", mRecordingSurface);

        if (useIntermediateSurface) {
            mIntermediateReader = ImageReader.newInstance(
                    mVideoSize.getWidth(), mVideoSize.getHeight(),
                    ImageFormat.PRIVATE, /*maxImages*/3, HardwareBuffer.USAGE_VIDEO_ENCODE);

            mIntermediateSurface = mIntermediateReader.getSurface();
            mIntermediateWriter = ImageWriter.newInstance(mRecordingSurface, /*maxImages*/3,
                    ImageFormat.PRIVATE);
            mQueuer = new ImageWriterQueuer(mIntermediateWriter);

            mIntermediateThread = new HandlerThread(TAG);
            mIntermediateThread.start();
            mIntermediateHandler = new Handler(mIntermediateThread.getLooper());
            mIntermediateReader.setOnImageAvailableListener(mQueuer, mIntermediateHandler);
        }
    }

    /**
     * Configure MediaRecorder recording session with CamcorderProfile, prepare
     * the recording surface. Use AVC for video compression, AAC for audio compression.
     * Both are required for android devices by android CDD.
     */
    private void prepareRecording(Size sz, int videoFrameRate, int captureRate)
            throws Exception {
        // Prepare MediaRecorder.
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);
        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.SURFACE);
        mMediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.THREE_GPP);
        mMediaRecorder.setOutputFile(mOutMediaFileName);
        mMediaRecorder.setVideoEncodingBitRate(getVideoBitRate(sz));
        mMediaRecorder.setVideoFrameRate(videoFrameRate);
        mMediaRecorder.setCaptureRate(captureRate);
        mMediaRecorder.setVideoSize(sz.getWidth(), sz.getHeight());
        mMediaRecorder.setVideoEncoder(MediaRecorder.VideoEncoder.H264);
        mMediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.AAC);
        if (mPersistentSurface != null) {
            mMediaRecorder.setInputSurface(mPersistentSurface);
            mRecordingSurface = mPersistentSurface;
        }
        mMediaRecorder.prepare();
        if (mPersistentSurface == null) {
            mRecordingSurface = mMediaRecorder.getSurface();
        }
        assertNotNull(""Recording surface must be non-null!"", mRecordingSurface);
        mVideoFrameRate = videoFrameRate;
        mVideoSize = sz;
    }

    private void startRecording(boolean useMediaRecorder,
            CameraCaptureSession.CaptureCallback listener, boolean useVideoStab) throws Exception {
        startRecording(useMediaRecorder, listener, useVideoStab, /*variableFpsRange*/null,
                /*useIntermediateSurface*/false);
    }

    private void startRecording(boolean useMediaRecorder,
            CameraCaptureSession.CaptureCallback listener, boolean useVideoStab,
            boolean useIntermediateSurface) throws Exception {
        startRecording(useMediaRecorder, listener, useVideoStab, /*variableFpsRange*/null,
                useIntermediateSurface);
    }

    private void startRecording(boolean useMediaRecorder,
            CameraCaptureSession.CaptureCallback listener, boolean useVideoStab,
            Range<Integer> variableFpsRange, boolean useIntermediateSurface) throws Exception {
        if (!mStaticInfo.isVideoStabilizationSupported() && useVideoStab) {
            throw new IllegalArgumentException(""Video stabilization is not supported"");
        }

        List<Surface> outputSurfaces = new ArrayList<Surface>(2);
        assertTrue(""Both preview and recording surfaces should be valid"",
                mPreviewSurface.isValid() && mRecordingSurface.isValid());
        outputSurfaces.add(mPreviewSurface);
        if (useIntermediateSurface) {
            outputSurfaces.add(mIntermediateSurface);
        } else {
            outputSurfaces.add(mRecordingSurface);
        }

        // Video snapshot surface
        if (mReaderSurface != null) {
            outputSurfaces.add(mReaderSurface);
        }
        mSessionListener = new BlockingSessionCallback();

        CaptureRequest.Builder recordingRequestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
        // Make sure camera output frame rate is set to correct value.
        Range<Integer> fpsRange = (variableFpsRange == null) ?
                Range.create(mVideoFrameRate, mVideoFrameRate) : variableFpsRange;

        recordingRequestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
        if (useVideoStab) {
            recordingRequestBuilder.set(CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE,
                    CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE_ON);
        }
        if (useIntermediateSurface) {
            recordingRequestBuilder.addTarget(mIntermediateSurface);
            if (mQueuer != null) {
                mQueuer.resetInvalidSurfaceFlag();
            }
        } else {
            recordingRequestBuilder.addTarget(mRecordingSurface);
        }
        recordingRequestBuilder.addTarget(mPreviewSurface);
        CaptureRequest recordingRequest = recordingRequestBuilder.build();
        mSession = configureCameraSessionWithParameters(mCamera, outputSurfaces, mSessionListener,
                mHandler, recordingRequest);
        mSession.setRepeatingRequest(recordingRequest, listener, mHandler);

        if (useMediaRecorder) {
            mMediaRecorder.start();
        } else {
            // TODO: need implement MediaCodec path.
        }
        mRecordingStartTime = SystemClock.elapsedRealtime();
    }

    /**
     * Start video recording with preview and video surfaces sharing the same
     * camera stream.
     *
     * @return true if success, false if sharing is not supported.
     */
    private boolean startSharedRecording(boolean useMediaRecorder,
            CameraCaptureSession.CaptureCallback listener, boolean useVideoStab,
            Range<Integer> variableFpsRange) throws Exception {
        if (!mStaticInfo.isVideoStabilizationSupported() && useVideoStab) {
            throw new IllegalArgumentException(""Video stabilization is not supported"");
        }

        List<OutputConfiguration> outputConfigs = new ArrayList<OutputConfiguration>(2);
        assertTrue(""Both preview and recording surfaces should be valid"",
                mPreviewSurface.isValid() && mRecordingSurface.isValid());
        OutputConfiguration sharedConfig = new OutputConfiguration(mPreviewSurface);
        sharedConfig.enableSurfaceSharing();
        sharedConfig.addSurface(mRecordingSurface);
        outputConfigs.add(sharedConfig);

        CaptureRequest.Builder recordingRequestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
        // Make sure camera output frame rate is set to correct value.
        Range<Integer> fpsRange = (variableFpsRange == null) ?
                Range.create(mVideoFrameRate, mVideoFrameRate) : variableFpsRange;
        recordingRequestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
        if (useVideoStab) {
            recordingRequestBuilder.set(CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE,
                    CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE_ON);
        }
        CaptureRequest recordingRequest = recordingRequestBuilder.build();

        mSessionListener = new BlockingSessionCallback();
        mSession = tryConfigureCameraSessionWithConfig(mCamera, outputConfigs, recordingRequest,
                mSessionListener, mHandler);

        if (mSession == null) {
            Log.i(TAG, ""Sharing between preview and video is not supported"");
            return false;
        }

        recordingRequestBuilder.addTarget(mRecordingSurface);
        recordingRequestBuilder.addTarget(mPreviewSurface);
        mSession.setRepeatingRequest(recordingRequestBuilder.build(), listener, mHandler);

        if (useMediaRecorder) {
            mMediaRecorder.start();
        } else {
            // TODO: need implement MediaCodec path.
        }
        mRecordingStartTime = SystemClock.elapsedRealtime();
        return true;
    }


    private void stopCameraStreaming() throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""Stopping camera streaming and waiting for idle"");
        }
        // Stop repeating, wait for captures to complete, and disconnect from
        // surfaces
        mSession.close();
        mSessionListener.getStateWaiter().waitForState(SESSION_CLOSED, SESSION_CLOSE_TIMEOUT_MS);
    }

    private int stopRecording(boolean useMediaRecorder) throws Exception {
        return stopRecording(useMediaRecorder, /*useIntermediateSurface*/false,
                /*stopStreaming*/true);
    }

    // Stop recording and return the estimated video duration in milliseconds.
    private int stopRecording(boolean useMediaRecorder, boolean useIntermediateSurface,
            boolean stopStreaming) throws Exception {
        long stopRecordingTime = SystemClock.elapsedRealtime();
        if (useMediaRecorder) {
            if (stopStreaming) {
                stopCameraStreaming();
            }
            if (useIntermediateSurface) {
                mIntermediateReader.setOnImageAvailableListener(null, null);
                mQueuer.expectInvalidSurface();
            }

            mMediaRecorder.stop();
            // Can reuse the MediaRecorder object after reset.
            mMediaRecorder.reset();
        } else {
            // TODO: need implement MediaCodec path.
        }

        if (useIntermediateSurface) {
            mIntermediateReader.close();
            mQueuer.close();
            mIntermediateWriter.close();
            mIntermediateSurface.release();
            mIntermediateReader = null;
            mIntermediateSurface = null;
            mIntermediateWriter = null;
            mIntermediateThread.quitSafely();
            mIntermediateHandler = null;
        }

        if (mPersistentSurface == null && mRecordingSurface != null) {
            mRecordingSurface.release();
            mRecordingSurface = null;
        }
        return (int) (stopRecordingTime - mRecordingStartTime);
    }

    private void releaseRecorder() {
        if (mMediaRecorder != null) {
            mMediaRecorder.release();
            mMediaRecorder = null;
        }
    }

    private void validateRecording(
            Size sz, float expectedDurationMs, float expectedFrameDurationMs,
            float frameDropTolerance) throws Exception {
        validateRecording(sz,
                expectedDurationMs,  /*fixed FPS recording*/0.f,
                expectedFrameDurationMs, /*fixed FPS recording*/0.f,
                frameDropTolerance);
    }

    private void validateRecording(
            Size sz,
            float expectedDurationMinMs,      // Min duration (maxFps)
            float expectedDurationMaxMs,      // Max duration (minFps). 0.f for fixed fps recording
            float expectedFrameDurationMinMs, // maxFps
            float expectedFrameDurationMaxMs, // minFps. 0.f for fixed fps recording
            float frameDropTolerance) throws Exception {
        File outFile = new File(mOutMediaFileName);
        assertTrue(""No video is recorded"", outFile.exists());
        float maxFrameDuration = expectedFrameDurationMinMs * (1.0f + FRAMEDURATION_MARGIN);
        if (expectedFrameDurationMaxMs > 0.f) {
            maxFrameDuration = expectedFrameDurationMaxMs * (1.0f + FRAMEDURATION_MARGIN);
        }

        if (expectedDurationMaxMs == 0.f) {
            expectedDurationMaxMs = expectedDurationMinMs;
        }

        MediaExtractor extractor = new MediaExtractor();
        try {
            extractor.setDataSource(mOutMediaFileName);
            long durationUs = 0;
            int width = -1, height = -1;
            int numTracks = extractor.getTrackCount();
            int selectedTrack = -1;
            final String VIDEO_MIME_TYPE = ""video"";
            for (int i = 0; i < numTracks; i++) {
                MediaFormat format = extractor.getTrackFormat(i);
                String mime = format.getString(MediaFormat.KEY_MIME);
                if (mime.contains(VIDEO_MIME_TYPE)) {
                    Log.i(TAG, ""video format is: "" + format.toString());
                    durationUs = format.getLong(MediaFormat.KEY_DURATION);
                    width = format.getInteger(MediaFormat.KEY_WIDTH);
                    height = format.getInteger(MediaFormat.KEY_HEIGHT);
                    selectedTrack = i;
                    extractor.selectTrack(i);
                    break;
                }
            }
            if (selectedTrack < 0) {
                throw new AssertionFailedError(
                        ""Cannot find video track!"");
            }

            Size videoSz = new Size(width, height);
            assertTrue(""Video size doesn't match, expected "" + sz.toString() +
                    "" got "" + videoSz.toString(), videoSz.equals(sz));
            float duration = (float) (durationUs / 1000);
            if (VERBOSE) {
                Log.v(TAG, String.format(""Video duration: recorded %fms, expected [%f,%f]ms"",
                                         duration, expectedDurationMinMs, expectedDurationMaxMs));
            }

            // Do rest of validation only for better-than-LEGACY devices
            if (mStaticInfo.isHardwareLevelLegacy()) return;

            // TODO: Don't skip this one for video snapshot on LEGACY
            assertTrue(String.format(
                    ""Camera %s: Video duration doesn't match: recorded %fms, expected [%f,%f]ms."",
                    mCamera.getId(), duration,
                    expectedDurationMinMs * (1.f - DURATION_MARGIN),
                    expectedDurationMaxMs * (1.f + DURATION_MARGIN)),
                    duration > expectedDurationMinMs * (1.f - DURATION_MARGIN) &&
                            duration < expectedDurationMaxMs * (1.f + DURATION_MARGIN));

            // Check for framedrop
            long lastSampleUs = 0;
            int frameDropCount = 0;
            int expectedFrameCount = (int) (expectedDurationMinMs / expectedFrameDurationMinMs);
            ArrayList<Long> timestamps = new ArrayList<Long>(expectedFrameCount);
            while (true) {
                timestamps.add(extractor.getSampleTime());
                if (!extractor.advance()) {
                    break;
                }
            }
            Collections.sort(timestamps);
            long prevSampleUs = timestamps.get(0);
            for (int i = 1; i < timestamps.size(); i++) {
                long currentSampleUs = timestamps.get(i);
                float frameDurationMs = (float) (currentSampleUs - prevSampleUs) / 1000;
                if (frameDurationMs > maxFrameDuration) {
                    Log.w(TAG, String.format(
                        ""Frame drop at %d: expectation %f, observed %f"",
                        i, expectedFrameDurationMinMs, frameDurationMs));
                    frameDropCount++;
                }
                prevSampleUs = currentSampleUs;
            }
            float frameDropRate = 100.f * frameDropCount / timestamps.size();
            Log.i(TAG, String.format(""Frame drop rate %d/%d (%f%%)"",
                frameDropCount, timestamps.size(), frameDropRate));
            assertTrue(String.format(
                    ""Camera %s: Video frame drop rate too high: %f%%, tolerance %f%%. "" +
                    ""Video size: %s, expectedDuration [%f,%f], expectedFrameDuration %f, "" +
                    ""frameDropCnt %d, frameCount %d"",
                    mCamera.getId(), frameDropRate, frameDropTolerance,
                    sz.toString(), expectedDurationMinMs, expectedDurationMaxMs,
                    expectedFrameDurationMinMs, frameDropCount, timestamps.size()),
                    frameDropRate < frameDropTolerance);
        } finally {
            extractor.release();
            if (!DEBUG_DUMP) {
                outFile.delete();
            }
        }
    }

    /**
     * Validate video snapshot capture image object validity and test.
     *
     * <p> Check for size, format and jpeg decoding</p>
     *
     * @param image The JPEG image to be verified.
     * @param size The JPEG capture size to be verified against.
     */
    private void validateVideoSnapshotCapture(Image image, Size size) {
        CameraTestUtils.validateImage(image, size.getWidth(), size.getHeight(),
                ImageFormat.JPEG, /*filePath*/null);
    }

    /**
     * Validate if video snapshot causes frame drop.
     * Here frame drop is defined as frame duration >= 2 * expected frame duration.
     * Return the estimated number of frames dropped during video snapshot
     */
    private int validateFrameDropAroundVideoSnapshot(
            SimpleCaptureCallback resultListener, long imageTimeStamp) {
        double expectedDurationMs = 1000.0 / mVideoFrameRate;
        CaptureResult prevResult = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
        long prevTS = getValueNotNull(prevResult, CaptureResult.SENSOR_TIMESTAMP);
        while (resultListener.hasMoreResults()) {
            CaptureResult currentResult =
                    resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            long currentTS = getValueNotNull(currentResult, CaptureResult.SENSOR_TIMESTAMP);
            if (currentTS == imageTimeStamp) {
                // validate the timestamp before and after, then return
                CaptureResult nextResult =
                        resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
                long nextTS = getValueNotNull(nextResult, CaptureResult.SENSOR_TIMESTAMP);
                double durationMs = (currentTS - prevTS) / 1000000.0;
                int totalFramesDropped = 0;

                // Snapshots in legacy mode pause the preview briefly.  Skip the duration
                // requirements for legacy mode unless this is fixed.
                if (!mStaticInfo.isHardwareLevelLegacy()) {
                    mCollector.expectTrue(
                            String.format(
                                    ""Video %dx%d Frame drop detected before video snapshot: "" +
                                            ""duration %.2fms (expected %.2fms)"",
                                    mVideoSize.getWidth(), mVideoSize.getHeight(),
                                    durationMs, expectedDurationMs
                            ),
                            durationMs <= (expectedDurationMs * MAX_NUM_FRAME_DROP_INTERVAL_ALLOWED)
                    );
                    // Log a warning is there is any frame drop detected.
                    if (durationMs >= expectedDurationMs * 2) {
                        Log.w(TAG, String.format(
                                ""Video %dx%d Frame drop detected before video snapshot: "" +
                                        ""duration %.2fms (expected %.2fms)"",
                                mVideoSize.getWidth(), mVideoSize.getHeight(),
                                durationMs, expectedDurationMs
                        ));
                    }

                    durationMs = (nextTS - currentTS) / 1000000.0;
                    mCollector.expectTrue(
                            String.format(
                                    ""Video %dx%d Frame drop detected after video snapshot: "" +
                                            ""duration %.2fms (expected %.2fms)"",
                                    mVideoSize.getWidth(), mVideoSize.getHeight(),
                                    durationMs, expectedDurationMs
                            ),
                            durationMs <= (expectedDurationMs * MAX_NUM_FRAME_DROP_INTERVAL_ALLOWED)
                    );
                    // Log a warning is there is any frame drop detected.
                    if (durationMs >= expectedDurationMs * 2) {
                        Log.w(TAG, String.format(
                                ""Video %dx%d Frame drop detected after video snapshot: "" +
                                        ""duration %fms (expected %fms)"",
                                mVideoSize.getWidth(), mVideoSize.getHeight(),
                                durationMs, expectedDurationMs
                        ));
                    }

                    double totalDurationMs = (nextTS - prevTS) / 1000000.0;
                    // Minus 2 for the expected 2 frames interval
                    totalFramesDropped = (int) (totalDurationMs / expectedDurationMs) - 2;
                    if (totalFramesDropped < 0) {
                        Log.w(TAG, ""totalFrameDropped is "" + totalFramesDropped +
                                "". Video frame rate might be too fast."");
                    }
                    totalFramesDropped = Math.max(0, totalFramesDropped);
                }
                return totalFramesDropped;
            }
            prevTS = currentTS;
        }
        throw new AssertionFailedError(
                ""Video snapshot timestamp does not match any of capture results!"");
    }

    /**
     * Validate frame jittering from the input simple listener's buffered results
     */
    private void validateJittering(SimpleCaptureCallback resultListener) {
        double expectedDurationMs = 1000.0 / mVideoFrameRate;
        CaptureResult prevResult = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
        long prevTS = getValueNotNull(prevResult, CaptureResult.SENSOR_TIMESTAMP);
        while (resultListener.hasMoreResults()) {
            CaptureResult currentResult =
                    resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            long currentTS = getValueNotNull(currentResult, CaptureResult.SENSOR_TIMESTAMP);
            double durationMs = (currentTS - prevTS) / 1000000.0;
            double durationError = Math.abs(durationMs - expectedDurationMs);
            long frameNumber = currentResult.getFrameNumber();
            mCollector.expectTrue(
                    String.format(
                            ""Resolution %dx%d Frame %d: jittering (%.2fms) exceeds bound [%.2fms,%.2fms]"",
                            mVideoSize.getWidth(), mVideoSize.getHeight(),
                            frameNumber, durationMs,
                            expectedDurationMs - FRAME_DURATION_ERROR_TOLERANCE_MS,
                            expectedDurationMs + FRAME_DURATION_ERROR_TOLERANCE_MS),
                    durationError <= FRAME_DURATION_ERROR_TOLERANCE_MS);
            prevTS = currentTS;
        }
    }

    /**
     * Calculate a video bit rate based on the size. The bit rate is scaled
     * based on ratio of video size to 1080p size.
     */
    private int getVideoBitRate(Size sz) {
        int rate = BIT_RATE_1080P;
        float scaleFactor = sz.getHeight() * sz.getWidth() / (float)(1920 * 1080);
        rate = (int)(rate * scaleFactor);

        // Clamp to the MIN, MAX range.
        return Math.max(BIT_RATE_MIN, Math.min(BIT_RATE_MAX, rate));
    }

    /**
     * Check if the encoder and camera are able to support this size and frame rate.
     * Assume the video compression format is AVC.
     */
    private boolean isSupported(Size sz, int captureRate, int encodingRate) throws Exception {
        // Check camera capability.
        if (!isSupportedByCamera(sz, captureRate)) {
            return false;
        }

        // Check encode capability.
        if (!isSupportedByAVCEncoder(sz, encodingRate)){
            return false;
        }

        if(VERBOSE) {
            Log.v(TAG, ""Both encoder and camera support "" + sz.toString() + ""@"" + encodingRate + ""@""
                    + getVideoBitRate(sz) / 1000 + ""Kbps"");
        }

        return true;
    }

    private boolean isSupportedByCamera(Size sz, int frameRate) {
        // Check if camera can support this sz and frame rate combination.
        StreamConfigurationMap config = mStaticInfo.
                getValueFromKeyNonNull(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);

        long minDuration = config.getOutputMinFrameDuration(MediaRecorder.class, sz);
        if (minDuration == 0) {
            return false;
        }

        int maxFrameRate = (int) (1e9f / minDuration);
        return maxFrameRate >= frameRate;
    }

    /**
     * Check if encoder can support this size and frame rate combination by querying
     * MediaCodec capability. Check is based on size and frame rate. Ignore the bit rate
     * as the bit rates targeted in this test are well below the bit rate max value specified
     * by AVC specification for certain level.
     */
    private static boolean isSupportedByAVCEncoder(Size sz, int frameRate) {
        MediaFormat format = MediaFormat.createVideoFormat(
                MediaFormat.MIMETYPE_VIDEO_AVC, sz.getWidth(), sz.getHeight());
        format.setInteger(MediaFormat.KEY_FRAME_RATE, frameRate);
        MediaCodecList mcl = new MediaCodecList(MediaCodecList.REGULAR_CODECS);
        return mcl.findEncoderForFormat(format) != null;
    }

    private static class ImageWriterQueuer implements ImageReader.OnImageAvailableListener {
        public ImageWriterQueuer(ImageWriter writer) {
            mWriter = writer;
        }

        public void resetInvalidSurfaceFlag() {
            synchronized (mLock) {
                mExpectInvalidSurface = false;
            }
        }

        // Indicate that the writer surface is about to get released
        // and become invalid.
        public void expectInvalidSurface() {
            // If we sync on 'mLock', we risk a possible deadlock
            // during 'mWriter.queueInputImage(image)' which is
            // called while the lock is held.
            mExpectInvalidSurface = true;
        }

        @Override
        public void onImageAvailable(ImageReader reader) {
            Image image = null;
            try {
                image = reader.acquireNextImage();
            } finally {
                synchronized (mLock) {
                    if (image != null && mWriter != null) {
                        try {
                            mWriter.queueInputImage(image);
                            mQueuedCount++;
                        } catch (IllegalStateException e) {
                            // Per API documentation ISE are possible
                            // in case the writer surface is not valid.
                            // Re-throw in case we have some other
                            // unexpected ISE.
                            if (mExpectInvalidSurface) {
                                Log.d(TAG, ""Invalid writer surface"");
                                image.close();
                            } else {
                                throw e;
                            }
                        }
                    } else if (image != null) {
                        image.close();
                    }
                }
            }
        }

        public int getQueuedCount() {
            synchronized (mLock) {
                return mQueuedCount;
            }
        }

        public void close() {
            synchronized (mLock) {
                mWriter = null;
            }
        }

        private Object      mLock = new Object();
        private ImageWriter mWriter = null;
        private int         mQueuedCount = 0;
        private boolean     mExpectInvalidSurface = false;
    }
}"	""	""	"cdd minimum resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.orientation.CameraOrientationActivity"	"setPassFailButtonClickListeners"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/orientation/CameraOrientationActivity.java"	""	"public void test/*
 * Copyright (C) 2012 The Android Open Source Project
 *
 * Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except
 * in compliance with the License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License
 * is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
 * or implied. See the License for the specific language governing permissions and limitations under
 * the License.
 */
package com.android.cts.verifier.camera.orientation;

import android.content.Intent;
import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.graphics.ImageFormat;
import android.graphics.Matrix;
import android.hardware.Camera;
import android.os.Bundle;
import android.os.Handler;
import android.util.Log;
import android.view.SurfaceHolder;
import android.view.SurfaceView;
import android.view.View;
import android.view.View.OnClickListener;
import android.widget.Button;
import android.widget.ImageButton;
import android.widget.ImageView;
import android.widget.LinearLayout.LayoutParams;
import android.widget.TextView;

import com.android.cts.verifier.PassFailButtons;
import com.android.cts.verifier.R;
import com.android.cts.verifier.TestResult;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.List;
import java.util.TreeSet;

/**
 * Tests for manual verification of the CDD-required camera output formats
 * for preview callbacks
 */
public class CameraOrientationActivity extends PassFailButtons.Activity
implements OnClickListener, SurfaceHolder.Callback {

    private static final String TAG = ""CameraOrientation"";
    private static final int STATE_OFF = 0;
    private static final int STATE_PREVIEW = 1;
    private static final int STATE_CAPTURE = 2;
    private static final int NUM_ORIENTATIONS = 4;
    private static final String STAGE_INDEX_EXTRA = ""stageIndex"";
    private static final int VGA_WIDTH = 640;
    private static final int VGA_HEIGHT = 480;

    private ImageButton mPassButton;
    private ImageButton mFailButton;
    private Button mTakePictureButton;

    private SurfaceView mCameraView;
    private ImageView mFormatView;
    private SurfaceHolder mSurfaceHolder;
    private Camera mCamera;
    private List<Camera.Size> mPreviewSizes;
    private List<Camera.Size> mPictureSizes;
    private Camera.Size mOptimalPreviewSize;
    private Camera.Size mOptimalPictureSize;
    private List<Integer> mPreviewOrientations;
    private int mNextPreviewOrientation;
    private int mNumCameras;
    private int mCurrentCameraId = -1;
    private int mState = STATE_OFF;
    private boolean mSizeAdjusted;

    private StringBuilder mReportBuilder = new StringBuilder();
    private final TreeSet<String> mTestedCombinations = new TreeSet<String>();
    private final TreeSet<String> mUntestedCombinations = new TreeSet<String>();

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        setContentView(R.layout.co_main);
        setPassFailButtonClickListeners();
        setInfoResources(R.string.camera_orientation, R.string.co_info, -1);
        mNumCameras = Camera.getNumberOfCameras();

        mPassButton         = (ImageButton) findViewById(R.id.pass_button);
        mFailButton         = (ImageButton) findViewById(R.id.fail_button);
        mTakePictureButton  = (Button) findViewById(R.id.take_picture_button);
        mFormatView         = (ImageView) findViewById(R.id.format_view);
        mCameraView         = (SurfaceView) findViewById(R.id.camera_view);

        mFormatView.setOnClickListener(this);
        mCameraView.setOnClickListener(this);
        mTakePictureButton.setOnClickListener(this);

        mSurfaceHolder = mCameraView.getHolder();
        mSurfaceHolder.addCallback(this);

        mPreviewOrientations = new ArrayList<Integer>();
        mPreviewOrientations.add(0);
        mPreviewOrientations.add(90);
        mPreviewOrientations.add(180);
        mPreviewOrientations.add(270);

        // This activity is reused multiple times
        // to test each camera/orientation combination
        final int stageIndex = getIntent().getIntExtra(STAGE_INDEX_EXTRA, 0);
        Settings settings = getSettings(stageIndex);

        // Hitting the pass button goes to the next test activity.
        // Only the last one uses the PassFailButtons click callback function,
        // which gracefully terminates the activity.
        if (stageIndex + 1 < mNumCameras * NUM_ORIENTATIONS) {
            setPassButtonGoesToNextStage(stageIndex);
        }

        String[] availableOrientations = new String[NUM_ORIENTATIONS];
        for (int i=0; i<availableOrientations.length; i++) {
            // append degree symbol
            availableOrientations[i] = Integer.toString(i * 90) + ""\u00b0"";
        }

        resetButtons();

        // Set initial values
        mSizeAdjusted = false;
        mCurrentCameraId = settings.mCameraId;
        TextView cameraLabel = (TextView) findViewById(R.id.camera_text);
        cameraLabel.setText(
                getString(R.string.co_camera_label)
                + "" "" + (mCurrentCameraId+1) + "" of "" + mNumCameras);

        mNextPreviewOrientation = settings.mOrientation;
        TextView orientationLabel =
                (TextView) findViewById(R.id.orientation_text);
        orientationLabel.setText(
                getString(R.string.co_orientation_label)
                + "" ""
                + Integer.toString(mNextPreviewOrientation+1)
                + "" of ""
                + Integer.toString(NUM_ORIENTATIONS)
                + "": ""
                + mPreviewOrientations.get(mNextPreviewOrientation) + ""\u00b0""
                + "" ""
                + getString(R.string.co_orientation_direction_label)
                );

        TextView instructionLabel =
                (TextView) findViewById(R.id.instruction_text);
        instructionLabel.setText(R.string.co_instruction_text_photo_label);

        mTakePictureButton.setEnabled(false);
        setUpCamera(mCurrentCameraId);
    }

    @Override
    public void onResume() {
        super.onResume();
        setUpCamera(mCurrentCameraId);
    }

    @Override
    public void onPause() {
        super.onPause();
        shutdownCamera();
    }

    @Override
    public String getTestDetails() {
        return mReportBuilder.toString();
    }

    private void setUpCamera(int id) {
        shutdownCamera();

        Log.v(TAG, ""Setting up Camera "" + id);
        mCurrentCameraId = id;

        try {
            mCamera = Camera.open(id);
        } catch (Exception e) {
            Log.e(TAG, ""Error opening camera"");
        }

        Camera.Parameters p = mCamera.getParameters();

        class SizeCompare implements Comparator<Camera.Size> {
            @Override
            public int compare(Camera.Size lhs, Camera.Size rhs) {
                if (lhs.width < rhs.width) return -1;
                if (lhs.width > rhs.width) return 1;
                if (lhs.height < rhs.height) return -1;
                if (lhs.height > rhs.height) return 1;
                return 0;
            }
        }
        SizeCompare s = new SizeCompare();
        TreeSet<Camera.Size> sortedResolutions = new TreeSet<Camera.Size>(s);

        // Get preview resolutions
        List<Camera.Size> unsortedSizes = p.getSupportedPreviewSizes();
        sortedResolutions.addAll(unsortedSizes);
        mPreviewSizes = new ArrayList<Camera.Size>(sortedResolutions);

        // Get picture resolutions
        unsortedSizes = p.getSupportedPictureSizes();
        sortedResolutions.clear();
        sortedResolutions.addAll(unsortedSizes);
        mPictureSizes = new ArrayList<Camera.Size>(sortedResolutions);

        startPreview();
    }

    private void shutdownCamera() {
        if (mCamera != null) {
            mCamera.setPreviewCallback(null);
            mCamera.stopPreview();
            mCamera.release();
            mCamera = null;
            mState = STATE_OFF;
        }
    }

    private void startPreview() {
        if (mState != STATE_OFF) {
            // Stop for a while to drain callbacks
            mCamera.setPreviewCallback(null);
            mCamera.stopPreview();
            mState = STATE_OFF;
            Handler h = new Handler();
            Runnable mDelayedPreview = new Runnable() {
                @Override
                public void run() {
                    startPreview();
                }
            };
            h.postDelayed(mDelayedPreview, 300);
            return;
        }

        mCamera.setPreviewCallback(mPreviewCallback);

        try {
            mCamera.setPreviewDisplay(mCameraView.getHolder());
        } catch (IOException ioe) {
            Log.e(TAG, ""Unable to connect camera to display"");
        }

        Camera.Parameters p = mCamera.getParameters();
        Log.v(TAG, ""Initializing picture format"");
        p.setPictureFormat(ImageFormat.JPEG);
        mOptimalPictureSize = getOptimalSize(mPictureSizes, VGA_WIDTH, VGA_HEIGHT);
        Log.v(TAG, ""Initializing picture size to ""
                + mOptimalPictureSize.width + ""x"" + mOptimalPictureSize.height);
        p.setPictureSize(mOptimalPictureSize.width, mOptimalPictureSize.height);
        mOptimalPreviewSize = getOptimalSize(mPreviewSizes, VGA_WIDTH, VGA_HEIGHT);
        Log.v(TAG, ""Initializing preview size to ""
                + mOptimalPreviewSize.width + ""x"" + mOptimalPreviewSize.height);
        p.setPreviewSize(mOptimalPreviewSize.width, mOptimalPreviewSize.height);

        Log.v(TAG, ""Setting camera parameters"");
        mCamera.setParameters(p);
        Log.v(TAG, ""Setting color filter"");
        mFormatView.setColorFilter(null);
        Log.v(TAG, ""Starting preview"");
        try {
            mCamera.startPreview();
        } catch (Exception e) {
            Log.d(TAG, ""Cannot start preview"", e);
        }

        // set preview orientation
        int degrees = mPreviewOrientations.get(mNextPreviewOrientation);
        mCamera.setDisplayOrientation(degrees);

        android.hardware.Camera.CameraInfo info =
                new android.hardware.Camera.CameraInfo();
        android.hardware.Camera.getCameraInfo(mCurrentCameraId, info);
        if (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {
            TextView cameraExtraLabel =
                    (TextView) findViewById(R.id.instruction_extra_text);
            cameraExtraLabel.setText(
                    getString(R.string.co_instruction_text_extra_label));
        }

        mState = STATE_PREVIEW;
    }

    @Override
    public void onClick(View view) {
        Log.v(TAG, ""Click detected"");

        if (view == mFormatView || view == mTakePictureButton) {
            if(mState == STATE_PREVIEW) {
                mTakePictureButton.setEnabled(false);
                Log.v(TAG, ""Taking picture"");
                mCamera.takePicture(null, null, null, mCameraCallback);
                mState = STATE_CAPTURE;
            }
        }

        if(view == mPassButton || view == mFailButton) {
            final int stageIndex =
                    getIntent().getIntExtra(STAGE_INDEX_EXTRA, 0);
            String[] cameraNames = new String[mNumCameras];
            int counter = 0;
            for (int i = 0; i < mNumCameras; i++) {
                cameraNames[i] = ""Camera "" + i;

                for(int j = 0; j < mPreviewOrientations.size(); j++) {
                    String combination = cameraNames[i] + "", ""
                            + mPreviewOrientations.get(j)
                            + ""\u00b0""
                            + ""\n"";

                    if(counter < stageIndex) {
                        // test already passed, or else wouldn't have made
                        // it to current stageIndex
                        mTestedCombinations.add(combination);
                    }

                    if(counter == stageIndex) {
                        // current test configuration
                        if(view == mPassButton) {
                            mTestedCombinations.add(combination);
                        }
                        else if(view == mFailButton) {
                            mUntestedCombinations.add(combination);
                        }
                    }

                    if(counter > stageIndex) {
                        // test not passed yet, since haven't made it to
                        // stageIndex
                        mUntestedCombinations.add(combination);
                    }

                    counter++;
                }
            }

            mReportBuilder = new StringBuilder();
            mReportBuilder.append(""Passed combinations:\n"");
            for (String combination : mTestedCombinations) {
                mReportBuilder.append(combination);
            }
            mReportBuilder.append(""Failed/untested combinations:\n"");
            for (String combination : mUntestedCombinations) {
                mReportBuilder.append(combination);
            }

            if(view == mPassButton) {
                TestResult.setPassedResult(this, ""CameraOrientationActivity"",
                        getTestDetails());
            }
            if(view == mFailButton) {
                TestResult.setFailedResult(this, ""CameraOrientationActivity"",
                        getTestDetails());
            }

            // restart activity to test next orientation
            Intent intent = new Intent(CameraOrientationActivity.this,
                    CameraOrientationActivity.class);
            intent.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP
                    | Intent.FLAG_ACTIVITY_FORWARD_RESULT);
            intent.putExtra(STAGE_INDEX_EXTRA, stageIndex + 1);
            startActivity(intent);
        }
    }

    private void resetButtons() {
        enablePassFailButtons(false);
    }

    private void enablePassFailButtons(boolean enable) {
        mPassButton.setEnabled(enable);
        mFailButton.setEnabled(enable);
    }

    // find a supported size with ratio less than tolerance threshold, and
    // which is closest to height and width of given dimensions without
    // being larger than either of given dimensions
    private Camera.Size getOptimalSize(List<Camera.Size> sizes, int w,
            int h) {
        final double ASPECT_TOLERANCE = 0.1;
        double targetRatio = (double) w / (double) h;
        if (sizes == null) return null;

        Camera.Size optimalSize = null;
        int minDiff = Integer.MAX_VALUE;
        int curDiff;

        int targetHeight = h;
        int targetWidth = w;

        boolean aspectRatio = true;
        boolean maintainCeiling = true;
        while(true) {
            for (Camera.Size size : sizes) {
                if(aspectRatio) {
                    double ratio = (double) size.width / size.height;
                    if (Math.abs(ratio - targetRatio) > ASPECT_TOLERANCE) {
                        continue;
                    }
                }
                curDiff = Math.abs(size.height - targetHeight) +
                        Math.abs(size.width - targetWidth);
                if (maintainCeiling && curDiff < minDiff
                        && size.height <= targetHeight
                        && size.width <= targetWidth) {
                    optimalSize = size;
                    minDiff = curDiff;
                } else if (maintainCeiling == false
                               && curDiff < minDiff) {
                    //try to get as close as possible
                    optimalSize = size;
                    minDiff = curDiff;
                }
            }
            if (optimalSize == null && aspectRatio == true) {
                // Cannot find a match, so repeat search and
                // ignore aspect ratio requirement
                aspectRatio = false;
            } else if (maintainCeiling == true) {
                //Camera resolutions are greater than ceiling provided
                //lets try to get as close as we can
                maintainCeiling = false;
            } else {
                break;
            }
        }

        return optimalSize;
    }

    @Override
    public void surfaceChanged(SurfaceHolder holder, int format, int width,
            int height) {
        startPreview();
    }

    private void setTestedConfiguration(int cameraId, int orientation) {
        String combination = ""Camera "" + cameraId + "", ""
                + orientation
                + ""\u00b0""
                + ""\n"";
        if (!mTestedCombinations.contains(combination)) {
            mTestedCombinations.add(combination);
            mUntestedCombinations.remove(combination);
        }
    }

    @Override
    public void surfaceCreated(SurfaceHolder holder) {
        // Auto-generated method stub
    }

    @Override
    public void surfaceDestroyed(SurfaceHolder holder) {
        // Auto-generated method stub
    }

    private final Camera.PreviewCallback mPreviewCallback =
            new Camera.PreviewCallback() {
        @Override
        public void onPreviewFrame(byte[] data, Camera camera) {
            // adjust camera preview to match output image's aspect ratio
            if(!mSizeAdjusted && mState == STATE_PREVIEW) {
                int viewWidth = mFormatView.getWidth();
                int viewHeight = mFormatView.getHeight();
                int newWidth, newHeight;

                if (viewWidth == 0 || viewHeight == 0){
                    return;
                }

                if (mPreviewOrientations.get(mNextPreviewOrientation) == 0
                    || mPreviewOrientations.get(mNextPreviewOrientation) == 180) {
                    // make preview width same as output image width,
                    // then calculate height using output image's height/width ratio
                    newWidth = viewWidth;
                    newHeight = (int) (viewWidth * ((double) mOptimalPreviewSize.height /
                            (double) mOptimalPreviewSize.width));
                }
                else {
                    newHeight = viewHeight;
                    newWidth = (int) (viewHeight * ((double) mOptimalPreviewSize.height /
                            (double) mOptimalPreviewSize.width));
                }

                LayoutParams layoutParams = new LayoutParams(newWidth, newHeight);
                mCameraView.setLayoutParams(layoutParams);
                mSizeAdjusted = true;
                mTakePictureButton.setEnabled(true);
            }
        }
    };

    private final Camera.PictureCallback mCameraCallback =
            new Camera.PictureCallback() {
        @Override
        public void onPictureTaken(byte[] data, Camera mCamera) {
            if (data != null) {
                Bitmap inputImage;
                inputImage = BitmapFactory.decodeByteArray(data, 0, data.length);

                int degrees = mPreviewOrientations.get(mNextPreviewOrientation);
                android.hardware.Camera.CameraInfo info =
                        new android.hardware.Camera.CameraInfo();
                android.hardware.Camera.getCameraInfo(mCurrentCameraId, info);
                float mirrorX[];
                if (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {
                    // mirror the image along vertical axis
                    mirrorX = new float[] {-1, 0, 0, 0, 1, 1, 0, 0, 1};
                    degrees = (360 - degrees) % 360; // compensate the mirror
                } else {
                    // leave image the same via identity matrix
                    mirrorX = new float[] {1, 0, 0, 0, 1, 0, 0, 0, 1};
                }

                // use matrix to transform the image
                Matrix matrixMirrorX = new Matrix();
                matrixMirrorX.setValues(mirrorX);
                Matrix mat = new Matrix();
                mat.postRotate(degrees);
                mat.postConcat(matrixMirrorX);

                Bitmap inputImageAdjusted = Bitmap.createBitmap(inputImage,
                        0,
                        0,
                        inputImage.getWidth(),
                        inputImage.getHeight(),
                        mat,
                        true);
                mFormatView.setImageBitmap(inputImageAdjusted);

                Log.v(TAG, ""Output image set"");
                enablePassFailButtons(true);

                TextView instructionLabel =
                        (TextView) findViewById(R.id.instruction_text);
                instructionLabel.setText(
                        R.string.co_instruction_text_passfail_label);
            }

            startPreview();
        }
    };

    private void setPassButtonGoesToNextStage(final int stageIndex) {
        findViewById(R.id.pass_button).setOnClickListener(this);
    }

    private Settings getSettings(int stageIndex) {
        int curCameraId = stageIndex / NUM_ORIENTATIONS;
        int curOrientation = stageIndex % NUM_ORIENTATIONS;
        return new Settings(stageIndex, curCameraId, curOrientation);
    }

    // Bundle of settings for testing a particular
    // camera/orientation combination
    class Settings {
        int mCameraId;
        int mOrientation;

        Settings(int stageIndex, int cameraId, int orientation) {
            mCameraId = cameraId;
            mOrientation = orientation;
        }
    }
}"	""	""	"cdd 12 resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CaptureRequestTest"	"testExtendedSceneModes"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CaptureRequestTest.java"	""	"public void testExtendedSceneModes() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                if (!mAllStaticInfo.get(id).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" does not support color outputs, skipping"");
                    continue;
                }
                openDevice(id);
                List<Range<Integer>> fpsRanges = getTargetFpsRangesUpTo30(mStaticInfo);
                extendedSceneModeTestByCamera(fpsRanges);
            } finally {
                closeDevice();
            }
        }
    }

    // TODO: add 3A state machine test.

    /**
     * Per camera dynamic black and white level test.
     */
    private void dynamicBlackWhiteLevelTestByCamera() throws Exception {
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = null;
        CaptureRequest.Builder previewBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder rawBuilder = null;
        Size previewSize =
                getMaxPreviewSize(mCamera.getId(), mCameraManager,
                getPreviewSizeBound(mWindowManager, PREVIEW_SIZE_BOUND));
        Size rawSize = null;
        boolean canCaptureBlackRaw =
                mStaticInfo.isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW) &&
                mStaticInfo.isOpticalBlackRegionSupported();
        if (canCaptureBlackRaw) {
            // Capture Raw16, then calculate the optical black, and use it to check with the dynamic
            // black level.
            rawBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
            rawSize = mStaticInfo.getRawDimensChecked();
            imageListener = new SimpleImageReaderListener();
            prepareRawCaptureAndStartPreview(previewBuilder, rawBuilder, previewSize, rawSize,
                    resultListener, imageListener);
        } else {
            startPreview(previewBuilder, previewSize, resultListener);
        }

        // Capture a sequence of frames with different sensitivities and validate the black/white
        // level values
        int[] sensitivities = getSensitivityTestValues();
        float[][] dynamicBlackLevels = new float[sensitivities.length][];
        int[] dynamicWhiteLevels = new int[sensitivities.length];
        float[][] opticalBlackLevels = new float[sensitivities.length][];
        for (int i = 0; i < sensitivities.length; i++) {
            CaptureResult result = null;
            if (canCaptureBlackRaw) {
                changeExposure(rawBuilder, DEFAULT_EXP_TIME_NS, sensitivities[i]);
                CaptureRequest rawRequest = rawBuilder.build();
                mSession.capture(rawRequest, resultListener, mHandler);
                result = resultListener.getCaptureResultForRequest(rawRequest,
                        NUM_RESULTS_WAIT_TIMEOUT);
                Image rawImage = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);

                // Get max (area-wise) optical black region
                Rect[] opticalBlackRegions = mStaticInfo.getCharacteristics().get(
                        CameraCharacteristics.SENSOR_OPTICAL_BLACK_REGIONS);
                Rect maxRegion = opticalBlackRegions[0];
                for (Rect region : opticalBlackRegions) {
                    if (region.width() * region.height() > maxRegion.width() * maxRegion.height()) {
                        maxRegion = region;
                    }
                }

                // Get average black pixel values in the region (region is multiple of 2x2)
                Image.Plane rawPlane = rawImage.getPlanes()[0];
                ByteBuffer rawBuffer = rawPlane.getBuffer();
                float[] avgBlackLevels = {0, 0, 0, 0};
                final int rowSize = rawPlane.getRowStride();
                final int bytePerPixel = rawPlane.getPixelStride();
                if (VERBOSE) {
                    Log.v(TAG, ""maxRegion: "" + maxRegion + "", Row stride: "" +
                            rawPlane.getRowStride());
                }
                for (int row = maxRegion.top; row < maxRegion.bottom; row += 2) {
                    for (int col = maxRegion.left; col < maxRegion.right; col += 2) {
                        int startOffset = row * rowSize + col * bytePerPixel;
                        avgBlackLevels[0] += rawBuffer.getShort(startOffset);
                        avgBlackLevels[1] += rawBuffer.getShort(startOffset + bytePerPixel);
                        startOffset += rowSize;
                        avgBlackLevels[2] += rawBuffer.getShort(startOffset);
                        avgBlackLevels[3] += rawBuffer.getShort(startOffset + bytePerPixel);
                    }
                }
                int numBlackBlocks = maxRegion.width() * maxRegion.height() / (2 * 2);
                for (int m = 0; m < avgBlackLevels.length; m++) {
                    avgBlackLevels[m] /= numBlackBlocks;
                }
                opticalBlackLevels[i] = avgBlackLevels;

                if (VERBOSE) {
                    Log.v(TAG, String.format(""Optical black level results for sensitivity (%d): %s"",
                            sensitivities[i], Arrays.toString(avgBlackLevels)));
                }

                rawImage.close();
            } else {
                changeExposure(previewBuilder, DEFAULT_EXP_TIME_NS, sensitivities[i]);
                CaptureRequest previewRequest = previewBuilder.build();
                mSession.capture(previewRequest, resultListener, mHandler);
                result = resultListener.getCaptureResultForRequest(previewRequest,
                        NUM_RESULTS_WAIT_TIMEOUT);
            }

            dynamicBlackLevels[i] = getValueNotNull(result,
                    CaptureResult.SENSOR_DYNAMIC_BLACK_LEVEL);
            dynamicWhiteLevels[i] = getValueNotNull(result,
                    CaptureResult.SENSOR_DYNAMIC_WHITE_LEVEL);
        }

        if (VERBOSE) {
            Log.v(TAG, ""Different sensitivities tested: "" + Arrays.toString(sensitivities));
            Log.v(TAG, ""Dynamic black level results: "" + Arrays.deepToString(dynamicBlackLevels));
            Log.v(TAG, ""Dynamic white level results: "" + Arrays.toString(dynamicWhiteLevels));
            if (canCaptureBlackRaw) {
                Log.v(TAG, ""Optical black level results "" +
                        Arrays.deepToString(opticalBlackLevels));
            }
        }

        // check the dynamic black level against global black level.
        // Implicit guarantee: if the dynamic black level is supported, fixed black level must be
        // supported as well (tested in ExtendedCameraCharacteristicsTest#testOpticalBlackRegions).
        BlackLevelPattern blackPattern = mStaticInfo.getCharacteristics().get(
                CameraCharacteristics.SENSOR_BLACK_LEVEL_PATTERN);
        int[] fixedBlackLevels = new int[4];
        int fixedWhiteLevel = mStaticInfo.getCharacteristics().get(
                CameraCharacteristics.SENSOR_INFO_WHITE_LEVEL);
        blackPattern.copyTo(fixedBlackLevels, 0);
        float maxBlackDeviation = 0;
        int maxWhiteDeviation = 0;
        for (int i = 0; i < dynamicBlackLevels.length; i++) {
            for (int j = 0; j < dynamicBlackLevels[i].length; j++) {
                if (maxBlackDeviation < Math.abs(fixedBlackLevels[j] - dynamicBlackLevels[i][j])) {
                    maxBlackDeviation = Math.abs(fixedBlackLevels[j] - dynamicBlackLevels[i][j]);
                }
            }
            if (maxWhiteDeviation < Math.abs(dynamicWhiteLevels[i] - fixedWhiteLevel)) {
                maxWhiteDeviation = Math.abs(dynamicWhiteLevels[i] - fixedWhiteLevel);
            }
        }
        mCollector.expectLessOrEqual(""Max deviation of the dynamic black level vs fixed black level""
                + "" exceed threshold.""
                + "" Dynamic black level results: "" + Arrays.deepToString(dynamicBlackLevels),
                fixedBlackLevels[0] * DYNAMIC_VS_FIXED_BLK_WH_LVL_ERROR_MARGIN, maxBlackDeviation);
        mCollector.expectLessOrEqual(""Max deviation of the dynamic white level exceed threshold.""
                + "" Dynamic white level results: "" + Arrays.toString(dynamicWhiteLevels),
                fixedWhiteLevel * DYNAMIC_VS_FIXED_BLK_WH_LVL_ERROR_MARGIN,
                (float)maxWhiteDeviation);

        // Validate against optical black levels if it is available
        if (canCaptureBlackRaw) {
            maxBlackDeviation = 0;
            for (int i = 0; i < dynamicBlackLevels.length; i++) {
                for (int j = 0; j < dynamicBlackLevels[i].length; j++) {
                    if (maxBlackDeviation <
                            Math.abs(opticalBlackLevels[i][j] - dynamicBlackLevels[i][j])) {
                        maxBlackDeviation =
                                Math.abs(opticalBlackLevels[i][j] - dynamicBlackLevels[i][j]);
                    }
                }
            }

            mCollector.expectLessOrEqual(""Max deviation of the dynamic black level vs optical black""
                    + "" exceed threshold.""
                    + "" Dynamic black level results: "" + Arrays.deepToString(dynamicBlackLevels)
                    + "" Optical black level results: "" + Arrays.deepToString(opticalBlackLevels),
                    fixedBlackLevels[0] * DYNAMIC_VS_OPTICAL_BLK_LVL_ERROR_MARGIN,
                    maxBlackDeviation);
        }
    }

    private void noiseReductionModeTestByCamera(List<Range<Integer>> fpsRanges) throws Exception {
        Size maxPrevSize = mOrderedPreviewSizes.get(0);
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        int[] availableModes = mStaticInfo.getAvailableNoiseReductionModesChecked();

        for (int mode : availableModes) {
            requestBuilder.set(CaptureRequest.NOISE_REDUCTION_MODE, mode);

            // Test that OFF and FAST mode should not slow down the frame rate.
            if (mode == CaptureRequest.NOISE_REDUCTION_MODE_OFF ||
                    mode == CaptureRequest.NOISE_REDUCTION_MODE_FAST) {
                verifyFpsNotSlowDown(requestBuilder, NUM_FRAMES_VERIFIED, fpsRanges);
            }

            SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
            startPreview(requestBuilder, maxPrevSize, resultListener);
            mSession.setRepeatingRequest(requestBuilder.build(), resultListener, mHandler);
            waitForSettingsApplied(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

            verifyCaptureResultForKey(CaptureResult.NOISE_REDUCTION_MODE, mode,
                    resultListener, NUM_FRAMES_VERIFIED);
        }

        stopPreview();
    }

    private void focusDistanceTestByCamera() throws Exception {
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        requestBuilder.set(CaptureRequest.CONTROL_AF_MODE, CaptureRequest.CONTROL_AF_MODE_OFF);
        int calibrationStatus = mStaticInfo.getFocusDistanceCalibrationChecked();
        float errorMargin = FOCUS_DISTANCE_ERROR_PERCENT_UNCALIBRATED;
        if (calibrationStatus ==
                CameraMetadata.LENS_INFO_FOCUS_DISTANCE_CALIBRATION_CALIBRATED) {
            errorMargin = FOCUS_DISTANCE_ERROR_PERCENT_CALIBRATED;
        } else if (calibrationStatus ==
                CameraMetadata.LENS_INFO_FOCUS_DISTANCE_CALIBRATION_APPROXIMATE) {
            errorMargin = FOCUS_DISTANCE_ERROR_PERCENT_APPROXIMATE;
        }

        // Test changing focus distance with repeating request
        focusDistanceTestRepeating(requestBuilder, errorMargin);

        if (calibrationStatus ==
                CameraMetadata.LENS_INFO_FOCUS_DISTANCE_CALIBRATION_CALIBRATED)  {
            // Test changing focus distance with burst request
            focusDistanceTestBurst(requestBuilder, errorMargin);
        }
    }

    private void focusDistanceTestRepeating(CaptureRequest.Builder requestBuilder,
            float errorMargin) throws Exception {
        CaptureRequest request;
        float[] testDistances = getFocusDistanceTestValuesInOrder(0, 0);
        Size maxPrevSize = mOrderedPreviewSizes.get(0);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        startPreview(requestBuilder, maxPrevSize, resultListener);

        float[] resultDistances = new float[testDistances.length];
        int[] resultLensStates = new int[testDistances.length];

        // Collect results
        for (int i = 0; i < testDistances.length; i++) {
            requestBuilder.set(CaptureRequest.LENS_FOCUS_DISTANCE, testDistances[i]);
            request = requestBuilder.build();
            resultListener = new SimpleCaptureCallback();
            mSession.setRepeatingRequest(request, resultListener, mHandler);
            waitForSettingsApplied(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
            waitForResultValue(resultListener, CaptureResult.LENS_STATE,
                    CaptureResult.LENS_STATE_STATIONARY, NUM_RESULTS_WAIT_TIMEOUT);
            CaptureResult result = resultListener.getCaptureResultForRequest(request,
                    NUM_RESULTS_WAIT_TIMEOUT);

            resultDistances[i] = getValueNotNull(result, CaptureResult.LENS_FOCUS_DISTANCE);
            resultLensStates[i] = getValueNotNull(result, CaptureResult.LENS_STATE);

            if (VERBOSE) {
                Log.v(TAG, ""Capture repeating request focus distance: "" + testDistances[i]
                        + "" result: "" + resultDistances[i] + "" lens state "" + resultLensStates[i]);
            }
        }

        verifyFocusDistance(testDistances, resultDistances, resultLensStates,
                /*ascendingOrder*/true, /*noOvershoot*/false, /*repeatStart*/0, /*repeatEnd*/0,
                errorMargin);

        if (mStaticInfo.areKeysAvailable(CameraCharacteristics.LENS_INFO_HYPERFOCAL_DISTANCE)) {

            // Test hyperfocal distance optionally
            float hyperFocalDistance = mStaticInfo.getHyperfocalDistanceChecked();
            if (hyperFocalDistance > 0) {
                requestBuilder.set(CaptureRequest.LENS_FOCUS_DISTANCE, hyperFocalDistance);
                request = requestBuilder.build();
                resultListener = new SimpleCaptureCallback();
                mSession.setRepeatingRequest(request, resultListener, mHandler);
                waitForSettingsApplied(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

                // Then wait for the lens.state to be stationary.
                waitForResultValue(resultListener, CaptureResult.LENS_STATE,
                        CaptureResult.LENS_STATE_STATIONARY, NUM_RESULTS_WAIT_TIMEOUT);
                CaptureResult result = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
                Float focusDistance = getValueNotNull(result, CaptureResult.LENS_FOCUS_DISTANCE);
                mCollector.expectInRange(""Focus distance for hyper focal should be close enough to"" +
                        "" requested value"", focusDistance,
                        hyperFocalDistance * (1.0f - errorMargin),
                        hyperFocalDistance * (1.0f + errorMargin));
            }
        }
    }

    private void focusDistanceTestBurst(CaptureRequest.Builder requestBuilder,
            float errorMargin) throws Exception {

        Size maxPrevSize = mOrderedPreviewSizes.get(0);
        float[] testDistances = getFocusDistanceTestValuesInOrder(NUM_FOCUS_DISTANCES_REPEAT,
                NUM_FOCUS_DISTANCES_REPEAT);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        startPreview(requestBuilder, maxPrevSize, resultListener);

        float[] resultDistances = new float[testDistances.length];
        int[] resultLensStates = new int[testDistances.length];

        final int maxPipelineDepth = mStaticInfo.getCharacteristics().get(
            CameraCharacteristics.REQUEST_PIPELINE_MAX_DEPTH);

        // Move lens to starting position, and wait for the lens.state to be stationary.
        CaptureRequest request;
        requestBuilder.set(CaptureRequest.LENS_FOCUS_DISTANCE, testDistances[0]);
        request = requestBuilder.build();
        mSession.setRepeatingRequest(request, resultListener, mHandler);
        waitForResultValue(resultListener, CaptureResult.LENS_STATE,
                CaptureResult.LENS_STATE_STATIONARY, NUM_RESULTS_WAIT_TIMEOUT);

        // Submit burst of requests with different focus distances
        List<CaptureRequest> burst = new ArrayList<>();
        for (int i = 0; i < testDistances.length; i ++) {
            requestBuilder.set(CaptureRequest.LENS_FOCUS_DISTANCE, testDistances[i]);
            burst.add(requestBuilder.build());
        }
        mSession.captureBurst(burst, resultListener, mHandler);

        for (int i = 0; i < testDistances.length; i++) {
            CaptureResult result = resultListener.getCaptureResultForRequest(
                    burst.get(i), maxPipelineDepth+1);

            resultDistances[i] = getValueNotNull(result, CaptureResult.LENS_FOCUS_DISTANCE);
            resultLensStates[i] = getValueNotNull(result, CaptureResult.LENS_STATE);

            if (VERBOSE) {
                Log.v(TAG, ""Capture burst request focus distance: "" + testDistances[i]
                        + "" result: "" + resultDistances[i] + "" lens state "" + resultLensStates[i]);
            }
        }

        verifyFocusDistance(testDistances, resultDistances, resultLensStates,
                /*ascendingOrder*/true, /*noOvershoot*/true,
                /*repeatStart*/NUM_FOCUS_DISTANCES_REPEAT, /*repeatEnd*/NUM_FOCUS_DISTANCES_REPEAT,
                errorMargin);

    }

    /**
     * Verify focus distance control.
     *
     * Assumption:
     * - First repeatStart+1 elements of requestedDistances share the same value
     * - Last repeatEnd+1 elements of requestedDistances share the same value
     * - All elements in between are monotonically increasing/decreasing depending on ascendingOrder.
     * - Focuser is at requestedDistances[0] at the beginning of the test.
     *
     * @param requestedDistances The requested focus distances
     * @param resultDistances The result focus distances
     * @param lensStates The result lens states
     * @param ascendingOrder The order of the expected focus distance request/output
     * @param noOvershoot Assert that focus control doesn't overshoot the requested value
     * @param repeatStart The number of times the starting focus distance is repeated
     * @param repeatEnd The number of times the ending focus distance is repeated
     * @param errorMargin The error margin between request and result
     */
    private void verifyFocusDistance(float[] requestedDistances, float[] resultDistances,
            int[] lensStates, boolean ascendingOrder, boolean noOvershoot, int repeatStart,
            int repeatEnd, float errorMargin) {

        float minValue = 0;
        float maxValue = mStaticInfo.getMinimumFocusDistanceChecked();
        float hyperfocalDistance = 0;
        if (mStaticInfo.areKeysAvailable(CameraCharacteristics.LENS_INFO_HYPERFOCAL_DISTANCE)) {
            hyperfocalDistance = mStaticInfo.getHyperfocalDistanceChecked();
        }

        // Verify lens and focus distance do not change for first repeatStart
        // results.
        for (int i = 0; i < repeatStart; i ++) {
            float marginMin = requestedDistances[i] * (1.0f - errorMargin);
            // HAL may choose to use hyperfocal distance for all distances between [0, hyperfocal].
            float marginMax =
                    Math.max(requestedDistances[i], hyperfocalDistance) * (1.0f + errorMargin);

            mCollector.expectEquals(""Lens moves even though focus_distance didn't change"",
                    lensStates[i], CaptureResult.LENS_STATE_STATIONARY);
            if (noOvershoot) {
                mCollector.expectInRange(""Focus distance in result should be close enough to "" +
                        ""requested value"", resultDistances[i], marginMin, marginMax);
            }
            mCollector.expectInRange(""Result focus distance is out of range"",
                    resultDistances[i], minValue, maxValue);
        }

        for (int i = repeatStart; i < resultDistances.length-1; i ++) {
            float marginMin = requestedDistances[i] * (1.0f - errorMargin);
            // HAL may choose to use hyperfocal distance for all distances between [0, hyperfocal].
            float marginMax =
                    Math.max(requestedDistances[i], hyperfocalDistance) * (1.0f + errorMargin);
            if (noOvershoot) {
                // Result focus distance shouldn't overshoot the request
                boolean condition;
                if (ascendingOrder) {
                    condition = resultDistances[i] <= marginMax;
               } else {
                    condition = resultDistances[i] >= marginMin;
                }
                mCollector.expectTrue(String.format(
                      ""Lens shouldn't move past request focus distance. result "" +
                      resultDistances[i] + "" vs target of "" +
                      (ascendingOrder ? marginMax : marginMin)), condition);
            }

            // Verify monotonically increased focus distance setting
            boolean condition;
            float compareDistance = resultDistances[i+1] - resultDistances[i];
            if (i < resultDistances.length-1-repeatEnd) {
                condition = (ascendingOrder ? compareDistance > 0 : compareDistance < 0);
            } else {
                condition = (ascendingOrder ? compareDistance >= 0 : compareDistance <= 0);
            }
            mCollector.expectTrue(String.format(""Adjacent [resultDistances, lens_state] results [""
                  + resultDistances[i] + "","" + lensStates[i] + ""], ["" + resultDistances[i+1] + "",""
                  + lensStates[i+1] + ""] monotonicity is broken""), condition);
        }

        mCollector.expectTrue(String.format(""All values of this array are equal: "" +
                resultDistances[0] + "" "" + resultDistances[resultDistances.length-1]),
                resultDistances[0] != resultDistances[resultDistances.length-1]);

        // Verify lens moved to destination location.
        mCollector.expectInRange(""Focus distance "" + resultDistances[resultDistances.length-1] +
                "" for minFocusDistance should be closed enough to requested value "" +
                requestedDistances[requestedDistances.length-1],
                resultDistances[resultDistances.length-1],
                requestedDistances[requestedDistances.length-1] * (1.0f - errorMargin),
                requestedDistances[requestedDistances.length-1] * (1.0f + errorMargin));
    }

    /**
     * Verify edge mode control results for fpsRanges
     */
    private void edgeModesTestByCamera(List<Range<Integer>> fpsRanges) throws Exception {
        Size maxPrevSize = mOrderedPreviewSizes.get(0);
        int[] edgeModes = mStaticInfo.getAvailableEdgeModesChecked();
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);

        for (int mode : edgeModes) {
            requestBuilder.set(CaptureRequest.EDGE_MODE, mode);

            // Test that OFF and FAST mode should not slow down the frame rate.
            if (mode == CaptureRequest.EDGE_MODE_OFF ||
                    mode == CaptureRequest.EDGE_MODE_FAST) {
                verifyFpsNotSlowDown(requestBuilder, NUM_FRAMES_VERIFIED, fpsRanges);
            }

            SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
            startPreview(requestBuilder, maxPrevSize, resultListener);
            mSession.setRepeatingRequest(requestBuilder.build(), resultListener, mHandler);
            waitForSettingsApplied(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

            verifyCaptureResultForKey(CaptureResult.EDGE_MODE, mode, resultListener,
                    NUM_FRAMES_VERIFIED);
       }

        stopPreview();
    }

    /**
     * Test color correction controls.
     *
     * <p>Test different color correction modes. For TRANSFORM_MATRIX, only test
     * the unit gain and identity transform.</p>
     */
    private void colorCorrectionTestByCamera() throws Exception {
        CaptureRequest request;
        CaptureResult result;
        Size maxPreviewSz = mOrderedPreviewSizes.get(0); // Max preview size.
        updatePreviewSurface(maxPreviewSz);
        CaptureRequest.Builder manualRequestBuilder = createRequestForPreview();
        CaptureRequest.Builder previewRequestBuilder = createRequestForPreview();
        SimpleCaptureCallback listener = new SimpleCaptureCallback();

        startPreview(previewRequestBuilder, maxPreviewSz, listener);

        // Default preview result should give valid color correction metadata.
        result = listener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
        validateColorCorrectionResult(result,
                previewRequestBuilder.get(CaptureRequest.COLOR_CORRECTION_MODE));
        int colorCorrectionMode = CaptureRequest.COLOR_CORRECTION_MODE_TRANSFORM_MATRIX;
        // TRANSFORM_MATRIX mode
        // Only test unit gain and identity transform
        List<Integer> availableControlModes = Arrays.asList(
                CameraTestUtils.toObject(mStaticInfo.getAvailableControlModesChecked()));
        List<Integer> availableAwbModes = Arrays.asList(
                CameraTestUtils.toObject(mStaticInfo.getAwbAvailableModesChecked()));
        boolean isManualCCSupported =
                availableControlModes.contains(CaptureRequest.CONTROL_MODE_OFF) ||
                availableAwbModes.contains(CaptureRequest.CONTROL_AWB_MODE_OFF);
        if (isManualCCSupported) {
            if (!availableControlModes.contains(CaptureRequest.CONTROL_MODE_OFF)) {
                // Only manual AWB mode is supported
                manualRequestBuilder.set(CaptureRequest.CONTROL_MODE,
                        CaptureRequest.CONTROL_MODE_AUTO);
                manualRequestBuilder.set(CaptureRequest.CONTROL_AWB_MODE,
                        CaptureRequest.CONTROL_AWB_MODE_OFF);
            } else {
                // All 3A manual controls are supported, it doesn't matter what we set for AWB mode.
                manualRequestBuilder.set(CaptureRequest.CONTROL_MODE,
                        CaptureRequest.CONTROL_MODE_OFF);
            }

            RggbChannelVector UNIT_GAIN = new RggbChannelVector(1.0f, 1.0f, 1.0f, 1.0f);

            ColorSpaceTransform IDENTITY_TRANSFORM = new ColorSpaceTransform(
                new Rational[] {
                    ONE_R, ZERO_R, ZERO_R,
                    ZERO_R, ONE_R, ZERO_R,
                    ZERO_R, ZERO_R, ONE_R
                });

            manualRequestBuilder.set(CaptureRequest.COLOR_CORRECTION_MODE, colorCorrectionMode);
            manualRequestBuilder.set(CaptureRequest.COLOR_CORRECTION_GAINS, UNIT_GAIN);
            manualRequestBuilder.set(CaptureRequest.COLOR_CORRECTION_TRANSFORM, IDENTITY_TRANSFORM);
            request = manualRequestBuilder.build();
            mSession.capture(request, listener, mHandler);
            result = listener.getCaptureResultForRequest(request, NUM_RESULTS_WAIT_TIMEOUT);
            RggbChannelVector gains = result.get(CaptureResult.COLOR_CORRECTION_GAINS);
            ColorSpaceTransform transform = result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM);
            validateColorCorrectionResult(result, colorCorrectionMode);
            mCollector.expectEquals(""control mode result/request mismatch"",
                    CaptureResult.CONTROL_MODE_OFF, result.get(CaptureResult.CONTROL_MODE));
            mCollector.expectEquals(""Color correction gain result/request mismatch"",
                    UNIT_GAIN, gains);
            mCollector.expectEquals(""Color correction gain result/request mismatch"",
                    IDENTITY_TRANSFORM, transform);

        }

        // FAST mode
        colorCorrectionMode = CaptureRequest.COLOR_CORRECTION_MODE_FAST;
        manualRequestBuilder.set(CaptureRequest.CONTROL_MODE, CaptureRequest.CONTROL_MODE_AUTO);
        manualRequestBuilder.set(CaptureRequest.COLOR_CORRECTION_MODE, colorCorrectionMode);
        request = manualRequestBuilder.build();
        mSession.capture(request, listener, mHandler);
        result = listener.getCaptureResultForRequest(request, NUM_RESULTS_WAIT_TIMEOUT);
        validateColorCorrectionResult(result, colorCorrectionMode);
        mCollector.expectEquals(""control mode result/request mismatch"",
                CaptureResult.CONTROL_MODE_AUTO, result.get(CaptureResult.CONTROL_MODE));

        // HIGH_QUALITY mode
        colorCorrectionMode = CaptureRequest.COLOR_CORRECTION_MODE_HIGH_QUALITY;
        manualRequestBuilder.set(CaptureRequest.CONTROL_MODE, CaptureRequest.CONTROL_MODE_AUTO);
        manualRequestBuilder.set(CaptureRequest.COLOR_CORRECTION_MODE, colorCorrectionMode);
        request = manualRequestBuilder.build();
        mSession.capture(request, listener, mHandler);
        result = listener.getCaptureResultForRequest(request, NUM_RESULTS_WAIT_TIMEOUT);
        validateColorCorrectionResult(result, colorCorrectionMode);
        mCollector.expectEquals(""control mode result/request mismatch"",
                CaptureResult.CONTROL_MODE_AUTO, result.get(CaptureResult.CONTROL_MODE));
    }

    private void validateColorCorrectionResult(CaptureResult result, int colorCorrectionMode) {
        final RggbChannelVector ZERO_GAINS = new RggbChannelVector(0, 0, 0, 0);
        final int TRANSFORM_SIZE = 9;
        Rational[] zeroTransform = new Rational[TRANSFORM_SIZE];
        Arrays.fill(zeroTransform, ZERO_R);
        final ColorSpaceTransform ZERO_TRANSFORM = new ColorSpaceTransform(zeroTransform);

        RggbChannelVector resultGain;
        if ((resultGain = mCollector.expectKeyValueNotNull(result,
                CaptureResult.COLOR_CORRECTION_GAINS)) != null) {
            mCollector.expectKeyValueNotEquals(result,
                    CaptureResult.COLOR_CORRECTION_GAINS, ZERO_GAINS);
        }

        ColorSpaceTransform resultTransform;
        if ((resultTransform = mCollector.expectKeyValueNotNull(result,
                CaptureResult.COLOR_CORRECTION_TRANSFORM)) != null) {
            mCollector.expectKeyValueNotEquals(result,
                    CaptureResult.COLOR_CORRECTION_TRANSFORM, ZERO_TRANSFORM);
        }

        mCollector.expectEquals(""color correction mode result/request mismatch"",
                colorCorrectionMode, result.get(CaptureResult.COLOR_CORRECTION_MODE));
    }

    /**
     * Test that flash can be turned off successfully with a given initial and final AE_CONTROL
     * states.
     *
     * This function expects that initialAeControl and flashOffAeControl will not be either
     * CaptureRequest.CONTROL_AE_MODE_ON or CaptureRequest.CONTROL_AE_MODE_OFF
     *
     * @param listener The Capture listener that is used to wait for capture result
     * @param initialAeControl The initial AE_CONTROL mode to start repeating requests with.
     * @param flashOffAeControl The final AE_CONTROL mode which is expected to turn flash off for
     *        TEMPLATE_PREVIEW repeating requests.
     */
    private void flashTurnOffTest(SimpleCaptureCallback listener, boolean isLegacy,
            int initialAeControl, int flashOffAeControl) throws Exception {
        CaptureResult result;
        final int NUM_FLASH_REQUESTS_TESTED = 10;
        CaptureRequest.Builder requestBuilder = createRequestForPreview();
        requestBuilder.set(CaptureRequest.CONTROL_MODE, CaptureRequest.CONTROL_MODE_AUTO);
        requestBuilder.set(CaptureRequest.CONTROL_AE_MODE, initialAeControl);

        mSession.setRepeatingRequest(requestBuilder.build(), listener, mHandler);
        waitForSettingsApplied(listener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

        // Turn on torch using FLASH_MODE_TORCH
        requestBuilder.set(CaptureRequest.CONTROL_AE_MODE, CaptureRequest.CONTROL_AE_MODE_ON);
        requestBuilder.set(CaptureRequest.FLASH_MODE, CaptureRequest.FLASH_MODE_TORCH);
        CaptureRequest torchOnRequest = requestBuilder.build();
        mSession.setRepeatingRequest(torchOnRequest, listener, mHandler);
        waitForSettingsApplied(listener, NUM_FRAMES_WAITED_FOR_TORCH);
        result = listener.getCaptureResultForRequest(torchOnRequest, NUM_RESULTS_WAIT_TIMEOUT);
        // Test that the flash actually turned on continuously.
        mCollector.expectEquals(""Flash state result must be FIRED"", CaptureResult.FLASH_STATE_FIRED,
                result.get(CaptureResult.FLASH_STATE));
        mSession.stopRepeating();
        // Turn off the torch
        requestBuilder.set(CaptureRequest.CONTROL_AE_MODE, flashOffAeControl);
        // TODO: jchowdhary@, b/130323585, this line can be removed.
        requestBuilder.set(CaptureRequest.FLASH_MODE, CaptureRequest.FLASH_MODE_OFF);
        int numAllowedTransitionStates = NUM_PARTIAL_FRAMES_NPFC;
        if (mStaticInfo.isPerFrameControlSupported()) {
           numAllowedTransitionStates = NUM_PARTIAL_FRAMES_PFC;

        }
        // We submit 2 * numAllowedTransitionStates + 1 requests since we have two torch mode
        // transitions. The additional request is to check for at least 1 expected (FIRED / READY)
        // state.
        int numTorchTestSamples =  2 * numAllowedTransitionStates  + 1;
        CaptureRequest flashOffRequest = requestBuilder.build();
        int flashModeOffRequests = captureRequestsSynchronizedBurst(flashOffRequest,
                numTorchTestSamples, listener, mHandler);
        // Turn it on again.
        requestBuilder.set(CaptureRequest.FLASH_MODE, CaptureRequest.FLASH_MODE_TORCH);
        // We need to have CONTROL_AE_MODE be either CONTROL_AE_MODE_ON or CONTROL_AE_MODE_OFF to
        // turn the torch on again.
        requestBuilder.set(CaptureRequest.CONTROL_AE_MODE, CaptureRequest.CONTROL_AE_MODE_ON);
        CaptureRequest flashModeTorchRequest = requestBuilder.build();
        int flashModeTorchRequests = captureRequestsSynchronizedBurst(flashModeTorchRequest,
                numTorchTestSamples, listener, mHandler);

        CaptureResult[] torchStateResults =
                new CaptureResult[flashModeTorchRequests + flashModeOffRequests];
        Arrays.fill(torchStateResults, null);
        int i = 0;
        for (; i < flashModeOffRequests; i++) {
            torchStateResults[i] =
                    listener.getCaptureResultForRequest(flashOffRequest, NUM_RESULTS_WAIT_TIMEOUT);
            mCollector.expectNotEquals(""Result for flashModeOff request null"",
                    torchStateResults[i], null);
        }
        for (int j = i; j < torchStateResults.length; j++) {
            torchStateResults[j] =
                    listener.getCaptureResultForRequest(flashModeTorchRequest,
                            NUM_RESULTS_WAIT_TIMEOUT);
            mCollector.expectNotEquals(""Result for flashModeTorch request null"",
                    torchStateResults[j], null);
        }
        if (isLegacy) {
            // For LEGACY devices, flash state is null for all situations except:
            // android.control.aeMode == ON_ALWAYS_FLASH, where flash.state will be FIRED
            // android.flash.mode == TORCH, where flash.state will be FIRED
            testLegacyTorchStates(torchStateResults, 0, flashModeOffRequests - 1, flashOffRequest);
            testLegacyTorchStates(torchStateResults, flashModeOffRequests,
                    torchStateResults.length -1,
                    flashModeTorchRequest);
        } else {
            checkTorchStates(torchStateResults, numAllowedTransitionStates, flashModeOffRequests,
                    flashModeTorchRequests);
        }
    }

    private void testLegacyTorchStates(CaptureResult []torchStateResults, int beg, int end,
            CaptureRequest request) {
        for (int i = beg; i <= end; i++) {
            Integer requestControlAeMode = request.get(CaptureRequest.CONTROL_AE_MODE);
            Integer requestFlashMode = request.get(CaptureRequest.FLASH_MODE);
            Integer resultFlashState = torchStateResults[i].get(CaptureResult.FLASH_STATE);
            if (requestControlAeMode == CaptureRequest.CONTROL_AE_MODE_ON_ALWAYS_FLASH ||
                    requestFlashMode == CaptureRequest.FLASH_MODE_TORCH) {
                mCollector.expectEquals(""For LEGACY devices, flash state must be FIRED when"" +
                        ""CONTROL_AE_MODE == CONTROL_AE_MODE_ON_ALWAYS_FLASH or FLASH_MODE == "" +
                        ""TORCH, CONTROL_AE_MODE = "" + requestControlAeMode + "" FLASH_MODE = "" +
                        requestFlashMode, CaptureResult.FLASH_STATE_FIRED, resultFlashState);
                continue;
            }
            mCollector.expectTrue(""For LEGACY devices, flash state must be null when"" +
                        ""CONTROL_AE_MODE != CONTROL_AE_MODE_ON_ALWAYS_FLASH or FLASH_MODE != "" +
                        ""TORCH, CONTROL_AE_MODE = "" + requestControlAeMode + "" FLASH_MODE = "" +
                        requestFlashMode,  resultFlashState == null);
        }
    }
    // We check that torch states appear in the order expected. We don't necessarily know how many
    // times each state might appear, however we make sure that the states do not appear out of
    // order.
    private void checkTorchTransitionStates(CaptureResult []torchStateResults, int beg, int end,
            List<Integer> stateOrder, boolean isTurningOff) {
        Integer flashState;
        Integer curIndex = 0;
        for (int i = beg; i <= end; i++) {
            flashState = torchStateResults[i].get(CaptureResult.FLASH_STATE);
            int index = stateOrder.indexOf(flashState);
            mCollector.expectNotEquals(""Invalid state "" + flashState + "" not in expected list"" +
                    stateOrder, index, -1);
            mCollector.expectGreaterOrEqual(""state "" + flashState  + "" index "" + index +
                    "" is expected to be >= "" + curIndex,
                    curIndex, index);
            curIndex = index;
        }
    }

    private void checkTorchStates(CaptureResult []torchResults, int numAllowedTransitionStates,
            int numTorchOffSamples, int numTorchOnSamples) {
        // We test for flash states from request:
        // Request:       O(0) O(1) O(2) O(n)....O(nOFF) T(0) T(1) T(2) ....T(n) .... T(nON)
        // Valid Result : P/R  P/R  P/R  R R R...P/R P/R   P/F  P/F  P/F      F         F
        // For the FLASH_STATE_OFF requests, once FLASH_STATE READY has been seen, for the
        // transition states while switching the torch off, it must not transition to
        // FLASH_STATE_PARTIAL again till the next transition period which turns the torch on.
        // P - FLASH_STATE_PARTIAL
        // R - FLASH_STATE_READY
        // F - FLASH_STATE_FIRED
        // O(k) - kth FLASH_MODE_OFF request
        // T(k) - kth FLASH_MODE_TORCH request
        // nOFF - number of torch off samples
        // nON - number of torch on samples
        Integer flashState;
        // Check on -> off transition states
        List<Integer> onToOffStateOrderList = new ArrayList<Integer>();
        onToOffStateOrderList.add(CaptureRequest.FLASH_STATE_PARTIAL);
        onToOffStateOrderList.add(CaptureRequest.FLASH_STATE_READY);
        checkTorchTransitionStates(torchResults, 0, numAllowedTransitionStates,
                onToOffStateOrderList, true);
        // The next frames (before transition) must have its flash state as FLASH_STATE_READY
        for (int i = numAllowedTransitionStates + 1;
                i < numTorchOffSamples - numAllowedTransitionStates; i++) {
            flashState = torchResults[numAllowedTransitionStates].get(CaptureResult.FLASH_STATE);
            mCollector.expectEquals(""flash state result must be READY"",
                    CaptureResult.FLASH_STATE_READY, flashState);
        }
        // check off -> on transition states, before the FLASH_MODE_TORCH request was sent
        List<Integer> offToOnPreStateOrderList = new ArrayList<Integer>();
        offToOnPreStateOrderList.add(CaptureRequest.FLASH_STATE_READY);
        offToOnPreStateOrderList.add(CaptureRequest.FLASH_STATE_PARTIAL);
        checkTorchTransitionStates(torchResults,
                numTorchOffSamples - numAllowedTransitionStates, numTorchOffSamples - 1,
                offToOnPreStateOrderList, false);
        // check off -> on transition states
        List<Integer> offToOnPostStateOrderList = new ArrayList<Integer>();
        offToOnPostStateOrderList.add(CaptureRequest.FLASH_STATE_PARTIAL);
        offToOnPostStateOrderList.add(CaptureRequest.FLASH_STATE_FIRED);
        checkTorchTransitionStates(torchResults,
                numTorchOffSamples, numTorchOffSamples + numAllowedTransitionStates,
                offToOnPostStateOrderList, false);
        // check on states after off -> on transition
        // The next frames must have its flash state as FLASH_STATE_FIRED
        for (int i = numTorchOffSamples + numAllowedTransitionStates + 1;
                i < torchResults.length - 1; i++) {
            flashState = torchResults[i].get(CaptureResult.FLASH_STATE);
            mCollector.expectEquals(""flash state result must be FIRED for frame "" + i,
                    CaptureRequest.FLASH_STATE_FIRED, flashState);
        }
    }

    /**
     * Test flash mode control by AE mode.
     * <p>
     * Only allow AE mode ON or OFF, because other AE mode could run into conflict with
     * flash manual control. This function expects the camera to already have an active
     * repeating request and be sending results to the listener.
     * </p>
     *
     * @param listener The Capture listener that is used to wait for capture result
     * @param aeMode The AE mode for flash to test with
     */
    private void flashTestByAeMode(SimpleCaptureCallback listener, int aeMode) throws Exception {
        CaptureResult result;
        final int NUM_FLASH_REQUESTS_TESTED = 10;
        CaptureRequest.Builder requestBuilder = createRequestForPreview();

        if (aeMode == CaptureRequest.CONTROL_AE_MODE_ON) {
            requestBuilder.set(CaptureRequest.CONTROL_AE_MODE, aeMode);
        } else if (aeMode == CaptureRequest.CONTROL_AE_MODE_OFF) {
            changeExposure(requestBuilder, DEFAULT_EXP_TIME_NS, DEFAULT_SENSITIVITY);
        } else {
            throw new IllegalArgumentException(""This test only works when AE mode is ON or OFF"");
        }

        mSession.setRepeatingRequest(requestBuilder.build(), listener, mHandler);
        waitForSettingsApplied(listener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

        // For camera that doesn't have flash unit, flash state should always be UNAVAILABLE.
        if (mStaticInfo.getFlashInfoChecked() == false) {
            for (int i = 0; i < NUM_FLASH_REQUESTS_TESTED; i++) {
                result = listener.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);
                mCollector.expectEquals(""No flash unit available, flash state must be UNAVAILABLE""
                        + ""for AE mode "" + aeMode, CaptureResult.FLASH_STATE_UNAVAILABLE,
                        result.get(CaptureResult.FLASH_STATE));
            }

            return;
        }

        // Test flash SINGLE mode control. Wait for flash state to be READY first.
        if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
            waitForResultValue(listener, CaptureResult.FLASH_STATE, CaptureResult.FLASH_STATE_READY,
                    NUM_RESULTS_WAIT_TIMEOUT);
        } // else the settings were already waited on earlier

        requestBuilder.set(CaptureRequest.FLASH_MODE, CaptureRequest.FLASH_MODE_SINGLE);
        CaptureRequest flashSinglerequest = requestBuilder.build();

        int flashModeSingleRequests = captureRequestsSynchronized(
                flashSinglerequest, listener, mHandler);
        waitForNumResults(listener, flashModeSingleRequests - 1);
        result = listener.getCaptureResultForRequest(flashSinglerequest, NUM_RESULTS_WAIT_TIMEOUT);
        // Result mode must be SINGLE, state must be FIRED.
        mCollector.expectEquals(""Flash mode result must be SINGLE"",
                CaptureResult.FLASH_MODE_SINGLE, result.get(CaptureResult.FLASH_MODE));
        mCollector.expectEquals(""Flash state result must be FIRED"",
                CaptureResult.FLASH_STATE_FIRED, result.get(CaptureResult.FLASH_STATE));

        // Test flash TORCH mode control.
        requestBuilder.set(CaptureRequest.FLASH_MODE, CaptureRequest.FLASH_MODE_TORCH);
        CaptureRequest torchRequest = requestBuilder.build();

        int flashModeTorchRequests = captureRequestsSynchronized(torchRequest,
                NUM_FLASH_REQUESTS_TESTED, listener, mHandler);
        waitForNumResults(listener, flashModeTorchRequests - NUM_FLASH_REQUESTS_TESTED);

        // Verify the results
        TorchSeqState state = TorchSeqState.RAMPING_UP;
        for (int i = 0; i < NUM_FLASH_REQUESTS_TESTED; i++) {
            result = listener.getCaptureResultForRequest(torchRequest,
                    NUM_RESULTS_WAIT_TIMEOUT);
            int flashMode = result.get(CaptureResult.FLASH_MODE);
            int flashState = result.get(CaptureResult.FLASH_STATE);
            // Result mode must be TORCH
            mCollector.expectEquals(""Flash mode result "" + i + "" must be TORCH"",
                    CaptureResult.FLASH_MODE_TORCH, result.get(CaptureResult.FLASH_MODE));
            if (state == TorchSeqState.RAMPING_UP &&
                    flashState == CaptureResult.FLASH_STATE_FIRED) {
                state = TorchSeqState.FIRED;
            } else if (state == TorchSeqState.FIRED &&
                    flashState == CaptureResult.FLASH_STATE_PARTIAL) {
                state = TorchSeqState.RAMPING_DOWN;
            }

            if (i == 0 && mStaticInfo.isPerFrameControlSupported()) {
                mCollector.expectTrue(
                        ""Per frame control device must enter FIRED state on first torch request"",
                        state == TorchSeqState.FIRED);
            }

            if (state == TorchSeqState.FIRED) {
                mCollector.expectEquals(""Flash state result "" + i + "" must be FIRED"",
                        CaptureResult.FLASH_STATE_FIRED, result.get(CaptureResult.FLASH_STATE));
            } else {
                mCollector.expectEquals(""Flash state result "" + i + "" must be PARTIAL"",
                        CaptureResult.FLASH_STATE_PARTIAL, result.get(CaptureResult.FLASH_STATE));
            }
        }
        mCollector.expectTrue(""Torch state FIRED never seen"",
                state == TorchSeqState.FIRED || state == TorchSeqState.RAMPING_DOWN);

        // Test flash OFF mode control
        requestBuilder.set(CaptureRequest.FLASH_MODE, CaptureRequest.FLASH_MODE_OFF);
        CaptureRequest flashOffrequest = requestBuilder.build();

        int flashModeOffRequests = captureRequestsSynchronized(flashOffrequest, listener, mHandler);
        waitForNumResults(listener, flashModeOffRequests - 1);
        result = listener.getCaptureResultForRequest(flashOffrequest, NUM_RESULTS_WAIT_TIMEOUT);
        mCollector.expectEquals(""Flash mode result must be OFF"", CaptureResult.FLASH_MODE_OFF,
                result.get(CaptureResult.FLASH_MODE));
    }

    private void verifyAntiBandingMode(SimpleCaptureCallback listener, int numFramesVerified,
            int mode, boolean isAeManual, long requestExpTime) throws Exception {
        // Skip the first a couple of frames as antibanding may not be fully up yet.
        final int NUM_FRAMES_SKIPPED = 5;
        for (int i = 0; i < NUM_FRAMES_SKIPPED; i++) {
            listener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
        }

        for (int i = 0; i < numFramesVerified; i++) {
            CaptureResult result = listener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            Long resultExpTime = result.get(CaptureResult.SENSOR_EXPOSURE_TIME);
            assertNotNull(""Exposure time shouldn't be null"", resultExpTime);
            Integer flicker = result.get(CaptureResult.STATISTICS_SCENE_FLICKER);
            // Scene flicker result should be always available.
            assertNotNull(""Scene flicker must not be null"", flicker);
            assertTrue(""Scene flicker is invalid"", flicker >= STATISTICS_SCENE_FLICKER_NONE &&
                    flicker <= STATISTICS_SCENE_FLICKER_60HZ);

            Integer antiBandMode = result.get(CaptureResult.CONTROL_AE_ANTIBANDING_MODE);
            assertNotNull(""antiBanding mode shouldn't be null"", antiBandMode);
            assertTrue(""antiBanding Mode invalid, should be == "" + mode + "", is: "" + antiBandMode,
                    antiBandMode == mode);
            if (isAeManual) {
                // First, round down not up, second, need close enough.
                validateExposureTime(requestExpTime, resultExpTime);
                return;
            }

            long expectedExpTime = resultExpTime; // Default, no exposure adjustment.
            if (mode == CONTROL_AE_ANTIBANDING_MODE_50HZ) {
                // result exposure time must be adjusted by 50Hz illuminant source.
                expectedExpTime =
                        getAntiFlickeringExposureTime(ANTI_FLICKERING_50HZ, resultExpTime);
            } else if (mode == CONTROL_AE_ANTIBANDING_MODE_60HZ) {
                // result exposure time must be adjusted by 60Hz illuminant source.
                expectedExpTime =
                        getAntiFlickeringExposureTime(ANTI_FLICKERING_60HZ, resultExpTime);
            } else if (mode == CONTROL_AE_ANTIBANDING_MODE_AUTO){
                /**
                 * Use STATISTICS_SCENE_FLICKER to tell the illuminant source
                 * and do the exposure adjustment.
                 */
                expectedExpTime = resultExpTime;
                if (flicker == STATISTICS_SCENE_FLICKER_60HZ) {
                    expectedExpTime =
                            getAntiFlickeringExposureTime(ANTI_FLICKERING_60HZ, resultExpTime);
                } else if (flicker == STATISTICS_SCENE_FLICKER_50HZ) {
                    expectedExpTime =
                            getAntiFlickeringExposureTime(ANTI_FLICKERING_50HZ, resultExpTime);
                }
            }

            if (Math.abs(resultExpTime - expectedExpTime) > EXPOSURE_TIME_ERROR_MARGIN_NS) {
                mCollector.addMessage(String.format(""Result exposure time %dns diverges too much""
                        + "" from expected exposure time %dns for mode %d when AE is auto"",
                        resultExpTime, expectedExpTime, mode));
            }
        }
    }

    private void antiBandingTestByMode(Size size, int mode)
            throws Exception {
        if(VERBOSE) {
            Log.v(TAG, ""Anti-banding test for mode "" + mode + "" for camera "" + mCamera.getId());
        }
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);

        requestBuilder.set(CaptureRequest.CONTROL_AE_ANTIBANDING_MODE, mode);

        // Test auto AE mode anti-banding behavior
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        startPreview(requestBuilder, size, resultListener);
        waitForSettingsApplied(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
        verifyAntiBandingMode(resultListener, NUM_FRAMES_VERIFIED, mode, /*isAeManual*/false,
                IGNORE_REQUESTED_EXPOSURE_TIME_CHECK);

        // Test manual AE mode anti-banding behavior
        // 65ms, must be supported by full capability devices.
        final long TEST_MANUAL_EXP_TIME_NS = 65000000L;
        long manualExpTime = mStaticInfo.getExposureClampToRange(TEST_MANUAL_EXP_TIME_NS);
        changeExposure(requestBuilder, manualExpTime);
        resultListener = new SimpleCaptureCallback();
        startPreview(requestBuilder, size, resultListener);
        waitForSettingsApplied(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
        verifyAntiBandingMode(resultListener, NUM_FRAMES_VERIFIED, mode, /*isAeManual*/true,
                manualExpTime);

        stopPreview();
    }

    /**
     * Test the all available AE modes and AE lock.
     * <p>
     * For manual AE mode, test iterates through different sensitivities and
     * exposure times, validate the result exposure time correctness. For
     * CONTROL_AE_MODE_ON_ALWAYS_FLASH mode, the AE lock and flash are tested.
     * For the rest of the AUTO mode, AE lock is tested.
     * </p>
     *
     * @param mode
     */
    private void aeModeAndLockTestByMode(int mode)
            throws Exception {
        switch (mode) {
            case CONTROL_AE_MODE_OFF:
                if (mStaticInfo.isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
                    // Test manual exposure control.
                    aeManualControlTest();
                } else {
                    Log.w(TAG,
                            ""aeModeAndLockTestByMode - can't test AE mode OFF without "" +
                            ""manual sensor control"");
                }
                break;
            case CONTROL_AE_MODE_ON:
            case CONTROL_AE_MODE_ON_AUTO_FLASH:
            case CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE:
            case CONTROL_AE_MODE_ON_ALWAYS_FLASH:
            case CONTROL_AE_MODE_ON_EXTERNAL_FLASH:
                // Test AE lock for above AUTO modes.
                aeAutoModeTestLock(mode);
                break;
            default:
                throw new UnsupportedOperationException(""Unhandled AE mode "" + mode);
        }
    }

    /**
     * Test AE auto modes.
     * <p>
     * Use single request rather than repeating request to test AE lock per frame control.
     * </p>
     */
    private void aeAutoModeTestLock(int mode) throws Exception {
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        if (mStaticInfo.isAeLockSupported()) {
            requestBuilder.set(CaptureRequest.CONTROL_AE_LOCK, false);
        }
        requestBuilder.set(CaptureRequest.CONTROL_AE_MODE, mode);
        configurePreviewOutput(requestBuilder);

        final int MAX_NUM_CAPTURES_DURING_LOCK = 5;
        for (int i = 1; i <= MAX_NUM_CAPTURES_DURING_LOCK; i++) {
            autoAeMultipleCapturesThenTestLock(requestBuilder, mode, i);
        }
    }

    /**
     * Issue multiple auto AE captures, then lock AE, validate the AE lock vs.
     * the first capture result after the AE lock. The right AE lock behavior is:
     * When it is locked, it locks to the current exposure value, and all subsequent
     * request with lock ON will have the same exposure value locked.
     */
    private void autoAeMultipleCapturesThenTestLock(
            CaptureRequest.Builder requestBuilder, int aeMode, int numCapturesDuringLock)
            throws Exception {
        if (numCapturesDuringLock < 1) {
            throw new IllegalArgumentException(""numCapturesBeforeLock must be no less than 1"");
        }
        if (VERBOSE) {
            Log.v(TAG, ""Camera "" + mCamera.getId() + "": Testing auto AE mode and lock for mode ""
                    + aeMode + "" with "" + numCapturesDuringLock + "" captures before lock"");
        }

        final int NUM_CAPTURES_BEFORE_LOCK = 2;
        SimpleCaptureCallback listener =  new SimpleCaptureCallback();

        CaptureResult[] resultsDuringLock = new CaptureResult[numCapturesDuringLock];
        boolean canSetAeLock = mStaticInfo.isAeLockSupported();

        // Reset the AE lock to OFF, since we are reusing this builder many times
        if (canSetAeLock) {
            requestBuilder.set(CaptureRequest.CONTROL_AE_LOCK, false);
        }

        // Just send several captures with auto AE, lock off.
        CaptureRequest request = requestBuilder.build();
        for (int i = 0; i < NUM_CAPTURES_BEFORE_LOCK; i++) {
            mSession.capture(request, listener, mHandler);
        }
        waitForNumResults(listener, NUM_CAPTURES_BEFORE_LOCK);

        if (!canSetAeLock) {
            // Without AE lock, the remaining tests items won't work
            return;
        }

        // Then fire several capture to lock the AE.
        requestBuilder.set(CaptureRequest.CONTROL_AE_LOCK, true);

        int requestCount = captureRequestsSynchronized(
                requestBuilder.build(), numCapturesDuringLock, listener, mHandler);

        int[] sensitivities = new int[numCapturesDuringLock];
        long[] expTimes = new long[numCapturesDuringLock];
        Arrays.fill(sensitivities, -1);
        Arrays.fill(expTimes, -1L);

        // Get the AE lock on result and validate the exposure values.
        waitForNumResults(listener, requestCount - numCapturesDuringLock);
        for (int i = 0; i < resultsDuringLock.length; i++) {
            resultsDuringLock[i] = listener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
        }

        for (int i = 0; i < numCapturesDuringLock; i++) {
            mCollector.expectKeyValueEquals(
                    resultsDuringLock[i], CaptureResult.CONTROL_AE_LOCK, true);
        }

        // Can't read manual sensor/exposure settings without manual sensor
        if (mStaticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS)) {
            int sensitivityLocked =
                    getValueNotNull(resultsDuringLock[0], CaptureResult.SENSOR_SENSITIVITY);
            long expTimeLocked =
                    getValueNotNull(resultsDuringLock[0], CaptureResult.SENSOR_EXPOSURE_TIME);
            for (int i = 1; i < resultsDuringLock.length; i++) {
                mCollector.expectKeyValueEquals(
                        resultsDuringLock[i], CaptureResult.SENSOR_EXPOSURE_TIME, expTimeLocked);
                mCollector.expectKeyValueEquals(
                        resultsDuringLock[i], CaptureResult.SENSOR_SENSITIVITY, sensitivityLocked);
            }
        }
    }

    /**
     * Iterate through exposure times and sensitivities for manual AE control.
     * <p>
     * Use single request rather than repeating request to test manual exposure
     * value change per frame control.
     * </p>
     */
    private void aeManualControlTest()
            throws Exception {
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        configurePreviewOutput(requestBuilder);

        // Warm up pipeline for more accurate timing
        SimpleCaptureCallback warmupListener =  new SimpleCaptureCallback();
        mSession.setRepeatingRequest(requestBuilder.build(), warmupListener, mHandler);
        warmupListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);

        // Do manual captures
        requestBuilder.set(CaptureRequest.CONTROL_AE_MODE, CONTROL_AE_MODE_OFF);
        SimpleCaptureCallback listener =  new SimpleCaptureCallback();

        long[] expTimesNs = getExposureTimeTestValues();
        int[] sensitivities = getSensitivityTestValues();
        // Submit single request at a time, then verify the result.
        for (int i = 0; i < expTimesNs.length; i++) {
            for (int j = 0; j < sensitivities.length; j++) {
                if (VERBOSE) {
                    Log.v(TAG, ""Camera "" + mCamera.getId() + "": Testing sensitivity ""
                            + sensitivities[j] + "", exposure time "" + expTimesNs[i] + ""ns"");
                }

                changeExposure(requestBuilder, expTimesNs[i], sensitivities[j]);
                mSession.capture(requestBuilder.build(), listener, mHandler);

                // make sure timeout is long enough for long exposure time - add a 2x safety margin
                // to exposure time
                long timeoutMs = WAIT_FOR_RESULT_TIMEOUT_MS + 2 * expTimesNs[i] / 1000000;
                CaptureResult result = listener.getCaptureResult(timeoutMs);
                long resultExpTimeNs = getValueNotNull(result, CaptureResult.SENSOR_EXPOSURE_TIME);
                int resultSensitivity = getValueNotNull(result, CaptureResult.SENSOR_SENSITIVITY);
                validateExposureTime(expTimesNs[i], resultExpTimeNs);
                validateSensitivity(sensitivities[j], resultSensitivity);
                validateFrameDurationForCapture(result);
            }
        }
        mSession.stopRepeating();

        // TODO: Add another case to test where we can submit all requests, then wait for
        // results, which will hide the pipeline latency. this is not only faster, but also
        // test high speed per frame control and synchronization.
    }


    /**
     * Verify black level lock control.
     */
    private void verifyBlackLevelLockResults(SimpleCaptureCallback listener, int numFramesVerified,
            int maxLockOffCnt) throws Exception {
        int noLockCnt = 0;
        for (int i = 0; i < numFramesVerified; i++) {
            CaptureResult result = listener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            Boolean blackLevelLock = result.get(CaptureResult.BLACK_LEVEL_LOCK);
            assertNotNull(""Black level lock result shouldn't be null"", blackLevelLock);

            // Count the lock == false result, which could possibly occur at most once.
            if (blackLevelLock == false) {
                noLockCnt++;
            }

            if(VERBOSE) {
                Log.v(TAG, ""Black level lock result: "" + blackLevelLock);
            }
        }
        assertTrue(""Black level lock OFF occurs "" + noLockCnt + "" times,  expect at most ""
                + maxLockOffCnt + "" for camera "" + mCamera.getId(), noLockCnt <= maxLockOffCnt);
    }

    /**
     * Verify shading map for different shading modes.
     */
    private void verifyShadingMap(SimpleCaptureCallback listener, int numFramesVerified,
            int shadingMode) throws Exception {

        for (int i = 0; i < numFramesVerified; i++) {
            CaptureResult result = listener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            mCollector.expectEquals(""Shading mode result doesn't match request"",
                    shadingMode, result.get(CaptureResult.SHADING_MODE));
            LensShadingMap mapObj = result.get(
                    CaptureResult.STATISTICS_LENS_SHADING_CORRECTION_MAP);
            assertNotNull(""Map object must not be null"", mapObj);
            int numElementsInMap = mapObj.getGainFactorCount();
            float[] map = new float[numElementsInMap];
            mapObj.copyGainFactors(map, /*offset*/0);
            assertNotNull(""Map must not be null"", map);
            assertFalse(String.format(
                    ""Map size %d should be less than %d"", numElementsInMap, MAX_SHADING_MAP_SIZE),
                    numElementsInMap >= MAX_SHADING_MAP_SIZE);
            assertFalse(String.format(""Map size %d should be no less than %d"", numElementsInMap,
                    MIN_SHADING_MAP_SIZE), numElementsInMap < MIN_SHADING_MAP_SIZE);

            if (shadingMode == CaptureRequest.SHADING_MODE_FAST ||
                    shadingMode == CaptureRequest.SHADING_MODE_HIGH_QUALITY) {
                // shading mode is FAST or HIGH_QUALITY, expect to receive a map with all
                // elements >= 1.0f

                int badValueCnt = 0;
                // Detect the bad values of the map data.
                for (int j = 0; j < numElementsInMap; j++) {
                    if (Float.isNaN(map[j]) || map[j] < 1.0f) {
                        badValueCnt++;
                    }
                }
                assertEquals(""Number of value in the map is "" + badValueCnt + "" out of ""
                        + numElementsInMap, /*expected*/0, /*actual*/badValueCnt);
            } else if (shadingMode == CaptureRequest.SHADING_MODE_OFF) {
                float[] unityMap = new float[numElementsInMap];
                Arrays.fill(unityMap, 1.0f);
                // shading mode is OFF, expect to receive a unity map.
                assertTrue(""Result map "" + Arrays.toString(map) + "" must be an unity map"",
                        Arrays.equals(unityMap, map));
            }
        }
    }

    /**
     * Test face detection for a camera.
     */
    private void faceDetectionTestByCamera() throws Exception {
        int[] faceDetectModes = mStaticInfo.getAvailableFaceDetectModesChecked();

        SimpleCaptureCallback listener;
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);

        Size maxPreviewSz = mOrderedPreviewSizes.get(0); // Max preview size.
        for (int mode : faceDetectModes) {
            requestBuilder.set(CaptureRequest.STATISTICS_FACE_DETECT_MODE, mode);
            if (VERBOSE) {
                Log.v(TAG, ""Start testing face detection mode "" + mode);
            }

            // Create a new listener for each run to avoid the results from one run spill
            // into another run.
            listener = new SimpleCaptureCallback();
            startPreview(requestBuilder, maxPreviewSz, listener);
            waitForSettingsApplied(listener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
            verifyFaceDetectionResults(listener, NUM_FACE_DETECTION_FRAMES_VERIFIED, mode);
        }

        stopPreview();
    }

    /**
     * Verify face detection results for different face detection modes.
     *
     * @param listener The listener to get capture result
     * @param numFramesVerified Number of results to be verified
     * @param faceDetectionMode Face detection mode to be verified against
     */
    private void verifyFaceDetectionResults(SimpleCaptureCallback listener, int numFramesVerified,
            int faceDetectionMode) {
        for (int i = 0; i < numFramesVerified; i++) {
            CaptureResult result = listener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            mCollector.expectEquals(""Result face detection mode should match the request"",
                    faceDetectionMode, result.get(CaptureResult.STATISTICS_FACE_DETECT_MODE));

            Face[] faces = result.get(CaptureResult.STATISTICS_FACES);
            List<Integer> faceIds = new ArrayList<Integer>(faces.length);
            List<Integer> faceScores = new ArrayList<Integer>(faces.length);
            if (faceDetectionMode == CaptureResult.STATISTICS_FACE_DETECT_MODE_OFF) {
                mCollector.expectEquals(""Number of detection faces should always 0 for OFF mode"",
                        0, faces.length);
            } else if (faceDetectionMode == CaptureResult.STATISTICS_FACE_DETECT_MODE_SIMPLE) {
                for (Face face : faces) {
                    mCollector.expectNotNull(""Face rectangle shouldn't be null"", face.getBounds());
                    faceScores.add(face.getScore());
                    mCollector.expectTrue(""Face id is expected to be -1 for SIMPLE mode"",
                            face.getId() == Face.ID_UNSUPPORTED);
                }
            } else if (faceDetectionMode == CaptureResult.STATISTICS_FACE_DETECT_MODE_FULL) {
                if (VERBOSE) {
                    Log.v(TAG, ""Number of faces detected: "" + faces.length);
                }

                for (Face face : faces) {
                    Rect faceBound;
                    boolean faceRectAvailable =  mCollector.expectTrue(""Face rectangle ""
                            + ""shouldn't be null"", face.getBounds() != null);
                    if (!faceRectAvailable) {
                        continue;
                    }
                    faceBound = face.getBounds();

                    faceScores.add(face.getScore());
                    faceIds.add(face.getId());

                    mCollector.expectTrue(""Face id is shouldn't be -1 for FULL mode"",
                            face.getId() != Face.ID_UNSUPPORTED);
                    boolean leftEyeAvailable =
                            mCollector.expectTrue(""Left eye position shouldn't be null"",
                                    face.getLeftEyePosition() != null);
                    boolean rightEyeAvailable =
                            mCollector.expectTrue(""Right eye position shouldn't be null"",
                                    face.getRightEyePosition() != null);
                    boolean mouthAvailable =
                            mCollector.expectTrue(""Mouth position shouldn't be null"",
                            face.getMouthPosition() != null);
                    // Eyes/mouth position should be inside of the face rect.
                    if (leftEyeAvailable) {
                        Point leftEye = face.getLeftEyePosition();
                        mCollector.expectTrue(""Left eye "" + leftEye + ""should be""
                                + ""inside of face rect "" + faceBound,
                                faceBound.contains(leftEye.x, leftEye.y));
                    }
                    if (rightEyeAvailable) {
                        Point rightEye = face.getRightEyePosition();
                        mCollector.expectTrue(""Right eye "" + rightEye + ""should be""
                                + ""inside of face rect "" + faceBound,
                                faceBound.contains(rightEye.x, rightEye.y));
                    }
                    if (mouthAvailable) {
                        Point mouth = face.getMouthPosition();
                        mCollector.expectTrue(""Mouth "" + mouth +  "" should be inside of""
                                + "" face rect "" + faceBound,
                                faceBound.contains(mouth.x, mouth.y));
                    }
                }
            }
            mCollector.expectValuesInRange(""Face scores are invalid"", faceScores,
                    Face.SCORE_MIN, Face.SCORE_MAX);
            mCollector.expectValuesUnique(""Face ids are invalid"", faceIds);
        }
    }

    /**
     * Test tone map mode and result by camera
     */
    private void toneMapTestByCamera() throws Exception {
        if (!mStaticInfo.isManualToneMapSupported()) {
            return;
        }

        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        int[] toneMapModes = mStaticInfo.getAvailableToneMapModesChecked();
        // Test AUTO modes first. Note that FAST/HQ must both present or not present
        for (int i = 0; i < toneMapModes.length; i++) {
            if (toneMapModes[i] == CaptureRequest.TONEMAP_MODE_FAST && i > 0) {
                int tmpMode = toneMapModes[0];
                toneMapModes[0] = CaptureRequest.TONEMAP_MODE_FAST;
                toneMapModes[i] = tmpMode;
            }
            if (toneMapModes[i] == CaptureRequest.TONEMAP_MODE_HIGH_QUALITY && i > 1) {
                int tmpMode = toneMapModes[1];
                toneMapModes[1] = CaptureRequest.TONEMAP_MODE_HIGH_QUALITY;
                toneMapModes[i] = tmpMode;
            }
        }
        for (int mode : toneMapModes) {
            if (VERBOSE) {
                Log.v(TAG, ""Testing tonemap mode "" + mode);
            }

            requestBuilder.set(CaptureRequest.TONEMAP_MODE, mode);
            switch (mode) {
                case CaptureRequest.TONEMAP_MODE_CONTRAST_CURVE:
                    TonemapCurve toneCurve = new TonemapCurve(TONEMAP_CURVE_LINEAR,
                            TONEMAP_CURVE_LINEAR, TONEMAP_CURVE_LINEAR);
                    requestBuilder.set(CaptureRequest.TONEMAP_CURVE, toneCurve);
                    testToneMapMode(NUM_FRAMES_VERIFIED, requestBuilder);

                    toneCurve = new TonemapCurve(TONEMAP_CURVE_SRGB,
                            TONEMAP_CURVE_SRGB, TONEMAP_CURVE_SRGB);
                    requestBuilder.set(CaptureRequest.TONEMAP_CURVE, toneCurve);
                    testToneMapMode(NUM_FRAMES_VERIFIED, requestBuilder);
                    break;
                case CaptureRequest.TONEMAP_MODE_GAMMA_VALUE:
                    requestBuilder.set(CaptureRequest.TONEMAP_GAMMA, 1.0f);
                    testToneMapMode(NUM_FRAMES_VERIFIED, requestBuilder);
                    requestBuilder.set(CaptureRequest.TONEMAP_GAMMA, 2.2f);
                    testToneMapMode(NUM_FRAMES_VERIFIED, requestBuilder);
                    requestBuilder.set(CaptureRequest.TONEMAP_GAMMA, 5.0f);
                    testToneMapMode(NUM_FRAMES_VERIFIED, requestBuilder);
                    break;
                case CaptureRequest.TONEMAP_MODE_PRESET_CURVE:
                    requestBuilder.set(CaptureRequest.TONEMAP_PRESET_CURVE,
                            CaptureRequest.TONEMAP_PRESET_CURVE_REC709);
                    testToneMapMode(NUM_FRAMES_VERIFIED, requestBuilder);
                    requestBuilder.set(CaptureRequest.TONEMAP_PRESET_CURVE,
                            CaptureRequest.TONEMAP_PRESET_CURVE_SRGB);
                    testToneMapMode(NUM_FRAMES_VERIFIED, requestBuilder);
                    break;
                default:
                    testToneMapMode(NUM_FRAMES_VERIFIED, requestBuilder);
                    break;
            }
        }


    }

    /**
     * Test tonemap mode with speficied request settings
     *
     * @param numFramesVerified Number of results to be verified
     * @param requestBuilder the request builder of settings to be tested
     */
    private void testToneMapMode (int numFramesVerified,
            CaptureRequest.Builder requestBuilder)  throws Exception  {
        final int MIN_TONEMAP_CURVE_POINTS = 2;
        final Float ZERO = new Float(0);
        final Float ONE = new Float(1.0f);

        SimpleCaptureCallback listener = new SimpleCaptureCallback();
        int tonemapMode = requestBuilder.get(CaptureRequest.TONEMAP_MODE);
        Size maxPreviewSz = mOrderedPreviewSizes.get(0); // Max preview size.
        startPreview(requestBuilder, maxPreviewSz, listener);
        waitForSettingsApplied(listener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

        int maxCurvePoints = mStaticInfo.getMaxTonemapCurvePointChecked();
        for (int i = 0; i < numFramesVerified; i++) {
            CaptureResult result = listener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            mCollector.expectEquals(""Capture result tonemap mode should match request"", tonemapMode,
                    result.get(CaptureResult.TONEMAP_MODE));
            TonemapCurve tc = getValueNotNull(result, CaptureResult.TONEMAP_CURVE);
            int pointCount = tc.getPointCount(TonemapCurve.CHANNEL_RED);
            float[] mapRed = new float[pointCount * TonemapCurve.POINT_SIZE];
            pointCount = tc.getPointCount(TonemapCurve.CHANNEL_GREEN);
            float[] mapGreen = new float[pointCount * TonemapCurve.POINT_SIZE];
            pointCount = tc.getPointCount(TonemapCurve.CHANNEL_BLUE);
            float[] mapBlue = new float[pointCount * TonemapCurve.POINT_SIZE];
            tc.copyColorCurve(TonemapCurve.CHANNEL_RED, mapRed, 0);
            tc.copyColorCurve(TonemapCurve.CHANNEL_GREEN, mapGreen, 0);
            tc.copyColorCurve(TonemapCurve.CHANNEL_BLUE, mapBlue, 0);
            if (tonemapMode == CaptureResult.TONEMAP_MODE_CONTRAST_CURVE) {
                /**
                 * TODO: need figure out a good way to measure the difference
                 * between request and result, as they may have different array
                 * size.
                 */
            } else if (tonemapMode == CaptureResult.TONEMAP_MODE_GAMMA_VALUE) {
                mCollector.expectEquals(""Capture result gamma value should match request"",
                        requestBuilder.get(CaptureRequest.TONEMAP_GAMMA),
                        result.get(CaptureResult.TONEMAP_GAMMA));
            } else if (tonemapMode == CaptureResult.TONEMAP_MODE_PRESET_CURVE) {
                mCollector.expectEquals(""Capture result preset curve should match request"",
                        requestBuilder.get(CaptureRequest.TONEMAP_PRESET_CURVE),
                        result.get(CaptureResult.TONEMAP_PRESET_CURVE));
            }

            // Tonemap curve result availability and basic validity check for all modes.
            mCollector.expectValuesInRange(""Tonemap curve red values are out of range"",
                    CameraTestUtils.toObject(mapRed), /*min*/ZERO, /*max*/ONE);
            mCollector.expectInRange(""Tonemap curve red length is out of range"",
                    mapRed.length, MIN_TONEMAP_CURVE_POINTS, maxCurvePoints * 2);
            mCollector.expectValuesInRange(""Tonemap curve green values are out of range"",
                    CameraTestUtils.toObject(mapGreen), /*min*/ZERO, /*max*/ONE);
            mCollector.expectInRange(""Tonemap curve green length is out of range"",
                    mapGreen.length, MIN_TONEMAP_CURVE_POINTS, maxCurvePoints * 2);
            mCollector.expectValuesInRange(""Tonemap curve blue values are out of range"",
                    CameraTestUtils.toObject(mapBlue), /*min*/ZERO, /*max*/ONE);
            mCollector.expectInRange(""Tonemap curve blue length is out of range"",
                    mapBlue.length, MIN_TONEMAP_CURVE_POINTS, maxCurvePoints * 2);

            // Make sure capture result tonemap has identical channels.
            if (mStaticInfo.isMonochromeCamera()) {
                mCollector.expectEquals(""Capture result tonemap of monochrome camera should "" +
                        ""have same dimension for all channels"", mapRed.length, mapGreen.length);
                mCollector.expectEquals(""Capture result tonemap of monochrome camera should "" +
                        ""have same dimension for all channels"", mapRed.length, mapBlue.length);

                if (mapRed.length == mapGreen.length && mapRed.length == mapBlue.length) {
                    boolean isIdentical = true;
                    for (int j = 0; j < mapRed.length; j++) {
                        isIdentical = (mapRed[j] == mapGreen[j] && mapRed[j] == mapBlue[j]);
                        if (!isIdentical)
                            break;
                    }
                    mCollector.expectTrue(""Capture result tonemap of monochrome camera should "" +
                            ""be identical between all channels"", isIdentical);
                }
            }
        }
        stopPreview();
    }

    /**
     * Test awb mode control.
     * <p>
     * Test each supported AWB mode, verify the AWB mode in capture result
     * matches request. When AWB is locked, the color correction gains and
     * transform should remain unchanged.
     * </p>
     */
    private void awbModeAndLockTestByCamera() throws Exception {
        int[] awbModes = mStaticInfo.getAwbAvailableModesChecked();
        Size maxPreviewSize = mOrderedPreviewSizes.get(0);
        boolean canSetAwbLock = mStaticInfo.isAwbLockSupported();
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        startPreview(requestBuilder, maxPreviewSize, /*listener*/null);

        for (int mode : awbModes) {
            SimpleCaptureCallback listener;
            requestBuilder.set(CaptureRequest.CONTROL_AWB_MODE, mode);
            listener = new SimpleCaptureCallback();
            mSession.setRepeatingRequest(requestBuilder.build(), listener, mHandler);
            waitForSettingsApplied(listener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

            // Verify AWB mode in capture result.
            verifyCaptureResultForKey(CaptureResult.CONTROL_AWB_MODE, mode, listener,
                    NUM_FRAMES_VERIFIED);

            if (mode == CameraMetadata.CONTROL_AWB_MODE_AUTO && canSetAwbLock) {
                // Verify color correction transform and gains stay unchanged after a lock.
                requestBuilder.set(CaptureRequest.CONTROL_AWB_LOCK, true);
                listener = new SimpleCaptureCallback();
                mSession.setRepeatingRequest(requestBuilder.build(), listener, mHandler);
                waitForSettingsApplied(listener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

                if (mStaticInfo.areKeysAvailable(CaptureResult.CONTROL_AWB_STATE)) {
                    waitForResultValue(listener, CaptureResult.CONTROL_AWB_STATE,
                            CaptureResult.CONTROL_AWB_STATE_LOCKED, NUM_RESULTS_WAIT_TIMEOUT);
                }

            }
            // Don't verify auto mode result if AWB lock is not supported
            if (mode != CameraMetadata.CONTROL_AWB_MODE_AUTO || canSetAwbLock) {
                verifyAwbCaptureResultUnchanged(listener, NUM_FRAMES_VERIFIED);
            }
        }
    }

    private void verifyAwbCaptureResultUnchanged(SimpleCaptureCallback listener,
            int numFramesVerified) {
        // Skip check if cc gains/transform/mode are not available
        if (!mStaticInfo.areKeysAvailable(
                CaptureResult.COLOR_CORRECTION_GAINS,
                CaptureResult.COLOR_CORRECTION_TRANSFORM,
                CaptureResult.COLOR_CORRECTION_MODE)) {
            return;
        }

        CaptureResult result = listener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
        RggbChannelVector lockedGains =
                getValueNotNull(result, CaptureResult.COLOR_CORRECTION_GAINS);
        ColorSpaceTransform lockedTransform =
                getValueNotNull(result, CaptureResult.COLOR_CORRECTION_TRANSFORM);

        for (int i = 0; i < numFramesVerified; i++) {
            result = listener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            // Color correction mode check is skipped here, as it is checked in colorCorrectionTest.
            validateColorCorrectionResult(result, result.get(CaptureResult.COLOR_CORRECTION_MODE));

            RggbChannelVector gains = getValueNotNull(result, CaptureResult.COLOR_CORRECTION_GAINS);
            ColorSpaceTransform transform =
                    getValueNotNull(result, CaptureResult.COLOR_CORRECTION_TRANSFORM);
            mCollector.expectEquals(""Color correction gains should remain unchanged after awb lock"",
                    lockedGains, gains);
            mCollector.expectEquals(""Color correction transform should remain unchanged after""
                    + "" awb lock"", lockedTransform, transform);
        }
    }

    /**
     * Test AF mode control.
     * <p>
     * Test all supported AF modes, verify the AF mode in capture result matches
     * request. When AF mode is one of the CONTROL_AF_MODE_CONTINUOUS_* mode,
     * verify if the AF can converge to PASSIVE_FOCUSED or PASSIVE_UNFOCUSED
     * state within certain amount of frames.
     * </p>
     */
    private void afModeTestByCamera() throws Exception {
        int[] afModes = mStaticInfo.getAfAvailableModesChecked();
        Size maxPreviewSize = mOrderedPreviewSizes.get(0);
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        startPreview(requestBuilder, maxPreviewSize, /*listener*/null);

        for (int mode : afModes) {
            SimpleCaptureCallback listener;
            requestBuilder.set(CaptureRequest.CONTROL_AF_MODE, mode);
            listener = new SimpleCaptureCallback();
            mSession.setRepeatingRequest(requestBuilder.build(), listener, mHandler);
            waitForSettingsApplied(listener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

            // Verify AF mode in capture result.
            verifyCaptureResultForKey(CaptureResult.CONTROL_AF_MODE, mode, listener,
                    NUM_FRAMES_VERIFIED);

            // Verify AF can finish a scan for CONTROL_AF_MODE_CONTINUOUS_* modes.
            // In LEGACY mode, a transition to one of the continuous AF modes does not necessarily
            // result in a passive AF call if the camera has already been focused, and the scene has
            // not changed enough to trigger an AF pass.  Skip this constraint for LEGACY.
            if (mStaticInfo.isHardwareLevelAtLeastLimited() &&
                    (mode == CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_PICTURE ||
                    mode == CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_VIDEO)) {
                List<Integer> afStateList = new ArrayList<Integer>();
                afStateList.add(CaptureResult.CONTROL_AF_STATE_PASSIVE_FOCUSED);
                afStateList.add(CaptureResult.CONTROL_AF_STATE_PASSIVE_UNFOCUSED);
                waitForAnyResultValue(listener, CaptureResult.CONTROL_AF_STATE, afStateList,
                        NUM_RESULTS_WAIT_TIMEOUT);
            }
        }
    }

    /**
     * Test video and optical stabilizations if they are supported by a given camera.
     */
    private void stabilizationTestByCamera() throws Exception {
        // video stabilization test.
        List<Key<?>> keys = mStaticInfo.getCharacteristics().getKeys();

        Integer[] videoStabModes = (keys.contains(CameraCharacteristics.
                CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES)) ?
                CameraTestUtils.toObject(mStaticInfo.getAvailableVideoStabilizationModesChecked()) :
                    new Integer[0];
        int[] opticalStabModes = (keys.contains(
                CameraCharacteristics.LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION)) ?
                mStaticInfo.getAvailableOpticalStabilizationChecked() : new int[0];

        Size maxPreviewSize = mOrderedPreviewSizes.get(0);
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        SimpleCaptureCallback listener = new SimpleCaptureCallback();
        startPreview(requestBuilder, maxPreviewSize, listener);

        for (Integer mode : videoStabModes) {
            listener = new SimpleCaptureCallback();
            requestBuilder.set(CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE, mode);
            mSession.setRepeatingRequest(requestBuilder.build(), listener, mHandler);
            waitForSettingsApplied(listener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
            // Video stabilization could return any modes.
            verifyAnyCaptureResultForKey(CaptureResult.CONTROL_VIDEO_STABILIZATION_MODE,
                    videoStabModes, listener, NUM_FRAMES_VERIFIED);
        }

        for (int mode : opticalStabModes) {
            listener = new SimpleCaptureCallback();
            requestBuilder.set(CaptureRequest.LENS_OPTICAL_STABILIZATION_MODE, mode);
            mSession.setRepeatingRequest(requestBuilder.build(), listener, mHandler);
            waitForSettingsApplied(listener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
            verifyCaptureResultForKey(CaptureResult.LENS_OPTICAL_STABILIZATION_MODE, mode,
                    listener, NUM_FRAMES_VERIFIED);
        }

        stopPreview();
    }

    private void digitalZoomTestByCamera(Size previewSize) throws Exception {
        final int ZOOM_STEPS = 15;
        final PointF[] TEST_ZOOM_CENTERS;
        final float maxZoom = mStaticInfo.getAvailableMaxDigitalZoomChecked();
        final float ZOOM_ERROR_MARGIN = 0.01f;
        if (Math.abs(maxZoom - 1.0f) < ZOOM_ERROR_MARGIN) {
            // It doesn't make much sense to test the zoom if the device effectively supports
            // no zoom.
            return;
        }

        final int croppingType = mStaticInfo.getScalerCroppingTypeChecked();
        if (croppingType == CameraCharacteristics.SCALER_CROPPING_TYPE_FREEFORM) {
            // Set the four corners in a way that the minimally allowed zoom factor is 2x.
            float normalizedLeft = 0.25f;
            float normalizedTop = 0.25f;
            float normalizedRight = 0.75f;
            float normalizedBottom = 0.75f;
            // If the max supported zoom is too small, make sure we at least test the max
            // Zoom is tested for the four corners.
            if (maxZoom < 2.0f) {
                normalizedLeft = 0.5f / maxZoom;
                normalizedTop = 0.5f / maxZoom;
                normalizedRight = 1.0f - normalizedLeft;
                normalizedBottom = 1.0f - normalizedTop;
            }
            TEST_ZOOM_CENTERS = new PointF[] {
                new PointF(0.5f, 0.5f),   // Center point
                new PointF(normalizedLeft, normalizedTop),     // top left corner zoom
                new PointF(normalizedRight, normalizedTop),    // top right corner zoom
                new PointF(normalizedLeft, normalizedBottom),  // bottom left corner zoom
                new PointF(normalizedRight, normalizedBottom), // bottom right corner zoom
            };

            if (VERBOSE) {
                Log.v(TAG, ""Testing zoom with CROPPING_TYPE = FREEFORM"");
            }
        } else {
            // CENTER_ONLY
            TEST_ZOOM_CENTERS = new PointF[] {
                    new PointF(0.5f, 0.5f),   // Center point
            };

            if (VERBOSE) {
                Log.v(TAG, ""Testing zoom with CROPPING_TYPE = CENTER_ONLY"");
            }
        }

        final Rect activeArraySize = mStaticInfo.getActiveArraySizeChecked();
        final Rect defaultCropRegion = new Rect(0, 0,
                activeArraySize.width(), activeArraySize.height());
        Rect[] cropRegions = new Rect[ZOOM_STEPS];
        MeteringRectangle[][] expectRegions = new MeteringRectangle[ZOOM_STEPS][];
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        SimpleCaptureCallback listener = new SimpleCaptureCallback();

        updatePreviewSurface(previewSize);
        configurePreviewOutput(requestBuilder);

        CaptureRequest[] requests = new CaptureRequest[ZOOM_STEPS];

        // Set algorithm regions
        final int METERING_RECT_RATIO = 10;
        final MeteringRectangle[][] defaultMeteringRects = new MeteringRectangle[][] {
                {
                    new MeteringRectangle (
                        /*x*/0, /*y*/0, activeArraySize.width(), activeArraySize.height(),
                        /*meteringWeight*/1), /* full active region */
                },
                {
                    new MeteringRectangle (
                        /*x*/0, /*y*/0, activeArraySize.width()/METERING_RECT_RATIO,
                        activeArraySize.height()/METERING_RECT_RATIO,
                        /*meteringWeight*/1),
                },
                {
                    new MeteringRectangle (
                        /*x*/(int)(activeArraySize.width() * (0.5f - 0.5f/METERING_RECT_RATIO)),
                        /*y*/(int)(activeArraySize.height() * (0.5f - 0.5f/METERING_RECT_RATIO)),
                        activeArraySize.width()/METERING_RECT_RATIO,
                        activeArraySize.height()/METERING_RECT_RATIO,
                        /*meteringWeight*/1),
                },
        };

        final int CAPTURE_SUBMIT_REPEAT;
        {
            int maxLatency = mStaticInfo.getSyncMaxLatency();
            if (maxLatency == CameraMetadata.SYNC_MAX_LATENCY_UNKNOWN) {
                CAPTURE_SUBMIT_REPEAT = NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY + 1;
            } else {
                CAPTURE_SUBMIT_REPEAT = maxLatency + 1;
            }
        }

        if (VERBOSE) {
            Log.v(TAG, ""Testing zoom with CAPTURE_SUBMIT_REPEAT = "" + CAPTURE_SUBMIT_REPEAT);
        }

        for (MeteringRectangle[] meteringRect : defaultMeteringRects) {
            for (int algo = 0; algo < NUM_ALGORITHMS; algo++) {
                update3aRegion(requestBuilder, algo,  meteringRect);
            }

            for (PointF center : TEST_ZOOM_CENTERS) {
                Rect previousCrop = null;

                for (int i = 0; i < ZOOM_STEPS; i++) {
                    /*
                     * Submit capture request
                     */
                    float zoomFactor = (float) (1.0f + (maxZoom - 1.0) * i / ZOOM_STEPS);
                    cropRegions[i] = getCropRegionForZoom(zoomFactor, center,
                            maxZoom, defaultCropRegion);
                    if (VERBOSE) {
                        Log.v(TAG, ""Testing Zoom for factor "" + zoomFactor + "" and center "" +
                                center + "" The cropRegion is "" + cropRegions[i] +
                                "" Preview size is "" + previewSize);
                    }
                    requestBuilder.set(CaptureRequest.SCALER_CROP_REGION, cropRegions[i]);
                    requests[i] = requestBuilder.build();
                    for (int j = 0; j < CAPTURE_SUBMIT_REPEAT; ++j) {
                        if (VERBOSE) {
                            Log.v(TAG, ""submit crop region "" + cropRegions[i]);
                        }
                        mSession.capture(requests[i], listener, mHandler);
                    }

                    /*
                     * Validate capture result
                     */
                    waitForNumResults(listener, CAPTURE_SUBMIT_REPEAT - 1); // Drop first few frames
                    TotalCaptureResult result = listener.getTotalCaptureResultForRequest(
                            requests[i], NUM_RESULTS_WAIT_TIMEOUT);
                    List<CaptureResult> partialResults = result.getPartialResults();

                    Rect cropRegion = getValueNotNull(result, CaptureResult.SCALER_CROP_REGION);
                    for (CaptureResult partialResult : partialResults) {
                        Rect cropRegionInPartial =
                                partialResult.get(CaptureResult.SCALER_CROP_REGION);
                        if (cropRegionInPartial != null) {
                            mCollector.expectEquals(""SCALER_CROP_REGION in partial result must ""
                                    + ""match in final result"", cropRegionInPartial, cropRegion);
                        }
                    }

                    /*
                     * Validate resulting crop regions
                     */
                    if (previousCrop != null) {
                        Rect currentCrop = cropRegion;
                        mCollector.expectTrue(String.format(
                                ""Crop region should shrink or stay the same "" +
                                        ""(previous = %s, current = %s)"",
                                        previousCrop, currentCrop),
                                previousCrop.equals(currentCrop) ||
                                    (previousCrop.width() > currentCrop.width() &&
                                     previousCrop.height() > currentCrop.height()));
                    }

                    if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                        mCollector.expectRectsAreSimilar(
                                ""Request and result crop region should be similar"",
                                cropRegions[i], cropRegion, CROP_REGION_ERROR_PERCENT_DELTA);
                    }

                    if (croppingType == SCALER_CROPPING_TYPE_CENTER_ONLY) {
                        mCollector.expectRectCentered(
                                ""Result crop region should be centered inside the active array"",
                                new Size(activeArraySize.width(), activeArraySize.height()),
                                cropRegion, CROP_REGION_ERROR_PERCENT_CENTERED);
                    }

                    /*
                     * Validate resulting metering regions
                     */

                    // Use the actual reported crop region to calculate the resulting metering region
                    expectRegions[i] = getExpectedOutputRegion(
                            /*requestRegion*/meteringRect,
                            /*cropRect*/     cropRegion);

                    // Verify Output 3A region is intersection of input 3A region and crop region
                    for (int algo = 0; algo < NUM_ALGORITHMS; algo++) {
                        validate3aRegion(result, partialResults, algo, expectRegions[i],
                                false/*scaleByZoomRatio*/);
                    }

                    previousCrop = cropRegion;
                }

                if (maxZoom > 1.0f) {
                    mCollector.expectTrue(
                            String.format(""Most zoomed-in crop region should be smaller"" +
                                            ""than active array w/h"" +
                                            ""(last crop = %s, active array = %s)"",
                                            previousCrop, activeArraySize),
                                (previousCrop.width() < activeArraySize.width() &&
                                 previousCrop.height() < activeArraySize.height()));
                }
            }
        }
    }

    private void zoomRatioTestByCamera(Size previewSize) throws Exception {
        final int ZOOM_STEPS = 15;
        final Range<Float> zoomRatioRange = mStaticInfo.getZoomRatioRangeChecked();
        // The error margin is derive from a VGA size camera zoomed all the way to 10x, in which
        // case the cropping error can be as large as 480/46 - 480/48 = 0.435.
        final float ZOOM_ERROR_MARGIN = 0.05f;

        final Rect activeArraySize = mStaticInfo.getActiveArraySizeChecked();
        final Rect defaultCropRegion =
                new Rect(0, 0, activeArraySize.width(), activeArraySize.height());
        final Rect zoom2xCropRegion =
                new Rect(activeArraySize.width()/4, activeArraySize.height()/4,
                        activeArraySize.width()*3/4, activeArraySize.height()*3/4);
        MeteringRectangle[][] expectRegions = new MeteringRectangle[ZOOM_STEPS][];
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        requestBuilder.set(CaptureRequest.SCALER_CROP_REGION, defaultCropRegion);
        SimpleCaptureCallback listener = new SimpleCaptureCallback();

        updatePreviewSurface(previewSize);
        configurePreviewOutput(requestBuilder);

        // Set algorithm regions to full active region
        final MeteringRectangle[] defaultMeteringRect = new MeteringRectangle[] {
                new MeteringRectangle (
                        /*x*/0, /*y*/0, activeArraySize.width(), activeArraySize.height(),
                        /*meteringWeight*/1)
        };

        for (int algo = 0; algo < NUM_ALGORITHMS; algo++) {
            update3aRegion(requestBuilder, algo,  defaultMeteringRect);
        }

        final int captureSubmitRepeat;
        {
            int maxLatency = mStaticInfo.getSyncMaxLatency();
            if (maxLatency == CameraMetadata.SYNC_MAX_LATENCY_UNKNOWN) {
                captureSubmitRepeat = NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY + 1;
            } else {
                captureSubmitRepeat = maxLatency + 1;
            }
        }

        float previousRatio = zoomRatioRange.getLower();
        for (int i = 0; i < ZOOM_STEPS; i++) {
            /*
             * Submit capture request
             */
            float zoomFactor = zoomRatioRange.getLower() + (zoomRatioRange.getUpper() -
                    zoomRatioRange.getLower()) * i / ZOOM_STEPS;
            if (VERBOSE) {
                Log.v(TAG, ""Testing Zoom ratio "" + zoomFactor + "" Preview size is "" + previewSize);
            }
            requestBuilder.set(CaptureRequest.CONTROL_ZOOM_RATIO, zoomFactor);
            requestBuilder.set(CaptureRequest.SCALER_CROP_REGION, defaultCropRegion);
            CaptureRequest request = requestBuilder.build();
            for (int j = 0; j < captureSubmitRepeat; ++j) {
                mSession.capture(request, listener, mHandler);
            }

            /*
             * Validate capture result
             */
            waitForNumResults(listener, captureSubmitRepeat - 1); // Drop first few frames
            TotalCaptureResult result = listener.getTotalCaptureResultForRequest(
                    request, NUM_RESULTS_WAIT_TIMEOUT);
            List<CaptureResult> partialResults = result.getPartialResults();
            float resultZoomRatio = getValueNotNull(result, CaptureResult.CONTROL_ZOOM_RATIO);
            Rect cropRegion = getValueNotNull(result, CaptureResult.SCALER_CROP_REGION);

            for (CaptureResult partialResult : partialResults) {
                Rect cropRegionInPartial =
                        partialResult.get(CaptureResult.SCALER_CROP_REGION);
                if (cropRegionInPartial != null) {
                    mCollector.expectEquals(""SCALER_CROP_REGION in partial result must ""
                            + ""match in final result"", cropRegionInPartial, cropRegion);
                }

                Float zoomRatioInPartial = partialResult.get(CaptureResult.CONTROL_ZOOM_RATIO);
                if (zoomRatioInPartial != null) {
                    mCollector.expectEquals(""CONTROL_ZOOM_RATIO in partial result must match""
                            + "" that in final result"", resultZoomRatio, zoomRatioInPartial);
                }
            }

            /*
             * Validate resulting crop regions and zoom ratio
             */
            mCollector.expectTrue(String.format(
                    ""Zoom ratio should increase or stay the same "" +
                            ""(previous = %f, current = %f)"",
                            previousRatio, resultZoomRatio),
                    Math.abs(previousRatio - resultZoomRatio) < ZOOM_ERROR_MARGIN ||
                        (previousRatio < resultZoomRatio));

            mCollector.expectTrue(String.format(
                    ""Request and result zoom ratio should be similar "" +
                    ""(requested = %f, result = %f"", zoomFactor, resultZoomRatio),
                    Math.abs(zoomFactor - resultZoomRatio)/zoomFactor <= ZOOM_ERROR_MARGIN);

            //In case zoom ratio is converted to crop region at HAL, due to error magnification
            //when converting to post-zoom crop region, scale the error threshold for crop region
            //check.
            float errorMultiplier = Math.max(1.0f, zoomFactor);
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                mCollector.expectRectsAreSimilar(
                        ""Request and result crop region should be similar"",
                        defaultCropRegion, cropRegion,
                        CROP_REGION_ERROR_PERCENT_DELTA * errorMultiplier);
            }

            mCollector.expectRectCentered(
                    ""Result crop region should be centered inside the active array"",
                    new Size(activeArraySize.width(), activeArraySize.height()),
                    cropRegion, CROP_REGION_ERROR_PERCENT_CENTERED * errorMultiplier);

            /*
             * Validate resulting metering regions
             */
            // Use the actual reported crop region to calculate the resulting metering region
            expectRegions[i] = getExpectedOutputRegion(
                    /*requestRegion*/defaultMeteringRect,
                    /*cropRect*/     cropRegion);

            // Verify Output 3A region is intersection of input 3A region and crop region
            boolean scaleByZoomRatio = zoomFactor > 1.0f;
            for (int algo = 0; algo < NUM_ALGORITHMS; algo++) {
                validate3aRegion(result, partialResults, algo, expectRegions[i], scaleByZoomRatio);
            }

            previousRatio = resultZoomRatio;

            /*
             * Set windowboxing cropRegion while zoomRatio is not 1.0x, and make sure the crop
             * region was overwritten.
             */
            if (zoomFactor != 1.0f) {
                requestBuilder.set(CaptureRequest.SCALER_CROP_REGION, zoom2xCropRegion);
                CaptureRequest requestWithCrop = requestBuilder.build();
                for (int j = 0; j < captureSubmitRepeat; ++j) {
                    mSession.capture(requestWithCrop, listener, mHandler);
                }

                waitForNumResults(listener, captureSubmitRepeat - 1); // Drop first few frames
                CaptureResult resultWithCrop = listener.getCaptureResultForRequest(
                        requestWithCrop, NUM_RESULTS_WAIT_TIMEOUT);
                float resultZoomRatioWithCrop = getValueNotNull(resultWithCrop,
                        CaptureResult.CONTROL_ZOOM_RATIO);
                Rect cropRegionWithCrop = getValueNotNull(resultWithCrop,
                        CaptureResult.SCALER_CROP_REGION);

                mCollector.expectTrue(String.format(
                        ""Result zoom ratio should remain the same (activeArrayCrop: %f, "" +
                        ""zoomedCrop: %f)"", resultZoomRatio, resultZoomRatioWithCrop),
                        Math.abs(resultZoomRatio - resultZoomRatioWithCrop) < ZOOM_ERROR_MARGIN);

                if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                    mCollector.expectRectsAreSimilar(
                            ""Result crop region should remain the same with or without crop"",
                            cropRegion, cropRegionWithCrop, CROP_REGION_ERROR_PERCENT_DELTA);
                }
            }
        }
    }

    private void digitalZoomPreviewCombinationTestByCamera() throws Exception {
        final double ASPECT_RATIO_THRESHOLD = 0.001;
        List<Double> aspectRatiosTested = new ArrayList<Double>();
        Size maxPreviewSize = mOrderedPreviewSizes.get(0);
        aspectRatiosTested.add((double)(maxPreviewSize.getWidth()) / maxPreviewSize.getHeight());

        for (Size size : mOrderedPreviewSizes) {
            // Max preview size was already tested in testDigitalZoom test. skip it.
            if (size.equals(maxPreviewSize)) {
                continue;
            }

            // Only test the largest size for each aspect ratio.
            double aspectRatio = (double)(size.getWidth()) / size.getHeight();
            if (isAspectRatioContained(aspectRatiosTested, aspectRatio, ASPECT_RATIO_THRESHOLD)) {
                continue;
            }

            if (VERBOSE) {
                Log.v(TAG, ""Test preview size "" + size.toString() + "" digital zoom"");
            }

            aspectRatiosTested.add(aspectRatio);
            digitalZoomTestByCamera(size);
        }
    }

    private static boolean isAspectRatioContained(List<Double> aspectRatioList,
            double aspectRatio, double delta) {
        for (Double ratio : aspectRatioList) {
            if (Math.abs(ratio - aspectRatio) < delta) {
                return true;
            }
        }

        return false;
    }

    private void sceneModeTestByCamera() throws Exception {
        int[] sceneModes = mStaticInfo.getAvailableSceneModesChecked();
        Size maxPreviewSize = mOrderedPreviewSizes.get(0);
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        SimpleCaptureCallback listener = new SimpleCaptureCallback();
        requestBuilder.set(CaptureRequest.CONTROL_MODE, CaptureRequest.CONTROL_MODE_USE_SCENE_MODE);
        startPreview(requestBuilder, maxPreviewSize, listener);

        for(int mode : sceneModes) {
            requestBuilder.set(CaptureRequest.CONTROL_SCENE_MODE, mode);
            listener = new SimpleCaptureCallback();
            mSession.setRepeatingRequest(requestBuilder.build(), listener, mHandler);
            waitForSettingsApplied(listener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

            verifyCaptureResultForKey(CaptureResult.CONTROL_SCENE_MODE,
                    mode, listener, NUM_FRAMES_VERIFIED);
            // This also serves as purpose of showing preview for NUM_FRAMES_VERIFIED
            verifyCaptureResultForKey(CaptureResult.CONTROL_MODE,
                    CaptureRequest.CONTROL_MODE_USE_SCENE_MODE, listener, NUM_FRAMES_VERIFIED);
        }
    }

    private void effectModeTestByCamera() throws Exception {
        int[] effectModes = mStaticInfo.getAvailableEffectModesChecked();
        Size maxPreviewSize = mOrderedPreviewSizes.get(0);
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        requestBuilder.set(CaptureRequest.CONTROL_MODE, CaptureRequest.CONTROL_MODE_AUTO);
        SimpleCaptureCallback listener = new SimpleCaptureCallback();
        startPreview(requestBuilder, maxPreviewSize, listener);

        for(int mode : effectModes) {
            requestBuilder.set(CaptureRequest.CONTROL_EFFECT_MODE, mode);
            listener = new SimpleCaptureCallback();
            mSession.setRepeatingRequest(requestBuilder.build(), listener, mHandler);
            waitForSettingsApplied(listener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

            verifyCaptureResultForKey(CaptureResult.CONTROL_EFFECT_MODE,
                    mode, listener, NUM_FRAMES_VERIFIED);
            // This also serves as purpose of showing preview for NUM_FRAMES_VERIFIED
            verifyCaptureResultForKey(CaptureResult.CONTROL_MODE,
                    CaptureRequest.CONTROL_MODE_AUTO, listener, NUM_FRAMES_VERIFIED);
        }
    }

    private void extendedSceneModeTestByCamera(List<Range<Integer>> fpsRanges) throws Exception {
        Capability[] extendedSceneModeCaps = mStaticInfo.getAvailableExtendedSceneModeCapsChecked();
        if (extendedSceneModeCaps.length == 0) {
            return;
        }

        Size maxPreviewSize = mOrderedPreviewSizes.get(0);
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);

        for (Capability cap : extendedSceneModeCaps) {
            int mode = cap.getMode();
            requestBuilder.set(CaptureRequest.CONTROL_EXTENDED_SCENE_MODE, mode);

            // Test that DISABLED and BOKEH_CONTINUOUS mode doesn't slow down the frame rate
            if (mode == CaptureRequest.CONTROL_EXTENDED_SCENE_MODE_DISABLED ||
                    mode == CaptureRequest.CONTROL_EXTENDED_SCENE_MODE_BOKEH_CONTINUOUS) {
                verifyFpsNotSlowDown(requestBuilder, NUM_FRAMES_VERIFIED, fpsRanges);
            }

            Range<Float> zoomRange = cap.getZoomRatioRange();
            float[] zoomRatios = new float[]{zoomRange.getLower(), zoomRange.getUpper()};
            for (float ratio : zoomRatios) {
                SimpleCaptureCallback listener = new SimpleCaptureCallback();
                requestBuilder.set(CaptureRequest.CONTROL_ZOOM_RATIO, ratio);
                startPreview(requestBuilder, maxPreviewSize, listener);
                waitForSettingsApplied(listener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

                verifyCaptureResultForKey(CaptureResult.CONTROL_EXTENDED_SCENE_MODE,
                        mode, listener, NUM_FRAMES_VERIFIED);
                verifyCaptureResultForKey(CaptureResult.CONTROL_ZOOM_RATIO,
                        ratio, listener, NUM_FRAMES_VERIFIED);
            }
        }
    }

    //----------------------------------------------------------------
    //---------Below are common functions for all tests.--------------
    //----------------------------------------------------------------

    /**
     * Enable exposure manual control and change exposure and sensitivity and
     * clamp the value into the supported range.
     */
    private void changeExposure(CaptureRequest.Builder requestBuilder,
            long expTime, int sensitivity) {
        // Check if the max analog sensitivity is available and no larger than max sensitivity.  The
        // max analog sensitivity is not actually used here. This is only an extra correctness
        // check.
        mStaticInfo.getMaxAnalogSensitivityChecked();

        expTime = mStaticInfo.getExposureClampToRange(expTime);
        sensitivity = mStaticInfo.getSensitivityClampToRange(sensitivity);

        requestBuilder.set(CaptureRequest.CONTROL_AE_MODE, CONTROL_AE_MODE_OFF);
        requestBuilder.set(CaptureRequest.SENSOR_EXPOSURE_TIME, expTime);
        requestBuilder.set(CaptureRequest.SENSOR_SENSITIVITY, sensitivity);
    }
    /**
     * Enable exposure manual control and change exposure time and
     * clamp the value into the supported range.
     *
     * <p>The sensitivity is set to default value.</p>
     */
    private void changeExposure(CaptureRequest.Builder requestBuilder, long expTime) {
        changeExposure(requestBuilder, expTime, DEFAULT_SENSITIVITY);
    }

    /**
     * Get the exposure time array that contains multiple exposure time steps in
     * the exposure time range, in nanoseconds.
     */
    private long[] getExposureTimeTestValues() {
        long[] testValues = new long[DEFAULT_NUM_EXPOSURE_TIME_STEPS + 1];
        long maxExpTime = mStaticInfo.getExposureMaximumOrDefault(DEFAULT_EXP_TIME_NS);
        long minExpTime = mStaticInfo.getExposureMinimumOrDefault(DEFAULT_EXP_TIME_NS);

        long range = maxExpTime - minExpTime;
        double stepSize = range / (double)DEFAULT_NUM_EXPOSURE_TIME_STEPS;
        for (int i = 0; i < testValues.length; i++) {
            testValues[i] = maxExpTime - (long)(stepSize * i);
            testValues[i] = mStaticInfo.getExposureClampToRange(testValues[i]);
        }

        return testValues;
    }

    /**
     * Generate test focus distances in range of [0, minFocusDistance] in increasing order.
     *
     * @param repeatMin number of times minValue will be repeated.
     * @param repeatMax number of times maxValue will be repeated.
     */
    private float[] getFocusDistanceTestValuesInOrder(int repeatMin, int repeatMax) {
        int totalCount = NUM_TEST_FOCUS_DISTANCES + 1 + repeatMin + repeatMax;
        float[] testValues = new float[totalCount];
        float minValue = 0;
        float maxValue = mStaticInfo.getMinimumFocusDistanceChecked();

        float range = maxValue - minValue;
        float stepSize = range / NUM_TEST_FOCUS_DISTANCES;

        for (int i = 0; i < repeatMin; i++) {
            testValues[i] = minValue;
        }
        for (int i = 0; i <= NUM_TEST_FOCUS_DISTANCES; i++) {
            testValues[repeatMin+i] = minValue + stepSize * i;
        }
        for (int i = 0; i < repeatMax; i++) {
            testValues[repeatMin+NUM_TEST_FOCUS_DISTANCES+1+i] =
                    maxValue;
        }

        return testValues;
    }

    /**
     * Get the sensitivity array that contains multiple sensitivity steps in the
     * sensitivity range.
     * <p>
     * Sensitivity number of test values is determined by
     * {@value #DEFAULT_SENSITIVITY_STEP_SIZE} and sensitivity range, and
     * bounded by {@value #DEFAULT_NUM_SENSITIVITY_STEPS}.
     * </p>
     */
    private int[] getSensitivityTestValues() {
        int maxSensitivity = mStaticInfo.getSensitivityMaximumOrDefault(
                DEFAULT_SENSITIVITY);
        int minSensitivity = mStaticInfo.getSensitivityMinimumOrDefault(
                DEFAULT_SENSITIVITY);

        int range = maxSensitivity - minSensitivity;
        int stepSize = DEFAULT_SENSITIVITY_STEP_SIZE;
        int numSteps = range / stepSize;
        // Bound the test steps to avoid supper long test.
        if (numSteps > DEFAULT_NUM_SENSITIVITY_STEPS) {
            numSteps = DEFAULT_NUM_SENSITIVITY_STEPS;
            stepSize = range / numSteps;
        }
        int[] testValues = new int[numSteps + 1];
        for (int i = 0; i < testValues.length; i++) {
            testValues[i] = maxSensitivity - stepSize * i;
            testValues[i] = mStaticInfo.getSensitivityClampToRange(testValues[i]);
        }

        return testValues;
    }

    /**
     * Validate the AE manual control exposure time.
     *
     * <p>Exposure should be close enough, and only round down if they are not equal.</p>
     *
     * @param request Request exposure time
     * @param result Result exposure time
     */
    private void validateExposureTime(long request, long result) {
        long expTimeDelta = request - result;
        long expTimeErrorMargin = (long)(Math.max(EXPOSURE_TIME_ERROR_MARGIN_NS, request
                * EXPOSURE_TIME_ERROR_MARGIN_RATE));
        // First, round down not up, second, need close enough.
        mCollector.expectTrue(""Exposture time is invalid for AE manaul control test, request: ""
                + request + "" result: "" + result,
                expTimeDelta < expTimeErrorMargin && expTimeDelta >= 0);
    }

    /**
     * Validate AE manual control sensitivity.
     *
     * @param request Request sensitivity
     * @param result Result sensitivity
     */
    private void validateSensitivity(int request, int result) {
        float sensitivityDelta = request - result;
        float sensitivityErrorMargin = request * SENSITIVITY_ERROR_MARGIN_RATE;
        // First, round down not up, second, need close enough.
        mCollector.expectTrue(""Sensitivity is invalid for AE manaul control test, request: ""
                + request + "" result: "" + result,
                sensitivityDelta < sensitivityErrorMargin && sensitivityDelta >= 0);
    }

    /**
     * Validate frame duration for a given capture.
     *
     * <p>Frame duration should be longer than exposure time.</p>
     *
     * @param result The capture result for a given capture
     */
    private void validateFrameDurationForCapture(CaptureResult result) {
        long expTime = getValueNotNull(result, CaptureResult.SENSOR_EXPOSURE_TIME);
        long frameDuration = getValueNotNull(result, CaptureResult.SENSOR_FRAME_DURATION);
        if (VERBOSE) {
            Log.v(TAG, ""frame duration: "" + frameDuration + "" Exposure time: "" + expTime);
        }

        mCollector.expectTrue(String.format(""Frame duration (%d) should be longer than exposure""
                + "" time (%d) for a given capture"", frameDuration, expTime),
                frameDuration >= expTime);

        validatePipelineDepth(result);
    }

    /**
     * Basic verification for the control mode capture result.
     *
     * @param key The capture result key to be verified against
     * @param requestMode The request mode for this result
     * @param listener The capture listener to get capture results
     * @param numFramesVerified The number of capture results to be verified
     */
    private <T> void verifyCaptureResultForKey(CaptureResult.Key<T> key, T requestMode,
            SimpleCaptureCallback listener, int numFramesVerified) {
        for (int i = 0; i < numFramesVerified; i++) {
            CaptureResult result = listener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            validatePipelineDepth(result);
            T resultMode = getValueNotNull(result, key);
            if (VERBOSE) {
                Log.v(TAG, ""Expect value: "" + requestMode.toString() + "" result value: ""
                        + resultMode.toString());
            }
            mCollector.expectEquals(""Key "" + key.getName() + "" result should match request"",
                    requestMode, resultMode);
        }
    }

    /**
     * Basic verification that the value of a capture result key should be one of the expected
     * values.
     *
     * @param key The capture result key to be verified against
     * @param expectedModes The list of any possible expected modes for this result
     * @param listener The capture listener to get capture results
     * @param numFramesVerified The number of captur"	""	""	"minimum"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.BurstCaptureRawTest"	"testTimestamp"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/BurstCaptureRawTest.java"	""	"public void testTimestamp() throws Exception {
        Log.i(TAG, ""Begin testTimestamp"");

        performTestRoutine(new TestTimestamp(), NONSTALL_RAW_FORMATS);

        Log.i(TAG, ""End testTimestamp"");
    }

    /*
     * Below are private infrastructure for all tests
     */

    /**
     * A structure encapsulates all the parameters for setting up preview, and RAW capture.
     */
    class CaptureSetup
    {
        public CaptureSetup(Size previewCaptureSize, Size rawCaptureSize,
                CaptureRequest.Builder previewRequestBuilder,
                CaptureRequest.Builder rawRequestBuilder,
                SimpleCaptureCallback previewCaptureCallback,
                SimpleCaptureCallback rawCaptureCallback,
                SimpleImageReaderListener rawReaderListener)
        {
            mPreviewCaptureSize = previewCaptureSize;
            mRawCaptureSize = rawCaptureSize;
            mPreviewRequestBuilder = previewRequestBuilder;
            mRawRequestBuilder = rawRequestBuilder;
            mPreviewCaptureCallback = previewCaptureCallback;
            mRawCaptureCallback = rawCaptureCallback;
            mRawReaderListener = rawReaderListener;
        }

        public Size getPreviewCaptureSize()
        {
            return mPreviewCaptureSize;
        }

        public Size getRawCaptureSize()
        {
            return mRawCaptureSize;
        }

        public CaptureRequest.Builder getPreviewRequestBuilder()
        {
            return mPreviewRequestBuilder;
        }

        public CaptureRequest.Builder getRawRequestBuilder() {
            return mRawRequestBuilder;
        }

        public SimpleCaptureCallback getPreviewCaptureCallback() {
            return mPreviewCaptureCallback;
        }

        public SimpleCaptureCallback getRawCaptureCallback() {
            return mRawCaptureCallback;
        }

        public SimpleImageReaderListener getRawReaderListener() {
            return mRawReaderListener;
        }

        private Size mPreviewCaptureSize;
        private Size mRawCaptureSize;
        private CaptureRequest.Builder mPreviewRequestBuilder;
        private CaptureRequest.Builder mRawRequestBuilder;

        /** all the non-testing requests are sent to here */
        private SimpleCaptureCallback mPreviewCaptureCallback;
        /** all the testing requests are sent to here */
        private SimpleCaptureCallback mRawCaptureCallback;
        /** all the testing framebuffers are sent to here */
        private SimpleImageReaderListener mRawReaderListener;
    }

    /**
     * Interface for the test routines that are being called by performTestRoutines(). Implement
     * different test cases in execute().
     */
    interface TestRoutine {
        public void execute(CaptureRequest.Builder rawBurstBuilder,
                SimpleCaptureCallback rawCaptureCallback,
                SimpleImageReaderListener rawReaderListener, int rawFormat) throws Exception;
    }

    /**
     * Implementation of metadata round down test.
     */
    class TestMetaDataRoundDownRoutine implements TestRoutine
    {
        @Override
        public void execute(CaptureRequest.Builder rawBurstBuilder,
                SimpleCaptureCallback rawCaptureCallback,
                SimpleImageReaderListener rawReaderListener, int rawFormat) throws Exception
        {
            // build burst capture
            ArrayList<CaptureRequest> rawRequestList = createBurstRequest(rawBurstBuilder);

            // submit captrue
            Log.i(TAG, ""Submitting Burst Request."");
            mSession.captureBurst(rawRequestList, rawCaptureCallback, mHandler);

            // verify metadata
            for (int i = 0; i < MAX_FRAMES_BURST; i++) {
                CaptureResult result = rawCaptureCallback.getCaptureResult(
                        CAPTURE_IMAGE_TIMEOUT_MS);

                long resultExposure = result.get(CaptureResult.SENSOR_EXPOSURE_TIME);
                int resultSensitivity = result.get(CaptureResult.SENSOR_SENSITIVITY);
                long desiredExposure = rawRequestList.get(i).get(
                        CaptureRequest.SENSOR_EXPOSURE_TIME);
                int desiredSensitivity = rawRequestList.get(i).get(
                        CaptureRequest.SENSOR_SENSITIVITY);

                Log.i(TAG, String.format(
                        ""Received capture result, exposure = %d, sensitivity = %d. ""
                                + ""Requested exposure = %d, sensitivity = %d."",
                        resultExposure,
                        resultSensitivity, desiredExposure, desiredSensitivity));

                mCollector.expectTrue(
                        String.format(""Exposure value is greater than requested: ""
                                + ""requested = %d, result = %d."",
                                desiredExposure, resultExposure),
                                resultExposure <= desiredExposure);

                mCollector.expectTrue(
                        String.format(""Sensitivity value is greater than requested: ""
                                + ""requested = %d, result = %d."",
                                desiredSensitivity, resultSensitivity),
                                resultSensitivity <= desiredSensitivity);
            }
        }
    }

    /**
     * Implementation of manual-auto switching test.
     */
    class TestManualAutoSwitch implements TestRoutine
    {
        @Override
        public void execute(CaptureRequest.Builder rawBurstBuilder,
                SimpleCaptureCallback rawCaptureCallback,
                SimpleImageReaderListener rawReaderListener, int rawFormat) throws Exception
        {
            // create a capture request builder to preserve all the original values
            CaptureRequest.Builder originBuilder = mCamera.createCaptureRequest(
                    CameraDevice.TEMPLATE_STILL_CAPTURE);
            copyBurstRequetBuilder(originBuilder, rawBurstBuilder);

            // build burst capture
            ArrayList<CaptureRequest> rawRequestList = createBurstRequest(rawBurstBuilder);

            // submit captrue but ignore
            mSession.captureBurst(rawRequestList, rawCaptureCallback, mHandler);

            // drain the capture result
            drainQueues(rawReaderListener, rawCaptureCallback);

            // reset and build capture with 3A
            copyBurstRequetBuilder(rawBurstBuilder, originBuilder);
            rawRequestList = createBurstRequestWith3A(rawBurstBuilder);

            // submit captrue but ignore
            mSession.captureBurst(rawRequestList, rawCaptureCallback, mHandler);

            // drain the capture result
            drainQueues(rawReaderListener, rawCaptureCallback);

            // reset and rebuild manual raw burst capture
            copyBurstRequetBuilder(rawBurstBuilder, originBuilder);
            rawRequestList = createBurstRequest(rawBurstBuilder);

            // submit capture
            Log.i(TAG, ""Submitting Burst Request."");
            mSession.captureBurst(rawRequestList, rawCaptureCallback, mHandler);

            // verify metadata
            for (int i = 0; i < MAX_FRAMES_BURST; i++) {
                CaptureResult result = rawCaptureCallback.getCaptureResult(
                        CAPTURE_IMAGE_TIMEOUT_MS);

                long resultExposure = result.get(CaptureResult.SENSOR_EXPOSURE_TIME);
                int resultSensitivity = result.get(CaptureResult.SENSOR_SENSITIVITY);
                int resultEdgeMode = result.get(CaptureResult.EDGE_MODE);
                int resultNoiseReductionMode = result.get(
                        CaptureResult.NOISE_REDUCTION_MODE);
                long desiredExposure = rawRequestList.get(i).get(
                        CaptureRequest.SENSOR_EXPOSURE_TIME);
                int desiredSensitivity = rawRequestList.get(i).get(
                        CaptureRequest.SENSOR_SENSITIVITY);

                Log.i(TAG, String.format(
                        ""Received capture result, exposure = %d, sensitivity = %d. ""
                                + ""Requested exposure = %d, sensitivity = %d."",
                        resultExposure,
                        resultSensitivity, desiredExposure, desiredSensitivity));

                mCollector.expectTrue(String.format(""Edge mode is not turned off.""),
                        resultEdgeMode == CaptureRequest.EDGE_MODE_OFF);

                mCollector.expectTrue(String.format(""Noise reduction is not turned off.""),
                        resultNoiseReductionMode
                        == CaptureRequest.NOISE_REDUCTION_MODE_OFF);

                mCollector.expectTrue(
                        String.format(""Exposure value is greater than requested: ""
                                + ""requested = %d, result = %d."",
                                desiredExposure, resultExposure),
                                resultExposure <= desiredExposure);

                mCollector.expectTrue(
                        String.format(""Sensitivity value is greater than requested: ""
                                + ""requested = %d, result = %d."",
                                desiredSensitivity, resultSensitivity),
                                resultSensitivity <= desiredSensitivity);
            }

        }
    }

    /**
     * Implementation of timestamp test
     */
    class TestTimestamp implements TestRoutine
    {
        private final double THRESHOLD = 5000000.0; // 5ms
        private final long EXPOSURE_MULTIPLIERS_PRIVATE[] = {
                1, 1, 1 };
        private final int SENSITIVITY_MLTIPLIERS_PRIVATE[] = {
                1, 1, 1 };
        private final int MAX_FRAMES_BURST_PRIVATE =
                EXPOSURE_MULTIPLIERS_PRIVATE.length * SENSITIVITY_MLTIPLIERS_PRIVATE.length;

        @Override
        public void execute(Builder rawBurstBuilder, SimpleCaptureCallback rawCaptureCallback,
                SimpleImageReaderListener rawReaderListener, int rawFormat) throws Exception {
            // prepare some local variables
            ArrayList<Long> sensorTime = new ArrayList<Long>(MAX_FRAMES_BURST_PRIVATE);

            // build burst capture
            ArrayList<CaptureRequest> rawRequestList = createBurstRequest(rawBurstBuilder,
                    EXPOSURE_MULTIPLIERS_PRIVATE, SENSITIVITY_MLTIPLIERS_PRIVATE);

            // submit capture while recording timestamp
            Log.i(TAG, ""Submitting Burst Request."");
            mSession.captureBurst(rawRequestList, rawCaptureCallback, mHandler);

            // receive frames while recording timestamp
            for (int i = 0; i < MAX_FRAMES_BURST_PRIVATE; i++) {
                CaptureResult result = rawCaptureCallback.getCaptureResult(
                        CAPTURE_IMAGE_TIMEOUT_MS);
                long resultExposure = result.get(CaptureResult.SENSOR_EXPOSURE_TIME);
                int resultSensitivity = result.get(CaptureResult.SENSOR_SENSITIVITY);
                long resultTimestamp = result.get(CaptureResult.SENSOR_TIMESTAMP);
                Log.i(TAG, String.format(
                        ""Received capture result, exposure = %d, sensitivity = %d, timestamp = %d"",
                        resultExposure, resultSensitivity, resultTimestamp));

                sensorTime.add(resultTimestamp);
            }

            // compare sensor time and compute the difference
            ArrayList<Long> deltaList = new ArrayList<Long>();
            for (int i = 1; i < MAX_FRAMES_BURST_PRIVATE; i++)
            {
                deltaList.add(sensorTime.get(i) - sensorTime.get(i - 1));
            }

            // compute the average and standard deviation of the differences
            double average = 0.0;
            for (int i = 0; i < deltaList.size(); i++)
            {
                average += deltaList.get(i);
            }
            average /= deltaList.size();

            double stddev = 0.0;
            for (int i = 0; i < deltaList.size(); i++)
            {
                double diff = deltaList.get(i) - average;
                stddev += diff * diff;
            }
            stddev = Math.sqrt(stddev / deltaList.size());

            Log.i(TAG, String.format(""average = %.2f, stddev = %.2f"", average, stddev));

            StringBuilder sensorTimestampMessage = new StringBuilder();
            for (int i = 0; i < sensorTime.size(); i++)
            {
                sensorTimestampMessage.append(""frame ["");
                sensorTimestampMessage.append(i);
                sensorTimestampMessage.append(""] SENSOR_TIMESTAMP = "");
                sensorTimestampMessage.append(sensorTime.get(i));
                sensorTimestampMessage.append(""\n"");
            }

            mCollector.expectLessOrEqual(
                    ""The standard deviation of frame interval is larger then threshold: "" +
                    String.format(""stddev = %.2f, threshold = %.2f.\n"", stddev, THRESHOLD) +
                    sensorTimestampMessage.toString(),
                    THRESHOLD, stddev);
        }
    }

    /**
     * Check sensor capability prior to the test.
     *
     * @return true if the it is has the capability to execute the test.
     */
    private boolean checkCapability(String id, ArrayList<Integer> supportedRawList,
            int[] testedFormats) {
        StaticMetadata staticInfo = mAllStaticInfo.get(id);
        // make sure the sensor has manual support
        if (!staticInfo.isHardwareLevelAtLeastFull()) {
            Log.w(TAG, ""Full hardware level is not supported"");
            return false;
        }

        // get the list of supported RAW format
        StreamConfigurationMap config = staticInfo.getValueFromKeyNonNull(
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);

        // check for the RAW support
        supportedRawList.clear();
        for (int rawFormat : testedFormats) {
            if (!config.isOutputSupportedFor(rawFormat)) {
                continue;
            }
            supportedRawList.add(rawFormat);
        }

        if (supportedRawList.size() == 0)
        {
            Log.w(TAG, ""RAW output is not supported!"");
            return false;
        }

        return true;
    }

    /**
     * Return the sensor format to human readable string.
     *
     * @param format Sensor image format.
     * @return Human readable string.
     */
    private String imageFormatToString(int format) {
        switch (format) {
            case ImageFormat.RAW10:
                return ""RAW10"";
            case ImageFormat.RAW12:
                return ""RAW12"";
            case ImageFormat.RAW_SENSOR:
                return ""RAW_SENSOR"";
        }

        return ""Unknown"";
    }

    /**
     * Setting up various classes prior to the request, e.g.: capture size, builder, callback and
     * listener
     *
     * @return initialized variables that can be directly fed into prepareCaptureAndStartPreview().
     */
    private CaptureSetup initCaptureSetupForPreviewAndRaw() throws Exception
    {
        // capture size
        Size previewSize = mOrderedPreviewSizes.get(0);
        Size rawSize = mStaticInfo.getRawDimensChecked();

        // builder
        CaptureRequest.Builder previewCaptureBuilder = mCamera.createCaptureRequest(
                CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder rawCaptureBuilder = mCamera.createCaptureRequest(
                CameraDevice.TEMPLATE_STILL_CAPTURE);

        // callback
        SimpleCaptureCallback previewCaptureCallback = new SimpleCaptureCallback();
        SimpleCaptureCallback rawCaptureCallback = new SimpleCaptureCallback();
        SimpleImageReaderListener rawReaderListener = new SimpleImageReaderListener();

        CaptureSetup setup = new CaptureSetup(previewSize, rawSize, previewCaptureBuilder,
                rawCaptureBuilder, previewCaptureCallback, rawCaptureCallback, rawReaderListener);

        return setup;
    }

    /**
     * Construct an array of burst request with manual exposure and sensitivity.
     * <p>
     * For each capture request, 3A and post processing (noise reduction, sharpening, etc) will be
     * turned off. Then exposure and sensitivity value will be configured, which are determined by
     * EXPOSURE_MULIPLIERS and SENSITIVITY_MULTIPLIERS.
     * </p>
     *
     * @param rawBurstBuilder The builder needs to have targets setup.
     * @return An array list capture request for burst.
     */
    private ArrayList<CaptureRequest> createBurstRequest(CaptureRequest.Builder rawBurstBuilder)
    {
        return createBurstRequest(rawBurstBuilder, EXPOSURE_MULTIPLIERS, SENSITIVITY_MLTIPLIERS);
    }

    private ArrayList<CaptureRequest> createBurstRequest(CaptureRequest.Builder rawBurstBuilder,
            long[] exposureMultipliers, int[] sensitivityMultipliers) {
        // set manual mode
        rawBurstBuilder.set(CaptureRequest.CONTROL_AE_MODE, CaptureRequest.CONTROL_AE_MODE_OFF);
        rawBurstBuilder.set(CaptureRequest.CONTROL_AWB_MODE, CaptureRequest.CONTROL_AWB_MODE_OFF);
        rawBurstBuilder.set(CaptureRequest.NOISE_REDUCTION_MODE,
                CaptureRequest.NOISE_REDUCTION_MODE_OFF);
        rawBurstBuilder.set(CaptureRequest.EDGE_MODE, CaptureRequest.EDGE_MODE_OFF);
        // exposure has higher priority over frame duration; therefore the frame readout time:
        // exposure time + overhead
        rawBurstBuilder.set(CaptureRequest.SENSOR_FRAME_DURATION, 0L);

        // get the exposure and sensitivity range
        Range<Long> exposureRangeNs = new Range<Long>(mStaticInfo.getExposureMinimumOrDefault(),
                mStaticInfo.getExposureMaximumOrDefault());

        Range<Integer> isoRange = new Range<Integer>(mStaticInfo.getSensitivityMinimumOrDefault(),
                mStaticInfo.getSensitivityMaximumOrDefault());

        Log.i(TAG, String.format(""Exposure time - max: %d, min: %d."", exposureRangeNs.getUpper(),
                exposureRangeNs.getLower()));
        Log.i(TAG, String.format(""Sensitivity - max: %d, min: %d."", isoRange.getUpper(),
                isoRange.getLower()));

        // building burst request
        int maxFramesBurst = exposureMultipliers.length * sensitivityMultipliers.length;
        Log.i(TAG, String.format(""Setting up burst = %d frames."", maxFramesBurst));
        ArrayList<CaptureRequest> rawRequestList = new ArrayList<CaptureRequest>(maxFramesBurst);

        for (int i = 0; i < exposureMultipliers.length; i++) {
            for (int j = 0; j < sensitivityMultipliers.length; j++) {
                long desiredExposure = Math.min(
                        exposureRangeNs.getLower() * exposureMultipliers[i],
                        exposureRangeNs.getUpper());

                int desiredSensitivity =
                        Math.min(isoRange.getLower() * sensitivityMultipliers[j],
                                isoRange.getUpper());

                rawBurstBuilder.set(CaptureRequest.SENSOR_EXPOSURE_TIME, desiredExposure);
                rawBurstBuilder.set(CaptureRequest.SENSOR_SENSITIVITY, desiredSensitivity);

                rawRequestList.add(rawBurstBuilder.build());
            }
        }
        return rawRequestList;
    }

    /**
     * Construct an array of burst request with 3A
     * <p>
     * For each capture request, 3A and post processing (noise reduction, sharpening, etc) will be
     * turned on.
     * </p>
     *
     * @param rawBurstBuilder The builder needs to have targets setup.
     * @return An array list capture request for burst.
     */
    private ArrayList<CaptureRequest> createBurstRequestWith3A(
            CaptureRequest.Builder rawBurstBuilder)
    {
        // set 3A mode to simulate regular still capture
        rawBurstBuilder.set(CaptureRequest.CONTROL_AE_MODE, CaptureRequest.CONTROL_AE_MODE_ON);
        rawBurstBuilder.set(CaptureRequest.CONTROL_AWB_MODE, CaptureRequest.CONTROL_AWB_MODE_AUTO);
        rawBurstBuilder.set(CaptureRequest.NOISE_REDUCTION_MODE,
                CaptureRequest.NOISE_REDUCTION_MODE_HIGH_QUALITY);
        rawBurstBuilder.set(CaptureRequest.EDGE_MODE, CaptureRequest.EDGE_MODE_HIGH_QUALITY);

        // building burst request
        Log.i(TAG, String.format(""Setting up burst = %d frames."", MAX_FRAMES_BURST));
        ArrayList<CaptureRequest> rawRequestList = new ArrayList<CaptureRequest>(MAX_FRAMES_BURST);

        for (int i = 0; i < MAX_FRAMES_BURST; i++) {
            rawRequestList.add(rawBurstBuilder.build());
        }

        return rawRequestList;
    }

    /**
     * An utility method to copy capture request builders. This is used for recovery purpose to
     * reverse the changes we made to the builder.
     *
     * @param dst the builder to write into.
     * @param src the builder that needs to be copied.
     */
    private void copyBurstRequetBuilder(CaptureRequest.Builder dst, CaptureRequest.Builder src)
    {
        dst.set(CaptureRequest.CONTROL_AE_MODE, src.get(CaptureRequest.CONTROL_AE_MODE));
        dst.set(CaptureRequest.CONTROL_AWB_MODE, src.get(CaptureRequest.CONTROL_AWB_MODE));
        dst.set(CaptureRequest.NOISE_REDUCTION_MODE, src.get(CaptureRequest.NOISE_REDUCTION_MODE));
        dst.set(CaptureRequest.EDGE_MODE, src.get(CaptureRequest.EDGE_MODE));
        dst.set(CaptureRequest.SENSOR_FRAME_DURATION,
                src.get(CaptureRequest.SENSOR_FRAME_DURATION));
        dst.set(CaptureRequest.SENSOR_EXPOSURE_TIME, src.get(CaptureRequest.SENSOR_EXPOSURE_TIME));
        dst.set(CaptureRequest.SENSOR_SENSITIVITY, src.get(CaptureRequest.SENSOR_SENSITIVITY));
    }

    /**
     * Draining the image reader and capture callback queue
     *
     * @param readerListener Image reader listener needs to be drained.
     * @param captureCallback Capture callback needs to be drained.
     * @throws Exception Exception from the queue.
     */
    private void drainQueues(SimpleImageReaderListener readerListener,
            SimpleCaptureCallback captureCallback) throws Exception
    {
        for (int i = 0; i < MAX_FRAMES_BURST; i++) {
            Image image = readerListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
            image.close();

            CaptureResult result = captureCallback.getCaptureResult(
                    CAPTURE_IMAGE_TIMEOUT_MS);
            long timestamp = result.get(CaptureResult.SENSOR_TIMESTAMP);
            Log.d(TAG, String.format(""timestamp = %d"", timestamp));
        }
    }

    /**
     * Stop preview and remove the target surfaces inside the CaptureRequest.Builder.
     *
     * @param previewBuilder Configured builder for preview.
     * @param rawBurstBuilder Configured builder for RAW.
     * @throws Exception Exceptions from stopPreview.
     */
    private void stopPreviewAndClearSurface(CaptureRequest.Builder previewBuilder,
            CaptureRequest.Builder rawBurstBuilder) throws Exception
    {
        previewBuilder.removeTarget(mPreviewSurface);
        rawBurstBuilder.removeTarget(mPreviewSurface);
        rawBurstBuilder.removeTarget(mReaderSurface);

        stopPreview();
    }

    private void performTestRoutine(TestRoutine routine, int[] testedFormats) throws Exception
    {
        final int PREPARE_TIMEOUT_MS = 10000;
        for (String id : mCameraIdsUnderTest) {
            try {
                ArrayList<Integer> supportedRawList = new ArrayList<Integer>(RAW_FORMATS.length);
                if (!checkCapability(id, supportedRawList, testedFormats)) {
                    Log.i(TAG, ""Capability is not supported on camera "" + id
                            + "". Skip the test."");
                    continue;
                }

                openDevice(id);
                // test each supported RAW format
                for (int rawFormat : supportedRawList) {
                    Log.i(TAG, ""Testing format "" + imageFormatToString(rawFormat) + ""."");

                    // prepare preview and still RAW capture
                    CaptureSetup captureSetup = initCaptureSetupForPreviewAndRaw();

                    Size previewCaptureSize = captureSetup.getPreviewCaptureSize();
                    Size rawCaptureSize = captureSetup.getRawCaptureSize();

                    CaptureRequest.Builder previewBuilder = captureSetup.getPreviewRequestBuilder();
                    CaptureRequest.Builder rawBurstBuilder = captureSetup.getRawRequestBuilder();

                    SimpleCaptureCallback previewCaptureCallback =
                            captureSetup.getPreviewCaptureCallback();
                    SimpleCaptureCallback rawCaptureCallback = captureSetup.getRawCaptureCallback();
                    SimpleImageReaderListener rawReaderListener = captureSetup
                            .getRawReaderListener();

                    // start preview and prepare RAW capture
                    prepareCaptureAndStartPreview(previewBuilder, rawBurstBuilder,
                            previewCaptureSize, rawCaptureSize, rawFormat, previewCaptureCallback,
                            MAX_FRAMES_BURST, rawReaderListener);

                    // Prepare still surface to prevent large allocations slow down capture
                    mSession.prepare(mReaderSurface);
                    mSessionListener.waitForSurfacePrepared(
                            mSession, mReaderSurface, PREPARE_TIMEOUT_MS);

                    // execute test routine
                    routine.execute(rawBurstBuilder, rawCaptureCallback, rawReaderListener,
                            rawFormat);

                    // clear out the surface and camera session
                    stopPreviewAndClearSurface(previewBuilder, rawBurstBuilder);
                    rawReaderListener.drain();
                    closeImageReader();
                }
            } finally {
                closeDevice();
            }
        }
    }
}"	""	""	"minimum 12"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"getCharacteristics"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"/*
 *.
 */

package android.hardware.camera2.cts;

import android.content.Context;
import android.content.pm.PackageManager;
import android.graphics.ImageFormat;
import android.graphics.Rect;
import android.graphics.SurfaceTexture;
import android.hardware.Camera;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraCharacteristics.Key;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CameraMetadata;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.cts.helpers.CameraErrorCollector;
import android.hardware.camera2.cts.helpers.StaticMetadata;
import android.hardware.camera2.cts.testcases.Camera2AndroidTestCase;
import android.hardware.camera2.params.BlackLevelPattern;
import android.hardware.camera2.params.ColorSpaceTransform;
import android.hardware.camera2.params.RecommendedStreamConfigurationMap;
import android.hardware.camera2.params.StreamConfigurationMap;
import android.media.CamcorderProfile;
import android.media.ImageReader;
import android.os.Build;
import android.util.ArraySet;
import android.util.DisplayMetrics;
import android.util.Log;
import android.util.Rational;
import android.util.Range;
import android.util.Size;
import android.util.Pair;
import android.util.Patterns;
import android.view.Display;
import android.view.Surface;
import android.view.WindowManager;

import com.android.compatibility.common.util.CddTest;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Objects;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.Set;

import org.junit.runners.Parameterized;
import org.junit.runner.RunWith;
import org.junit.Test;

import static android.hardware.camera2.cts.helpers.AssertHelpers.*;
import static android.hardware.camera2.cts.CameraTestUtils.SimpleCaptureCallback;
import static android.hardware.cts.helpers.CameraUtils.matchParametersToCharacteristics;

import static junit.framework.Assert.*;

import static org.mockito.Mockito.*;

/**
 * Extended tests for static camera characteristics.
 */
@RunWith(Parameterized.class)
public class ExtendedCameraCharacteristicsTest extends Camera2AndroidTestCase {
    private static final String TAG = ""ExChrsTest""; // must be short so next line doesn't throw
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);

    private static final String PREFIX_ANDROID = ""android"";

    /*
     * Constants for static RAW metadata.
     */
    private static final int MIN_ALLOWABLE_WHITELEVEL = 32; // must have sensor bit depth > 5

    private List<CameraCharacteristics> mCharacteristics;

    private static final Size FULLHD = new Size(1920, 1080);
    private static final Size FULLHD_ALT = new Size(1920, 1088);
    private static final Size HD = new Size(1280, 720);
    private static final Size VGA = new Size(640, 480);
    private static final Size QVGA = new Size(320, 240);
    private static final Size UHD = new Size(3840, 2160);
    private static final Size DC4K = new Size(4096, 2160);

    private static final long MIN_BACK_SENSOR_RESOLUTION = 2000000;
    private static final long MIN_FRONT_SENSOR_RESOLUTION = VGA.getHeight() * VGA.getWidth();
    private static final long LOW_LATENCY_THRESHOLD_MS = 200;
    private static final float LATENCY_TOLERANCE_FACTOR = 1.1f; // 10% tolerance
    private static final int MAX_NUM_IMAGES = 5;
    private static final long PREVIEW_RUN_MS = 500;
    private static final long FRAME_DURATION_30FPS_NSEC = (long) 1e9 / 30;

    private static final long MIN_BACK_SENSOR_PERF_CLASS_RESOLUTION = 12000000;
    private static final long MIN_FRONT_SENSOR_S_PERF_CLASS_RESOLUTION = 5000000;
    private static final long MIN_FRONT_SENSOR_R_PERF_CLASS_RESOLUTION = 4000000;

    private static final long MIN_UHR_SENSOR_RESOLUTION = 24000000;
    /*
     * HW Levels short hand
     */
    private static final int LEGACY = CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY;
    private static final int LIMITED = CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED;
    private static final int FULL = CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_FULL;
    private static final int LEVEL_3 = CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_3;
    private static final int EXTERNAL = CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL;
    private static final int OPT = Integer.MAX_VALUE;  // For keys that are optional on all hardware levels.

    /*
     * Capabilities short hand
     */
    private static final int NONE = -1;
    private static final int BC =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE;
    private static final int MANUAL_SENSOR =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR;
    private static final int MANUAL_POSTPROC =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_POST_PROCESSING;
    private static final int RAW =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW;
    private static final int YUV_REPROCESS =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING;
    private static final int OPAQUE_REPROCESS =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING;
    private static final int CONSTRAINED_HIGH_SPEED =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_CONSTRAINED_HIGH_SPEED_VIDEO;
    private static final int MONOCHROME =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MONOCHROME;
    private static final int HIGH_SPEED_FPS_LOWER_MIN = 30;
    private static final int HIGH_SPEED_FPS_UPPER_MIN = 120;

    @Override
    public void setUp() throws Exception {
        super.setUp();
        mCharacteristics = new ArrayList<>();
        for (int i = 0; i < mAllCameraIds.length; i++) {
            mCharacteristics.add(mAllStaticInfo.get(mAllCameraIds[i]).getCharacteristics());
        }
    }

    @Override
    public void tearDown() throws Exception {
        super.tearDown();
        mCharacteristics = null;
    }

    /**
     * Test that the available stream configurations contain a few required formats and sizes.
     */
    @CddTest(requirement=""7.5.1/C-1-2"")"	""	""	"cdd 12 resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testAvailableStreamConfigs"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"public void testAvailableStreamConfigs() throws Exception {
        boolean firstBackFacingCamera = true;
        for (int i = 0; i < mAllCameraIds.length; i++) {
            CameraCharacteristics c = mCharacteristics.get(i);
            StreamConfigurationMap config =
                    c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            assertNotNull(String.format(""No stream configuration map found for: ID %s"",
                    mAllCameraIds[i]), config);
            int[] outputFormats = config.getOutputFormats();

            int[] actualCapabilities = c.get(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
            assertNotNull(""android.request.availableCapabilities must never be null"",
                    actualCapabilities);

            // Check required formats exist (JPEG, and YUV_420_888).
            if (!arrayContains(actualCapabilities, BC)) {
                Log.i(TAG, ""Camera "" + mAllCameraIds[i] +
                    "": BACKWARD_COMPATIBLE capability not supported, skipping test"");
                continue;
            }

            boolean isMonochromeWithY8 = arrayContains(actualCapabilities, MONOCHROME)
                    && arrayContains(outputFormats, ImageFormat.Y8);
            boolean isHiddenPhysicalCamera = !arrayContains(mCameraIdsUnderTest, mAllCameraIds[i]);
            boolean supportHeic = arrayContains(outputFormats, ImageFormat.HEIC);

            assertArrayContains(
                    String.format(""No valid YUV_420_888 preview formats found for: ID %s"",
                            mAllCameraIds[i]), outputFormats, ImageFormat.YUV_420_888);
            if (isMonochromeWithY8) {
                assertArrayContains(
                        String.format(""No valid Y8 preview formats found for: ID %s"",
                                mAllCameraIds[i]), outputFormats, ImageFormat.Y8);
            }
            assertArrayContains(String.format(""No JPEG image format for: ID %s"",
                    mAllCameraIds[i]), outputFormats, ImageFormat.JPEG);

            Size[] yuvSizes = config.getOutputSizes(ImageFormat.YUV_420_888);
            Size[] y8Sizes = config.getOutputSizes(ImageFormat.Y8);
            Size[] jpegSizes = config.getOutputSizes(ImageFormat.JPEG);
            Size[] heicSizes = config.getOutputSizes(ImageFormat.HEIC);
            Size[] privateSizes = config.getOutputSizes(ImageFormat.PRIVATE);

            CameraTestUtils.assertArrayNotEmpty(yuvSizes,
                    String.format(""No sizes for preview format %x for: ID %s"",
                            ImageFormat.YUV_420_888, mAllCameraIds[i]));
            if (isMonochromeWithY8) {
                CameraTestUtils.assertArrayNotEmpty(y8Sizes,
                    String.format(""No sizes for preview format %x for: ID %s"",
                            ImageFormat.Y8, mAllCameraIds[i]));
            }

            Rect activeRect = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE);
            Size pixelArraySize = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.SENSOR_INFO_PIXEL_ARRAY_SIZE);

            int activeArrayHeight = activeRect.height();
            int activeArrayWidth = activeRect.width();
            long sensorResolution = pixelArraySize.getHeight() * pixelArraySize.getWidth() ;
            Integer lensFacing = c.get(CameraCharacteristics.LENS_FACING);
            assertNotNull(""Can't get lens facing info for camera id: "" + mAllCameraIds[i],
                    lensFacing);

            // Check that the sensor sizes are atleast what the CDD specifies
            switch(lensFacing) {
                case CameraCharacteristics.LENS_FACING_FRONT:
                    assertTrue(""Front Sensor resolution should be at least "" +
                            MIN_FRONT_SENSOR_RESOLUTION + "" pixels, is ""+ sensorResolution,
                            sensorResolution >= MIN_FRONT_SENSOR_RESOLUTION);
                    break;
                case CameraCharacteristics.LENS_FACING_BACK:
                    if (firstBackFacingCamera) {
                        assertTrue(""Back Sensor resolution should be at least ""
                                + MIN_BACK_SENSOR_RESOLUTION +
                                "" pixels, is ""+ sensorResolution,
                                sensorResolution >= MIN_BACK_SENSOR_RESOLUTION);
                        firstBackFacingCamera = false;
                    }
                    break;
                default:
                    break;
            }

            Integer hwLevel = c.get(CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL);

            if (activeArrayWidth >= FULLHD.getWidth() &&
                    activeArrayHeight >= FULLHD.getHeight()) {
                assertArrayContainsAnyOf(String.format(
                        ""Required FULLHD size not found for format %x for: ID %s"",
                        ImageFormat.JPEG, mAllCameraIds[i]), jpegSizes,
                        new Size[] {FULLHD, FULLHD_ALT});
                if (supportHeic) {
                    assertArrayContainsAnyOf(String.format(
                            ""Required FULLHD size not found for format %x for: ID %s"",
                            ImageFormat.HEIC, mAllCameraIds[i]), heicSizes,
                            new Size[] {FULLHD, FULLHD_ALT});
                }
            }

            if (activeArrayWidth >= HD.getWidth() &&
                    activeArrayHeight >= HD.getHeight()) {
                assertArrayContains(String.format(
                        ""Required HD size not found for format %x for: ID %s"",
                        ImageFormat.JPEG, mAllCameraIds[i]), jpegSizes, HD);
                if (supportHeic) {
                    assertArrayContains(String.format(
                            ""Required HD size not found for format %x for: ID %s"",
                            ImageFormat.HEIC, mAllCameraIds[i]), heicSizes, HD);
                }
            }

            if (activeArrayWidth >= VGA.getWidth() &&
                    activeArrayHeight >= VGA.getHeight()) {
                assertArrayContains(String.format(
                        ""Required VGA size not found for format %x for: ID %s"",
                        ImageFormat.JPEG, mAllCameraIds[i]), jpegSizes, VGA);
                if (supportHeic) {
                    assertArrayContains(String.format(
                            ""Required VGA size not found for format %x for: ID %s"",
                            ImageFormat.HEIC, mAllCameraIds[i]), heicSizes, VGA);
                }
            }

            if (activeArrayWidth >= QVGA.getWidth() &&
                    activeArrayHeight >= QVGA.getHeight()) {
                assertArrayContains(String.format(
                        ""Required QVGA size not found for format %x for: ID %s"",
                        ImageFormat.JPEG, mAllCameraIds[i]), jpegSizes, QVGA);
                if (supportHeic) {
                    assertArrayContains(String.format(
                            ""Required QVGA size not found for format %x for: ID %s"",
                            ImageFormat.HEIC, mAllCameraIds[i]), heicSizes, QVGA);
                }

            }

            ArrayList<Size> jpegSizesList = new ArrayList<>(Arrays.asList(jpegSizes));
            ArrayList<Size> yuvSizesList = new ArrayList<>(Arrays.asList(yuvSizes));
            ArrayList<Size> privateSizesList = new ArrayList<>(Arrays.asList(privateSizes));
            boolean isExternalCamera = (hwLevel ==
                    CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL);
            Size maxVideoSize = null;
            if (isExternalCamera || isHiddenPhysicalCamera) {
                // TODO: for now, use FULLHD 30 as largest possible video size for external camera.
                // For hidden physical camera, since we don't require CamcorderProfile to be
                // available, use FULLHD 30 as maximum video size as well.
                List<Size> videoSizes = CameraTestUtils.getSupportedVideoSizes(
                        mAllCameraIds[i], mCameraManager, FULLHD);
                for (Size sz : videoSizes) {
                    long minFrameDuration = config.getOutputMinFrameDuration(
                            android.media.MediaRecorder.class, sz);
                    // Give some margin for rounding error
                    if (minFrameDuration < (1e9 / 29.9)) {
                        maxVideoSize = sz;
                        break;
                    }
                }
            } else {
                int cameraId = Integer.valueOf(mAllCameraIds[i]);
                CamcorderProfile maxVideoProfile = CamcorderProfile.get(
                        cameraId, CamcorderProfile.QUALITY_HIGH);
                maxVideoSize = new Size(
                        maxVideoProfile.videoFrameWidth, maxVideoProfile.videoFrameHeight);
            }
            if (maxVideoSize == null) {
                fail(""Camera "" + mAllCameraIds[i] + "" does not support any 30fps video output"");
            }

            // Handle FullHD special case first
            if (jpegSizesList.contains(FULLHD)) {
                if (compareHardwareLevel(hwLevel, LEVEL_3) >= 0 || hwLevel == FULL ||
                        (hwLevel == LIMITED &&
                        maxVideoSize.getWidth() >= FULLHD.getWidth() &&
                        maxVideoSize.getHeight() >= FULLHD.getHeight())) {
                    boolean yuvSupportFullHD = yuvSizesList.contains(FULLHD) ||
                            yuvSizesList.contains(FULLHD_ALT);
                    boolean privateSupportFullHD = privateSizesList.contains(FULLHD) ||
                            privateSizesList.contains(FULLHD_ALT);
                    assertTrue(""Full device FullHD YUV size not found"", yuvSupportFullHD);
                    assertTrue(""Full device FullHD PRIVATE size not found"", privateSupportFullHD);

                    if (isMonochromeWithY8) {
                        ArrayList<Size> y8SizesList = new ArrayList<>(Arrays.asList(y8Sizes));
                        boolean y8SupportFullHD = y8SizesList.contains(FULLHD) ||
                                y8SizesList.contains(FULLHD_ALT);
                        assertTrue(""Full device FullHD Y8 size not found"", y8SupportFullHD);
                    }
                }
                // remove all FullHD or FullHD_Alt sizes for the remaining of the test
                jpegSizesList.remove(FULLHD);
                jpegSizesList.remove(FULLHD_ALT);
            }

            // Check all sizes other than FullHD
            if (hwLevel == LIMITED) {
                // Remove all jpeg sizes larger than max video size
                ArrayList<Size> toBeRemoved = new ArrayList<>();
                for (Size size : jpegSizesList) {
                    if (size.getWidth() >= maxVideoSize.getWidth() &&
                            size.getHeight() >= maxVideoSize.getHeight()) {
                        toBeRemoved.add(size);
                    }
                }
                jpegSizesList.removeAll(toBeRemoved);
            }

            if (compareHardwareLevel(hwLevel, LEVEL_3) >= 0 || hwLevel == FULL ||
                    hwLevel == LIMITED) {
                if (!yuvSizesList.containsAll(jpegSizesList)) {
                    for (Size s : jpegSizesList) {
                        if (!yuvSizesList.contains(s)) {
                            fail(""Size "" + s + "" not found in YUV format"");
                        }
                    }
                }

                if (isMonochromeWithY8) {
                    ArrayList<Size> y8SizesList = new ArrayList<>(Arrays.asList(y8Sizes));
                    if (!y8SizesList.containsAll(jpegSizesList)) {
                        for (Size s : jpegSizesList) {
                            if (!y8SizesList.contains(s)) {
                                fail(""Size "" + s + "" not found in Y8 format"");
                            }
                        }
                    }
                }
            }

            if (!privateSizesList.containsAll(yuvSizesList)) {
                for (Size s : yuvSizesList) {
                    if (!privateSizesList.contains(s)) {
                        fail(""Size "" + s + "" not found in PRIVATE format"");
                    }
                }
            }
        }
    }

    private void verifyCommonRecommendedConfiguration(String id, CameraCharacteristics c,
            RecommendedStreamConfigurationMap config, boolean checkNoInput,
            boolean checkNoHighRes, boolean checkNoHighSpeed, boolean checkNoPrivate,
            boolean checkNoDepth) {
        StreamConfigurationMap fullConfig = c.get(
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        assertNotNull(String.format(""No stream configuration map found for ID: %s!"", id),
                fullConfig);

        Set<Integer> recommendedOutputFormats = config.getOutputFormats();

        if (checkNoInput) {
            Set<Integer> inputFormats = config.getInputFormats();
            assertTrue(String.format(""Recommended configuration must not include any input "" +
                    ""streams for ID: %s"", id),
                    ((inputFormats == null) || (inputFormats.size() == 0)));
        }

        if (checkNoHighRes) {
            for (int format : recommendedOutputFormats) {
                Set<Size> highResSizes = config.getHighResolutionOutputSizes(format);
                assertTrue(String.format(""Recommended configuration should not include any "" +
                        ""high resolution sizes, which cannot operate at full "" +
                        ""BURST_CAPTURE rate for ID: %s"", id),
                        ((highResSizes == null) || (highResSizes.size() == 0)));
            }
        }

        if (checkNoHighSpeed) {
            Set<Size> highSpeedSizes = config.getHighSpeedVideoSizes();
            assertTrue(String.format(""Recommended configuration must not include any high "" +
                    ""speed configurations for ID: %s"", id),
                    ((highSpeedSizes == null) || (highSpeedSizes.size() == 0)));
        }

        int[] exhaustiveOutputFormats = fullConfig.getOutputFormats();
        for (Integer formatInteger : recommendedOutputFormats) {
            int format = formatInteger.intValue();
            assertArrayContains(String.format(""Unsupported recommended output format: %d for "" +
                    ""ID: %s "", format, id), exhaustiveOutputFormats, format);
            Set<Size> recommendedSizes = config.getOutputSizes(format);

            switch (format) {
                case ImageFormat.PRIVATE:
                    if (checkNoPrivate) {
                        fail(String.format(""Recommended configuration must not include "" +
                                ""PRIVATE format entries for ID: %s"", id));
                    }

                    Set<Size> classOutputSizes = config.getOutputSizes(ImageReader.class);
                    assertCollectionContainsAnyOf(String.format(""Recommended output sizes for "" +
                            ""ImageReader class don't match the output sizes for the "" +
                            ""corresponding format for ID: %s"", id), classOutputSizes,
                            recommendedSizes);
                    break;
                case ImageFormat.DEPTH16:
                case ImageFormat.DEPTH_POINT_CLOUD:
                    if (checkNoDepth) {
                        fail(String.format(""Recommended configuration must not include any DEPTH "" +
                                ""formats for ID: %s"", id));
                    }
                    break;
                default:
            }
            Size [] exhaustiveSizes = fullConfig.getOutputSizes(format);
            for (Size sz : recommendedSizes) {
                assertArrayContains(String.format(""Unsupported recommended size %s for "" +
                        ""format: %d for ID: %s"", sz.toString(), format, id),
                        exhaustiveSizes, sz);

                long recommendedMinDuration = config.getOutputMinFrameDuration(format, sz);
                long availableMinDuration = fullConfig.getOutputMinFrameDuration(format, sz);
                assertTrue(String.format(""Recommended minimum frame duration %d for size "" +
                        ""%s format: %d doesn't match with currently available minimum"" +
                        "" frame duration of %d for ID: %s"", recommendedMinDuration,
                        sz.toString(), format, availableMinDuration, id),
                        (recommendedMinDuration == availableMinDuration));
                long recommendedStallDuration = config.getOutputStallDuration(format, sz);
                long availableStallDuration = fullConfig.getOutputStallDuration(format, sz);
                assertTrue(String.format(""Recommended stall duration %d for size %s"" +
                        "" format: %d doesn't match with currently available stall "" +
                        ""duration of %d for ID: %s"", recommendedStallDuration,
                        sz.toString(), format, availableStallDuration, id),
                        (recommendedStallDuration == availableStallDuration));

                ImageReader reader = ImageReader.newInstance(sz.getWidth(), sz.getHeight(), format,
                        /*maxImages*/1);
                Surface readerSurface = reader.getSurface();
                assertTrue(String.format(""ImageReader surface using format %d and size %s is not"" +
                        "" supported for ID: %s"", format, sz.toString(), id),
                        config.isOutputSupportedFor(readerSurface));
                if (format == ImageFormat.PRIVATE) {
                    long classMinDuration = config.getOutputMinFrameDuration(ImageReader.class, sz);
                    assertTrue(String.format(""Recommended minimum frame duration %d for size "" +
                            ""%s format: %d doesn't match with the duration %d for "" +
                            ""ImageReader class of the same size"", recommendedMinDuration,
                            sz.toString(), format, classMinDuration),
                            classMinDuration == recommendedMinDuration);
                    long classStallDuration = config.getOutputStallDuration(ImageReader.class, sz);
                    assertTrue(String.format(""Recommended stall duration %d for size "" +
                            ""%s format: %d doesn't match with the stall duration %d for "" +
                            ""ImageReader class of the same size"", recommendedStallDuration,
                            sz.toString(), format, classStallDuration),
                            classStallDuration == recommendedStallDuration);
                }
            }
        }
    }

    private void verifyRecommendedPreviewConfiguration(String cameraId, CameraCharacteristics c,
            RecommendedStreamConfigurationMap previewConfig) {
        verifyCommonRecommendedConfiguration(cameraId, c, previewConfig, /*checkNoInput*/ true,
                /*checkNoHighRes*/ true, /*checkNoHighSpeed*/ true, /*checkNoPrivate*/ false,
                /*checkNoDepth*/ true);

        Set<Integer> outputFormats = previewConfig.getOutputFormats();
        assertTrue(String.format(""No valid YUV_420_888 and PRIVATE preview "" +
                ""formats found in recommended preview configuration for ID: %s"", cameraId),
                outputFormats.containsAll(Arrays.asList(new Integer(ImageFormat.YUV_420_888),
                        new Integer(ImageFormat.PRIVATE))));
    }

    private void verifyRecommendedVideoConfiguration(String cameraId, CameraCharacteristics c,
            RecommendedStreamConfigurationMap videoConfig) {
        verifyCommonRecommendedConfiguration(cameraId, c, videoConfig, /*checkNoInput*/ true,
                /*checkNoHighRes*/ true, /*checkNoHighSpeed*/ false, /*checkNoPrivate*/false,
                /*checkNoDepth*/ true);

        Set<Size> highSpeedSizes = videoConfig.getHighSpeedVideoSizes();
        StreamConfigurationMap fullConfig = c.get(
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        assertNotNull(""No stream configuration map found!"", fullConfig);
        Size [] availableHighSpeedSizes = fullConfig.getHighSpeedVideoSizes();
        if ((highSpeedSizes != null) && (highSpeedSizes.size() > 0)) {
            for (Size sz : highSpeedSizes) {
                assertArrayContains(String.format(""Recommended video configuration includes "" +
                        ""unsupported high speed configuration with size %s for ID: %s"",
                        sz.toString(), cameraId), availableHighSpeedSizes, sz);
                Set<Range<Integer>>  highSpeedFpsRanges =
                    videoConfig.getHighSpeedVideoFpsRangesFor(sz);
                Range<Integer> [] availableHighSpeedFpsRanges =
                    fullConfig.getHighSpeedVideoFpsRangesFor(sz);
                for (Range<Integer> fpsRange : highSpeedFpsRanges) {
                    assertArrayContains(String.format(""Recommended video configuration includes "" +
                            ""unsupported high speed fps range [%d %d] for ID: %s"",
                            fpsRange.getLower().intValue(), fpsRange.getUpper().intValue(),
                            cameraId), availableHighSpeedFpsRanges, fpsRange);
                }
            }
        }

        final int[] profileList = {
            CamcorderProfile.QUALITY_2160P,
            CamcorderProfile.QUALITY_1080P,
            CamcorderProfile.QUALITY_480P,
            CamcorderProfile.QUALITY_720P,
            CamcorderProfile.QUALITY_CIF,
            CamcorderProfile.QUALITY_HIGH,
            CamcorderProfile.QUALITY_LOW,
            CamcorderProfile.QUALITY_QCIF,
            CamcorderProfile.QUALITY_QVGA,
        };
        Set<Size> privateSizeSet = videoConfig.getOutputSizes(ImageFormat.PRIVATE);
        for (int profile : profileList) {
            int idx = Integer.valueOf(cameraId);
            if (CamcorderProfile.hasProfile(idx, profile)) {
                CamcorderProfile videoProfile = CamcorderProfile.get(idx, profile);
                Size profileSize  = new Size(videoProfile.videoFrameWidth,
                        videoProfile.videoFrameHeight);
                assertCollectionContainsAnyOf(String.format(""Recommended video configuration "" +
                        ""doesn't include supported video profile size %s with Private format "" +
                        ""for ID: %s"", profileSize.toString(), cameraId), privateSizeSet,
                        Arrays.asList(profileSize));
            }
        }
    }

    private Pair<Boolean, Size> isSizeWithinSensorMargin(Size sz, Size sensorSize) {
        final float SIZE_ERROR_MARGIN = 0.03f;
        float croppedWidth = (float)sensorSize.getWidth();
        float croppedHeight = (float)sensorSize.getHeight();
        float sensorAspectRatio = (float)sensorSize.getWidth() / (float)sensorSize.getHeight();
        float maxAspectRatio = (float)sz.getWidth() / (float)sz.getHeight();
        if (sensorAspectRatio < maxAspectRatio) {
            croppedHeight = (float)sensorSize.getWidth() / maxAspectRatio;
        } else if (sensorAspectRatio > maxAspectRatio) {
            croppedWidth = (float)sensorSize.getHeight() * maxAspectRatio;
        }
        Size croppedSensorSize = new Size((int)croppedWidth, (int)croppedHeight);

        Boolean match = new Boolean(
            (sz.getWidth() <= croppedSensorSize.getWidth() * (1.0 + SIZE_ERROR_MARGIN) &&
             sz.getWidth() >= croppedSensorSize.getWidth() * (1.0 - SIZE_ERROR_MARGIN) &&
             sz.getHeight() <= croppedSensorSize.getHeight() * (1.0 + SIZE_ERROR_MARGIN) &&
             sz.getHeight() >= croppedSensorSize.getHeight() * (1.0 - SIZE_ERROR_MARGIN)));

        return Pair.create(match, croppedSensorSize);
    }

    private void verifyRecommendedSnapshotConfiguration(String cameraId, CameraCharacteristics c,
            RecommendedStreamConfigurationMap snapshotConfig) {
        verifyCommonRecommendedConfiguration(cameraId, c, snapshotConfig, /*checkNoInput*/ true,
                /*checkNoHighRes*/ false, /*checkNoHighSpeed*/ true, /*checkNoPrivate*/false,
                /*checkNoDepth*/ false);
        Rect activeRect = CameraTestUtils.getValueNotNull(
                c, CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE);
        Size arraySize = new Size(activeRect.width(), activeRect.height());


        ArraySet<Size> snapshotSizeSet = new ArraySet<>(snapshotConfig.getOutputSizes(
                    ImageFormat.JPEG));
        Set<Size> highResSnapshotSizeSet = snapshotConfig.getHighResolutionOutputSizes(
                ImageFormat.JPEG);
        if (highResSnapshotSizeSet != null) {
            snapshotSizeSet.addAll(highResSnapshotSizeSet);
        }
        Size[] snapshotSizes = new Size[snapshotSizeSet.size()];
        snapshotSizes = snapshotSizeSet.toArray(snapshotSizes);
        Size maxJpegSize = CameraTestUtils.getMaxSize(snapshotSizes);
        assertTrue(String.format(""Maximum recommended Jpeg size %s should be within 3 percent "" +
                ""of the area of the advertised array size %s for ID: %s"",
                maxJpegSize.toString(), arraySize.toString(), cameraId),
                isSizeWithinSensorMargin(maxJpegSize, arraySize).first.booleanValue());
    }

    private void verifyRecommendedVideoSnapshotConfiguration(String cameraId,
            CameraCharacteristics c,
            RecommendedStreamConfigurationMap videoSnapshotConfig,
            RecommendedStreamConfigurationMap videoConfig) {
        verifyCommonRecommendedConfiguration(cameraId, c, videoSnapshotConfig,
                /*checkNoInput*/ true, /*checkNoHighRes*/ false, /*checkNoHighSpeed*/ true,
                /*checkNoPrivate*/ true, /*checkNoDepth*/ true);

        Set<Integer> outputFormats = videoSnapshotConfig.getOutputFormats();
        assertCollectionContainsAnyOf(String.format(""No valid JPEG format found "" +
                ""in recommended video snapshot configuration for ID: %s"", cameraId),
                outputFormats, Arrays.asList(new Integer(ImageFormat.JPEG)));
        assertTrue(String.format(""Recommended video snapshot configuration must only advertise "" +
                ""JPEG format for ID: %s"", cameraId), outputFormats.size() == 1);

        Set<Size> privateVideoSizeSet = videoConfig.getOutputSizes(ImageFormat.PRIVATE);
        Size[] privateVideoSizes = new Size[privateVideoSizeSet.size()];
        privateVideoSizes = privateVideoSizeSet.toArray(privateVideoSizes);
        Size maxVideoSize = CameraTestUtils.getMaxSize(privateVideoSizes);
        Set<Size> outputSizes = videoSnapshotConfig.getOutputSizes(ImageFormat.JPEG);
        assertCollectionContainsAnyOf(String.format(""The maximum recommended video size %s "" +
                ""should be present in the recommended video snapshot configurations for ID: %s"",
                maxVideoSize.toString(), cameraId), outputSizes, Arrays.asList(maxVideoSize));
    }

    private void verifyRecommendedRawConfiguration(String cameraId,
            CameraCharacteristics c, RecommendedStreamConfigurationMap rawConfig) {
        verifyCommonRecommendedConfiguration(cameraId, c, rawConfig, /*checkNoInput*/ true,
                /*checkNoHighRes*/ false, /*checkNoHighSpeed*/ true, /*checkNoPrivate*/ true,
                /*checkNoDepth*/ true);

        Set<Integer> outputFormats = rawConfig.getOutputFormats();
        for (Integer outputFormatInteger : outputFormats) {
            int outputFormat = outputFormatInteger.intValue();
            switch (outputFormat) {
                case ImageFormat.RAW10:
                case ImageFormat.RAW12:
                case ImageFormat.RAW_PRIVATE:
                case ImageFormat.RAW_SENSOR:
                    break;
                default:
                    fail(String.format(""Recommended raw configuration map must not contain "" +
                            "" non-RAW formats like: %d for ID: %s"", outputFormat, cameraId));

            }
        }
    }

    private void verifyRecommendedZSLConfiguration(String cameraId, CameraCharacteristics c,
            RecommendedStreamConfigurationMap zslConfig) {
        verifyCommonRecommendedConfiguration(cameraId, c, zslConfig, /*checkNoInput*/ false,
                /*checkNoHighRes*/ false, /*checkNoHighSpeed*/ true, /*checkNoPrivate*/ false,
                /*checkNoDepth*/ false);

        StreamConfigurationMap fullConfig =
            c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        assertNotNull(String.format(""No stream configuration map found for ID: %s!"", cameraId),
                fullConfig);
        Set<Integer> inputFormats = zslConfig.getInputFormats();
        int [] availableInputFormats = fullConfig.getInputFormats();
        for (Integer inputFormatInteger : inputFormats) {
            int inputFormat = inputFormatInteger.intValue();
            assertArrayContains(String.format(""Recommended ZSL configuration includes "" +
                    ""unsupported input format %d for ID: %s"", inputFormat, cameraId),
                    availableInputFormats, inputFormat);

            Set<Size> inputSizes = zslConfig.getInputSizes(inputFormat);
            Size [] availableInputSizes = fullConfig.getInputSizes(inputFormat);
            assertTrue(String.format(""Recommended ZSL configuration input format %d includes "" +
                    ""invalid input sizes for ID: %s"", inputFormat, cameraId),
                    ((inputSizes != null) && (inputSizes.size() > 0)));
            for (Size inputSize : inputSizes) {
                assertArrayContains(String.format(""Recommended ZSL configuration includes "" +
                        ""unsupported input format %d with size %s ID: %s"", inputFormat,
                        inputSize.toString(), cameraId), availableInputSizes, inputSize);
            }
            Set<Integer> validOutputFormats = zslConfig.getValidOutputFormatsForInput(inputFormat);
            int [] availableValidOutputFormats = fullConfig.getValidOutputFormatsForInput(
                    inputFormat);
            for (Integer outputFormatInteger : validOutputFormats) {
                int outputFormat = outputFormatInteger.intValue();
                assertArrayContains(String.format(""Recommended ZSL configuration includes "" +
                        ""unsupported output format %d for input %s ID: %s"", outputFormat,
                        inputFormat, cameraId), availableValidOutputFormats, outputFormat);
            }
        }
    }

    private void checkFormatLatency(int format, long latencyThresholdMs,
            RecommendedStreamConfigurationMap configMap) throws Exception {
        Set<Size> availableSizes = configMap.getOutputSizes(format);
        assertNotNull(String.format(""No available sizes for output format: %d"", format),
                availableSizes);

        ImageReader previewReader = null;
        long threshold = (long) (latencyThresholdMs * LATENCY_TOLERANCE_FACTOR);
        // for each resolution, check that the end-to-end latency doesn't exceed the given threshold
        for (Size sz : availableSizes) {
            try {
                // Create ImageReaders, capture session and requests
                final ImageReader.OnImageAvailableListener mockListener = mock(
                        ImageReader.OnImageAvailableListener.class);
                createDefaultImageReader(sz, format, MAX_NUM_IMAGES, mockListener);
                Size previewSize = mOrderedPreviewSizes.get(0);
                previewReader = createImageReader(previewSize, ImageFormat.YUV_420_888,
                        MAX_NUM_IMAGES, new CameraTestUtils.ImageDropperListener());
                Surface previewSurface = previewReader.getSurface();
                List<Surface> surfaces = new ArrayList<Surface>();
                surfaces.add(previewSurface);
                surfaces.add(mReaderSurface);
                createSession(surfaces);
                CaptureRequest.Builder captureBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
                captureBuilder.addTarget(previewSurface);
                CaptureRequest request = captureBuilder.build();

                // Let preview run for a while
                startCapture(request, /*repeating*/ true, new SimpleCaptureCallback(), mHandler);
                Thread.sleep(PREVIEW_RUN_MS);

                // Start capture.
                captureBuilder = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
                captureBuilder.addTarget(mReaderSurface);
                request = captureBuilder.build();

                for (int i = 0; i < MAX_NUM_IMAGES; i++) {
                    startCapture(request, /*repeating*/ false, new SimpleCaptureCallback(),
                            mHandler);
                    verify(mockListener, timeout(threshold).times(1)).onImageAvailable(
                            any(ImageReader.class));
                    reset(mockListener);
                }

                // stop capture.
                stopCapture(/*fast*/ false);
            } finally {
                closeDefaultImageReader();

                if (previewReader != null) {
                    previewReader.close();
                    previewReader = null;
                }
            }

        }
    }

    private void verifyRecommendedLowLatencyConfiguration(String cameraId, CameraCharacteristics c,
            RecommendedStreamConfigurationMap lowLatencyConfig) throws Exception {
        verifyCommonRecommendedConfiguration(cameraId, c, lowLatencyConfig, /*checkNoInput*/ true,
                /*checkNoHighRes*/ false, /*checkNoHighSpeed*/ true, /*checkNoPrivate*/ false,
                /*checkNoDepth*/ true);

        try {
            openDevice(cameraId);

            Set<Integer> formats = lowLatencyConfig.getOutputFormats();
            for (Integer format : formats) {
                checkFormatLatency(format.intValue(), LOW_LATENCY_THRESHOLD_MS, lowLatencyConfig);
            }
        } finally {
            closeDevice(cameraId);
        }

    }"	""	""	"cdd minimum 12 resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testRotateAndCropCharacteristics"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"public void testRotateAndCropCharacteristics() {
        for (int i = 0; i < mAllCameraIds.length; i++) {
            Log.i(TAG, ""testRotateAndCropCharacteristics: Testing camera ID "" + mAllCameraIds[i]);

            CameraCharacteristics c = mCharacteristics.get(i);

            if (!arrayContains(mCameraIdsUnderTest, mAllCameraIds[i])) {
                // Skip hidden physical cameras
                continue;
            }

            int[] availableRotateAndCropModes = c.get(
                    CameraCharacteristics.SCALER_AVAILABLE_ROTATE_AND_CROP_MODES);
            assertTrue(""availableRotateAndCropModes must not be null"",
                     availableRotateAndCropModes != null);
            boolean foundAuto = false;
            boolean foundNone = false;
            boolean found90 = false;
            for (int mode :  availableRotateAndCropModes) {
                switch(mode) {
                    case CameraCharacteristics.SCALER_ROTATE_AND_CROP_NONE:
                        foundNone = true;
                        break;
                    case CameraCharacteristics.SCALER_ROTATE_AND_CROP_90:
                        found90 = true;
                        break;
                    case CameraCharacteristics.SCALER_ROTATE_AND_CROP_AUTO:
                        foundAuto = true;
                        break;
                }
            }
            if (availableRotateAndCropModes.length > 1) {
                assertTrue(""To support SCALER_ROTATE_AND_CROP: NONE, 90, and AUTO must be included"",
                        foundNone && found90 && foundAuto);
            } else {
                assertTrue(""If only one SCALER_ROTATE_AND_CROP value is supported, it must be NONE"",
                        foundNone);
            }
        }
    }

    /**
     * Check that all devices available through the legacy API are also
     * accessible via Camera2.
     */
    @CddTest(requirement=""7.5.4/C-0-11"")"	""	""	"cdd"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testLegacyCameraDeviceParity"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"public void testLegacyCameraDeviceParity() {
        if (mAdoptShellPerm) {
            // There is no current way to determine in camera1 api if a device is a system camera
            // Skip test, http://b/141496896
            return;
        }
        if (mOverrideCameraId != null) {
            // A single camera is being tested. Skip test.
            return;
        }
        int legacyDeviceCount = Camera.getNumberOfCameras();
        assertTrue(""More legacy devices: "" + legacyDeviceCount + "" compared to Camera2 devices: "" +
                mCharacteristics.size(), legacyDeviceCount <= mCharacteristics.size());

        ArrayList<CameraCharacteristics> chars = new ArrayList<> (mCharacteristics);
        for (int i = 0; i < legacyDeviceCount; i++) {
            Camera camera = null;
            Camera.Parameters legacyParams = null;
            Camera.CameraInfo legacyInfo = new Camera.CameraInfo();
            try {
                Camera.getCameraInfo(i, legacyInfo);
                camera = Camera.open(i);
                legacyParams = camera.getParameters();

                assertNotNull(""Camera parameters for device: "" + i + ""  must not be null"",
                        legacyParams);
            } finally {
                if (camera != null) {
                    camera.release();
                }
            }

            // Camera Ids between legacy devices and Camera2 device could be
            // different try to match devices by using other common traits.
            CameraCharacteristics found = null;
            for (CameraCharacteristics ch : chars) {
                if (matchParametersToCharacteristics(legacyParams, legacyInfo, ch)) {
                    found = ch;
                    break;
                }
            }
            assertNotNull(""No matching Camera2 device for legacy device id: "" + i, found);

            chars.remove(found);
        }
    }

    /**
     * Check camera orientation against device orientation
     */
    @CddTest(requirement=""7.5.5/C-1-1"")"	""	""	"cdd"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testCameraOrientationAlignedWithDevice"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"public void testCameraOrientationAlignedWithDevice() {
        WindowManager windowManager =
                (WindowManager) mContext.getSystemService(Context.WINDOW_SERVICE);
        Display display = windowManager.getDefaultDisplay();
        DisplayMetrics metrics = new DisplayMetrics();
        display.getMetrics(metrics);

        // For square screen, test is guaranteed to pass
        if (metrics.widthPixels == metrics.heightPixels) {
            return;
        }

        // Handle display rotation
        int displayRotation = display.getRotation();
        if (displayRotation == Surface.ROTATION_90 || displayRotation == Surface.ROTATION_270) {
            int tmp = metrics.widthPixels;
            metrics.widthPixels = metrics.heightPixels;
            metrics.heightPixels = tmp;
        }
        boolean isDevicePortrait = metrics.widthPixels < metrics.heightPixels;

        for (int i = 0; i < mAllCameraIds.length; i++) {
            CameraCharacteristics c = mCharacteristics.get(i);
            // Camera size
            Size pixelArraySize = c.get(CameraCharacteristics.SENSOR_INFO_PIXEL_ARRAY_SIZE);
            // Camera orientation
            int sensorOrientation = c.get(CameraCharacteristics.SENSOR_ORIENTATION);

            // For square sensor, test is guaranteed to pass
            if (pixelArraySize.getWidth() == pixelArraySize.getHeight()) {
                continue;
            }

            // Camera size adjusted for device native orientation.
            Size adjustedSensorSize;
            if (sensorOrientation == 90 || sensorOrientation == 270) {
                adjustedSensorSize = new Size(
                        pixelArraySize.getHeight(), pixelArraySize.getWidth());
            } else {
                adjustedSensorSize = pixelArraySize;
            }

            boolean isCameraPortrait =
                    adjustedSensorSize.getWidth() < adjustedSensorSize.getHeight();
            assertFalse(""Camera "" + mAllCameraIds[i] + ""'s long dimension must ""
                    + ""align with screen's long dimension"", isDevicePortrait^isCameraPortrait);
        }
    }

    /**
     * Check camera characteristics for R and S Performance class requirements as specified
     * in CDD camera section 7.5
     */"	""	""	"cdd"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testCameraPerfClassCharacteristics"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"@CddTest(requirement=""7.5"")
    public void testCameraPerfClassCharacteristics() throws Exception {
        if (mAdoptShellPerm) {
            // Skip test for system camera. Performance class is only applicable for public camera
            // ids.
            return;
        }
        boolean isRPerfClass = CameraTestUtils.isRPerfClass();
        boolean isSPerfClass = CameraTestUtils.isSPerfClass();
        if (!isRPerfClass && !isSPerfClass) {
            return;
        }

        boolean hasPrimaryRear = false;
        boolean hasPrimaryFront = false;
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            String cameraId = mCameraIdsUnderTest[i];
            boolean isPrimaryRear = CameraTestUtils.isPrimaryRearFacingCamera(
                    mCameraManager, cameraId);
            boolean isPrimaryFront = CameraTestUtils.isPrimaryFrontFacingCamera(
                    mCameraManager, cameraId);
            if (!isPrimaryRear && !isPrimaryFront) {
                continue;
            }

            CameraCharacteristics c = mCharacteristics.get(i);
            StaticMetadata staticInfo = mAllStaticInfo.get(cameraId);

            // H-1-1, H-1-2
            Size pixelArraySize = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.SENSOR_INFO_PIXEL_ARRAY_SIZE);
            long sensorResolution = pixelArraySize.getHeight() * pixelArraySize.getWidth();
            StreamConfigurationMap config = staticInfo.getValueFromKeyNonNull(
                    CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            assertNotNull(""No stream configuration map found for ID "" + cameraId, config);
            List<Size> videoSizes = CameraTestUtils.getSupportedVideoSizes(cameraId,
                    mCameraManager, null /*bound*/);

            if (isPrimaryRear) {
                hasPrimaryRear = true;
                mCollector.expectTrue(""Primary rear camera resolution should be at least "" +
                        MIN_BACK_SENSOR_PERF_CLASS_RESOLUTION + "" pixels, is ""+
                        sensorResolution,
                        sensorResolution >= MIN_BACK_SENSOR_PERF_CLASS_RESOLUTION);

                // 4K @ 30fps
                boolean supportUHD = videoSizes.contains(UHD);
                boolean supportDC4K = videoSizes.contains(DC4K);
                mCollector.expectTrue(""Primary rear camera should support 4k video recording"",
                        supportUHD || supportDC4K);
                if (supportUHD || supportDC4K) {
                    long minFrameDuration = config.getOutputMinFrameDuration(
                            android.media.MediaRecorder.class, supportDC4K ? DC4K : UHD);
                    mCollector.expectTrue(""Primary rear camera should support 4k video @ 30fps"",
                            minFrameDuration < (1e9 / 29.9));
                }
            } else {
                hasPrimaryFront = true;
                if (isSPerfClass) {
                    mCollector.expectTrue(""Primary front camera resolution should be at least "" +
                            MIN_FRONT_SENSOR_S_PERF_CLASS_RESOLUTION + "" pixels, is ""+
                            sensorResolution,
                            sensorResolution >= MIN_FRONT_SENSOR_S_PERF_CLASS_RESOLUTION);
                } else {
                    mCollector.expectTrue(""Primary front camera resolution should be at least "" +
                            MIN_FRONT_SENSOR_R_PERF_CLASS_RESOLUTION + "" pixels, is ""+
                            sensorResolution,
                            sensorResolution >= MIN_FRONT_SENSOR_R_PERF_CLASS_RESOLUTION);
                }
                // 1080P @ 30fps
                boolean supportFULLHD = videoSizes.contains(FULLHD);
                mCollector.expectTrue(""Primary front camera should support 1080P video recording"",
                        supportFULLHD);
                if (supportFULLHD) {
                    long minFrameDuration = config.getOutputMinFrameDuration(
                            android.media.MediaRecorder.class, FULLHD);
                    mCollector.expectTrue(""Primary front camera should support 1080P video @ 30fps"",
                            minFrameDuration < (1e9 / 29.9));
                }
            }

            String facingString = hasPrimaryRear ? ""rear"" : ""front"";
            // H-1-3
            if (isSPerfClass || (isRPerfClass && isPrimaryRear)) {
                mCollector.expectTrue(""Primary "" + facingString +
                        "" camera should be at least FULL, but is "" +
                        toStringHardwareLevel(staticInfo.getHardwareLevelChecked()),
                        staticInfo.isHardwareLevelAtLeastFull());
            } else {
                mCollector.expectTrue(""Primary "" + facingString +
                        "" camera should be at least LIMITED, but is "" +
                        toStringHardwareLevel(staticInfo.getHardwareLevelChecked()),
                        staticInfo.isHardwareLevelAtLeastLimited());
            }

            // H-1-4
            Integer timestampSource = c.get(CameraCharacteristics.SENSOR_INFO_TIMESTAMP_SOURCE);
            mCollector.expectTrue(
                    ""Primary "" + facingString + "" camera should support real-time timestamp source"",
                    timestampSource != null &&
                    timestampSource.equals(CameraMetadata.SENSOR_INFO_TIMESTAMP_SOURCE_REALTIME));

            // H-1-8
            if (isSPerfClass && isPrimaryRear) {
                mCollector.expectTrue(""Primary rear camera should support RAW capability"",
                        staticInfo.isCapabilitySupported(RAW));
            }
        }
        mCollector.expectTrue(""There must be a primary rear camera for performance class."",
                hasPrimaryRear);
        mCollector.expectTrue(""There must be a primary front camera for performance class."",
                hasPrimaryFront);
    }

    /**
     * Get lens distortion coefficients, as a list of 6 floats; returns null if no valid
     * distortion field is available
     */
    private float[] getLensDistortion(CameraCharacteristics c) {
        float[] distortion = null;
        float[] newDistortion = c.get(CameraCharacteristics.LENS_DISTORTION);
        if (Build.VERSION.DEVICE_INITIAL_SDK_INT > Build.VERSION_CODES.O_MR1 || newDistortion != null) {
            // New devices need to use fixed radial distortion definition; old devices can
            // opt-in to it
            if (newDistortion != null && newDistortion.length == 5) {
                distortion = new float[6];
                distortion[0] = 1.0f;
                for (int i = 1; i < 6; i++) {
                    distortion[i] = newDistortion[i-1];
                }
            }
        } else {
            // Select old field only if on older first SDK and new definition not available
            distortion = c.get(CameraCharacteristics.LENS_RADIAL_DISTORTION);
        }
        return distortion;
    }

    /**
     * Create an invalid size that's close to one of the good sizes in the list, but not one of them
     */
    private Size findInvalidSize(Size[] goodSizes) {
        return findInvalidSize(Arrays.asList(goodSizes));
    }

    /**
     * Create an invalid size that's close to one of the good sizes in the list, but not one of them
     */
    private Size findInvalidSize(List<Size> goodSizes) {
        Size invalidSize = new Size(goodSizes.get(0).getWidth() + 1, goodSizes.get(0).getHeight());
        while(goodSizes.contains(invalidSize)) {
            invalidSize = new Size(invalidSize.getWidth() + 1, invalidSize.getHeight());
        }
        return invalidSize;
    }

    /**
     * Check key is present in characteristics if the hardware level is at least {@code hwLevel};
     * check that the key is present if the actual capabilities are one of {@code capabilities}.
     *
     * @return value of the {@code key} from {@code c}
     */
    private <T> T expectKeyAvailable(CameraCharacteristics c, CameraCharacteristics.Key<T> key,
            int hwLevel, int... capabilities) {

        Integer actualHwLevel = c.get(CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL);
        assertNotNull(""android.info.supportedHardwareLevel must never be null"", actualHwLevel);

        int[] actualCapabilities = c.get(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
        assertNotNull(""android.request.availableCapabilities must never be null"",
                actualCapabilities);

        List<Key<?>> allKeys = c.getKeys();

        T value = c.get(key);

        // For LIMITED-level targeted keys, rely on capability check, not level
        if ((compareHardwareLevel(actualHwLevel, hwLevel) >= 0) && (hwLevel != LIMITED)) {
            mCollector.expectTrue(
                    String.format(""Key (%s) must be in characteristics for this hardware level "" +
                            ""(required minimal HW level %s, actual HW level %s)"",
                            key.getName(), toStringHardwareLevel(hwLevel),
                            toStringHardwareLevel(actualHwLevel)),
                    value != null);
            mCollector.expectTrue(
                    String.format(""Key (%s) must be in characteristics list of keys for this "" +
                            ""hardware level (required minimal HW level %s, actual HW level %s)"",
                            key.getName(), toStringHardwareLevel(hwLevel),
                            toStringHardwareLevel(actualHwLevel)),
                    allKeys.contains(key));
        } else if (arrayContainsAnyOf(actualCapabilities, capabilities)) {
            if (!(hwLevel == LIMITED && compareHardwareLevel(actualHwLevel, hwLevel) < 0)) {
                // Don't enforce LIMITED-starting keys on LEGACY level, even if cap is defined
                mCollector.expectTrue(
                    String.format(""Key (%s) must be in characteristics for these capabilities "" +
                            ""(required capabilities %s, actual capabilities %s)"",
                            key.getName(), Arrays.toString(capabilities),
                            Arrays.toString(actualCapabilities)),
                    value != null);
                mCollector.expectTrue(
                    String.format(""Key (%s) must be in characteristics list of keys for "" +
                            ""these capabilities (required capabilities %s, actual capabilities %s)"",
                            key.getName(), Arrays.toString(capabilities),
                            Arrays.toString(actualCapabilities)),
                    allKeys.contains(key));
            }
        } else {
            if (actualHwLevel == LEGACY && hwLevel != OPT) {
                if (value != null || allKeys.contains(key)) {
                    Log.w(TAG, String.format(
                            ""Key (%s) is not required for LEGACY devices but still appears"",
                            key.getName()));
                }
            }
            // OK: Key may or may not be present.
        }
        return value;
    }

    private static boolean arrayContains(int[] arr, int needle) {
        if (arr == null) {
            return false;
        }

        for (int elem : arr) {
            if (elem == needle) {
                return true;
            }
        }

        return false;
    }

    private static <T> boolean arrayContains(T[] arr, T needle) {
        if (arr == null) {
            return false;
        }

        for (T elem : arr) {
            if (elem.equals(needle)) {
                return true;
            }
        }

        return false;
    }

    private static boolean arrayContainsAnyOf(int[] arr, int[] needles) {
        for (int needle : needles) {
            if (arrayContains(arr, needle)) {
                return true;
            }
        }
        return false;
    }

    /**
     * The key name has a prefix of either ""android."" or a valid TLD; other prefixes are not valid.
     */
    private static void assertKeyPrefixValid(String keyName) {
        assertStartsWithAndroidOrTLD(
                ""All metadata keys must start with 'android.' (built-in keys) "" +
                ""or valid TLD (vendor-extended keys)"", keyName);
    }

    private static void assertTrueForKey(String msg, CameraCharacteristics.Key<?> key,
            boolean actual) {
        assertTrue(msg + "" (key = '"" + key.getName() + ""')"", actual);
    }

    private static <T> void assertOneOf(String msg, T[] expected, T actual) {
        for (int i = 0; i < expected.length; ++i) {
            if (Objects.equals(expected[i], actual)) {
                return;
            }
        }

        fail(String.format(""%s: (expected one of %s, actual %s)"",
                msg, Arrays.toString(expected), actual));
    }

    private static <T> void assertStartsWithAndroidOrTLD(String msg, String keyName) {
        String delimiter = ""."";
        if (keyName.startsWith(PREFIX_ANDROID + delimiter)) {
            return;
        }
        Pattern tldPattern = Pattern.compile(Patterns.TOP_LEVEL_DOMAIN_STR);
        Matcher match = tldPattern.matcher(keyName);
        if (match.find(0) && (0 == match.start()) && (!match.hitEnd())) {
            if (keyName.regionMatches(match.end(), delimiter, 0, delimiter.length())) {
                return;
            }
        }

        fail(String.format(""%s: (expected to start with %s or valid TLD, but value was %s)"",
                msg, PREFIX_ANDROID + delimiter, keyName));
    }

    /** Return a positive int if left > right, 0 if left==right, negative int if left < right */
    private static int compareHardwareLevel(int left, int right) {
        return remapHardwareLevel(left) - remapHardwareLevel(right);
    }

    /** Remap HW levels worst<->best, 0 = LEGACY, 1 = LIMITED, 2 = FULL, ..., N = LEVEL_N */
    private static int remapHardwareLevel(int level) {
        switch (level) {
            case OPT:
                return Integer.MAX_VALUE;
            case LEGACY:
                return 0; // lowest
            case EXTERNAL:
                return 1; // second lowest
            case LIMITED:
                return 2;
            case FULL:
                return 3; // good
            case LEVEL_3:
                return 4;
            default:
                fail(""Unknown HW level: "" + level);
        }
        return -1;
    }

    private static String toStringHardwareLevel(int level) {
        switch (level) {
            case LEGACY:
                return ""LEGACY"";
            case LIMITED:
                return ""LIMITED"";
            case FULL:
                return ""FULL"";
            case EXTERNAL:
                return ""EXTERNAL"";
            default:
                if (level >= LEVEL_3) {
                    return String.format(""LEVEL_%d"", level);
                }
        }

        // unknown
        Log.w(TAG, ""Unknown hardware level "" + level);
        return Integer.toString(level);
    }
}"	""	""	"cdd rear resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testKeys"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"public void testKeys() {
        for (int i = 0; i < mAllCameraIds.length; i++) {
            CameraCharacteristics c = mCharacteristics.get(i);
            mCollector.setCameraId(mAllCameraIds[i]);

            if (VERBOSE) {
                Log.v(TAG, ""testKeys - testing characteristics for camera "" + mAllCameraIds[i]);
            }

            List<CameraCharacteristics.Key<?>> allKeys = c.getKeys();
            assertNotNull(""Camera characteristics keys must not be null"", allKeys);
            assertFalse(""Camera characteristics keys must have at least 1 key"",
                    allKeys.isEmpty());

            for (CameraCharacteristics.Key<?> key : allKeys) {
                assertKeyPrefixValid(key.getName());

                // All characteristics keys listed must never be null
                mCollector.expectKeyValueNotNull(c, key);

                // TODO: add a check that key must not be @hide
            }

            /*
             * List of keys that must be present in camera characteristics (not null).
             *
             * Keys for LIMITED, FULL devices might be available despite lacking either
             * the hardware level or the capability. This is *OK*. This only lists the
             * *minimal* requirements for a key to be listed.
             *
             * LEGACY devices are a bit special since they map to api1 devices, so we know
             * for a fact most keys are going to be illegal there so they should never be
             * available.
             *
             * For LIMITED-level keys, if the level is >= LIMITED, then the capabilities are used to
             * do the actual checking.
             */
            {
                //                                           (Key Name)                                     (HW Level)  (Capabilities <Var-Arg>)
                expectKeyAvailable(c, CameraCharacteristics.COLOR_CORRECTION_AVAILABLE_ABERRATION_MODES     , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AVAILABLE_MODES                         , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AE_AVAILABLE_ANTIBANDING_MODES          , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AE_AVAILABLE_MODES                      , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES          , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AE_COMPENSATION_RANGE                   , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AE_COMPENSATION_STEP                    , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AE_LOCK_AVAILABLE                       , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AF_AVAILABLE_MODES                      , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AVAILABLE_EFFECTS                       , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AVAILABLE_SCENE_MODES                   , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES     , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AWB_AVAILABLE_MODES                     , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AWB_LOCK_AVAILABLE                      , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_MAX_REGIONS_AE                          , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_MAX_REGIONS_AF                          , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_MAX_REGIONS_AWB                         , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.EDGE_AVAILABLE_EDGE_MODES                       , FULL     ,   NONE                 );
                expectKeyAvailable(c, CameraCharacteristics.FLASH_INFO_AVAILABLE                            , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES             , OPT      ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL                   , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.INFO_VERSION                                    , OPT      ,   NONE                 );
                expectKeyAvailable(c, CameraCharacteristics.JPEG_AVAILABLE_THUMBNAIL_SIZES                  , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.LENS_FACING                                     , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.LENS_INFO_AVAILABLE_APERTURES                   , FULL     ,   MANUAL_SENSOR        );
                expectKeyAvailable(c, CameraCharacteristics.LENS_INFO_AVAILABLE_FILTER_DENSITIES            , FULL     ,   MANUAL_SENSOR        );
                expectKeyAvailable(c, CameraCharacteristics.LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION       , LIMITED  ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.LENS_INFO_FOCUS_DISTANCE_CALIBRATION            , LIMITED  ,   MANUAL_SENSOR        );
                expectKeyAvailable(c, CameraCharacteristics.LENS_INFO_HYPERFOCAL_DISTANCE                   , LIMITED  ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.LENS_INFO_MINIMUM_FOCUS_DISTANCE                , LIMITED  ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES                  , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.REQUEST_MAX_NUM_INPUT_STREAMS                   , OPT      ,   YUV_REPROCESS, OPAQUE_REPROCESS);
                expectKeyAvailable(c, CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP                 , OPT      ,   CONSTRAINED_HIGH_SPEED);
                expectKeyAvailable(c, CameraCharacteristics.REQUEST_MAX_NUM_OUTPUT_PROC                     , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.REQUEST_MAX_NUM_OUTPUT_PROC_STALLING            , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.REQUEST_MAX_NUM_OUTPUT_RAW                      , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.REQUEST_PARTIAL_RESULT_COUNT                    , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.REQUEST_PIPELINE_MAX_DEPTH                      , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SCALER_AVAILABLE_MAX_DIGITAL_ZOOM               , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP                 , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SCALER_CROPPING_TYPE                            , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_BLACK_LEVEL_PATTERN                      , FULL     ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE                   , OPT      ,   BC, RAW              );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT            , FULL     ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE                 , FULL     ,   MANUAL_SENSOR        );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_MAX_FRAME_DURATION                  , FULL     ,   MANUAL_SENSOR        );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_PIXEL_ARRAY_SIZE                    , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE                   , FULL     ,   MANUAL_SENSOR        );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_WHITE_LEVEL                         , OPT      ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_TIMESTAMP_SOURCE                    , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_MAX_ANALOG_SENSITIVITY                   , FULL     ,   MANUAL_SENSOR        );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_ORIENTATION                              , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SHADING_AVAILABLE_MODES                         , LIMITED  ,   MANUAL_POSTPROC, RAW );
                expectKeyAvailable(c, CameraCharacteristics.STATISTICS_INFO_AVAILABLE_FACE_DETECT_MODES     , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.STATISTICS_INFO_AVAILABLE_HOT_PIXEL_MAP_MODES   , OPT      ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.STATISTICS_INFO_AVAILABLE_LENS_SHADING_MAP_MODES, LIMITED  ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.STATISTICS_INFO_MAX_FACE_COUNT                  , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SYNC_MAX_LATENCY                                , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.TONEMAP_AVAILABLE_TONE_MAP_MODES                , FULL     ,   MANUAL_POSTPROC      );
                expectKeyAvailable(c, CameraCharacteristics.TONEMAP_MAX_CURVE_POINTS                        , FULL     ,   MANUAL_POSTPROC      );

                // Future: Use column editors for modifying above, ignore line length to keep 1 key per line

                // TODO: check that no other 'android' keys are listed in #getKeys if they aren't in the above list
            }

            int[] actualCapabilities = c.get(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
            assertNotNull(""android.request.availableCapabilities must never be null"",
                    actualCapabilities);
            boolean isMonochrome = arrayContains(actualCapabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MONOCHROME);
            if (!isMonochrome) {
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_CALIBRATION_TRANSFORM1                   , OPT      ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_COLOR_TRANSFORM1                         , OPT      ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_FORWARD_MATRIX1                          , OPT      ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_REFERENCE_ILLUMINANT1                    , OPT      ,   RAW                  );


                // Only check for these if the second reference illuminant is included
                if (allKeys.contains(CameraCharacteristics.SENSOR_REFERENCE_ILLUMINANT2)) {
                    expectKeyAvailable(c, CameraCharacteristics.SENSOR_REFERENCE_ILLUMINANT2                    , OPT      ,   RAW                  );
                    expectKeyAvailable(c, CameraCharacteristics.SENSOR_COLOR_TRANSFORM2                         , OPT      ,   RAW                  );
                    expectKeyAvailable(c, CameraCharacteristics.SENSOR_CALIBRATION_TRANSFORM2                   , OPT      ,   RAW                  );
                    expectKeyAvailable(c, CameraCharacteristics.SENSOR_FORWARD_MATRIX2                          , OPT      ,   RAW                  );
                }
            }

            // Required key if any of RAW format output is supported
            StreamConfigurationMap config =
                    c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            assertNotNull(String.format(""No stream configuration map found for: ID %s"",
                    mAllCameraIds[i]), config);
            if (config.isOutputSupportedFor(ImageFormat.RAW_SENSOR) ||
                    config.isOutputSupportedFor(ImageFormat.RAW10)  ||
                    config.isOutputSupportedFor(ImageFormat.RAW12)  ||
                    config.isOutputSupportedFor(ImageFormat.RAW_PRIVATE)) {
                expectKeyAvailable(c,
                        CameraCharacteristics.CONTROL_POST_RAW_SENSITIVITY_BOOST_RANGE, OPT, BC);
            }

            // External Camera exceptional keys
            Integer hwLevel = c.get(CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL);
            boolean isExternalCamera = (hwLevel ==
                    CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL);
            if (!isExternalCamera) {
                expectKeyAvailable(c, CameraCharacteristics.LENS_INFO_AVAILABLE_FOCAL_LENGTHS               , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_AVAILABLE_TEST_PATTERN_MODES             , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_PHYSICAL_SIZE                       , OPT      ,   BC                   );
            }


            // Verify version is a short text string.
            if (allKeys.contains(CameraCharacteristics.INFO_VERSION)) {
                final String TEXT_REGEX = ""[\\p{Alnum}\\p{Punct}\\p{Space}]*"";
                final int MAX_VERSION_LENGTH = 256;

                String version = c.get(CameraCharacteristics.INFO_VERSION);
                mCollector.expectTrue(""Version contains non-text characters: "" + version,
                        version.matches(TEXT_REGEX));
                mCollector.expectLessOrEqual(""Version too long: "" + version, MAX_VERSION_LENGTH,
                        version.length());
            }
        }
    }

    /**
     * Test values for static metadata used by the RAW capability.
     */"	""	""	"minimum 12"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testStaticBurstCharacteristics"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"public void testStaticBurstCharacteristics() throws Exception {
        for (int i = 0; i < mAllCameraIds.length; i++) {
            CameraCharacteristics c = mCharacteristics.get(i);
            int[] actualCapabilities = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);

            // Check if the burst capability is defined
            boolean haveBurstCapability = arrayContains(actualCapabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_BURST_CAPTURE);
            boolean haveBC = arrayContains(actualCapabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE);

            if(haveBurstCapability && !haveBC) {
                fail(""Must have BACKWARD_COMPATIBLE capability if BURST_CAPTURE capability is defined"");
            }

            if (!haveBC) continue;

            StreamConfigurationMap config =
                    c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            assertNotNull(String.format(""No stream configuration map found for: ID %s"",
                    mAllCameraIds[i]), config);
            Rect activeRect = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE);
            Size sensorSize = new Size(activeRect.width(), activeRect.height());

            // Ensure that max YUV size matches max JPEG size
            Size maxYuvSize = CameraTestUtils.getMaxSize(
                    config.getOutputSizes(ImageFormat.YUV_420_888));
            Size maxFastYuvSize = maxYuvSize;

            Size[] slowYuvSizes = config.getHighResolutionOutputSizes(ImageFormat.YUV_420_888);
            Size maxSlowYuvSizeLessThan24M = null;
            if (haveBurstCapability && slowYuvSizes != null && slowYuvSizes.length > 0) {
                Size maxSlowYuvSize = CameraTestUtils.getMaxSize(slowYuvSizes);
                final int SIZE_24MP_BOUND = 24000000;
                maxSlowYuvSizeLessThan24M =
                        CameraTestUtils.getMaxSizeWithBound(slowYuvSizes, SIZE_24MP_BOUND);
                maxYuvSize = CameraTestUtils.getMaxSize(new Size[]{maxYuvSize, maxSlowYuvSize});
            }

            Size maxJpegSize = CameraTestUtils.getMaxSize(CameraTestUtils.getSupportedSizeForFormat(
                    ImageFormat.JPEG, mAllCameraIds[i], mCameraManager));

            boolean haveMaxYuv = maxYuvSize != null ?
                (maxJpegSize.getWidth() <= maxYuvSize.getWidth() &&
                        maxJpegSize.getHeight() <= maxYuvSize.getHeight()) : false;

            Pair<Boolean, Size> maxYuvMatchSensorPair = isSizeWithinSensorMargin(maxYuvSize,
                    sensorSize);

            // No need to do null check since framework will generate the key if HAL don't supply
            boolean haveAeLock = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.CONTROL_AE_LOCK_AVAILABLE);
            boolean haveAwbLock = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.CONTROL_AWB_LOCK_AVAILABLE);

            // Ensure that some >=8MP YUV output is fast enough - needs to be at least 20 fps

            long maxFastYuvRate =
                    config.getOutputMinFrameDuration(ImageFormat.YUV_420_888, maxFastYuvSize);
            final long MIN_8MP_DURATION_BOUND_NS = 50000000; // 50 ms, 20 fps
            boolean haveFastYuvRate = maxFastYuvRate <= MIN_8MP_DURATION_BOUND_NS;

            final int SIZE_8MP_BOUND = 8000000;
            boolean havefast8MPYuv = (maxFastYuvSize.getWidth() * maxFastYuvSize.getHeight()) >
                    SIZE_8MP_BOUND;

            // Ensure that max YUV output smaller than 24MP is fast enough
            // - needs to be at least 10 fps
            final long MIN_MAXSIZE_DURATION_BOUND_NS = 100000000; // 100 ms, 10 fps
            long maxYuvRate = maxFastYuvRate;
            if (maxSlowYuvSizeLessThan24M != null) {
                maxYuvRate = config.getOutputMinFrameDuration(
                        ImageFormat.YUV_420_888, maxSlowYuvSizeLessThan24M);
            }
            boolean haveMaxYuvRate = maxYuvRate <= MIN_MAXSIZE_DURATION_BOUND_NS;

            // Ensure that there's an FPS range that's fast enough to capture at above
            // minFrameDuration, for full-auto bursts at the fast resolutions
            Range[] fpsRanges = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES);
            float minYuvFps = 1.f / maxFastYuvRate;

            boolean haveFastAeTargetFps = false;
            for (Range<Integer> r : fpsRanges) {
                if (r.getLower() >= minYuvFps) {
                    haveFastAeTargetFps = true;
                    break;
                }
            }

            // Ensure that maximum sync latency is small enough for fast setting changes, even if
            // it's not quite per-frame

            Integer maxSyncLatencyValue = c.get(CameraCharacteristics.SYNC_MAX_LATENCY);
            assertNotNull(String.format(""No sync latency declared for ID %s"", mAllCameraIds[i]),
                    maxSyncLatencyValue);

            int maxSyncLatency = maxSyncLatencyValue;
            final long MAX_LATENCY_BOUND = 4;
            boolean haveFastSyncLatency =
                (maxSyncLatency <= MAX_LATENCY_BOUND) && (maxSyncLatency >= 0);

            if (haveBurstCapability) {
                assertTrue(""Must have slow YUV size array when BURST_CAPTURE capability is defined!"",
                        slowYuvSizes != null);
                assertTrue(
                        String.format(""BURST-capable camera device %s does not have maximum YUV "" +
                                ""size that is at least max JPEG size"",
                                mAllCameraIds[i]),
                        haveMaxYuv);
                assertTrue(
                        String.format(""BURST-capable camera device %s max-resolution "" +
                                ""YUV frame rate is too slow"" +
                                ""(%d ns min frame duration reported, less than %d ns expected)"",
                                mAllCameraIds[i], maxYuvRate, MIN_MAXSIZE_DURATION_BOUND_NS),
                        haveMaxYuvRate);
                assertTrue(
                        String.format(""BURST-capable camera device %s >= 8MP YUV output "" +
                                ""frame rate is too slow"" +
                                ""(%d ns min frame duration reported, less than %d ns expected)"",
                                mAllCameraIds[i], maxYuvRate, MIN_8MP_DURATION_BOUND_NS),
                        haveFastYuvRate);
                assertTrue(
                        String.format(""BURST-capable camera device %s does not list an AE target "" +
                                "" FPS range with min FPS >= %f, for full-AUTO bursts"",
                                mAllCameraIds[i], minYuvFps),
                        haveFastAeTargetFps);
                assertTrue(
                        String.format(""BURST-capable camera device %s YUV sync latency is too long"" +
                                ""(%d frames reported, [0, %d] frames expected)"",
                                mAllCameraIds[i], maxSyncLatency, MAX_LATENCY_BOUND),
                        haveFastSyncLatency);
                assertTrue(
                        String.format(""BURST-capable camera device %s max YUV size %s should be"" +
                                ""close to active array size %s or cropped active array size %s"",
                                mAllCameraIds[i], maxYuvSize.toString(), sensorSize.toString(),
                                maxYuvMatchSensorPair.second.toString()),
                        maxYuvMatchSensorPair.first.booleanValue());
                assertTrue(
                        String.format(""BURST-capable camera device %s does not support AE lock"",
                                mAllCameraIds[i]),
                        haveAeLock);
                assertTrue(
                        String.format(""BURST-capable camera device %s does not support AWB lock"",
                                mAllCameraIds[i]),
                        haveAwbLock);
            } else {
                assertTrue(""Must have null slow YUV size array when no BURST_CAPTURE capability!"",
                        slowYuvSizes == null);
                assertTrue(
                        String.format(""Camera device %s has all the requirements for BURST"" +
                                "" capability but does not report it!"", mAllCameraIds[i]),
                        !(haveMaxYuv && haveMaxYuvRate && haveFastYuvRate && haveFastAeTargetFps &&
                                haveFastSyncLatency && maxYuvMatchSensorPair.first.booleanValue() &&
                                haveAeLock && haveAwbLock));
            }
        }
    }

    /**
     * Check reprocessing capabilities.
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testReprocessingCharacteristics"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"public void testReprocessingCharacteristics() {
        for (int i = 0; i < mAllCameraIds.length; i++) {
            Log.i(TAG, ""testReprocessingCharacteristics: Testing camera ID "" + mAllCameraIds[i]);

            CameraCharacteristics c = mCharacteristics.get(i);
            int[] capabilities = c.get(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
            assertNotNull(""android.request.availableCapabilities must never be null"",
                    capabilities);
            boolean supportYUV = arrayContains(capabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING);
            boolean supportOpaque = arrayContains(capabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING);
            StreamConfigurationMap configs =
                    c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            Integer maxNumInputStreams =
                    c.get(CameraCharacteristics.REQUEST_MAX_NUM_INPUT_STREAMS);
            int[] availableEdgeModes = c.get(CameraCharacteristics.EDGE_AVAILABLE_EDGE_MODES);
            int[] availableNoiseReductionModes = c.get(
                    CameraCharacteristics.NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES);

            int[] inputFormats = configs.getInputFormats();
            int[] outputFormats = configs.getOutputFormats();
            boolean isMonochromeWithY8 = arrayContains(capabilities, MONOCHROME)
                    && arrayContains(outputFormats, ImageFormat.Y8);

            boolean supportZslEdgeMode = false;
            boolean supportZslNoiseReductionMode = false;
            boolean supportHiQNoiseReductionMode = false;
            boolean supportHiQEdgeMode = false;

            if (availableEdgeModes != null) {
                supportZslEdgeMode = Arrays.asList(CameraTestUtils.toObject(availableEdgeModes)).
                        contains(CaptureRequest.EDGE_MODE_ZERO_SHUTTER_LAG);
                supportHiQEdgeMode = Arrays.asList(CameraTestUtils.toObject(availableEdgeModes)).
                        contains(CaptureRequest.EDGE_MODE_HIGH_QUALITY);
            }

            if (availableNoiseReductionModes != null) {
                supportZslNoiseReductionMode = Arrays.asList(
                        CameraTestUtils.toObject(availableNoiseReductionModes)).contains(
                        CaptureRequest.NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG);
                supportHiQNoiseReductionMode = Arrays.asList(
                        CameraTestUtils.toObject(availableNoiseReductionModes)).contains(
                        CaptureRequest.NOISE_REDUCTION_MODE_HIGH_QUALITY);
            }

            if (supportYUV || supportOpaque) {
                mCollector.expectTrue(""Support reprocessing but max number of input stream is "" +
                        maxNumInputStreams, maxNumInputStreams != null && maxNumInputStreams > 0);
                mCollector.expectTrue(""Support reprocessing but EDGE_MODE_ZERO_SHUTTER_LAG is "" +
                        ""not supported"", supportZslEdgeMode);
                mCollector.expectTrue(""Support reprocessing but "" +
                        ""NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG is not supported"",
                        supportZslNoiseReductionMode);

                // For reprocessing, if we only require OFF and ZSL mode, it will be just like jpeg
                // encoding. We implicitly require FAST to make reprocessing meaningful, which means
                // that we also require HIGH_QUALITY.
                mCollector.expectTrue(""Support reprocessing but EDGE_MODE_HIGH_QUALITY is "" +
                        ""not supported"", supportHiQEdgeMode);
                mCollector.expectTrue(""Support reprocessing but "" +
                        ""NOISE_REDUCTION_MODE_HIGH_QUALITY is not supported"",
                        supportHiQNoiseReductionMode);

                // Verify mandatory input formats are supported
                mCollector.expectTrue(""YUV_420_888 input must be supported for YUV reprocessing"",
                        !supportYUV || arrayContains(inputFormats, ImageFormat.YUV_420_888));
                mCollector.expectTrue(""Y8 input must be supported for YUV reprocessing on "" +
                        ""MONOCHROME devices with Y8 support"", !supportYUV || !isMonochromeWithY8
                        || arrayContains(inputFormats, ImageFormat.Y8));
                mCollector.expectTrue(""PRIVATE input must be supported for OPAQUE reprocessing"",
                        !supportOpaque || arrayContains(inputFormats, ImageFormat.PRIVATE));

                // max capture stall must be reported if one of the reprocessing is supported.
                final int MAX_ALLOWED_STALL_FRAMES = 4;
                Integer maxCaptureStall = c.get(CameraCharacteristics.REPROCESS_MAX_CAPTURE_STALL);
                mCollector.expectTrue(""max capture stall must be non-null and no larger than ""
                        + MAX_ALLOWED_STALL_FRAMES,
                        maxCaptureStall != null && maxCaptureStall <= MAX_ALLOWED_STALL_FRAMES);

                for (int input : inputFormats) {
                    // Verify mandatory output formats are supported
                    int[] outputFormatsForInput = configs.getValidOutputFormatsForInput(input);
                    mCollector.expectTrue(
                        ""YUV_420_888 output must be supported for reprocessing"",
                        input == ImageFormat.Y8
                        || arrayContains(outputFormatsForInput, ImageFormat.YUV_420_888));
                    mCollector.expectTrue(
                        ""Y8 output must be supported for reprocessing on MONOCHROME devices with""
                        + "" Y8 support"", !isMonochromeWithY8 || input == ImageFormat.YUV_420_888
                        || arrayContains(outputFormatsForInput, ImageFormat.Y8));
                    mCollector.expectTrue(""JPEG output must be supported for reprocessing"",
                            arrayContains(outputFormatsForInput, ImageFormat.JPEG));

                    // Verify camera can output the reprocess input formats and sizes.
                    Size[] inputSizes = configs.getInputSizes(input);
                    Size[] outputSizes = configs.getOutputSizes(input);
                    Size[] highResOutputSizes = configs.getHighResolutionOutputSizes(input);
                    mCollector.expectTrue(""no input size supported for format "" + input,
                            inputSizes.length > 0);
                    mCollector.expectTrue(""no output size supported for format "" + input,
                            outputSizes.length > 0);

                    for (Size inputSize : inputSizes) {
                        mCollector.expectTrue(""Camera must be able to output the supported "" +
                                ""reprocessing input size"",
                                arrayContains(outputSizes, inputSize) ||
                                arrayContains(highResOutputSizes, inputSize));
                    }
                }
            } else {
                mCollector.expectTrue(""Doesn't support reprocessing but report input format: "" +
                        Arrays.toString(inputFormats), inputFormats.length == 0);
                mCollector.expectTrue(""Doesn't support reprocessing but max number of input "" +
                        ""stream is "" + maxNumInputStreams,
                        maxNumInputStreams == null || maxNumInputStreams == 0);
                mCollector.expectTrue(""Doesn't support reprocessing but "" +
                        ""EDGE_MODE_ZERO_SHUTTER_LAG is supported"", !supportZslEdgeMode);
                mCollector.expectTrue(""Doesn't support reprocessing but "" +
                        ""NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG is supported"",
                        !supportZslNoiseReductionMode);
            }
        }
    }

    /**
     * Check ultra high resolution sensor characteristics.
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testUltraHighResolutionSensorCharacteristics"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"public void testUltraHighResolutionSensorCharacteristics() {
        for (int i = 0; i < mAllCameraIds.length; i++) {
            CameraCharacteristics c = mCharacteristics.get(i);
            String cameraId = mAllCameraIds[i];
            int[] capabilities = c.get(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
            assertNotNull(""android.request.availableCapabilities must never be null"",
                    capabilities);
            boolean isUltraHighResolutionSensor = arrayContains(capabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR);

            boolean supportsRemosaic = arrayContains(capabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_REMOSAIC_REPROCESSING);

            if (!isUltraHighResolutionSensor) {
                Log.i(TAG, ""Camera id "" + cameraId + "" not ultra high resolution. Skipping "" +
                        ""testUltraHighResolutionSensorCharacteristics"");
                continue;
            }
            assertArrayContains(
                    String.format(""Ultra high resolution sensor, camera id %s"" +
                    "" must also have the RAW capability"", cameraId), capabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW);
            StreamConfigurationMap configs =
                    c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP_MAXIMUM_RESOLUTION);
            assertNotNull(""Maximum resolution stream configuration map must not be null for ultra"" +
                    "" high resolution sensor camera "" + cameraId, configs);
            Size uhrPixelArraySize = CameraTestUtils.getValueNotNull(
                c, CameraCharacteristics.SENSOR_INFO_PIXEL_ARRAY_SIZE_MAXIMUM_RESOLUTION);
            long uhrSensorSize = uhrPixelArraySize.getHeight() * uhrPixelArraySize.getWidth();

            assertTrue(""ULTRA_HIGH_RESOLUTION_SENSOR pixel array size should be at least "" +
                    MIN_UHR_SENSOR_RESOLUTION + "" pixels, is "" + uhrSensorSize + "", for camera id ""
                    + cameraId, uhrSensorSize >= MIN_UHR_SENSOR_RESOLUTION);

            int[] outputFormats = configs.getOutputFormats();
            assertArrayContains(String.format(""No max res JPEG image format for ultra high"" +
                  "" resolution sensor: ID %s"", cameraId), outputFormats, ImageFormat.JPEG);
            assertArrayContains(String.format(""No max res YUV_420_88 image format for ultra high"" +
                  "" resolution sensor: ID %s"", cameraId), outputFormats, ImageFormat.YUV_420_888);
            assertArrayContains(String.format(""No max res RAW_SENSOR image format for ultra high"" +
                  "" resolution sensor: ID %s"", cameraId), outputFormats, ImageFormat.RAW_SENSOR);

            if (supportsRemosaic) {
                testRemosaicReprocessingCharacteristics(cameraId, c);
            }
      }

    }
    /**
     * Check remosaic reprocessing capabilities. Check that ImageFormat.RAW_SENSOR is supported as
     * input and output.
     */
    private void testRemosaicReprocessingCharacteristics(String cameraId, CameraCharacteristics c) {
        StreamConfigurationMap configs =
                c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP_MAXIMUM_RESOLUTION);
        Integer maxNumInputStreams =
                c.get(CameraCharacteristics.REQUEST_MAX_NUM_INPUT_STREAMS);
        int[] inputFormats = configs.getInputFormats();
        int[] outputFormats = configs.getOutputFormats();

        mCollector.expectTrue(""Support reprocessing but max number of input stream is "" +
                maxNumInputStreams, maxNumInputStreams != null && maxNumInputStreams > 0);

        // Verify mandatory input formats are supported
        mCollector.expectTrue(""RAW_SENSOR input support needed for REMOSAIC reprocessing"",
                arrayContains(inputFormats, ImageFormat.RAW_SENSOR));
        // max capture stall must be reported if one of the reprocessing is supported.
        final int MAX_ALLOWED_STALL_FRAMES = 4;
        Integer maxCaptureStall = c.get(CameraCharacteristics.REPROCESS_MAX_CAPTURE_STALL);
        mCollector.expectTrue(""max capture stall must be non-null and no larger than ""
                + MAX_ALLOWED_STALL_FRAMES,
                maxCaptureStall != null && maxCaptureStall <= MAX_ALLOWED_STALL_FRAMES);

        for (int input : inputFormats) {
            // Verify mandatory output formats are supported
            int[] outputFormatsForInput = configs.getValidOutputFormatsForInput(input);

            // Verify camera can output the reprocess input formats and sizes.
            Size[] inputSizes = configs.getInputSizes(input);
            Size[] outputSizes = configs.getOutputSizes(input);
            Size[] highResOutputSizes = configs.getHighResolutionOutputSizes(input);
            mCollector.expectTrue(""no input size supported for format "" + input,
                    inputSizes.length > 0);
            mCollector.expectTrue(""no output size supported for format "" + input,
                    outputSizes.length > 0);

            for (Size inputSize : inputSizes) {
                mCollector.expectTrue(""Camera must be able to output the supported "" +
                        ""reprocessing input size"",
                        arrayContains(outputSizes, inputSize) ||
                        arrayContains(highResOutputSizes, inputSize));
            }
        }
    }


    /**
     * Check depth output capability
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testStreamConfigurationMap"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"public void testStreamConfigurationMap() throws Exception {
        for (int i = 0; i < mAllCameraIds.length; i++) {
            Log.i(TAG, ""testStreamConfigurationMap: Testing camera ID "" + mAllCameraIds[i]);
            CameraCharacteristics c = mCharacteristics.get(i);
            StreamConfigurationMap config =
                    c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            assertNotNull(String.format(""No stream configuration map found for: ID %s"",
                            mAllCameraIds[i]), config);

            int[] actualCapabilities = c.get(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
            assertNotNull(""android.request.availableCapabilities must never be null"",
                    actualCapabilities);

            if (arrayContains(actualCapabilities, BC)) {
                assertTrue(""ImageReader must be supported"",
                    config.isOutputSupportedFor(android.media.ImageReader.class));
                assertTrue(""MediaRecorder must be supported"",
                    config.isOutputSupportedFor(android.media.MediaRecorder.class));
                assertTrue(""MediaCodec must be supported"",
                    config.isOutputSupportedFor(android.media.MediaCodec.class));
                assertTrue(""Allocation must be supported"",
                    config.isOutputSupportedFor(android.renderscript.Allocation.class));
                assertTrue(""SurfaceHolder must be supported"",
                    config.isOutputSupportedFor(android.view.SurfaceHolder.class));
                assertTrue(""SurfaceTexture must be supported"",
                    config.isOutputSupportedFor(android.graphics.SurfaceTexture.class));

                assertTrue(""YUV_420_888 must be supported"",
                    config.isOutputSupportedFor(ImageFormat.YUV_420_888));
                assertTrue(""JPEG must be supported"",
                    config.isOutputSupportedFor(ImageFormat.JPEG));
            } else {
                assertTrue(""YUV_420_88 may not be supported if BACKWARD_COMPATIBLE capability is not listed"",
                    !config.isOutputSupportedFor(ImageFormat.YUV_420_888));
                assertTrue(""JPEG may not be supported if BACKWARD_COMPATIBLE capability is not listed"",
                    !config.isOutputSupportedFor(ImageFormat.JPEG));
            }

            // Check RAW

            if (arrayContains(actualCapabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                assertTrue(""RAW_SENSOR must be supported if RAW capability is advertised"",
                    config.isOutputSupportedFor(ImageFormat.RAW_SENSOR));
            }

            // Cross check public formats and sizes

            int[] supportedFormats = config.getOutputFormats();
            for (int format : supportedFormats) {
                assertTrue(""Format "" + format + "" fails cross check"",
                        config.isOutputSupportedFor(format));
                List<Size> supportedSizes = CameraTestUtils.getAscendingOrderSizes(
                        Arrays.asList(config.getOutputSizes(format)), /*ascending*/true);
                if (arrayContains(actualCapabilities,
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_BURST_CAPTURE)) {
                    supportedSizes.addAll(
                        Arrays.asList(config.getHighResolutionOutputSizes(format)));
                    supportedSizes = CameraTestUtils.getAscendingOrderSizes(
                        supportedSizes, /*ascending*/true);
                }
                assertTrue(""Supported format "" + format + "" has no sizes listed"",
                        supportedSizes.size() > 0);
                for (int j = 0; j < supportedSizes.size(); j++) {
                    Size size = supportedSizes.get(j);
                    if (VERBOSE) {
                        Log.v(TAG,
                                String.format(""Testing camera %s, format %d, size %s"",
                                        mAllCameraIds[i], format, size.toString()));
                    }

                    long stallDuration = config.getOutputStallDuration(format, size);
                    switch(format) {
                        case ImageFormat.YUV_420_888:
                            assertTrue(""YUV_420_888 may not have a non-zero stall duration"",
                                    stallDuration == 0);
                            break;
                        case ImageFormat.JPEG:
                        case ImageFormat.RAW_SENSOR:
                            final float TOLERANCE_FACTOR = 2.0f;
                            long prevDuration = 0;
                            if (j > 0) {
                                prevDuration = config.getOutputStallDuration(
                                        format, supportedSizes.get(j - 1));
                            }
                            long nextDuration = Long.MAX_VALUE;
                            if (j < (supportedSizes.size() - 1)) {
                                nextDuration = config.getOutputStallDuration(
                                        format, supportedSizes.get(j + 1));
                            }
                            long curStallDuration = config.getOutputStallDuration(format, size);
                            // Stall duration should be in a reasonable range: larger size should
                            // normally have larger stall duration.
                            mCollector.expectInRange(""Stall duration (format "" + format +
                                    "" and size "" + size + "") is not in the right range"",
                                    curStallDuration,
                                    (long) (prevDuration / TOLERANCE_FACTOR),
                                    (long) (nextDuration * TOLERANCE_FACTOR));
                            break;
                        default:
                            assertTrue(""Negative stall duration for format "" + format,
                                    stallDuration >= 0);
                            break;
                    }
                    long minDuration = config.getOutputMinFrameDuration(format, size);
                    if (arrayContains(actualCapabilities,
                            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
                        assertTrue(""MANUAL_SENSOR capability, need positive min frame duration for""
                                + ""format "" + format + "" for size "" + size + "" minDuration "" +
                                minDuration,
                                minDuration > 0);
                    } else {
                        assertTrue(""Need non-negative min frame duration for format "" + format,
                                minDuration >= 0);
                    }

                    // todo: test opaque image reader when it's supported.
                    if (format != ImageFormat.PRIVATE) {
                        ImageReader testReader = ImageReader.newInstance(
                            size.getWidth(),
                            size.getHeight(),
                            format,
                            1);
                        Surface testSurface = testReader.getSurface();

                        assertTrue(
                            String.format(""isOutputSupportedFor fails for config %s, format %d"",
                                    size.toString(), format),
                            config.isOutputSupportedFor(testSurface));

                        testReader.close();
                    }
                } // sizes

                // Try an invalid size in this format, should round
                Size invalidSize = findInvalidSize(supportedSizes);
                int MAX_ROUNDING_WIDTH = 1920;
                // todo: test opaque image reader when it's supported.
                if (format != ImageFormat.PRIVATE &&
                        invalidSize.getWidth() <= MAX_ROUNDING_WIDTH) {
                    ImageReader testReader = ImageReader.newInstance(
                                                                     invalidSize.getWidth(),
                                                                     invalidSize.getHeight(),
                                                                     format,
                                                                     1);
                    Surface testSurface = testReader.getSurface();

                    assertTrue(
                               String.format(""isOutputSupportedFor fails for config %s, %d"",
                                       invalidSize.toString(), format),
                               config.isOutputSupportedFor(testSurface));

                    testReader.close();
                }
            } // formats

            // Cross-check opaque format and sizes
            if (arrayContains(actualCapabilities, BC)) {
                SurfaceTexture st = new SurfaceTexture(1);
                Surface surf = new Surface(st);

                Size[] opaqueSizes = CameraTestUtils.getSupportedSizeForClass(SurfaceTexture.class,
                        mAllCameraIds[i], mCameraManager);
                assertTrue(""Opaque format has no sizes listed"",
                        opaqueSizes.length > 0);
                for (Size size : opaqueSizes) {
                    long stallDuration = config.getOutputStallDuration(SurfaceTexture.class, size);
                    assertTrue(""Opaque output may not have a non-zero stall duration"",
                            stallDuration == 0);

                    long minDuration = config.getOutputMinFrameDuration(SurfaceTexture.class, size);
                    if (arrayContains(actualCapabilities,
                                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
                        assertTrue(""MANUAL_SENSOR capability, need positive min frame duration for""
                                + ""opaque format"",
                                minDuration > 0);
                    } else {
                        assertTrue(""Need non-negative min frame duration for opaque format "",
                                minDuration >= 0);
                    }
                    st.setDefaultBufferSize(size.getWidth(), size.getHeight());

                    assertTrue(
                            String.format(""isOutputSupportedFor fails for SurfaceTexture config %s"",
                                    size.toString()),
                            config.isOutputSupportedFor(surf));

                } // opaque sizes

                // Try invalid opaque size, should get rounded
                Size invalidSize = findInvalidSize(opaqueSizes);
                st.setDefaultBufferSize(invalidSize.getWidth(), invalidSize.getHeight());
                assertTrue(
                        String.format(""isOutputSupportedFor fails for SurfaceTexture config %s"",
                                invalidSize.toString()),
                        config.isOutputSupportedFor(surf));

            }
        } // mCharacteristics
    }

    /**
     * Test high speed capability and cross-check the high speed sizes and fps ranges from
     * the StreamConfigurationMap.
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.its.ItsService"	"doCheckSensorExistence"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/its/ItsService.java"	""	"public void test/*
 *.
 */

package com.android.cts.verifier.camera.its;

import android.app.Notification;
import android.app.NotificationChannel;
import android.app.NotificationManager;
import android.app.Service;
import android.content.Context;
import android.content.Intent;
import android.content.pm.ServiceInfo;
import android.graphics.ImageFormat;
import android.graphics.Rect;
import android.hardware.SensorPrivacyManager;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CaptureFailure;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.DngCreator;
import android.hardware.camera2.TotalCaptureResult;
import android.hardware.camera2.cts.PerformanceTest;
import android.hardware.camera2.params.InputConfiguration;
import android.hardware.camera2.params.MeteringRectangle;
import android.hardware.camera2.params.OutputConfiguration;
import android.hardware.camera2.params.SessionConfiguration;
import android.hardware.Sensor;
import android.hardware.SensorEvent;
import android.hardware.SensorEventListener;
import android.hardware.SensorManager;
import android.media.AudioAttributes;
import android.media.Image;
import android.media.ImageReader;
import android.media.ImageWriter;
import android.media.Image.Plane;
import android.net.Uri;
import android.os.Build;
import android.os.Bundle;
import android.os.ConditionVariable;
import android.os.Handler;
import android.os.HandlerThread;
import android.os.IBinder;
import android.os.Message;
import android.os.SystemClock;
import android.os.Vibrator;
import android.util.Log;
import android.util.Rational;
import android.util.Size;
import android.util.SparseArray;
import android.view.Surface;

import androidx.test.InstrumentationRegistry;

import com.android.ex.camera2.blocking.BlockingCameraManager;
import com.android.ex.camera2.blocking.BlockingCameraManager.BlockingOpenException;
import com.android.ex.camera2.blocking.BlockingStateCallback;
import com.android.ex.camera2.blocking.BlockingSessionCallback;

import com.android.compatibility.common.util.ReportLog.Metric;
import com.android.cts.verifier.camera.its.StatsImage;
import com.android.cts.verifier.camera.performance.CameraTestInstrumentation;
import com.android.cts.verifier.camera.performance.CameraTestInstrumentation.MetricListener;
import com.android.cts.verifier.R;

import org.json.JSONArray;
import org.json.JSONObject;
import org.junit.runner.JUnitCore;
import org.junit.runner.Request;
import org.junit.runner.Result;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.io.PrintWriter;
import java.math.BigInteger;
import java.net.ServerSocket;
import java.net.Socket;
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.nio.FloatBuffer;
import java.nio.charset.Charset;
import java.security.MessageDigest;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Executor;
import java.util.concurrent.LinkedBlockingDeque;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.Semaphore;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;

public class ItsService extends Service implements SensorEventListener {
    public static final String TAG = ItsService.class.getSimpleName();

    // Version number to keep host/server communication in sync
    // This string must be in sync with python side device.py
    // Updated when interface between script and ItsService is changed
    private final String ITS_SERVICE_VERSION = ""1.0"";

    private final int SERVICE_NOTIFICATION_ID = 37; // random int that is unique within app
    private NotificationChannel mChannel;

    // Timeouts, in seconds.
    private static final int TIMEOUT_CALLBACK = 20;
    private static final int TIMEOUT_3A = 10;

    // Time given for background requests to warm up pipeline
    private static final long PIPELINE_WARMUP_TIME_MS = 2000;

    // State transition timeouts, in ms.
    private static final long TIMEOUT_IDLE_MS = 2000;
    private static final long TIMEOUT_STATE_MS = 500;
    private static final long TIMEOUT_SESSION_CLOSE = 3000;

    // Timeout to wait for a capture result after the capture buffer has arrived, in ms.
    private static final long TIMEOUT_CAP_RES = 2000;

    private static final int MAX_CONCURRENT_READER_BUFFERS = 10;

    // Supports at most RAW+YUV+JPEG, one surface each, plus optional background stream
    private static final int MAX_NUM_OUTPUT_SURFACES = 4;

    // Performance class R version number
    private static final int PERFORMANCE_CLASS_R = Build.VERSION_CODES.R;
    // Performance class S version number
    private static final int PERFORMANCE_CLASS_S = Build.VERSION_CODES.R + 1;

    public static final int SERVERPORT = 6000;

    public static final String REGION_KEY = ""regions"";
    public static final String REGION_AE_KEY = ""ae"";
    public static final String REGION_AWB_KEY = ""awb"";
    public static final String REGION_AF_KEY = ""af"";
    public static final String LOCK_AE_KEY = ""aeLock"";
    public static final String LOCK_AWB_KEY = ""awbLock"";
    public static final String TRIGGER_KEY = ""triggers"";
    public static final String PHYSICAL_ID_KEY = ""physicalId"";
    public static final String TRIGGER_AE_KEY = ""ae"";
    public static final String TRIGGER_AF_KEY = ""af"";
    public static final String VIB_PATTERN_KEY = ""pattern"";
    public static final String EVCOMP_KEY = ""evComp"";
    public static final String AUDIO_RESTRICTION_MODE_KEY = ""mode"";

    private CameraManager mCameraManager = null;
    private HandlerThread mCameraThread = null;
    private Handler mCameraHandler = null;
    private BlockingCameraManager mBlockingCameraManager = null;
    private BlockingStateCallback mCameraListener = null;
    private CameraDevice mCamera = null;
    private CameraCaptureSession mSession = null;
    private ImageReader[] mOutputImageReaders = null;
    private SparseArray<String> mPhysicalStreamMap = new SparseArray<String>();
    private ImageReader mInputImageReader = null;
    private CameraCharacteristics mCameraCharacteristics = null;
    private HashMap<String, CameraCharacteristics> mPhysicalCameraChars =
            new HashMap<String, CameraCharacteristics>();
    private ItsUtils.ItsCameraIdList mItsCameraIdList = null;

    private Vibrator mVibrator = null;

    private HandlerThread mSaveThreads[] = new HandlerThread[MAX_NUM_OUTPUT_SURFACES];
    private Handler mSaveHandlers[] = new Handler[MAX_NUM_OUTPUT_SURFACES];
    private HandlerThread mResultThread = null;
    private Handler mResultHandler = null;

    private volatile boolean mThreadExitFlag = false;

    private volatile ServerSocket mSocket = null;
    private volatile SocketRunnable mSocketRunnableObj = null;
    private Semaphore mSocketQueueQuota = null;
    private int mMemoryQuota = -1;
    private LinkedList<Integer> mInflightImageSizes = new LinkedList<>();
    private volatile BlockingQueue<ByteBuffer> mSocketWriteQueue =
            new LinkedBlockingDeque<ByteBuffer>();
    private final Object mSocketWriteEnqueueLock = new Object();
    private final Object mSocketWriteDrainLock = new Object();

    private volatile BlockingQueue<Object[]> mSerializerQueue =
            new LinkedBlockingDeque<Object[]>();

    private AtomicInteger mCountCallbacksRemaining = new AtomicInteger();
    private AtomicInteger mCountRawOrDng = new AtomicInteger();
    private AtomicInteger mCountRaw10 = new AtomicInteger();
    private AtomicInteger mCountRaw12 = new AtomicInteger();
    private AtomicInteger mCountJpg = new AtomicInteger();
    private AtomicInteger mCountYuv = new AtomicInteger();
    private AtomicInteger mCountCapRes = new AtomicInteger();
    private boolean mCaptureRawIsDng;
    private boolean mCaptureRawIsStats;
    private int mCaptureStatsGridWidth;
    private int mCaptureStatsGridHeight;
    private CaptureResult mCaptureResults[] = null;

    private volatile ConditionVariable mInterlock3A = new ConditionVariable(true);

    final Object m3AStateLock = new Object();
    private volatile boolean mConvergedAE = false;
    private volatile boolean mConvergedAF = false;
    private volatile boolean mConvergedAWB = false;
    private volatile boolean mLockedAE = false;
    private volatile boolean mLockedAWB = false;
    private volatile boolean mNeedsLockedAE = false;
    private volatile boolean mNeedsLockedAWB = false;

    class MySensorEvent {
        public Sensor sensor;
        public int accuracy;
        public long timestamp;
        public float values[];
    }

    // For capturing motion sensor traces.
    private SensorManager mSensorManager = null;
    private Sensor mAccelSensor = null;
    private Sensor mMagSensor = null;
    private Sensor mGyroSensor = null;
    private volatile LinkedList<MySensorEvent> mEvents = null;
    private volatile Object mEventLock = new Object();
    private volatile boolean mEventsEnabled = false;
    private HandlerThread mSensorThread = null;
    private Handler mSensorHandler = null;

    private SensorPrivacyManager mSensorPrivacyManager;

    // Camera test instrumentation
    private CameraTestInstrumentation mCameraInstrumentation;
    // Camera PerformanceTest metric
    private final ArrayList<Metric> mResults = new ArrayList<Metric>();

    private static final int SERIALIZER_SURFACES_ID = 2;
    private static final int SERIALIZER_PHYSICAL_METADATA_ID = 3;

    public interface CaptureCallback {
        void onCaptureAvailable(Image capture, String physicalCameraId);
    }

    public abstract class CaptureResultListener extends CameraCaptureSession.CaptureCallback {}

    @Override
    public IBinder onBind(Intent intent) {
        return null;
    }

    @Override
    public void onCreate() {
        try {
            mThreadExitFlag = false;

            // Get handle to camera manager.
            mCameraManager = (CameraManager) this.getSystemService(Context.CAMERA_SERVICE);
            if (mCameraManager == null) {
                throw new ItsException(""Failed to connect to camera manager"");
            }
            mBlockingCameraManager = new BlockingCameraManager(mCameraManager);
            mCameraListener = new BlockingStateCallback();

            // Register for motion events.
            mEvents = new LinkedList<MySensorEvent>();
            mSensorManager = (SensorManager)getSystemService(Context.SENSOR_SERVICE);
            mAccelSensor = mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER);
            mMagSensor = mSensorManager.getDefaultSensor(Sensor.TYPE_MAGNETIC_FIELD);
            mGyroSensor = mSensorManager.getDefaultSensor(Sensor.TYPE_GYROSCOPE);
            mSensorThread = new HandlerThread(""SensorThread"");
            mSensorThread.start();
            mSensorHandler = new Handler(mSensorThread.getLooper());
            mSensorManager.registerListener(this, mAccelSensor,
                    /*100hz*/ 10000, mSensorHandler);
            mSensorManager.registerListener(this, mMagSensor,
                    SensorManager.SENSOR_DELAY_NORMAL, mSensorHandler);
            mSensorManager.registerListener(this, mGyroSensor,
                    /*200hz*/5000, mSensorHandler);

            // Get a handle to the system vibrator.
            mVibrator = (Vibrator)getSystemService(Context.VIBRATOR_SERVICE);

            // Create threads to receive images and save them.
            for (int i = 0; i < MAX_NUM_OUTPUT_SURFACES; i++) {
                mSaveThreads[i] = new HandlerThread(""SaveThread"" + i);
                mSaveThreads[i].start();
                mSaveHandlers[i] = new Handler(mSaveThreads[i].getLooper());
            }

            // Create a thread to handle object serialization.
            (new Thread(new SerializerRunnable())).start();;

            // Create a thread to receive capture results and process them.
            mResultThread = new HandlerThread(""ResultThread"");
            mResultThread.start();
            mResultHandler = new Handler(mResultThread.getLooper());

            // Create a thread for the camera device.
            mCameraThread = new HandlerThread(""ItsCameraThread"");
            mCameraThread.start();
            mCameraHandler = new Handler(mCameraThread.getLooper());

            // Create a thread to process commands, listening on a TCP socket.
            mSocketRunnableObj = new SocketRunnable();
            (new Thread(mSocketRunnableObj)).start();
        } catch (ItsException e) {
            Logt.e(TAG, ""Service failed to start: "", e);
        }

        NotificationManager notificationManager =
                (NotificationManager) getSystemService(Context.NOTIFICATION_SERVICE);
        mChannel = new NotificationChannel(
                ""ItsServiceChannel"", ""ItsService"", NotificationManager.IMPORTANCE_LOW);
        // Configure the notification channel.
        mChannel.setDescription(""ItsServiceChannel"");
        mChannel.enableVibration(false);
        notificationManager.createNotificationChannel(mChannel);

        mSensorPrivacyManager = getSystemService(SensorPrivacyManager.class);
    }

    @Override
    public int onStartCommand(Intent intent, int flags, int startId) {
        try {
            // Just log a message indicating that the service is running and is able to accept
            // socket connections.
            while (!mThreadExitFlag && mSocket==null) {
                Thread.sleep(1);
            }
            if (!mThreadExitFlag){
                Logt.i(TAG, ""ItsService ready"");
            } else {
                Logt.e(TAG, ""Starting ItsService in bad state"");
            }

            Notification notification = new Notification.Builder(this, mChannel.getId())
                    .setContentTitle(""CameraITS Service"")
                    .setContentText(""CameraITS Service is running"")
                    .setSmallIcon(R.drawable.icon)
                    .setOngoing(true).build();
            startForeground(SERVICE_NOTIFICATION_ID, notification,
                    ServiceInfo.FOREGROUND_SERVICE_TYPE_CAMERA);
        } catch (java.lang.InterruptedException e) {
            Logt.e(TAG, ""Error starting ItsService (interrupted)"", e);
        }
        return START_STICKY;
    }

    @Override
    public void onDestroy() {
        mThreadExitFlag = true;
        for (int i = 0; i < MAX_NUM_OUTPUT_SURFACES; i++) {
            if (mSaveThreads[i] != null) {
                mSaveThreads[i].quit();
                mSaveThreads[i] = null;
            }
        }
        if (mSensorThread != null) {
            mSensorThread.quitSafely();
            mSensorThread = null;
        }
        if (mResultThread != null) {
            mResultThread.quitSafely();
            mResultThread = null;
        }
        if (mCameraThread != null) {
            mCameraThread.quitSafely();
            mCameraThread = null;
        }
    }

    public void openCameraDevice(String cameraId) throws ItsException {
        Logt.i(TAG, String.format(""Opening camera %s"", cameraId));

        try {
            if (mMemoryQuota == -1) {
                // Initialize memory quota on this device
                if (mItsCameraIdList == null) {
                    mItsCameraIdList = ItsUtils.getItsCompatibleCameraIds(mCameraManager);
                }
                if (mItsCameraIdList.mCameraIds.size() == 0) {
                    throw new ItsException(""No camera devices"");
                }
                for (String camId : mItsCameraIdList.mCameraIds) {
                    CameraCharacteristics chars =  mCameraManager.getCameraCharacteristics(camId);
                    Size maxYuvSize = ItsUtils.getMaxOutputSize(
                            chars, ImageFormat.YUV_420_888);
                    // 4 bytes per pixel for RGBA8888 Bitmap and at least 3 Bitmaps per CDD
                    int quota = maxYuvSize.getWidth() * maxYuvSize.getHeight() * 4 * 3;
                    if (quota > mMemoryQuota) {
                        mMemoryQuota = quota;
                    }
                }
            }
        } catch (CameraAccessException e) {
            throw new ItsException(""Failed to get device ID list"", e);
        }

        try {
            mCamera = mBlockingCameraManager.openCamera(cameraId, mCameraListener, mCameraHandler);
            mCameraCharacteristics = mCameraManager.getCameraCharacteristics(cameraId);

            boolean isLogicalCamera = hasCapability(
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_LOGICAL_MULTI_CAMERA);
            if (isLogicalCamera) {
                Set<String> physicalCameraIds = mCameraCharacteristics.getPhysicalCameraIds();
                for (String id : physicalCameraIds) {
                    mPhysicalCameraChars.put(id, mCameraManager.getCameraCharacteristics(id));
                }
            }
            mSocketQueueQuota = new Semaphore(mMemoryQuota, true);
        } catch (CameraAccessException e) {
            throw new ItsException(""Failed to open camera"", e);
        } catch (BlockingOpenException e) {
            throw new ItsException(""Failed to open camera (after blocking)"", e);
        }
        mSocketRunnableObj.sendResponse(""cameraOpened"", """");
    }

    public void closeCameraDevice() throws ItsException {
        try {
            if (mCamera != null) {
                Logt.i(TAG, ""Closing camera"");
                mCamera.close();
                mCamera = null;
            }
        } catch (Exception e) {
            throw new ItsException(""Failed to close device"");
        }
        mSocketRunnableObj.sendResponse(""cameraClosed"", """");
    }

    class SerializerRunnable implements Runnable {
        // Use a separate thread to perform JSON serialization (since this can be slow due to
        // the reflection).
        @Override
        public void run() {
            Logt.i(TAG, ""Serializer thread starting"");
            while (! mThreadExitFlag) {
                try {
                    Object objs[] = mSerializerQueue.take();
                    JSONObject jsonObj = new JSONObject();
                    String tag = null;
                    for (int i = 0; i < objs.length; i++) {
                        Object obj = objs[i];
                        if (obj instanceof String) {
                            if (tag != null) {
                                throw new ItsException(""Multiple tags for socket response"");
                            }
                            tag = (String)obj;
                        } else if (obj instanceof CameraCharacteristics) {
                            jsonObj.put(""cameraProperties"", ItsSerializer.serialize(
                                    (CameraCharacteristics)obj));
                        } else if (obj instanceof CaptureRequest) {
                            jsonObj.put(""captureRequest"", ItsSerializer.serialize(
                                    (CaptureRequest)obj));
                        } else if (obj instanceof CaptureResult) {
                            jsonObj.put(""captureResult"", ItsSerializer.serialize(
                                    (CaptureResult)obj));
                        } else if (obj instanceof JSONArray) {
                            if (tag == ""captureResults"") {
                                if (i == SERIALIZER_SURFACES_ID) {
                                    jsonObj.put(""outputs"", (JSONArray)obj);
                                } else if (i == SERIALIZER_PHYSICAL_METADATA_ID) {
                                    jsonObj.put(""physicalResults"", (JSONArray)obj);
                                } else {
                                    throw new ItsException(
                                            ""Unsupported JSONArray for captureResults"");
                                }
                            } else {
                                jsonObj.put(""outputs"", (JSONArray)obj);
                            }
                        } else {
                            throw new ItsException(""Invalid object received for serialization"");
                        }
                    }
                    if (tag == null) {
                        throw new ItsException(""No tag provided for socket response"");
                    }
                    mSocketRunnableObj.sendResponse(tag, null, jsonObj, null);
                    Logt.i(TAG, String.format(""Serialized %s"", tag));
                } catch (org.json.JSONException e) {
                    Logt.e(TAG, ""Error serializing object"", e);
                    break;
                } catch (ItsException e) {
                    Logt.e(TAG, ""Error serializing object"", e);
                    break;
                } catch (java.lang.InterruptedException e) {
                    Logt.e(TAG, ""Error serializing object (interrupted)"", e);
                    break;
                }
            }
            Logt.i(TAG, ""Serializer thread terminated"");
        }
    }

    class SocketWriteRunnable implements Runnable {

        // Use a separate thread to service a queue of objects to be written to the socket,
        // writing each sequentially in order. This is needed since different handler functions
        // (called on different threads) will need to send data back to the host script.

        public Socket mOpenSocket = null;
        private Thread mThread = null;

        public SocketWriteRunnable(Socket openSocket) {
            mOpenSocket = openSocket;
        }

        public void setOpenSocket(Socket openSocket) {
            mOpenSocket = openSocket;
        }

        @Override
        public void run() {
            Logt.i(TAG, ""Socket writer thread starting"");
            while (true) {
                try {
                    ByteBuffer b = mSocketWriteQueue.take();
                    synchronized(mSocketWriteDrainLock) {
                        if (mOpenSocket == null) {
                            Logt.e(TAG, ""No open socket connection!"");
                            continue;
                        }
                        if (b.hasArray()) {
                            mOpenSocket.getOutputStream().write(b.array(), 0, b.capacity());
                        } else {
                            byte[] barray = new byte[b.capacity()];
                            b.get(barray);
                            mOpenSocket.getOutputStream().write(barray);
                        }
                        mOpenSocket.getOutputStream().flush();
                        Logt.i(TAG, String.format(""Wrote to socket: %d bytes"", b.capacity()));
                        Integer imgBufSize = mInflightImageSizes.peek();
                        if (imgBufSize != null && imgBufSize == b.capacity()) {
                            mInflightImageSizes.removeFirst();
                            if (mSocketQueueQuota != null) {
                                mSocketQueueQuota.release(imgBufSize);
                            }
                        }
                    }
                } catch (IOException e) {
                    Logt.e(TAG, ""Error writing to socket"", e);
                    mOpenSocket = null;
                    break;
                } catch (java.lang.InterruptedException e) {
                    Logt.e(TAG, ""Error writing to socket (interrupted)"", e);
                    mOpenSocket = null;
                    break;
                }
            }
            Logt.i(TAG, ""Socket writer thread terminated"");
        }

        public synchronized void checkAndStartThread() {
            if (mThread == null || mThread.getState() == Thread.State.TERMINATED) {
                mThread = new Thread(this);
            }
            if (mThread.getState() == Thread.State.NEW) {
                mThread.start();
            }
        }

    }

    class SocketRunnable implements Runnable {

        // Format of sent messages (over the socket):
        // * Serialized JSON object on a single line (newline-terminated)
        // * For byte buffers, the binary data then follows
        //
        // Format of received messages (from the socket):
        // * Serialized JSON object on a single line (newline-terminated)

        private Socket mOpenSocket = null;
        private SocketWriteRunnable mSocketWriteRunnable = null;

        @Override
        public void run() {
            Logt.i(TAG, ""Socket thread starting"");
            try {
                mSocket = new ServerSocket(SERVERPORT);
            } catch (IOException e) {
                Logt.e(TAG, ""Failed to create socket"", e);
            }

            // Create a new thread to handle writes to this socket.
            mSocketWriteRunnable = new SocketWriteRunnable(null);

            while (!mThreadExitFlag) {
                // Receive the socket-open request from the host.
                try {
                    Logt.i(TAG, ""Waiting for client to connect to socket"");
                    mOpenSocket = mSocket.accept();
                    if (mOpenSocket == null) {
                        Logt.e(TAG, ""Socket connection error"");
                        break;
                    }
                    mSocketWriteQueue.clear();
                    mInflightImageSizes.clear();
                    mSocketWriteRunnable.setOpenSocket(mOpenSocket);
                    mSocketWriteRunnable.checkAndStartThread();
                    Logt.i(TAG, ""Socket connected"");
                } catch (IOException e) {
                    Logt.e(TAG, ""Socket open error: "", e);
                    break;
                }

                // Process commands over the open socket.
                while (!mThreadExitFlag) {
                    try {
                        BufferedReader input = new BufferedReader(
                                new InputStreamReader(mOpenSocket.getInputStream()));
                        if (input == null) {
                            Logt.e(TAG, ""Failed to get socket input stream"");
                            break;
                        }
                        String line = input.readLine();
                        if (line == null) {
                            Logt.i(TAG, ""Socket readline returned null (host disconnected)"");
                            break;
                        }
                        processSocketCommand(line);
                    } catch (IOException e) {
                        Logt.e(TAG, ""Socket read error: "", e);
                        break;
                    } catch (ItsException e) {
                        Logt.e(TAG, ""Script error: "", e);
                        break;
                    }
                }

                // Close socket and go back to waiting for a new connection.
                try {
                    synchronized(mSocketWriteDrainLock) {
                        mSocketWriteQueue.clear();
                        mInflightImageSizes.clear();
                        mOpenSocket.close();
                        mOpenSocket = null;
                        mSocketWriteRunnable.setOpenSocket(null);
                        Logt.i(TAG, ""Socket disconnected"");
                    }
                } catch (java.io.IOException e) {
                    Logt.e(TAG, ""Exception closing socket"");
                }
            }

            // It's an overall error state if the code gets here; no recevery.
            // Try to do some cleanup, but the service probably needs to be restarted.
            Logt.i(TAG, ""Socket server loop exited"");
            mThreadExitFlag = true;
            try {
                synchronized(mSocketWriteDrainLock) {
                    if (mOpenSocket != null) {
                        mOpenSocket.close();
                        mOpenSocket = null;
                        mSocketWriteRunnable.setOpenSocket(null);
                    }
                }
            } catch (java.io.IOException e) {
                Logt.w(TAG, ""Exception closing socket"");
            }
            try {
                if (mSocket != null) {
                    mSocket.close();
                    mSocket = null;
                }
            } catch (java.io.IOException e) {
                Logt.w(TAG, ""Exception closing socket"");
            }
        }

        public void processSocketCommand(String cmd)
                throws ItsException {
            // Default locale must be set to ""en-us""
            Locale locale = Locale.getDefault();
            if (!Locale.US.equals(locale)) {
                Logt.e(TAG, ""Default language is not set to "" + Locale.US + ""!"");
                stopSelf();
            }

            // Each command is a serialized JSON object.
            try {
                JSONObject cmdObj = new JSONObject(cmd);
                Logt.i(TAG, ""Start processing command"" + cmdObj.getString(""cmdName""));
                if (""open"".equals(cmdObj.getString(""cmdName""))) {
                    String cameraId = cmdObj.getString(""cameraId"");
                    openCameraDevice(cameraId);
                } else if (""close"".equals(cmdObj.getString(""cmdName""))) {
                    closeCameraDevice();
                } else if (""getCameraProperties"".equals(cmdObj.getString(""cmdName""))) {
                    doGetProps();
                } else if (""getCameraPropertiesById"".equals(cmdObj.getString(""cmdName""))) {
                    doGetPropsById(cmdObj);
                } else if (""startSensorEvents"".equals(cmdObj.getString(""cmdName""))) {
                    doStartSensorEvents();
                } else if (""checkSensorExistence"".equals(cmdObj.getString(""cmdName""))) {
                    doCheckSensorExistence();
                } else if (""getSensorEvents"".equals(cmdObj.getString(""cmdName""))) {
                    doGetSensorEvents();
                } else if (""do3A"".equals(cmdObj.getString(""cmdName""))) {
                    do3A(cmdObj);
                } else if (""doCapture"".equals(cmdObj.getString(""cmdName""))) {
                    doCapture(cmdObj);
                } else if (""doVibrate"".equals(cmdObj.getString(""cmdName""))) {
                    doVibrate(cmdObj);
                } else if (""setAudioRestriction"".equals(cmdObj.getString(""cmdName""))) {
                    doSetAudioRestriction(cmdObj);
                } else if (""getCameraIds"".equals(cmdObj.getString(""cmdName""))) {
                    doGetCameraIds();
                } else if (""doReprocessCapture"".equals(cmdObj.getString(""cmdName""))) {
                    doReprocessCapture(cmdObj);
                } else if (""getItsVersion"".equals(cmdObj.getString(""cmdName""))) {
                    mSocketRunnableObj.sendResponse(""ItsVersion"", ITS_SERVICE_VERSION);
                } else if (""isStreamCombinationSupported"".equals(cmdObj.getString(""cmdName""))) {
                    doCheckStreamCombination(cmdObj);
                } else if (""isCameraPrivacyModeSupported"".equals(cmdObj.getString(""cmdName""))) {
                    doCheckCameraPrivacyModeSupport();
                } else if (""isPerformanceClassPrimaryCamera"".equals(cmdObj.getString(""cmdName""))) {
                    String cameraId = cmdObj.getString(""cameraId"");
                    doCheckPerformanceClassPrimaryCamera(cameraId);
                } else if (""measureCameraLaunchMs"".equals(cmdObj.getString(""cmdName""))) {
                    String cameraId = cmdObj.getString(""cameraId"");
                    doMeasureCameraLaunchMs(cameraId);
                } else if (""measureCamera1080pJpegCaptureMs"".equals(cmdObj.getString(""cmdName""))) {
                    String cameraId = cmdObj.getString(""cameraId"");
                    doMeasureCamera1080pJpegCaptureMs(cameraId);
                } else {
                    throw new ItsException(""Unknown command: "" + cmd);
                }
                Logt.i(TAG, ""Finish processing command"" + cmdObj.getString(""cmdName""));
            } catch (org.json.JSONException e) {
                Logt.e(TAG, ""Invalid command: "", e);
            }
        }

        public void sendResponse(String tag, String str, JSONObject obj, ByteBuffer bbuf)
                throws ItsException {
            try {
                JSONObject jsonObj = new JSONObject();
                jsonObj.put(""tag"", tag);
                if (str != null) {
                    jsonObj.put(""strValue"", str);
                }
                if (obj != null) {
                    jsonObj.put(""objValue"", obj);
                }
                if (bbuf != null) {
                    jsonObj.put(""bufValueSize"", bbuf.capacity());
                }
                ByteBuffer bstr = ByteBuffer.wrap(
                        (jsonObj.toString()+""\n"").getBytes(Charset.defaultCharset()));
                synchronized(mSocketWriteEnqueueLock) {
                    if (bstr != null) {
                        mSocketWriteQueue.put(bstr);
                    }
                    if (bbuf != null) {
                        mInflightImageSizes.add(bbuf.capacity());
                        mSocketWriteQueue.put(bbuf);
                    }
                }
            } catch (org.json.JSONException e) {
                throw new ItsException(""JSON error: "", e);
            } catch (java.lang.InterruptedException e) {
                throw new ItsException(""Socket error: "", e);
            }
        }

        public void sendResponse(String tag, String str)
                throws ItsException {
            sendResponse(tag, str, null, null);
        }

        public void sendResponse(String tag, JSONObject obj)
                throws ItsException {
            sendResponse(tag, null, obj, null);
        }

        public void sendResponseCaptureBuffer(String tag, ByteBuffer bbuf)
                throws ItsException {
            sendResponse(tag, null, null, bbuf);
        }

        public void sendResponse(LinkedList<MySensorEvent> events)
                throws ItsException {
            Logt.i(TAG, ""Sending "" + events.size() + "" sensor events"");
            try {
                JSONArray accels = new JSONArray();
                JSONArray mags = new JSONArray();
                JSONArray gyros = new JSONArray();
                for (MySensorEvent event : events) {
                    JSONObject obj = new JSONObject();
                    obj.put(""time"", event.timestamp);
                    obj.put(""x"", event.values[0]);
                    obj.put(""y"", event.values[1]);
                    obj.put(""z"", event.values[2]);
                    if (event.sensor.getType() == Sensor.TYPE_ACCELEROMETER) {
                        accels.put(obj);
                    } else if (event.sensor.getType() == Sensor.TYPE_MAGNETIC_FIELD) {
                        mags.put(obj);
                    } else if (event.sensor.getType() == Sensor.TYPE_GYROSCOPE) {
                        gyros.put(obj);
                    }
                }
                JSONObject obj = new JSONObject();
                obj.put(""accel"", accels);
                obj.put(""mag"", mags);
                obj.put(""gyro"", gyros);
                sendResponse(""sensorEvents"", null, obj, null);
            } catch (org.json.JSONException e) {
                throw new ItsException(""JSON error: "", e);
            }
            Logt.i(TAG, ""Sent sensor events"");
        }

        public void sendResponse(CameraCharacteristics props)
                throws ItsException {
            try {
                Object objs[] = new Object[2];
                objs[0] = ""cameraProperties"";
                objs[1] = props;
                mSerializerQueue.put(objs);
            } catch (InterruptedException e) {
                throw new ItsException(""Interrupted: "", e);
            }
        }

        public void sendResponseCaptureResult(CameraCharacteristics props,
                                              CaptureRequest request,
                                              TotalCaptureResult result,
                                              ImageReader[] readers)
                throws ItsException {
            try {
                JSONArray jsonSurfaces = new JSONArray();
                for (int i = 0; i < readers.length; i++) {
                    JSONObject jsonSurface = new JSONObject();
                    jsonSurface.put(""width"", readers[i].getWidth());
                    jsonSurface.put(""height"", readers[i].getHeight());
                    int format = readers[i].getImageFormat();
                    if (format == ImageFormat.RAW_SENSOR) {
                        if (mCaptureRawIsStats) {
                            int aaw = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .width();
                            int aah = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .height();
                            jsonSurface.put(""format"", ""rawStats"");
                            jsonSurface.put(""width"", aaw/mCaptureStatsGridWidth);
                            jsonSurface.put(""height"", aah/mCaptureStatsGridHeight);
                        } else if (mCaptureRawIsDng) {
                            jsonSurface.put(""format"", ""dng"");
                        } else {
                            jsonSurface.put(""format"", ""raw"");
                        }
                    } else if (format == ImageFormat.RAW10) {
                        jsonSurface.put(""format"", ""raw10"");
                    } else if (format == ImageFormat.RAW12) {
                        jsonSurface.put(""format"", ""raw12"");
                    } else if (format == ImageFormat.JPEG) {
                        jsonSurface.put(""format"", ""jpeg"");
                    } else if (format == ImageFormat.YUV_420_888) {
                        jsonSurface.put(""format"", ""yuv"");
                    } else if (format == ImageFormat.Y8) {
                        jsonSurface.put(""format"", ""y8"");
                    } else {
                        throw new ItsException(""Invalid format"");
                    }
                    jsonSurfaces.put(jsonSurface);
                }

                Map<String, CaptureResult> physicalMetadata =
                        result.getPhysicalCameraResults();
                JSONArray jsonPhysicalMetadata = new JSONArray();
                for (Map.Entry<String, CaptureResult> pair : physicalMetadata.entrySet()) {
                    JSONObject jsonOneMetadata = new JSONObject();
                    jsonOneMetadata.put(pair.getKey(), ItsSerializer.serialize(pair.getValue()));
                    jsonPhysicalMetadata.put(jsonOneMetadata);
                }
                Object objs[] = new Object[4];
                objs[0] = ""captureResults"";
                objs[1] = result;
                objs[SERIALIZER_SURFACES_ID] = jsonSurfaces;
                objs[SERIALIZER_PHYSICAL_METADATA_ID] = jsonPhysicalMetadata;
                mSerializerQueue.put(objs);
            } catch (org.json.JSONException e) {
                throw new ItsException(""JSON error: "", e);
            } catch (InterruptedException e) {
                throw new ItsException(""Interrupted: "", e);
            }
        }
    }

    public ImageReader.OnImageAvailableListener
            createAvailableListener(final CaptureCallback listener) {
        return new ImageReader.OnImageAvailableListener() {
            @Override
            public void onImageAvailable(ImageReader reader) {
                Image i = null;
                try {
                    i = reader.acquireNextImage();
                    String physicalCameraId = new String();
                    for (int idx = 0; idx < mOutputImageReaders.length; idx++) {
                        if (mOutputImageReaders[idx] == reader) {
                            physicalCameraId = mPhysicalStreamMap.get(idx);
                        }
                    }
                    listener.onCaptureAvailable(i, physicalCameraId);
                } finally {
                    if (i != null) {
                        i.close();
                    }
                }
            }
        };
    }

    private ImageReader.OnImageAvailableListener
            createAvailableListenerDropper() {
        return new ImageReader.OnImageAvailableListener() {
            @Override
            public void onImageAvailable(ImageReader reader) {
                Image i = reader.acquireNextImage();
                i.close();
            }
        };
    }

    private void doStartSensorEvents() throws ItsException {
        synchronized(mEventLock) {
            mEventsEnabled = true;
        }
        mSocketRunnableObj.sendResponse(""sensorEventsStarted"", """");
    }

    private void doCheckSensorExistence() throws ItsException {
        try {
            JSONObject obj = new JSONObject();
            obj.put(""accel"", mAccelSensor != null);
            obj.put(""mag"", mMagSensor != null);
            obj.put(""gyro"", mGyroSensor != null);
            obj.put(""vibrator"", mVibrator.hasVibrator());
            mSocketRunnableObj.sendResponse(""sensorExistence"", null, obj, null);
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        }
    }

    private void doGetSensorEvents() throws ItsException {
        synchronized(mEventLock) {
            mSocketRunnableObj.sendResponse(mEvents);
            mEvents.clear();
            mEventsEnabled = false;
        }
    }

    private void doGetProps() throws ItsException {
        mSocketRunnableObj.sendResponse(mCameraCharacteristics);
    }

    private void doGetPropsById(JSONObject params) throws ItsException {
        String[] devices;
        try {
            // Intentionally not using ItsUtils.getItsCompatibleCameraIds here so it's possible to
            // write some simple script to query camera characteristics even for devices exempted
            // from ITS today.
            devices = mCameraManager.getCameraIdList();
            if (devices == null || devices.length == 0) {
                throw new ItsException(""No camera devices"");
            }
        } catch (CameraAccessException e) {
            throw new ItsException(""Failed to get device ID list"", e);
        }

        try {
            String cameraId = params.getString(""cameraId"");
            CameraCharacteristics characteristics =
                    mCameraManager.getCameraCharacteristics(cameraId);
            mSocketRunnableObj.sendResponse(characteristics);
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        } catch (IllegalArgumentException e) {
            throw new ItsException(""Illegal argument error:"", e);
        } catch (CameraAccessException e) {
            throw new ItsException(""Access error: "", e);
        }
    }

    private void doGetCameraIds() throws ItsException {
        if (mItsCameraIdList == null) {
            mItsCameraIdList = ItsUtils.getItsCompatibleCameraIds(mCameraManager);
        }
        if (mItsCameraIdList.mCameraIdCombos.size() == 0) {
            throw new ItsException(""No camera devices"");
        }

        try {
            JSONObject obj = new JSONObject();
            JSONArray array = new JSONArray();
            for (String id : mItsCameraIdList.mCameraIdCombos) {
                array.put(id);
            }
            obj.put(""cameraIdArray"", array);
            mSocketRunnableObj.sendResponse(""cameraIds"", obj);
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        }
    }

    private static class HandlerExecutor implements Executor {
        private final Handler mHandler;

        public HandlerExecutor(Handler handler) {
            mHandler = handler;
        }

        @Override
        public void execute(Runnable runCmd) {
            mHandler.post(runCmd);
        }
    }

    private void doCheckStreamCombination(JSONObject params) throws ItsException {
        try {
            JSONObject obj = new JSONObject();
            JSONArray jsonOutputSpecs = ItsUtils.getOutputSpecs(params);
            prepareImageReadersWithOutputSpecs(jsonOutputSpecs, /*inputSize*/null,
                    /*inputFormat*/0, /*maxInputBuffers*/0, /*backgroundRequest*/false);
            int numSurfaces = mOutputImageReaders.length;
            List<OutputConfiguration> outputConfigs =
                    new ArrayList<OutputConfiguration>(numSurfaces);
            for (int i = 0; i < numSurfaces; i++) {
                OutputConfiguration config = new OutputConfiguration(
                        mOutputImageReaders[i].getSurface());
                if (mPhysicalStreamMap.get(i) != null) {
                    config.setPhysicalCameraId(mPhysicalStreamMap.get(i));
                }
                outputConfigs.add(config);
            }

            BlockingSessionCallback sessionListener = new BlockingSessionCallback();
            SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_REGULAR, outputConfigs,
                new HandlerExecutor(mCameraHandler), sessionListener);
            boolean supported = mCamera.isSessionConfigurationSupported(sessionConfig);

            String supportString = supported ? ""supportedCombination"" : ""unsupportedCombination"";
            mSocketRunnableObj.sendResponse(""streamCombinationSupport"", supportString);

        } catch (UnsupportedOperationException e) {
            mSocketRunnableObj.sendResponse(""streamCombinationSupport"", ""unsupportedOperation"");
        } catch (IllegalArgumentException e) {
            throw new ItsException(""Error checking stream combination"", e);
        } catch (CameraAccessException e) {
            throw new ItsException(""Error checking stream combination"", e);
        }
    }

    private void doCheckCameraPrivacyModeSupport() throws ItsException {
        boolean hasPrivacySupport = mSensorPrivacyManager
                .supportsSensorToggle(SensorPrivacyManager.Sensors.CAMERA);
        mSocketRunnableObj.sendResponse(""cameraPrivacyModeSupport"",
                hasPrivacySupport ? ""true"" : ""false"");
    }

    private void doCheckPerformanceClassPrimaryCamera(String cameraId) throws ItsException {
        boolean  isPerfClass = (Build.VERSION.MEDIA_PERFORMANCE_CLASS == PERFORMANCE_CLASS_S
                || Build.VERSION.MEDIA_PERFORMANCE_CLASS == PERFORMANCE_CLASS_R);

        if (mItsCameraIdList == null) {
            mItsCameraIdList = ItsUtils.getItsCompatibleCameraIds(mCameraManager);
        }
        if (mItsCameraIdList.mCameraIds.size() == 0) {
            throw new ItsException(""No camera devices"");
        }
        if (!mItsCameraIdList.mCameraIds.contains(cameraId)) {
            throw new ItsException(""Invalid cameraId "" + cameraId);
        }

        boolean isPrimaryCamera = false;
        try {
            CameraCharacteristics c = mCameraManager.getCameraCharacteristics(cameraId);
            Integer cameraFacing = c.get(CameraCharacteristics.LENS_FACING);
            for (String id : mItsCameraIdList.mCameraIds) {
                c = mCameraManager.getCameraCharacteristics(id);
                Integer facing = c.get(CameraCharacteristics.LENS_FACING);
                if (cameraFacing.equals(facing)) {
                    if (cameraId.equals(id)) {
                        isPrimaryCamera = true;
                    } else {
                        isPrimaryCamera = false;
                    }
                    break;
                }
            }
        } catch (CameraAccessException e) {
            throw new ItsException(""Failed to get camera characteristics"", e);
        }

        mSocketRunnableObj.sendResponse(""performanceClassPrimaryCamera"",
                (isPerfClass && isPrimaryCamera) ? ""true"" : ""false"");
    }

    private double invokeCameraPerformanceTest(Class testClass, String testName,
            String cameraId, String metricName) throws ItsException {
        mResults.clear();
        mCameraInstrumentation = new CameraTestInstrumentation();
        MetricListener metricListener = new MetricListener() {
            @Override
            public void onResultMetric(Metric metric) {
                mResults.add(metric);
            }
        };
        mCameraInstrumentation.initialize(this, metricListener);

        Bundle bundle = new Bundle();
        bundle.putString(""camera-id"", cameraId);
        bundle.putString(""perf-measure"", ""on"");
        bundle.putString(""perf-class-test"", ""on"");
        InstrumentationRegistry.registerInstance(mCameraInstrumentation, bundle);

        JUnitCore testRunner = new JUnitCore();
        Log.v(TAG, String.format(""Execute Test: %s#%s"", testClass.getSimpleName(), testName));
        Request request = Request.method(testClass, testName);
        Result runResult = testRunner.run(request);
        if (!runResult.wasSuccessful()) {
            throw new ItsException(""Camera PerformanceTest "" + testClass.getSimpleName() +
                    ""#"" + testName + "" failed"");
        }

        for (Metric m : mResults) {
            if (m.getMessage().equals(metricName) && m.getValues().length == 1) {
                return m.getValues()[0];
            }
        }

        throw new ItsException(""Failed to look up "" + metricName +
                "" in Camera PerformanceTest result!"");
    }

    private void doMeasureCameraLaunchMs(String cameraId) throws ItsException {
        double launchMs = invokeCameraPerformanceTest(PerformanceTest.class,
                ""testCameraLaunch"", cameraId, ""camera_launch_average_time_for_all_cameras"");
        mSocketRunnableObj.sendResponse(""cameraLaunchMs"", Double.toString(launchMs));
    }

    private void doMeasureCamera1080pJpegCaptureMs(String cameraId) throws ItsException {
        double jpegCaptureMs = invokeCameraPerformanceTest(PerformanceTest.class,
                ""testSingleCapture"", cameraId,
                ""camera_capture_average_latency_for_all_cameras_jpeg"");
        mSocketRunnableObj.sendResponse(""camera1080pJpegCaptureMs"", Double.toString(jpegCaptureMs));
    }

    private void prepareImageReaders(Size[] outputSizes, int[] outputFormats, Size inputSize,
            int inputFormat, int maxInputBuffers) {
        closeImageReaders();
        mOutputImageReaders = new ImageReader[outputSizes.length];
        for (int i = 0; i < outputSizes.length; i++) {
            // Check if the output image reader can be shared with the input image reader.
            if (outputSizes[i].equals(inputSize) && outputFormats[i] == inputFormat) {
                mOutputImageReaders[i] = ImageReader.newInstance(outputSizes[i].getWidth(),
                        outputSizes[i].getHeight(), outputFormats[i],
                        MAX_CONCURRENT_READER_BUFFERS + maxInputBuffers);
                mInputImageReader = mOutputImageReaders[i];
            } else {
                mOutputImageReaders[i] = ImageReader.newInstance(outputSizes[i].getWidth(),
                        outputSizes[i].getHeight(), outputFormats[i],
                        MAX_CONCURRENT_READER_BUFFERS);
            }
        }

        if (inputSize != null && mInputImageReader == null) {
            mInputImageReader = ImageReader.newInstance(inputSize.getWidth(), inputSize.getHeight(),
                    inputFormat, maxInputBuffers);
        }
    }

    private void closeImageReaders() {
        if (mOutputImageReaders != null) {
            for (int i = 0; i < mOutputImageReaders.length; i++) {
                if (mOutputImageReaders[i] != null) {
                    mOutputImageReaders[i].close();
                    mOutputImageReaders[i] = null;
                }
            }
        }
        if (mInputImageReader != null) {
            mInputImageReader.close();
            mInputImageReader = null;
        }
    }

    private void do3A(JSONObject params) throws ItsException {
        ThreeAResultListener threeAListener = new ThreeAResultListener();
        try {
            // Start a 3A action, and wait for it to converge.
            // Get the converged values for each ""A"", and package into JSON result for caller.

            // Configure streams on physical sub-camera if PHYSICAL_ID_KEY is specified.
            String physicalId = null;
            CameraCharacteristics c = mCameraCharacteristics;
            if (params.has(PHYSICAL_ID_KEY)) {
                physicalId = params.getString(PHYSICAL_ID_KEY);
                c = mPhysicalCameraChars.get(physicalId);
            }

            // 3A happens on full-res frames.
            Size sizes[] = ItsUtils.getYuvOutputSizes(c);
            int outputFormats[] = new int[1];
            outputFormats[0] = ImageFormat.YUV_420_888;
            Size[] outputSizes = new Size[1];
            outputSizes[0] = sizes[0];
            int width = outputSizes[0].getWidth();
            int height = outputSizes[0].getHeight();

            prepareImageReaders(outputSizes, outputFormats, /*inputSize*/null, /*inputFormat*/0,
                    /*maxInputBuffers*/0);

            List<OutputConfiguration> outputConfigs = new ArrayList<OutputConfiguration>(1);
            OutputConfiguration config =
                    new OutputConfiguration(mOutputImageReaders[0].getSurface());
            if (physicalId != null) {
                config.setPhysicalCameraId(physicalId);
            }
            outputConfigs.add(config);
            BlockingSessionCallback sessionListener = new BlockingSessionCallback();
            mCamera.createCaptureSessionByOutputConfigurations(
                    outputConfigs, sessionListener, mCameraHandler);
            mSession = sessionListener.waitAndGetSession(TIMEOUT_IDLE_MS);

            // Add a listener that just recycles buffers; they aren't saved anywhere.
            ImageReader.OnImageAvailableListener readerListener =
                    createAvailableListenerDropper();
            mOutputImageReaders[0].setOnImageAvailableListener(readerListener, mSaveHandlers[0]);

            // Get the user-specified regions for AE, AWB, AF.
            // Note that the user specifies normalized [x,y,w,h], which is converted below
            // to an [x0,y0,x1,y1] region in sensor coords. The capture request region
            // also has a fifth ""weight"" element: [x0,y0,x1,y1,w].
            // Use logical camera's active array size for 3A regions.
            Rect activeArray = mCameraCharacteristics.get(
                    CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE);
            int aaWidth = activeArray.right - activeArray.left;
            int aaHeight = activeArray.bottom - activeArray.top;
            MeteringRectangle[] regionAE = new MeteringRectangle[]{
                    new MeteringRectangle(0,0,aaWidth,aaHeight,1)};
            MeteringRectangle[] regionAF = new MeteringRectangle[]{
                    new MeteringRectangle(0,0,aaWidth,aaHeight,1)};
            MeteringRectangle[] regionAWB = new MeteringRectangle[]{
                    new MeteringRectangle(0,0,aaWidth,aaHeight,1)};
            if (params.has(REGION_KEY)) {
                JSONObject regions = params.getJSONObject(REGION_KEY);
                if (regions.has(REGION_AE_KEY)) {
                    regionAE = ItsUtils.getJsonWeightedRectsFromArray(
                            regions.getJSONArray(REGION_AE_KEY), true, aaWidth, aaHeight);
                }
                if (regions.has(REGION_AF_KEY)) {
                    regionAF = ItsUtils.getJsonWeightedRectsFromArray(
                            regions.getJSONArray(REGION_AF_KEY), true, aaWidth, aaHeight);
                }
                if (regions.has(REGION_AWB_KEY)) {
                    regionAWB = ItsUtils.getJsonWeightedRectsFromArray(
                            regions.getJSONArray(REGION_AWB_KEY), true, aaWidth, aaHeight);
                }
            }

            // An EV compensation can be specified as part of AE convergence.
            int evComp = params.optInt(EVCOMP_KEY, 0);
            if (evComp != 0) {
                Logt.i(TAG, String.format(""Running 3A with AE exposure compensation value: %d"", evComp));
            }

            // By default, AE and AF both get triggered, but the user can optionally override this.
            // Also, AF won't get triggered if the lens is fixed-focus.
            boolean doAE = true;
            boolean doAF = true;
            if (params.has(TRIGGER_KEY)) {
                JSONObject triggers = params.getJSONObject(TRIGGER_KEY);
                if (triggers.has(TRIGGER_AE_KEY)) {
                    doAE = triggers.getBoolean(TRIGGER_AE_KEY);
                }
                if (triggers.has(TRIGGER_AF_KEY)) {
                    doAF = triggers.getBoolean(TRIGGER_AF_KEY);
                }
            }
            Float minFocusDistance = c.get(
                    CameraCharacteristics.LENS_INFO_MINIMUM_FOCUS_DISTANCE);
            boolean isFixedFocusLens = minFocusDistance != null && minFocusDistance == 0.0;
            if (doAF && isFixedFocusLens) {
                // Send a fake result back for the code that is waiting for this message to see
                // that AF has converged.
                Logt.i(TAG, ""Ignoring request for AF on fixed-focus camera"");
                mSocketRunnableObj.sendResponse(""afResult"", ""0.0"");
                doAF = false;
            }

            mInterlock3A.open();
            synchronized(m3AStateLock) {
                // If AE or AWB lock is specified, then the 3A will converge first and then lock these
                // values, waiting until the HAL has reported that the lock was successful.
                mNeedsLockedAE = params.optBoolean(LOCK_AE_KEY, false);
                mNeedsLockedAWB = params.optBoolean(LOCK_AWB_KEY, false);
                mConvergedAE = false;
                mConvergedAWB = false;
                mConvergedAF = false;
                mLockedAE = false;
                mLockedAWB = false;
            }
            long tstart = System.currentTimeMillis();
            boolean triggeredAE = false;
            boolean triggeredAF = false;

            Logt.i(TAG, String.format(""Initiating 3A: AE:%d, AF:%d, AWB:1, AELOCK:%d, AWBLOCK:%d"",
                    doAE?1:0, doAF?1:0, mNeedsLockedAE?1:0, mNeedsLockedAWB?1:0));

            // Keep issuing capture requests until 3A has converged.
            while (true) {

                // Block until can take the next 3A frame. Only want one outstanding frame
                // at a time, to simplify the logic here.
                if (!mInterlock3A.block(TIMEOUT_3A * 1000) ||
                        System.currentTimeMillis() - tstart > TIMEOUT_3A * 1000) {
                    throw new ItsException(
                            ""3A failed to converge after "" + TIMEOUT_3A + "" seconds.\n"" +
                            ""AE converge state: "" + mConvergedAE + "", \n"" +
                            ""AF convergence state: "" + mConvergedAF + "", \n"" +
                            ""AWB convergence state: "" + mConvergedAWB + ""."");
                }
                mInterlock3A.close();

                synchronized(m3AStateLock) {
                    // If not converged yet, issue another capture request.
                    if (       (doAE && (!triggeredAE || !mConvergedAE))
                            || !mConvergedAWB
                            || (doAF && (!triggeredAF || !mConvergedAF))
                            || (doAE && mNeedsLockedAE && !mLockedAE)
                            || (mNeedsLockedAWB && !mLockedAWB)) {

                        // Baseline capture request for 3A.
                        CaptureRequest.Builder req = mCamera.createCaptureRequest(
                                CameraDevice.TEMPLATE_PREVIEW);
                        req.set(CaptureRequest.FLASH_MODE, CaptureRequest.FLASH_MODE_OFF);
                        req.set(CaptureRequest.CONTROL_MODE, CaptureRequest.CONTROL_MODE_AUTO);
                        req.set(CaptureRequest.CONTROL_CAPTURE_INTENT,
                                CaptureRequest.CONTROL_CAPTURE_INTENT_PREVIEW);
                        req.set(CaptureRequest.CONTROL_AE_MODE,
                                CaptureRequest.CONTROL_AE_MODE_ON);
                        req.set(CaptureRequest.CONTROL_AE_EXPOSURE_COMPENSATION, 0);
                        req.set(CaptureRequest.CONTROL_AE_LOCK, false);
                        req.set(CaptureRequest.CONTROL_AE_REGIONS, regionAE);
                        req.set(CaptureRequest.CONTROL_AF_MODE,
                                CaptureRequest.CONTROL_AF_MODE_AUTO);
                        req.set(CaptureRequest.CONTROL_AF_REGIONS, regionAF);
                        req.set(CaptureRequest.CONTROL_AWB_MODE,
                                CaptureRequest.CONTROL_AWB_MODE_AUTO);
                        req.set(CaptureRequest.CONTROL_AWB_LOCK, false);
                        req.set(CaptureRequest.CONTROL_AWB_REGIONS, regionAWB);
                        // ITS only turns OIS on when it's explicitly requested
                        req.set(CaptureRequest.LENS_OPTICAL_STABILIZATION_MODE,
                                CaptureRequest.LENS_OPTICAL_STABILIZATION_MODE_OFF);

                        if (evComp != 0) {
                            req.set(CaptureRequest.CONTROL_AE_EXPOSURE_COMPENSATION, evComp);
                        }

                        if (mConvergedAE && mNeedsLockedAE) {
                            req.set(CaptureRequest.CONTROL_AE_LOCK, true);
                        }
                        if (mConvergedAWB && mNeedsLockedAWB) {
                            req.set(CaptureRequest.CONTROL_AWB_LOCK, true);
                        }

                        boolean triggering = false;
                        // Trigger AE first.
                        if (doAE && !triggeredAE) {
                            Logt.i(TAG, ""Triggering AE"");
                            req.set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,
                                    CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_START);
                            triggeredAE = true;
                            triggering = true;
                        }

                        // After AE has converged, trigger AF.
                        if (doAF && !triggeredAF && (!doAE || (triggeredAE && mConvergedAE))) {
                            Logt.i(TAG, ""Triggering AF"");
                            req.set(CaptureRequest.CONTROL_AF_TRIGGER,
                                    CaptureRequest.CONTROL_AF_TRIGGER_START);
                            triggeredAF = true;
                            triggering = true;
                        }

                        req.addTarget(mOutputImageReaders[0].getSurface());

                        if (triggering) {
                            // Send single request for AE/AF trigger
                            mSession.capture(req.build(),
                                    threeAListener, mResultHandler);
                        } else {
                            // Use repeating request for non-trigger requests
                            mSession.setRepeatingRequest(req.build(),
                                    threeAListener, mResultHandler);
                        }
                    } else {
                        mSocketRunnableObj.sendResponse(""3aConverged"", """");
                        Logt.i(TAG, ""3A converged"");
                        break;
                    }
                }
            }
        } catch (android.hardware.camera2.CameraAccessException e) {
            throw new ItsException(""Access error: "", e);
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        } finally {
            mSocketRunnableObj.sendResponse(""3aDone"", """");
            // stop listener from updating 3A states
            threeAListener.stop();
            if (mSession != null) {
                mSession.close();
            }
        }
    }

    private void doVibrate(JSONObject params) throws ItsException {
        try {
            if (mVibrator == null) {
                throw new ItsException(""Unable to start vibrator"");
            }
            JSONArray patternArray = params.getJSONArray(VIB_PATTERN_KEY);
            int len = patternArray.length();
            long pattern[] = new long[len];
            for (int i = 0; i < len; i++) {
                pattern[i] = patternArray.getLong(i);
            }
            Logt.i(TAG, String.format(""Starting vibrator, pattern length %d"",len));

            // Mark the vibrator as alarm to test the audio restriction API
            // TODO: consider making this configurable
            AudioAttributes audioAttributes = new AudioAttributes.Builder()
                    .setUsage(AudioAttributes.USAGE_ALARM).build();
            mVibrator.vibrate(pattern, -1, audioAttributes);
            mSocketRunnableObj.sendResponse(""vibrationStarted"", """");
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        }
    }

    private void doSetAudioRestriction(JSONObject params) throws ItsException {
        try {
            if (mCamera == null) {
                throw new ItsException(""Camera is closed"");
            }
            int mode = params.getInt(AUDIO_RESTRICTION_MODE_KEY);
            mCamera.setCameraAudioRestriction(mode);
            Logt.i(TAG, String.format(""Set audio restriction mode to %d"", mode));

            mSocketRunnableObj.sendResponse(""audioRestrictionSet"", """");
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        } catch (android.hardware.camera2.CameraAccessException e) {
            throw new ItsException(""Access error: "", e);
        }
    }

    /**
     * Parse jsonOutputSpecs to get output surface sizes and formats. Create input and output
     * image readers for the parsed output surface sizes, output formats, and the given input
     * size and format.
     */
    private void prepareImageReadersWithOutputSpecs(JSONArray jsonOutputSpecs, Size inputSize,
            int inputFormat, int maxInputBuffers, boolean backgroundRequest) throws ItsException {
        Size outputSizes[];
        int outputFormats[];
        int numSurfaces = 0;
        mPhysicalStreamMap.clear();

        if (jsonOutputSpecs != null) {
            try {
                numSurfaces = jsonOutputSpecs.length();
                if (backgroundRequest) {
                    numSurfaces += 1;
                }
                if (numSurfaces > MAX_NUM_OUTPUT_SURFACES) {
                    throw new ItsException(""Too many output surfaces"");
                }

                outputSizes = new Size[numSurfaces];
                outputFormats = new int[numSurfaces];
                for (int i = 0; i < numSurfaces; i++) {
                    // Append optional background stream at the end
                    if (backgroundRequest && i == numSurfaces - 1) {
                        outputFormats[i] = ImageFormat.YUV_420_888;
                        outputSizes[i] = new Size(640, 480);
                        continue;
                    }
                    // Get the specified surface.
                    JSONObject surfaceObj = jsonOutputSpecs.getJSONObject(i);
                    String physicalCameraId = surfaceObj.optString(""physicalCamera"");
                    CameraCharacteristics cameraCharacteristics =  mCameraCharacteristics;
                    mPhysicalStreamMap.put(i, physicalCameraId);
                    if (!physicalCameraId.isEmpty()) {
                        cameraCharacteristics = mPhysicalCameraChars.get(physicalCameraId);
                    }

                    String sformat = surfaceObj.optString(""format"");
                    Size sizes[];
                    if (""yuv"".equals(sformat) || """".equals(sformat)) {
                        // Default to YUV if no format is specified.
                        outputFormats[i] = ImageFormat.YUV_420_888;
                        sizes = ItsUtils.getYuvOutputSizes(cameraCharacteristics);
                    } else if (""jpg"".equals(sformat) || ""jpeg"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.JPEG;
                        sizes = ItsUtils.getJpegOutputSizes(cameraCharacteristics);
                    } else if (""raw"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.RAW_SENSOR;
                        sizes = ItsUtils.getRaw16OutputSizes(cameraCharacteristics);
                    } else if (""raw10"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.RAW10;
                        sizes = ItsUtils.getRaw10OutputSizes(cameraCharacteristics);
                    } else if (""raw12"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.RAW12;
                        sizes = ItsUtils.getRaw12OutputSizes(cameraCharacteristics);
                    } else if (""dng"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.RAW_SENSOR;
                        sizes = ItsUtils.getRaw16OutputSizes(cameraCharacteristics);
                        mCaptureRawIsDng = true;
                    } else if (""rawStats"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.RAW_SENSOR;
                        sizes = ItsUtils.getRaw16OutputSizes(cameraCharacteristics);
                        mCaptureRawIsStats = true;
                        mCaptureStatsGridWidth = surfaceObj.optInt(""gridWidth"");
                        mCaptureStatsGridHeight = surfaceObj.optInt(""gridHeight"");
                    } else if (""y8"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.Y8;
                        sizes = ItsUtils.getY8OutputSizes(cameraCharacteristics);
                    } else {
                        throw new ItsException(""Unsupported format: "" + sformat);
                    }
                    // If the size is omitted, then default to the largest allowed size for the
                    // format.
                    int width = surfaceObj.optInt(""width"");
                    int height = surfaceObj.optInt(""height"");
                    if (width <= 0) {
                        if (sizes == null || sizes.length == 0) {
                            throw new ItsException(String.format(
                                    ""Zero stream configs available for requested format: %s"",
                                    sformat));
                        }
                        width = ItsUtils.getMaxSize(sizes).getWidth();
                    }
                    if (height <= 0) {
                        height = ItsUtils.getMaxSize(sizes).getHeight();
                    }
                    // The stats computation only applies to the active array region.
                    int aaw = ItsUtils.getActiveArrayCropRegion(cameraCharacteristics).width();
                    int aah = ItsUtils.getActiveArrayCropRegion(cameraCharacteristics).height();
                    if (mCaptureStatsGridWidth <= 0 || mCaptureStatsGridWidth > aaw) {
                        mCaptureStatsGridWidth = aaw;
                    }
                    if (mCaptureStatsGridHeight <= 0 || mCaptureStatsGridHeight > aah) {
                        mCaptureStatsGridHeight = aah;
                    }

                    outputSizes[i] = new Size(width, height);
                }
            } catch (org.json.JSONException e) {
                throw new ItsException(""JSON error"", e);
            }
        } else {
            // No surface(s) specified at all.
            // Default: a single output surface which is full-res YUV.
            Size maxYuvSize = ItsUtils.getMaxOutputSize(
                    mCameraCharacteristics, ImageFormat.YUV_420_888);
            numSurfaces = backgroundRequest ? 2 : 1;

            outputSizes = new Size[numSurfaces];
            outputFormats = new int[numSurfaces];
            outputSizes[0] = maxYuvSize;
            outputFormats[0] = ImageFormat.YUV_420_888;
            if (backgroundRequest) {
                outputSizes[1] = new Size(640, 480);
                outputFormats[1] = ImageFormat.YUV_420_888;
            }
        }

        prepareImageReaders(outputSizes, outputFormats, inputSize, inputFormat, maxInputBuffers);
    }

    /**
     * Wait until mCountCallbacksRemaining is 0 or a specified amount of time has elapsed between
     * each callback.
     */
    private void waitForCallbacks(long timeoutMs) throws ItsException {
        synchronized(mCountCallbacksRemaining) {
            int currentCount = mCountCallbacksRemaining.get();
            while (currentCount > 0) {
                try {
                    mCountCallbacksRemaining.wait(timeoutMs);
                } catch (InterruptedException e) {
                    throw new ItsException(""Waiting for callbacks was interrupted."", e);
                }

                int newCount = mCountCallbacksRemaining.get();
                if (newCount == currentCount) {
                    throw new ItsException(""No callback received within timeout "" +
                            timeoutMs + ""ms"");
                }
                currentCount = newCount;
            }
        }
    }

    private void doCapture(JSONObject params) throws ItsException {
        try {
            // Parse the JSON to get the list of capture requests.
            List<CaptureRequest.Builder> requests = ItsSerializer.deserializeRequestList(
                    mCamera, params, ""captureRequests"");

            // optional background preview requests
            List<CaptureRequest.Builder> backgroundRequests = ItsSerializer.deserializeRequestList(
                    mCamera, params, ""repeatRequests"");
            boolean backgroundRequest = backgroundRequests.size() > 0;

            int numSurfaces = 0;
            int numCaptureSurfaces = 0;
            BlockingSessionCallback sessionListener = new BlockingSessionCallback();
            try {
                mCountRawOrDng.set(0);
                mCountJpg.set(0);
                mCountYuv.set(0);
                mCountRaw10.set(0);
                mCountRaw12.set(0);
                mCountCapRes.set(0);
                mCaptureRawIsDng = false;
                mCaptureRawIsStats = false;
                mCaptureResults = new CaptureResult[requests.size()];

                JSONArray jsonOutputSpecs = ItsUtils.getOutputSpecs(params);

                prepareImageReadersWithOutputSpecs(jsonOutputSpecs, /*inputSize*/null,
                        /*inputFormat*/0, /*maxInputBuffers*/0, backgroundRequest);
                numSurfaces = mOutputImageReaders.length;
                numCaptureSurfaces = numSurfaces - (backgroundRequest ? 1 : 0);

                List<OutputConfiguration> outputConfigs =
                        new ArrayList<OutputConfiguration>(numSurfaces);
                for (int i = 0; i < numSurfaces; i++) {
                    OutputConfiguration config = new OutputConfiguration(
                            mOutputImageReaders[i].getSurface());
                    if (mPhysicalStreamMap.get(i) != null) {
                        config.setPhysicalCameraId(mPhysicalStreamMap.get(i));
                    }
                    outputConfigs.add(config);
                }
                mCamera.createCaptureSessionByOutputConfigurations(outputConfigs,
                        sessionListener, mCameraHandler);
                mSession = sessionListener.waitAndGetSession(TIMEOUT_IDLE_MS);

                for (int i = 0; i < numSurfaces; i++) {
                    ImageReader.OnImageAvailableListener readerListener;
                    if (backgroundRequest && i == numSurfaces - 1) {
                        readerListener = createAvailableListenerDropper();
                    } else {
                        readerListener = createAvailableListener(mCaptureCallback);
                    }
                    mOutputImageReaders[i].setOnImageAvailableListener(readerListener,
                            mSaveHandlers[i]);
                }

                // Plan for how many callbacks need to be received throughout the duration of this
                // sequence of capture requests. There is one callback per image surface, and one
                // callback for the CaptureResult, for each capture.
                int numCaptures = requests.size();
                mCountCallbacksRemaining.set(numCaptures * (numCaptureSurfaces + 1));

            } catch (CameraAccessException e) {
                throw new ItsException(""Error configuring outputs"", e);
            }

            // Start background requests and let it warm up pipeline
            if (backgroundRequest) {
                List<CaptureRequest> bgRequestList =
                        new ArrayList<CaptureRequest>(backgroundRequests.size());
                for (int i = 0; i < backgroundRequests.size(); i++) {
                    CaptureRequest.Builder req = backgroundRequests.get(i);
                    req.addTarget(mOutputImageReaders[numCaptureSurfaces].getSurface());
                    bgRequestList.add(req.build());
                }
                mSession.setRepeatingBurst(bgRequestList, null, null);
                // warm up the pipeline
                Thread.sleep(PIPELINE_WARMUP_TIME_MS);
            }

            // Initiate the captures.
            long maxExpTimeNs = -1;
            List<CaptureRequest> requestList =
                    new ArrayList<>(requests.size());
            for (int i = 0; i < requests.size(); i++) {
                CaptureRequest.Builder req = requests.get(i);
                // For DNG captures, need the LSC map to be available.
                if (mCaptureRawIsDng) {
                    req.set(CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE, 1);
                }
                Long expTimeNs = req.get(CaptureRequest.SENSOR_EXPOSURE_TIME);
                if (expTimeNs != null && expTimeNs > maxExpTimeNs) {
                    maxExpTimeNs = expTimeNs;
                }

                for (int j = 0; j < numCaptureSurfaces; j++) {
                    req.addTarget(mOutputImageReaders[j].getSurface());
                }
                requestList.add(req.build());
            }
            mSession.captureBurst(requestList, mCaptureResultListener, mResultHandler);

            long timeout = TIMEOUT_CALLBACK * 1000;
            if (maxExpTimeNs > 0) {
                timeout += maxExpTimeNs / 1000000; // ns to ms
            }
            // Make sure all callbacks have been hit (wait until captures are done).
            // If no timeouts are received after a timeout, then fail.
            waitForCallbacks(timeout);

            // Close session and wait until session is fully closed
            mSession.close();
            sessionListener.getStateWaiter().waitForState(
                    BlockingSessionCallback.SESSION_CLOSED, TIMEOUT_SESSION_CLOSE);

        } catch (android.hardware.camera2.CameraAccessException e) {
            throw new ItsException(""Access error: "", e);
        } catch (InterruptedException e) {
            throw new ItsException(""Unexpected InterruptedException: "", e);
        }
    }

    /**
     * Perform reprocess captures.
     *
     * It takes captureRequests in a JSON object and perform capture requests in two steps:
     * regular capture request to get reprocess input and reprocess capture request to get
     * reprocess outputs.
     *
     * Regular capture requests:
     *   1. For each capture request in the JSON object, create a full-size capture request with
     *      the settings in the JSON object.
     *   2. Remember and clear noise reduction, edge enhancement, and effective exposure factor
     *      from the regular capture requests. (Those settings will be used for reprocess requests.)
     *   3. Submit the regular capture requests.
     *
     * Reprocess capture requests:
     *   4. Wait for the regular capture results and use them to create reprocess capture requests.
     *   5. Wait for the regular capture output images and queue them to the image writer.
     *   6. Set the noise reduction, edge enhancement, and effective exposure factor from #2.
     *   7. Submit the reprocess capture requests.
     *
     * The output images and results for the regular capture requests won't be written to socket.
     * The output images and results for the reprocess capture requests will be written to socket.
     */
    private void doReprocessCapture(JSONObject params) throws ItsException {
        ImageWriter imageWriter = null;
        ArrayList<Integer> noiseReductionModes = new ArrayList<>();
        ArrayList<Integer> edgeModes = new ArrayList<>();
        ArrayList<Float> effectiveExposureFactors = new ArrayList<>();

        mCountRawOrDng.set(0);
        mCountJpg.set(0);
        mCountYuv.set(0);
        mCountRaw10.set(0);
        mCountRaw12.set(0);
        mCountCapRes.set(0);
        mCaptureRawIsDng = false;
        mCaptureRawIsStats = false;

        try {
            // Parse the JSON to get the list of capture requests.
            List<CaptureRequest.Builder> inputRequests =
                    ItsSerializer.deserializeRequestList(mCamera, params, ""captureRequests"");

            // Prepare the image readers for reprocess input and reprocess outputs.
            int inputFormat = getReprocessInputFormat(params);
            Size inputSize = ItsUtils.getMaxOutputSize(mCameraCharacteristics, inputFormat);
            JSONArray jsonOutputSpecs = ItsUtils.getOutputSpecs(params);
            prepareImageReadersWithOutputSpecs(jsonOutputSpecs, inputSize, inputFormat,
                    inputRequests.size(), /*backgroundRequest*/false);

            // Prepare a reprocessable session.
            int numOutputSurfaces = mOutputImageReaders.length;
            InputConfiguration inputConfig = new InputConfiguration(inputSize.getWidth(),
                    inputSize.getHeight(), inputFormat);
            List<Surface> outputSurfaces = new ArrayList<Surface>();
            boolean addSurfaceForInput = true;
            for (int i = 0; i < numOutputSurfaces; i++) {
                outputSurfaces.add(mOutputImageReaders[i].getSurface());
                if (mOutputImageReaders[i] == mInputImageReader) {
                    // If input and one of the outputs share the same image reader, avoid
                    // adding the same surfaces twice.
                    addSurfaceForInput = false;
                }
            }

            if (addSurfaceForInput) {
                // Besides the output surfaces specified in JSON object, add an additional one
                // for reprocess input.
                outputSurfaces.add(mInputImageReader.getSurface());
            }

            BlockingSessionCallback sessionListener = new BlockingSessionCallback();
            mCamera.createReprocessableCaptureSession(inputConfig, outputSurfaces, sessionListener,
                    mCameraHandler);
            mSession = sessionListener.waitAndGetSession(TIMEOUT_IDLE_MS);

            // Create an image writer for reprocess input.
            Surface inputSurface = mSession.getInputSurface();
            imageWriter = ImageWriter.newInstance(inputSurface, inputRequests.size());

            // Set up input reader listener and capture callback listener to get
            // reprocess input buffers and the results in order to create reprocess capture
            // requests.
            ImageReaderListenerWaiter inputReaderListener = new ImageReaderListenerWaiter();
            mInputImageReader.setOnImageAvailableListener(inputReaderListener, mSaveHandlers[0]);

            CaptureCallbackWaiter captureCallbackWaiter = new CaptureCallbackWaiter();
            // Prepare the reprocess input request
            for (CaptureRequest.Builder inputReqest : inputRequests) {
                // Remember and clear noise reduction, edge enhancement, and effective exposure
                // factors.
                noiseReductionModes.add(inputReqest.get(CaptureRequest.NOISE_REDUCTION_MODE));
                edgeModes.add(inputReqest.get(CaptureRequest.EDGE_MODE));
                effectiveExposureFactors.add(inputReqest.get(
                        CaptureRequest.REPROCESS_EFFECTIVE_EXPOSURE_FACTOR));

                inputReqest.set(CaptureRequest.NOISE_REDUCTION_MODE,
                        CaptureRequest.NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG);
                inputReqest.set(CaptureRequest.EDGE_MODE, CaptureRequest.EDGE_MODE_ZERO_SHUTTER_LAG);
                inputReqest.set(CaptureRequest.REPROCESS_EFFECTIVE_EXPOSURE_FACTOR, null);
                inputReqest.addTarget(mInputImageReader.getSurface());
                mSession.capture(inputReqest.build(), captureCallbackWaiter, mResultHandler);
            }

            // Wait for reprocess input images
            ArrayList<CaptureRequest.Builder> reprocessOutputRequests = new ArrayList<>();
            for (int i = 0; i < inputRequests.size(); i++) {
                TotalCaptureResult result =
                        captureCallbackWaiter.getResult(TIMEOUT_CALLBACK * 1000);
                reprocessOutputRequests.add(mCamera.createReprocessCaptureRequest(result));
                imageWriter.queueInputImage(inputReaderListener.getImage(TIMEOUT_CALLBACK * 1000));
            }

            // Start performing reprocess captures.

            mCaptureResults = new CaptureResult[inputRequests.size()];

            // Prepare reprocess capture requests.
            for (int i = 0; i < numOutputSurfaces; i++) {
                ImageReader.OnImageAvailableListener outputReaderListener =
                        createAvailableListener(mCaptureCallback);
                mOutputImageReaders[i].setOnImageAvailableListener(outputReaderListener,
                        mSaveHandlers[i]);
            }

            // Plan for how many callbacks need to be received throughout the duration of this
            // sequence of capture requests. There is one callback per image surface, and one
            // callback for the CaptureResult, for each capture.
            int numCaptures = reprocessOutputRequests.size();
            mCountCallbacksRemaining.set(numCaptures * (numOutputSurfaces + 1));

            // Initiate the captures.
            for (int i = 0; i < reprocessOutputRequests.size(); i++) {
                CaptureRequest.Builder req = reprocessOutputRequests.get(i);
                for (ImageReader outputImageReader : mOutputImageReaders) {
                    req.addTarget(outputImageReader.getSurface());
                }

                req.set(CaptureRequest.NOISE_REDUCTION_MODE, noiseReductionModes.get(i));
                req.set(CaptureRequest.EDGE_MODE, edgeModes.get(i));
                req.set(CaptureRequest.REPROCESS_EFFECTIVE_EXPOSURE_FACTOR,
                        effectiveExposureFactors.get(i));

                mSession.capture(req.build(), mCaptureResultListener, mResultHandler);
            }

            // Make sure all callbacks have been hit (wait until captures are done).
            // If no timeouts are received after a timeout, then fail.
            waitForCallbacks(TIMEOUT_CALLBACK * 1000);
        } catch (android.hardware.camera2.CameraAccessException e) {
            throw new ItsException(""Access error: "", e);
        } finally {
            closeImageReaders();
            if (mSession != null) {
                mSession.close();
                mSession = null;
            }
            if (imageWriter != null) {
                imageWriter.close();
            }
        }
    }

    @Override
    public final void onAccuracyChanged(Sensor sensor, int accuracy) {
        Logt.i(TAG, ""Sensor "" + sensor.getName() + "" accuracy changed to "" + accuracy);
    }

    @Override
    public final void onSensorChanged(SensorEvent event) {
        synchronized(mEventLock) {
            if (mEventsEnabled) {
                MySensorEvent ev2 = new MySensorEvent();
                ev2.sensor = event.sensor;
                ev2.accuracy = event.accuracy;
                ev2.timestamp = event.timestamp;
                ev2.values = new float[event.values.length];
                System.arraycopy(event.values, 0, ev2.values, 0, event.values.length);
                mEvents.add(ev2);
            }
        }
    }

    private final CaptureCallback mCaptureCallback = new CaptureCallback() {
        @Override
        public void onCaptureAvailable(Image capture, String physicalCameraId) {
            try {
                int format = capture.getFormat();
                if (format == ImageFormat.JPEG) {
                    Logt.i(TAG, ""Received JPEG capture"");
                    byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                    ByteBuffer buf = ByteBuffer.wrap(img);
                    int count = mCountJpg.getAndIncrement();
                    mSocketRunnableObj.sendResponseCaptureBuffer(""jpegImage""+physicalCameraId, buf);
                } else if (format == ImageFormat.YUV_420_888) {
                    Logt.i(TAG, ""Received YUV capture"");
                    byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                    ByteBuffer buf = ByteBuffer.wrap(img);
                    mSocketRunnableObj.sendResponseCaptureBuffer(
                            ""yuvImage""+physicalCameraId, buf);
                } else if (format == ImageFormat.RAW10) {
                    Logt.i(TAG, ""Received RAW10 capture"");
                    byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                    ByteBuffer buf = ByteBuffer.wrap(img);
                    int count = mCountRaw10.getAndIncrement();
                    mSocketRunnableObj.sendResponseCaptureBuffer(
                            ""raw10Image""+physicalCameraId, buf);
                } else if (format == ImageFormat.RAW12) {
                    Logt.i(TAG, ""Received RAW12 capture"");
                    byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                    ByteBuffer buf = ByteBuffer.wrap(img);
                    int count = mCountRaw12.getAndIncrement();
                    mSocketRunnableObj.sendResponseCaptureBuffer(""raw12Image""+physicalCameraId, buf);
                } else if (format == ImageFormat.RAW_SENSOR) {
                    Logt.i(TAG, ""Received RAW16 capture"");
                    int count = mCountRawOrDng.getAndIncrement();
                    if (! mCaptureRawIsDng) {
                        byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                        if (! mCaptureRawIsStats) {
                            ByteBuffer buf = ByteBuffer.wrap(img);
                            mSocketRunnableObj.sendResponseCaptureBuffer(
                                    ""rawImage"" + physicalCameraId, buf);
                        } else {
                            // Compute the requested stats on the raw frame, and return the results
                            // in a new ""stats image"".
                            long startTimeMs = SystemClock.elapsedRealtime();
                            int w = capture.getWidth();
                            int h = capture.getHeight();
                            int aaw = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .width();
                            int aah = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .height();
                            int aax = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .left;
                            int aay = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .top;

                            if (w == aaw) {
                                aax = 0;
                            }
                            if (h == aah) {
                                aay = 0;
                            }

                            int gw = mCaptureStatsGridWidth;
                            int gh = mCaptureStatsGridHeight;
                            float[] stats = StatsImage.computeStatsImage(
                                                             img, w, h, aax, aay, aaw, aah, gw, gh);
                            long endTimeMs = SystemClock.elapsedRealtime();
                            Log.e(TAG, ""Raw stats computation takes "" + (endTimeMs - startTimeMs) + "" ms"");
                            int statsImgSize = stats.length * 4;
                            if (mSocketQueueQuota != null) {
                                mSocketQueueQuota.release(img.length);
                                mSocketQueueQuota.acquire(statsImgSize);
                            }
                            ByteBuffer bBuf = ByteBuffer.allocate(statsImgSize);
                            bBuf.order(ByteOrder.nativeOrder());
                            FloatBuffer fBuf = bBuf.asFloatBuffer();
                            fBuf.put(stats);
                            fBuf.position(0);
                            mSocketRunnableObj.sendResponseCaptureBuffer(
                                    ""rawStatsImage""+physicalCameraId, bBuf);
                        }
                    } else {
                        // Wait until the corresponding capture result is ready, up to a timeout.
                        long t0 = android.os.SystemClock.elapsedRealtime();
                        while (! mThreadExitFlag
                                && android.os.SystemClock.elapsedRealtime()-t0 < TIMEOUT_CAP_RES) {
                            if (mCaptureResults[count] != null) {
                                Logt.i(TAG, ""Writing capture as DNG"");
                                DngCreator dngCreator = new DngCreator(
                                        mCameraCharacteristics, mCaptureResults[count]);
                                ByteArrayOutputStream dngStream = new ByteArrayOutputStream();
                                dngCreator.writeImage(dngStream, capture);
                                byte[] dngArray = dngStream.toByteArray();
                                if (mSocketQueueQuota != null) {
                                    // Ideally we should acquire before allocating memory, but
                                    // here the DNG size is unknown before toByteArray call, so
                                    // we have to register the size afterward. This should still
                                    // works most of the time since all DNG images are handled by
                                    // the same handler thread, so we are at most one buffer over
                                    // the quota.
                                    mSocketQueueQuota.acquire(dngArray.length);
                                }
                                ByteBuffer dngBuf = ByteBuffer.wrap(dngArray);
                                mSocketRunnableObj.sendResponseCaptureBuffer(""dngImage"", dngBuf);
                                break;
                            } else {
                                Thread.sleep(1);
                            }
                        }
                    }
                } else if (format == ImageFormat.Y8) {
                    Logt.i(TAG, ""Received Y8 capture"");
                    byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                    ByteBuffer buf = ByteBuffer.wrap(img);
                    mSocketRunnableObj.sendResponseCaptureBuffer(
                            ""y8Image""+physicalCameraId, buf);
                } else {
                    throw new ItsException(""Unsupported image format: "" + format);
                }

                synchronized(mCountCallbacksRemaining) {
                    mCountCallbacksRemaining.decrementAndGet();
                    mCountCallbacksRemaining.notify();
                }
            } catch (IOException e) {
                Logt.e(TAG, ""Script error: "", e);
            } catch (InterruptedException e) {
                Logt.e(TAG, ""Script error: "", e);
            } catch (ItsException e) {
                Logt.e(TAG, ""Script error: "", e);
            }
        }
    };

    private static float r2f(Rational r) {
        return (float)r.getNumerator() / (float)r.getDenominator();
    }

    private boolean hasCapability(int capability) throws ItsException {
        int[] capabilities = mCameraCharacteristics.get(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
        if (capabilities == null) {
            throw new ItsException(""Failed to get capabilities"");
        }
        for (int c : capabilities) {
            if (c == capability) {
                return true;
            }
        }
        return false;
    }

    private String buildLogString(CaptureResult result) throws ItsException {
        StringBuilder logMsg = new StringBuilder();
        logMsg.append(String.format(
                ""Capt result: AE=%d, AF=%d, AWB=%d, "",
                result.get(CaptureResult.CONTROL_AE_STATE),
                result.get(CaptureResult.CONTROL_AF_STATE),
                result.get(CaptureResult.CONTROL_AWB_STATE)));

        boolean readSensorSettings = hasCapability(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS);

        if (readSensorSettings) {
            logMsg.append(String.format(
                    ""sens=%d, exp=%.1fms, dur=%.1fms, "",
                    result.get(CaptureResult.SENSOR_SENSITIVITY),
                    result.get(CaptureResult.SENSOR_EXPOSURE_TIME).longValue() / 1000000.0f,
                    result.get(CaptureResult.SENSOR_FRAME_DURATION).longValue() /
                                1000000.0f));
        }
        if (result.get(CaptureResult.COLOR_CORRECTION_GAINS) != null) {
            logMsg.append(String.format(
                    ""gains=[%.1f, %.1f, %.1f, %.1f], "",
                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getRed(),
                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getGreenEven(),
                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getGreenOdd(),
                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getBlue()));
        } else {
            logMsg.append(""gains=[], "");
        }
        if (result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM) != null) {
            logMsg.append(String.format(
                    ""xform=[%.1f, %.1f, %.1f, %.1f, %.1f, %.1f, %.1f, %.1f, %.1f], "",
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(0,0)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(1,0)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(2,0)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(0,1)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(1,1)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(2,1)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(0,2)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(1,2)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(2,2))));
        } else {
            logMsg.append(""xform=[], "");
        }
        logMsg.append(String.format(
                ""foc=%.1f"",
                result.get(CaptureResult.LENS_FOCUS_DISTANCE)));
        return logMsg.toString();
    }

    private class ThreeAResultListener extends CaptureResultListener {
        private volatile boolean stopped = false;
        private boolean aeResultSent = false;
        private boolean awbResultSent = false;
        private boolean afResultSent = false;

        @Override
        public void onCaptureStarted(CameraCaptureSession session, CaptureRequest request,
                long timestamp, long frameNumber) {
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                TotalCaptureResult result) {
            try {
                if (stopped) {
                    return;
                }

                if (request == null || result == null) {
                    throw new ItsException(""Request/result is invalid"");
                }

                Logt.i(TAG, buildLogString(result));

                synchronized(m3AStateLock) {
                    if (result.get(CaptureResult.CONTROL_AE_STATE) != null) {
                        mConvergedAE = result.get(CaptureResult.CONTROL_AE_STATE) ==
                                                  CaptureResult.CONTROL_AE_STATE_CONVERGED ||
                                       result.get(CaptureResult.CONTROL_AE_STATE) ==
                                                  CaptureResult.CONTROL_AE_STATE_FLASH_REQUIRED ||
                                       result.get(CaptureResult.CONTROL_AE_STATE) ==
                                                  CaptureResult.CONTROL_AE_STATE_LOCKED;
                        mLockedAE = result.get(CaptureResult.CONTROL_AE_STATE) ==
                                               CaptureResult.CONTROL_AE_STATE_LOCKED;
                    }
                    if (result.get(CaptureResult.CONTROL_AF_STATE) != null) {
                        mConvergedAF = result.get(CaptureResult.CONTROL_AF_STATE) ==
                                                  CaptureResult.CONTROL_AF_STATE_FOCUSED_LOCKED;
                    }
                    if (result.get(CaptureResult.CONTROL_AWB_STATE) != null) {
                        mConvergedAWB = result.get(CaptureResult.CONTROL_AWB_STATE) ==
                                                   CaptureResult.CONTROL_AWB_STATE_CONVERGED ||
                                        result.get(CaptureResult.CONTROL_AWB_STATE) ==
                                                   CaptureResult.CONTROL_AWB_STATE_LOCKED;
                        mLockedAWB = result.get(CaptureResult.CONTROL_AWB_STATE) ==
                                                CaptureResult.CONTROL_AWB_STATE_LOCKED;
                    }

                    if (mConvergedAE && (!mNeedsLockedAE || mLockedAE) && !aeResultSent) {
                        aeResultSent = true;
                        if (result.get(CaptureResult.SENSOR_SENSITIVITY) != null
                                && result.get(CaptureResult.SENSOR_EXPOSURE_TIME) != null) {
                            mSocketRunnableObj.sendResponse(""aeResult"", String.format(""%d %d"",
                                    result.get(CaptureResult.SENSOR_SENSITIVITY).intValue(),
                                    result.get(CaptureResult.SENSOR_EXPOSURE_TIME).intValue()
                                    ));
                        } else {
                            Logt.i(TAG, String.format(
                                    ""AE converged but NULL exposure values, sensitivity:%b, expTime:%b"",
                                    result.get(CaptureResult.SENSOR_SENSITIVITY) == null,
                                    result.get(CaptureResult.SENSOR_EXPOSURE_TIME) == null));
                        }
                    }

                    if (mConvergedAF && !afResultSent) {
                        afResultSent = true;
                        if (result.get(CaptureResult.LENS_FOCUS_DISTANCE) != null) {
                            mSocketRunnableObj.sendResponse(""afResult"", String.format(""%f"",
                                    result.get(CaptureResult.LENS_FOCUS_DISTANCE)
                                    ));
                        } else {
                            Logt.i(TAG, ""AF converged but NULL focus distance values"");
                        }
                    }

                    if (mConvergedAWB && (!mNeedsLockedAWB || mLockedAWB) && !awbResultSent) {
                        awbResultSent = true;
                        if (result.get(CaptureResult.COLOR_CORRECTION_GAINS) != null
                                && result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM) != null) {
                            mSocketRunnableObj.sendResponse(""awbResult"", String.format(
                                    ""%f %f %f %f %f %f %f %f %f %f %f %f %f"",
                                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getRed(),
                                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getGreenEven(),
                                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getGreenOdd(),
                                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getBlue(),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(0,0)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(1,0)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(2,0)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(0,1)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(1,1)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(2,1)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(0,2)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(1,2)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(2,2))));
                        } else {
                            Logt.i(TAG, String.format(
                                    ""AWB converged but NULL color correction values, gains:%b, ccm:%b"",
                                    result.get(CaptureResult.COLOR_CORRECTION_GAINS) == null,
                                    result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM) == null));
                        }
                    }
                }

                mInterlock3A.open();
            } catch (ItsException e) {
                Logt.e(TAG, ""Script error: "", e);
            } catch (Exception e) {
                Logt.e(TAG, ""Script error: "", e);
            }
        }

        @Override
        public void onCaptureFailed(CameraCaptureSession session, CaptureRequest request,
                CaptureFailure failure) {
            Logt.e(TAG, ""Script error: capture failed"");
        }

        public void stop() {
            stopped = true;
        }
    }

    private final CaptureResultListener mCaptureResultListener = new CaptureResultListener() {
        @Override
        public void onCaptureStarted(CameraCaptureSession session, CaptureRequest request,
                long timestamp, long frameNumber) {
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                TotalCaptureResult result) {
            try {
                if (request == null || result == null) {
                    throw new ItsException(""Request/result is invalid"");
                }

                Logt.i(TAG, buildLogString(result));

                int count = mCountCapRes.getAndIncrement();
                mCaptureResults[count] = result;
                mSocketRunnableObj.sendResponseCaptureResult(mCameraCharacteristics,
                        request, result, mOutputImageReaders);
                synchronized(mCountCallbacksRemaining) {
                    mCountCallbacksRemaining.decrementAndGet();
                    mCountCallbacksRemaining.notify();
                }
            } catch (ItsException e) {
                Logt.e(TAG, ""Script error: "", e);
            } catch (Exception e) {
                Logt.e(TAG, ""Script error: "", e);
            }
        }

        @Override
        public void onCaptureFailed(CameraCaptureSession session, CaptureRequest request,
                CaptureFailure failure) {
            Logt.e(TAG, ""Script error: capture failed"");
        }
    };

    private class CaptureCallbackWaiter extends CameraCaptureSession.CaptureCallback {
        private final LinkedBlockingQueue<TotalCaptureResult> mResultQueue =
                new LinkedBlockingQueue<>();

        @Override
        public void onCaptureStarted(CameraCaptureSession session, CaptureRequest request,
                long timestamp, long frameNumber) {
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                TotalCaptureResult result) {
            try {
                mResultQueue.put(result);
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onImageAvailable"");
            }
        }

        @Override
        public void onCaptureFailed(CameraCaptureSession session, CaptureRequest request,
                CaptureFailure failure) {
            Logt.e(TAG, ""Script error: capture failed"");
        }

        public TotalCaptureResult getResult(long timeoutMs) throws ItsException {
            TotalCaptureResult result;
            try {
                result = mResultQueue.poll(timeoutMs, TimeUnit.MILLISECONDS);
            } catch (InterruptedException e) {
                throw new ItsException(e);
            }

            if (result == null) {
                throw new ItsException(""Getting an image timed out after "" + timeoutMs +
                        ""ms"");
            }

            return result;
        }
    }

    private static class ImageReaderListenerWaiter implements ImageReader.OnImageAvailableListener {
        private final LinkedBlockingQueue<Image> mImageQueue = new LinkedBlockingQueue<>();

        @Override
        public void onImageAvailable(ImageReader reader) {
            try {
                mImageQueue.put(reader.acquireNextImage());
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onImageAvailable"");
            }
        }

        public Image getImage(long timeoutMs) throws ItsException {
            Image image;
            try {
                image = mImageQueue.poll(timeoutMs, TimeUnit.MILLISECONDS);
            } catch (InterruptedException e) {
                throw new ItsException(e);
            }

            if (image == null) {
                throw new ItsException(""Getting an image timed out after "" + timeoutMs +
                        ""ms"");
            }
            return image;
        }
    }

    private int getReprocessInputFormat(JSONObject params) throws ItsException {
        String reprocessFormat;
        try {
            reprocessFormat = params.getString(""reprocessFormat"");
        } catch (org.json.JSONException e) {
            throw new ItsException(""Error parsing reprocess format: "" + e);
        }

        if (reprocessFormat.equals(""yuv"")) {
            return ImageFormat.YUV_420_888;
        } else if (reprocessFormat.equals(""private"")) {
            return ImageFormat.PRIVATE;
        }

        throw new ItsException(""Uknown reprocess format: "" + reprocessFormat);
    }
}"	""	""	"cdd MEDIA_PERFORMANCE_CLASS minimum 12"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.testcases.Camera2AndroidTestCase"	"isInstantApp"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/testcases/Camera2AndroidTestCase.java"	""	"public void test/*
 *.
 */

package android.hardware.camera2.cts.testcases;

import static android.hardware.camera2.cts.CameraTestUtils.*;
import static com.android.ex.camera2.blocking.BlockingStateCallback.*;

import android.content.Context;
import android.graphics.ImageFormat;
import android.graphics.Rect;
import android.hardware.cts.helpers.CameraParameterizedTestCase;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraCaptureSession.CaptureCallback;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.params.InputConfiguration;
import android.hardware.camera2.params.OutputConfiguration;
import android.hardware.camera2.params.SessionConfiguration;
import android.util.Size;
import android.hardware.camera2.cts.Camera2ParameterizedTestCase;
import android.hardware.camera2.cts.CameraTestUtils;
import android.hardware.camera2.cts.helpers.CameraErrorCollector;
import android.hardware.camera2.cts.helpers.StaticMetadata;
import android.hardware.camera2.cts.helpers.StaticMetadata.CheckLevel;
import android.media.Image;
import android.media.Image.Plane;
import android.media.ImageReader;
import android.os.Handler;
import android.os.HandlerThread;
import android.test.AndroidTestCase;
import android.util.Log;
import android.view.Surface;
import android.view.WindowManager;
import androidx.test.InstrumentationRegistry;

import com.android.ex.camera2.blocking.BlockingSessionCallback;
import com.android.ex.camera2.blocking.BlockingStateCallback;

import java.io.File;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;

import org.junit.Ignore;
import org.junit.Test;

// TODO: Can we de-duplicate this with Camera2AndroidBasicTestCase keeping in mind CtsVerifier ?
public class Camera2AndroidTestCase extends Camera2ParameterizedTestCase {
    private static final String TAG = ""Camera2AndroidTestCase"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);

    // Default capture size: VGA size is required by CDD.
    protected static final Size DEFAULT_CAPTURE_SIZE = new Size(640, 480);
    protected static final int CAPTURE_WAIT_TIMEOUT_MS = 7000;

    protected CameraDevice mCamera;
    protected CameraCaptureSession mCameraSession;
    protected BlockingSessionCallback mCameraSessionListener;
    protected BlockingStateCallback mCameraListener;
    // include both standalone camera IDs and ""hidden"" physical camera IDs
    protected String[] mAllCameraIds;
    protected HashMap<String, StaticMetadata> mAllStaticInfo;
    protected ImageReader mReader;
    protected Surface mReaderSurface;
    protected Handler mHandler;
    protected HandlerThread mHandlerThread;
    protected StaticMetadata mStaticInfo;
    protected CameraErrorCollector mCollector;
    protected List<Size> mOrderedPreviewSizes; // In descending order.
    protected List<Size> mOrderedVideoSizes; // In descending order.
    protected List<Size> mOrderedStillSizes; // In descending order.
    protected String mDebugFileNameBase;

    protected WindowManager mWindowManager;

    /**
     * Set up the camera2 test case required environments, including CameraManager,
     * HandlerThread, Camera IDs, and CameraStateCallback etc.
     */
    @Override
    public void setUp() throws Exception {
        setUp(false);
    }

    /**
     * Set up the camera2 test case required environments, including CameraManager,
     * HandlerThread, Camera IDs, and CameraStateCallback etc.
     * @param useAll whether all camera ids are to be used for system camera tests
     */
    public void setUp(boolean useAll) throws Exception {
        super.setUp(useAll);
        mWindowManager = (WindowManager) mContext.getSystemService(Context.WINDOW_SERVICE);

        mHandlerThread = new HandlerThread(TAG);
        mHandlerThread.start();
        mHandler = new Handler(mHandlerThread.getLooper());
        mCameraListener = new BlockingStateCallback();
        mCollector = new CameraErrorCollector();

        File filesDir = mContext.getPackageManager().isInstantApp()
                ? mContext.getFilesDir()
                : mContext.getExternalFilesDir(null);

        mDebugFileNameBase = filesDir.getPath();

        mAllStaticInfo = new HashMap<String, StaticMetadata>();
        List<String> hiddenPhysicalIds = new ArrayList<>();
        for (String cameraId : mCameraIdsUnderTest) {
            CameraCharacteristics props = mCameraManager.getCameraCharacteristics(cameraId);
            StaticMetadata staticMetadata = new StaticMetadata(props,
                    CheckLevel.ASSERT, /*collector*/null);
            mAllStaticInfo.put(cameraId, staticMetadata);

            for (String physicalId : props.getPhysicalCameraIds()) {
                if (!Arrays.asList(mCameraIdsUnderTest).contains(physicalId) &&
                        !hiddenPhysicalIds.contains(physicalId)) {
                    hiddenPhysicalIds.add(physicalId);
                    props = mCameraManager.getCameraCharacteristics(physicalId);
                    staticMetadata = new StaticMetadata(
                            mCameraManager.getCameraCharacteristics(physicalId),
                            CheckLevel.ASSERT, /*collector*/null);
                    mAllStaticInfo.put(physicalId, staticMetadata);
                }
            }
        }
        mAllCameraIds = new String[mCameraIdsUnderTest.length + hiddenPhysicalIds.size()];
        System.arraycopy(mCameraIdsUnderTest, 0, mAllCameraIds, 0, mCameraIdsUnderTest.length);
        for (int i = 0; i < hiddenPhysicalIds.size(); i++) {
            mAllCameraIds[mCameraIdsUnderTest.length + i] = hiddenPhysicalIds.get(i);
        }
    }

    @Override
    public void tearDown() throws Exception {
        try {
            if (mHandlerThread != null) {
                mHandlerThread.quitSafely();
            }
            mHandler = null;
            closeDefaultImageReader();

            if (mCollector != null) {
                mCollector.verify();
            }
        } catch (Throwable e) {
            // When new Exception(e) is used, exception info will be printed twice.
            throw new Exception(e.getMessage());
        } finally {
            super.tearDown();
        }
    }

    /**
     * Start capture with given {@link #CaptureRequest}.
     *
     * @param request The {@link #CaptureRequest} to be captured.
     * @param repeating If the capture is single capture or repeating.
     * @param listener The {@link #CaptureCallback} camera device used to notify callbacks.
     * @param handler The handler camera device used to post callbacks.
     */
    protected void startCapture(CaptureRequest request, boolean repeating,
            CaptureCallback listener, Handler handler) throws Exception {
        if (VERBOSE) Log.v(TAG, ""Starting capture from device"");

        if (repeating) {
            mCameraSession.setRepeatingRequest(request, listener, handler);
        } else {
            mCameraSession.capture(request, listener, handler);
        }
    }

    /**
     * Stop the current active capture.
     *
     * @param fast When it is true, {@link CameraDevice#flush} is called, the stop capture
     * could be faster.
     */
    protected void stopCapture(boolean fast) throws Exception {
        if (VERBOSE) Log.v(TAG, ""Stopping capture"");

        if (fast) {
            /**
             * Flush is useful for canceling long exposure single capture, it also could help
             * to make the streaming capture stop sooner.
             */
            mCameraSession.abortCaptures();
            mCameraSessionListener.getStateWaiter().
                    waitForState(BlockingSessionCallback.SESSION_READY, CAMERA_IDLE_TIMEOUT_MS);
        } else {
            mCameraSession.close();
            mCameraSessionListener.getStateWaiter().
                    waitForState(BlockingSessionCallback.SESSION_CLOSED, CAMERA_IDLE_TIMEOUT_MS);
        }
    }

    /**
     * Open a {@link #CameraDevice camera device} and get the StaticMetadata for a given camera id.
     * The default mCameraListener is used to wait for states.
     *
     * @param cameraId The id of the camera device to be opened.
     */
    protected void openDevice(String cameraId) throws Exception {
        openDevice(cameraId, mCameraListener);
    }

    /**
     * Open a {@link #CameraDevice} and get the StaticMetadata for a given camera id and listener.
     *
     * @param cameraId The id of the camera device to be opened.
     * @param listener The {@link #BlockingStateCallback} used to wait for states.
     */
    protected void openDevice(String cameraId, BlockingStateCallback listener) throws Exception {
        mCamera = CameraTestUtils.openCamera(
                mCameraManager, cameraId, listener, mHandler);
        mCollector.setCameraId(cameraId);
        mStaticInfo = mAllStaticInfo.get(cameraId);
        if (mStaticInfo.isColorOutputSupported()) {
            mOrderedPreviewSizes = getSupportedPreviewSizes(
                    cameraId, mCameraManager,
                    getPreviewSizeBound(mWindowManager, PREVIEW_SIZE_BOUND));
            mOrderedVideoSizes = getSupportedVideoSizes(cameraId, mCameraManager, PREVIEW_SIZE_BOUND);
            mOrderedStillSizes = getSupportedStillSizes(cameraId, mCameraManager, null);
        }

        if (VERBOSE) {
            Log.v(TAG, ""Camera "" + cameraId + "" is opened"");
        }
    }

    /**
     * Create a {@link #CameraCaptureSession} using the currently open camera.
     *
     * @param outputSurfaces The set of output surfaces to configure for this session
     */
    protected void createSession(List<Surface> outputSurfaces) throws Exception {
        mCameraSessionListener = new BlockingSessionCallback();
        mCameraSession = CameraTestUtils.configureCameraSession(mCamera, outputSurfaces,
                mCameraSessionListener, mHandler);
    }

    /**
     * Create a reprocessable {@link #CameraCaptureSession} using the currently open camera.
     *
     * @param inputConfiguration The inputConfiguration for this session
     * @param outputSurfaces The set of output surfaces to configure for this session
     */
    protected void createReprocessableSession(InputConfiguration inputConfig,
            List<Surface> outputSurfaces) throws Exception {
        mCameraSessionListener = new BlockingSessionCallback();
        mCameraSession = CameraTestUtils.configureReprocessableCameraSession(
                mCamera, inputConfig, outputSurfaces, mCameraSessionListener, mHandler);
    }

    /**
     * Create a {@link #CameraCaptureSession} using the currently open camera with
     * OutputConfigurations.
     *
     * @param outputSurfaces The set of output surfaces to configure for this session
     */
    protected void createSessionByConfigs(List<OutputConfiguration> outputConfigs) throws Exception {
        mCameraSessionListener = new BlockingSessionCallback();
        mCameraSession = CameraTestUtils.configureCameraSessionWithConfig(mCamera, outputConfigs,
                mCameraSessionListener, mHandler);
    }

    /**
     * Close a {@link #CameraDevice camera device} and clear the associated StaticInfo field for a
     * given camera id. The default mCameraListener is used to wait for states.
     * <p>
     * This function must be used along with the {@link #openDevice} for the
     * same camera id.
     * </p>
     *
     * @param cameraId The id of the {@link #CameraDevice camera device} to be closed.
     */
    protected void closeDevice(String cameraId) {
        closeDevice(cameraId, mCameraListener);
    }

    /**
     * Close a {@link #CameraDevice camera device} and clear the associated StaticInfo field for a
     * given camera id and listener.
     * <p>
     * This function must be used along with the {@link #openDevice} for the
     * same camera id.
     * </p>
     *
     * @param cameraId The id of the camera device to be closed.
     * @param listener The BlockingStateCallback used to wait for states.
     */
    protected void closeDevice(String cameraId, BlockingStateCallback listener) {
        if (mCamera != null) {
            if (!cameraId.equals(mCamera.getId())) {
                throw new IllegalStateException(""Try to close a device that is not opened yet"");
            }
            mCamera.close();
            listener.waitForState(STATE_CLOSED, CAMERA_CLOSE_TIMEOUT_MS);
            mCamera = null;
            mCameraSession = null;
            mCameraSessionListener = null;
            mStaticInfo = null;
            mOrderedPreviewSizes = null;
            mOrderedVideoSizes = null;
            mOrderedStillSizes = null;

            if (VERBOSE) {
                Log.v(TAG, ""Camera "" + cameraId + "" is closed"");
            }
        }
    }

    /**
     * Create an {@link ImageReader} object and get the surface.
     * <p>
     * This function creates {@link ImageReader} object and surface, then assign
     * to the default {@link mReader} and {@link mReaderSurface}. It closes the
     * current default active {@link ImageReader} if it exists.
     * </p>
     *
     * @param size The size of this ImageReader to be created.
     * @param format The format of this ImageReader to be created
     * @param maxNumImages The max number of images that can be acquired
     *            simultaneously.
     * @param listener The listener used by this ImageReader to notify
     *            callbacks.
     */
    protected void createDefaultImageReader(Size size, int format, int maxNumImages,
            ImageReader.OnImageAvailableListener listener) throws Exception {
        closeDefaultImageReader();

        mReader = createImageReader(size, format, maxNumImages, listener);
        mReaderSurface = mReader.getSurface();
        if (VERBOSE) Log.v(TAG, ""Created ImageReader size "" + size.toString());
    }

    /**
     * Create an {@link ImageReader} object and get the surface.
     * <p>
     * This function creates {@link ImageReader} object and surface, then assign
     * to the default {@link mReader} and {@link mReaderSurface}. It closes the
     * current default active {@link ImageReader} if it exists.
     * </p>
     *
     * @param size The size of this ImageReader to be created.
     * @param format The format of this ImageReader to be created
     * @param maxNumImages The max number of images that can be acquired
     *            simultaneously.
     * @param usage The usage flag of the ImageReader
     * @param listener The listener used by this ImageReader to notify
     *            callbacks.
     */
    protected void createDefaultImageReader(Size size, int format, int maxNumImages, long usage,
            ImageReader.OnImageAvailableListener listener) throws Exception {
        closeDefaultImageReader();

        mReader = createImageReader(size, format, maxNumImages, usage, listener);
        mReaderSurface = mReader.getSurface();
        if (VERBOSE) Log.v(TAG, ""Created ImageReader size "" + size.toString());
    }

    /**
     * Create an {@link ImageReader} object.
     *
     * <p>This function creates image reader object for given format, maxImages, and size.</p>
     *
     * @param size The size of this ImageReader to be created.
     * @param format The format of this ImageReader to be created
     * @param maxNumImages The max number of images that can be acquired simultaneously.
     * @param listener The listener used by this ImageReader to notify callbacks.
     */

    protected ImageReader createImageReader(Size size, int format, int maxNumImages,
            ImageReader.OnImageAvailableListener listener) throws Exception {

        ImageReader reader = null;
        reader = ImageReader.newInstance(size.getWidth(), size.getHeight(),
                format, maxNumImages);

        reader.setOnImageAvailableListener(listener, mHandler);
        if (VERBOSE) Log.v(TAG, ""Created ImageReader size "" + size.toString());
        return reader;
    }

    /**
     * Create an {@link ImageReader} object.
     *
     * <p>This function creates image reader object for given format, maxImages, usage and size.</p>
     *
     * @param size The size of this ImageReader to be created.
     * @param format The format of this ImageReader to be created
     * @param maxNumImages The max number of images that can be acquired simultaneously.
     * @param usage The usage flag of the ImageReader
     * @param listener The listener used by this ImageReader to notify callbacks.
     */

    protected ImageReader createImageReader(Size size, int format, int maxNumImages, long usage,
            ImageReader.OnImageAvailableListener listener) throws Exception {
        ImageReader reader = null;
        reader = ImageReader.newInstance(size.getWidth(), size.getHeight(),
                format, maxNumImages, usage);

        reader.setOnImageAvailableListener(listener, mHandler);
        if (VERBOSE) Log.v(TAG, ""Created ImageReader size "" + size.toString());
        return reader;
    }

    /**
     * Close the pending images then close current default {@link ImageReader} object.
     */
    protected void closeDefaultImageReader() {
        closeImageReader(mReader);
        mReader = null;
        mReaderSurface = null;
    }

    /**
     * Close an image reader instance.
     *
     * @param reader
     */
    protected void closeImageReader(ImageReader reader) {
        if (reader != null) {
            try {
                // Close all possible pending images first.
                Image image = reader.acquireLatestImage();
                if (image != null) {
                    image.close();
                }
            } finally {
                reader.close();
                reader = null;
            }
        }
    }

    protected void checkImageReaderSessionConfiguration(String msg) throws Exception {
        checkImageReaderSessionConfiguration(msg, /*physicalCameraId*/null);
    }

    protected void checkImageReaderSessionConfiguration(String msg, String physicalCameraId)
            throws Exception {
        List<OutputConfiguration> outputConfigs = new ArrayList<OutputConfiguration>();
        OutputConfiguration config = new OutputConfiguration(mReaderSurface);
        if (physicalCameraId != null) {
            config.setPhysicalCameraId(physicalCameraId);
        }
        outputConfigs.add(config);
        checkSessionConfigurationSupported(mCamera, mHandler, outputConfigs, /*inputConfig*/ null,
                SessionConfiguration.SESSION_REGULAR, /*expectedResult*/ true, msg);
    }

    protected CaptureRequest prepareCaptureRequest() throws Exception {
        return prepareCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
    }

    protected CaptureRequest prepareCaptureRequest(int template) throws Exception {
        List<Surface> outputSurfaces = new ArrayList<Surface>();
        Surface surface = mReader.getSurface();
        assertNotNull(""Fail to get surface from ImageReader"", surface);
        outputSurfaces.add(surface);
        return prepareCaptureRequestForSurfaces(outputSurfaces, template)
                .build();
    }

    protected CaptureRequest.Builder prepareCaptureRequestForSurfaces(List<Surface> surfaces,
            int template)
            throws Exception {
        createSession(surfaces);

        CaptureRequest.Builder captureBuilder =
                mCamera.createCaptureRequest(template);
        assertNotNull(""Fail to get captureRequest"", captureBuilder);
        for (Surface surface : surfaces) {
            captureBuilder.addTarget(surface);
        }

        return captureBuilder;
    }

    protected CaptureRequest.Builder prepareCaptureRequestForConfigs(
            List<OutputConfiguration> outputConfigs, int template) throws Exception {
        createSessionByConfigs(outputConfigs);

        CaptureRequest.Builder captureBuilder =
                mCamera.createCaptureRequest(template);
        assertNotNull(""Fail to get captureRequest"", captureBuilder);
        for (OutputConfiguration config : outputConfigs) {
            for (Surface s : config.getSurfaces()) {
                captureBuilder.addTarget(s);
            }
        }

        return captureBuilder;
    }

    /**
     * Test the invalid Image access: accessing a closed image must result in
     * {@link IllegalStateException}.
     *
     * @param closedImage The closed image.
     * @param closedBuffer The ByteBuffer from a closed Image. buffer invalid
     *            access will be skipped if it is null.
     */
    protected void imageInvalidAccessTestAfterClose(Image closedImage,
            Plane closedPlane, ByteBuffer closedBuffer) {
        if (closedImage == null) {
            throw new IllegalArgumentException("" closedImage must be non-null"");
        }
        if (closedBuffer != null && !closedBuffer.isDirect()) {
            throw new IllegalArgumentException(""The input ByteBuffer should be direct ByteBuffer"");
        }

        if (closedPlane != null) {
            // Plane#getBuffer test
            try {
                closedPlane.getBuffer(); // An ISE should be thrown here.
                fail(""Image should throw IllegalStateException when calling getBuffer""
                        + "" after the image is closed"");
            } catch (IllegalStateException e) {
                // Expected.
            }

            // Plane#getPixelStride test
            try {
                closedPlane.getPixelStride(); // An ISE should be thrown here.
                fail(""Image should throw IllegalStateException when calling getPixelStride""
                        + "" after the image is closed"");
            } catch (IllegalStateException e) {
                // Expected.
            }

            // Plane#getRowStride test
            try {
                closedPlane.getRowStride(); // An ISE should be thrown here.
                fail(""Image should throw IllegalStateException when calling getRowStride""
                        + "" after the image is closed"");
            } catch (IllegalStateException e) {
                // Expected.
            }
        }

        // ByteBuffer access test
        if (closedBuffer != null) {
            try {
                closedBuffer.get(); // An ISE should be thrown here.
                fail(""Image should throw IllegalStateException when accessing a byte buffer""
                        + "" after the image is closed"");
            } catch (IllegalStateException e) {
                // Expected.
            }
        }

        // Image#getFormat test
        try {
            closedImage.getFormat();
            fail(""Image should throw IllegalStateException when calling getFormat""
                    + "" after the image is closed"");
        } catch (IllegalStateException e) {
            // Expected.
        }

        // Image#getWidth test
        try {
            closedImage.getWidth();
            fail(""Image should throw IllegalStateException when calling getWidth""
                    + "" after the image is closed"");
        } catch (IllegalStateException e) {
            // Expected.
        }

        // Image#getHeight test
        try {
            closedImage.getHeight();
            fail(""Image should throw IllegalStateException when calling getHeight""
                    + "" after the image is closed"");
        } catch (IllegalStateException e) {
            // Expected.
        }

        // Image#getTimestamp test
        try {
            closedImage.getTimestamp();
            fail(""Image should throw IllegalStateException when calling getTimestamp""
                    + "" after the image is closed"");
        } catch (IllegalStateException e) {
            // Expected.
        }

        // Image#getTimestamp test
        try {
            closedImage.getTimestamp();
            fail(""Image should throw IllegalStateException when calling getTimestamp""
                    + "" after the image is closed"");
        } catch (IllegalStateException e) {
            // Expected.
        }

        // Image#getCropRect test
        try {
            closedImage.getCropRect();
            fail(""Image should throw IllegalStateException when calling getCropRect""
                    + "" after the image is closed"");
        } catch (IllegalStateException e) {
            // Expected.
        }

        // Image#setCropRect test
        try {
            Rect rect = new Rect();
            closedImage.setCropRect(rect);
            fail(""Image should throw IllegalStateException when calling setCropRect""
                    + "" after the image is closed"");
        } catch (IllegalStateException e) {
            // Expected.
        }

        // Image#getPlanes test
        try {
            closedImage.getPlanes();
            fail(""Image should throw IllegalStateException when calling getPlanes""
                    + "" after the image is closed"");
        } catch (IllegalStateException e) {
            // Expected.
        }
    }
}"	""	""	"cdd"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"testPreviewFpsRange"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"public void testPreviewFpsRange() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            testPreviewFpsRangeByCamera(id);
        }
    }

    private void testPreviewFpsRangeByCamera(int cameraId) throws Exception {
        initializeMessageLooper(cameraId);

        // Test if the parameters exists and minimum fps <= maximum fps.
        final int INTERVAL_ERROR_THRESHOLD = 10;
        int[] defaultFps = new int[2];
        Parameters parameters = mCamera.getParameters();
        parameters.getPreviewFpsRange(defaultFps);
        List<int[]> fpsList = parameters.getSupportedPreviewFpsRange();
        assertTrue(fpsList.size() > 0);
        boolean found = false;
        for(int[] fps: fpsList) {
            assertTrue(fps[Parameters.PREVIEW_FPS_MIN_INDEX] > 0);
            assertTrue(fps[Parameters.PREVIEW_FPS_MIN_INDEX] <=
                       fps[Parameters.PREVIEW_FPS_MAX_INDEX]);
            if (!found && Arrays.equals(defaultFps, fps)) {
                found = true;
            }
        }
        assertTrue(""Preview fps range must be in the supported list."", found);

        // Test if the list is properly sorted.
        for (int i = 0; i < fpsList.size() - 1; i++) {
            int minFps1 = fpsList.get(i)[Parameters.PREVIEW_FPS_MIN_INDEX];
            int maxFps1 = fpsList.get(i)[Parameters.PREVIEW_FPS_MAX_INDEX];
            int minFps2 = fpsList.get(i + 1)[Parameters.PREVIEW_FPS_MIN_INDEX];
            int maxFps2 = fpsList.get(i + 1)[Parameters.PREVIEW_FPS_MAX_INDEX];
            assertTrue(maxFps1 < maxFps2
                    || (maxFps1 == maxFps2 && minFps1 < minFps2));
        }

        // Test if the actual fps is within fps range.
        Size size = parameters.getPreviewSize();
        int format = mCamera.getParameters().getPreviewFormat();
        int bitsPerPixel = ImageFormat.getBitsPerPixel(format);
        byte[] buffer1 = new byte[size.width * size.height * bitsPerPixel / 8];
        byte[] buffer2 = new byte[size.width * size.height * bitsPerPixel / 8];
        byte[] buffer3 = new byte[size.width * size.height * bitsPerPixel / 8];
        FpsRangePreviewCb callback = new FpsRangePreviewCb();
        int[] readBackFps = new int[2];
        for (int[] fps: fpsList) {
            parameters = mCamera.getParameters();
            parameters.setPreviewFpsRange(fps[Parameters.PREVIEW_FPS_MIN_INDEX],
                                          fps[Parameters.PREVIEW_FPS_MAX_INDEX]);
            callback.reset(fps[Parameters.PREVIEW_FPS_MIN_INDEX] / 1000.0,
                           fps[Parameters.PREVIEW_FPS_MAX_INDEX] / 1000.0);
            mCamera.setParameters(parameters);
            parameters = mCamera.getParameters();
            parameters.getPreviewFpsRange(readBackFps);
            MoreAsserts.assertEquals(fps, readBackFps);
            mCamera.addCallbackBuffer(buffer1);
            mCamera.addCallbackBuffer(buffer2);
            mCamera.addCallbackBuffer(buffer3);
            mCamera.setPreviewCallbackWithBuffer(callback);
            mCamera.startPreview();
            try {
                // Test the frame rate for a while.
                Thread.sleep(3000);
            } catch(Exception e) {
                // ignore
            }
            mCamera.stopPreview();
            // See if any frame duration violations occurred during preview run
            AssertionFailedError e = callback.getDurationException();
            if (e != null) throw(e);
            int numIntervalError = callback.getNumIntervalError();
            if (numIntervalError > INTERVAL_ERROR_THRESHOLD) {
                fail(String.format(
                        ""Too many preview callback frame intervals out of bounds: "" +
                                ""Count is %d, limit is %d"",
                        numIntervalError, INTERVAL_ERROR_THRESHOLD));
            }
        }

        // Test the invalid fps cases.
        parameters = mCamera.getParameters();
        parameters.setPreviewFpsRange(-1, -1);
        try {
            mCamera.setParameters(parameters);
            fail(""Should throw an exception if fps range is negative."");
        } catch (RuntimeException e) {
            // expected
        }
        parameters.setPreviewFpsRange(10, 5);
        try {
            mCamera.setParameters(parameters);
            fail(""Should throw an exception if fps range is invalid."");
        } catch (RuntimeException e) {
            // expected
        }

        terminateMessageLooper();
    }

    private final class FpsRangePreviewCb
            implements android.hardware.Camera.PreviewCallback {
        private double mMinFps, mMaxFps, mMaxFrameInterval, mMinFrameInterval;
        // An array storing the arrival time of the frames in the last second.
        private ArrayList<Long> mFrames = new ArrayList<Long>();
        private long firstFrameArrivalTime;
        private AssertionFailedError mDurationException = null;
        private int numIntervalError;

        public void reset(double minFps, double maxFps) {
            this.mMinFps = minFps;
            this.mMaxFps = maxFps;
            mMaxFrameInterval = 1000.0 / mMinFps;
            mMinFrameInterval = 1000.0 / mMaxFps;
            Log.v(TAG, ""Min fps="" + mMinFps + "". Max fps="" + mMaxFps
                    + "". Min frame interval="" + mMinFrameInterval
                    + "". Max frame interval="" + mMaxFrameInterval);
            mFrames.clear();
            firstFrameArrivalTime = 0;
            mDurationException = null;
            numIntervalError = 0;
        }

        // This method tests if the actual fps is between minimum and maximum.
        // It also tests if the frame interval is too long.
        public void onPreviewFrame(byte[] data, android.hardware.Camera camera) {
            long arrivalTime = SystemClock.elapsedRealtime();
            camera.addCallbackBuffer(data);
            if (firstFrameArrivalTime == 0) firstFrameArrivalTime = arrivalTime;

            // Remove the frames that arrived before the last second.
            Iterator<Long> it = mFrames.iterator();
            while(it.hasNext()) {
                long time = it.next();
                if (arrivalTime - time > 1000 && mFrames.size() > 2) {
                    it.remove();
                } else {
                    break;
                }
            }

            // Start the test after one second.
            if (arrivalTime - firstFrameArrivalTime > 1000) {
                assertTrue(mFrames.size() >= 2);

                // Check the frame interval and fps. The interval check
                // considers the time variance passing frames from the camera
                // hardware to the callback. It should be a constant time, not a
                // ratio. The fps check is more strict because individual
                // variance is averaged out.

                // Check if the frame interval is too large or too small.
                // x100 = percent, intervalMargin should be bigger than
                // fpsMargin considering that fps will be in the order of 10.
                double intervalMargin = 0.9;
                if (mIsExternalCamera) {
                    intervalMargin = 0.8;
                }
                long lastArrivalTime = mFrames.get(mFrames.size() - 1);
                double interval = arrivalTime - lastArrivalTime;
                if (VERBOSE) Log.v(TAG, ""Frame interval="" + interval);

                try {
                    if (interval > mMaxFrameInterval * (1.0 + intervalMargin) ||
                            interval < mMinFrameInterval * (1.0 - intervalMargin)) {
                        Log.i(TAG, ""Bad frame interval="" + interval + ""ms. Out out range "" +
                                mMinFrameInterval * (1.0 - intervalMargin) + ""/"" +
                                mMaxFrameInterval * (1.0 + intervalMargin));
                        numIntervalError++;
                    }
                    // Check if the fps is within range.
                    double fpsMargin = 0.5; // x100 = percent
                    if (mIsExternalCamera) {
                        fpsMargin = 0.6;
                    }
                    double avgInterval = (double)(arrivalTime - mFrames.get(0))
                            / mFrames.size();
                    double fps = 1000.0 / avgInterval;
                    assertTrue(""Actual fps ("" + fps + "") should be larger "" +
                            ""than min fps ("" + mMinFps + "")"",
                            fps >= mMinFps * (1.0 - fpsMargin));
                    assertTrue(""Actual fps ("" + fps + "") should be smaller"" +
                            ""than max fps ("" + mMaxFps + "")"",
                            fps <= mMaxFps * (1.0 + fpsMargin));
                } catch (AssertionFailedError e) {
                    // Need to throw this only in the test body, instead of in
                    // the callback
                    if (mDurationException == null) {
                        mDurationException = e;
                    }
                }
            }
            // Add the arrival time of this frame to the list.
            mFrames.add(arrivalTime);
        }

        public AssertionFailedError getDurationException() {
            return mDurationException;
        }
        public int getNumIntervalError() {
            return numIntervalError;
        }
    }

    private void assertEquals(Size expected, Size actual) {
        assertEquals(expected.width, actual.width);
        assertEquals(expected.height, actual.height);
    }

    private void assertEquals(float[] expected, float[] actual) {
        assertEquals(expected.length, actual.length);
        for (int i = 0; i < expected.length; i++) {
            assertEquals(expected[i], actual[i], 0.000001f);
        }
    }

    private void assertNoLetters(String value, String key) {
        for (int i = 0; i < value.length(); i++) {
            char c = value.charAt(i);
            assertFalse(""Parameter contains invalid characters. key,value=(""
                    + key + "","" + value + "")"",
                    Character.isLetter(c) && c != 'x');
        }
    }

    @UiThreadTest"	""	""	"minimum"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"TestShutterCallback"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"/*
 *.
 */

package android.hardware.cts;

import android.content.pm.PackageManager;
import android.graphics.BitmapFactory;
import android.graphics.ImageFormat;
import android.graphics.Rect;
import android.hardware.Camera;
import android.hardware.Camera.Area;
import android.hardware.Camera.CameraInfo;
import android.hardware.Camera.ErrorCallback;
import android.hardware.Camera.Face;
import android.hardware.Camera.FaceDetectionListener;
import android.hardware.Camera.Parameters;
import android.hardware.Camera.PictureCallback;
import android.hardware.Camera.ShutterCallback;
import android.hardware.Camera.Size;
import android.hardware.cts.helpers.CameraUtils;
import android.media.CamcorderProfile;
import android.media.ExifInterface;
import android.media.MediaRecorder;
import android.os.Build;
import android.os.ConditionVariable;
import android.os.Looper;
import android.os.SystemClock;
import android.test.MoreAsserts;
import android.test.UiThreadTest;
import android.test.suitebuilder.annotation.LargeTest;
import android.util.Log;
import android.view.SurfaceHolder;

import androidx.test.rule.ActivityTestRule;

import junit.framework.Assert;
import junit.framework.AssertionFailedError;

import org.junit.After;
import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.text.ParseException;
import java.text.ParsePosition;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Date;
import java.util.Iterator;
import java.util.List;
import java.util.TimeZone;

import com.android.compatibility.common.util.WindowUtil;

/**
 * This test case must run with hardware. It can't be tested in emulator
 */
@LargeTest
public class CameraTest extends Assert {
    private static final String TAG = ""CameraTest"";
    private static final String PACKAGE = ""android.hardware.cts"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);
    private String mRecordingPath = null;
    private String mJpegPath = null;
    private byte[] mJpegData;

    private static final int PREVIEW_CALLBACK_NOT_RECEIVED = 0;
    private static final int PREVIEW_CALLBACK_RECEIVED = 1;
    private static final int PREVIEW_CALLBACK_DATA_NULL = 2;
    private static final int PREVIEW_CALLBACK_INVALID_FRAME_SIZE = 3;
    private int mPreviewCallbackResult = PREVIEW_CALLBACK_NOT_RECEIVED;

    private boolean mShutterCallbackResult = false;
    private boolean mRawPictureCallbackResult = false;
    private boolean mJpegPictureCallbackResult = false;
    private static final int NO_ERROR = -1;
    private int mCameraErrorCode = NO_ERROR;
    private boolean mAutoFocusSucceeded = false;

    private static final int WAIT_FOR_COMMAND_TO_COMPLETE = 5000;  // Milliseconds.
    private static final int WAIT_FOR_FOCUS_TO_COMPLETE = 5000;
    private static final int WAIT_FOR_SNAPSHOT_TO_COMPLETE = 5000;

    private static final int FOCUS_AREA = 0;
    private static final int METERING_AREA = 1;

    private static final int AUTOEXPOSURE_LOCK = 0;
    private static final int AUTOWHITEBALANCE_LOCK = 1;

    // For external camera recording
    private static final int VIDEO_BIT_RATE_IN_BPS = 128000;

    // Some exif tags that are not defined by ExifInterface but supported.
    private static final String TAG_DATETIME_DIGITIZED = ""DateTimeDigitized"";
    private static final String TAG_SUBSEC_TIME = ""SubSecTime"";
    private static final String TAG_SUBSEC_TIME_ORIG = ""SubSecTimeOriginal"";
    private static final String TAG_SUBSEC_TIME_DIG = ""SubSecTimeDigitized"";

    private PreviewCallback mPreviewCallback = new PreviewCallback();
    private TestShutterCallback mShutterCallback = new TestShutterCallback();
    private RawPictureCallback mRawPictureCallback = new RawPictureCallback();
    private JpegPictureCallback mJpegPictureCallback = new JpegPictureCallback();
    private TestErrorCallback mErrorCallback = new TestErrorCallback();
    private AutoFocusCallback mAutoFocusCallback = new AutoFocusCallback();
    private AutoFocusMoveCallback mAutoFocusMoveCallback = new AutoFocusMoveCallback();

    private Looper mLooper = null;
    private final ConditionVariable mPreviewDone = new ConditionVariable();
    private final ConditionVariable mFocusDone = new ConditionVariable();
    private final ConditionVariable mSnapshotDone = new ConditionVariable();
    private int[] mCameraIds;

    Camera mCamera;
    boolean mIsExternalCamera;

    @Rule
    public ActivityTestRule<CameraCtsActivity> mActivityRule =
            new ActivityTestRule<>(CameraCtsActivity.class);

    @Before
    public void setUp() throws Exception {
        // Some of the tests run on the UI thread. In case some of the operations take a long time to complete,
        // wait for window to receive focus. This ensure that the focus event from input flinger has been handled,
        // and avoids getting ANR.
        WindowUtil.waitForFocus(mActivityRule.getActivity());
        mCameraIds = CameraUtils.deriveCameraIdsUnderTest();
    }

    @After
    public void tearDown() throws Exception {
        if (mCamera != null) {
            mCamera.release();
            mCamera = null;
        }
    }

    /*
     * Initializes the message looper so that the Camera object can
     * receive the callback messages.
     */
    private void initializeMessageLooper(final int cameraId) throws IOException {
        LooperInfo looperInfo = new LooperInfo();
        initializeMessageLooper(cameraId, mErrorCallback, looperInfo);
        mIsExternalCamera = looperInfo.isExternalCamera;
        mCamera = looperInfo.camera;
        mLooper = looperInfo.looper;
    }

    private final class LooperInfo {
        Camera camera = null;
        Looper looper = null;
        boolean isExternalCamera = false;
    };

    /*
     * Initializes the message looper so that the Camera object can
     * receive the callback messages.
     */
    private void initializeMessageLooper(final int cameraId, final ErrorCallback errorCallback,
            LooperInfo looperInfo) throws IOException {
        final ConditionVariable startDone = new ConditionVariable();
        final CameraCtsActivity activity = mActivityRule.getActivity();
        new Thread() {
            @Override
            public void run() {
                Log.v(TAG, ""start loopRun for cameraId "" + cameraId);
                // Set up a looper to be used by camera.
                Looper.prepare();
                // Save the looper so that we can terminate this thread
                // after we are done with it.
                looperInfo.looper = Looper.myLooper();
                try {
                    looperInfo.isExternalCamera = CameraUtils.isExternal(
                            activity.getApplicationContext(), cameraId);
                } catch (Exception e) {
                    Log.e(TAG, ""Unable to query external camera!"" + e);
                }

                try {
                    looperInfo.camera = Camera.open(cameraId);
                    looperInfo.camera.setErrorCallback(errorCallback);
                } catch (RuntimeException e) {
                    Log.e(TAG, ""Fail to open camera id "" + cameraId + "": "" + e);
                }
                Log.v(TAG, ""camera"" + cameraId + "" is opened"");
                startDone.open();
                Looper.loop(); // Blocks forever until Looper.quit() is called.
                if (VERBOSE) Log.v(TAG, ""initializeMessageLooper: quit."");
            }
        }.start();

        Log.v(TAG, ""start waiting for looper"");
        if (!startDone.block(WAIT_FOR_COMMAND_TO_COMPLETE)) {
            Log.v(TAG, ""initializeMessageLooper: start timeout"");
            fail(""initializeMessageLooper: start timeout"");
        }
        assertNotNull(""Fail to open camera "" + cameraId, looperInfo.camera);
        looperInfo.camera.setPreviewDisplay(activity.getSurfaceView().getHolder());
        File parent = activity.getPackageManager().isInstantApp()
                ? activity.getFilesDir()
                : activity.getExternalFilesDir(null);

        mJpegPath = parent.getPath() + ""/test.jpg"";
        mRecordingPath = parent.getPath() + ""/test_video.mp4"";
    }

    /*
     * Terminates the message looper thread.
     */
    private void terminateMessageLooper() throws Exception {
        terminateMessageLooper(false);
    }

    /*
     * Terminates the message looper thread, optionally allowing evict error
     */
    private void terminateMessageLooper(boolean allowEvict) throws Exception {
        LooperInfo looperInfo = new LooperInfo();
        looperInfo.camera = mCamera;
        looperInfo.looper = mLooper;
        terminateMessageLooper(allowEvict, mCameraErrorCode, looperInfo);
        mCamera = null;
    }

    /*
     * Terminates the message looper thread, optionally allowing evict error
     */
    private void terminateMessageLooper(final boolean allowEvict, final int errorCode,
            final LooperInfo looperInfo) throws Exception {
        looperInfo.looper.quit();
        // Looper.quit() is asynchronous. The looper may still has some
        // preview callbacks in the queue after quit is called. The preview
        // callback still uses the camera object (setHasPreviewCallback).
        // After camera is released, RuntimeException will be thrown from
        // the method. So we need to join the looper thread here.
        looperInfo.looper.getThread().join();
        looperInfo.camera.release();
        looperInfo.camera = null;
        if (allowEvict) {
            assertTrue(""Got unexpected camera error callback."",
                    (NO_ERROR == errorCode ||
                    Camera.CAMERA_ERROR_EVICTED == errorCode));
        } else {
            assertEquals(""Got camera error callback."", NO_ERROR, errorCode);
        }
    }

    // Align 'x' to 'to', which should be a power of 2
    private static int align(int x, int to) {
        return (x + (to-1)) & ~(to - 1);
    }
    private static int calculateBufferSize(int width, int height,
                                           int format, int bpp) {

        if (VERBOSE) {
            Log.v(TAG, ""calculateBufferSize: w="" + width + "",h="" + height
            + "",f="" + format + "",bpp="" + bpp);
        }

        if (format == ImageFormat.YV12) {
            /*
            http://developer.android.com/reference/android/graphics/ImageFormat.html#YV12
            */

            int stride = align(width, 16);

            int y_size = stride * height;
            int c_stride = align(stride/2, 16);
            int c_size = c_stride * height/2;
            int size = y_size + c_size * 2;

            if (VERBOSE) {
                Log.v(TAG, ""calculateBufferSize: YV12 size= "" + size);
            }

            return size;

        }
        else {
            return width * height * bpp / 8;
        }
    }

    //Implement the previewCallback
    private final class PreviewCallback
            implements android.hardware.Camera.PreviewCallback {
        public void onPreviewFrame(byte [] data, Camera camera) {
            if (data == null) {
                mPreviewCallbackResult = PREVIEW_CALLBACK_DATA_NULL;
                mPreviewDone.open();
                return;
            }
            Size size = camera.getParameters().getPreviewSize();
            int format = camera.getParameters().getPreviewFormat();
            int bitsPerPixel = ImageFormat.getBitsPerPixel(format);
            if (calculateBufferSize(size.width, size.height,
                    format, bitsPerPixel) != data.length) {
                Log.e(TAG, ""Invalid frame size "" + data.length + "". width="" + size.width
                        + "". height="" + size.height + "". bitsPerPixel="" + bitsPerPixel);
                mPreviewCallbackResult = PREVIEW_CALLBACK_INVALID_FRAME_SIZE;
                mPreviewDone.open();
                return;
            }
            mPreviewCallbackResult = PREVIEW_CALLBACK_RECEIVED;
            mCamera.stopPreview();
            if (VERBOSE) Log.v(TAG, ""notify the preview callback"");
            mPreviewDone.open();
            if (VERBOSE) Log.v(TAG, ""Preview callback stop"");
        }
    }

    //Implement the shutterCallback
    private final class TestShutterCallback implements ShutterCallback {
        public void onShutter() {
            mShutterCallbackResult = true;
            if (VERBOSE) Log.v(TAG, ""onShutter called"");
        }
    }

    //Implement the RawPictureCallback
    private final class RawPictureCallback implements PictureCallback {
        public void onPictureTaken(byte [] rawData, Camera camera) {
            mRawPictureCallbackResult = true;
            if (VERBOSE) Log.v(TAG, ""RawPictureCallback callback"");
        }
    }

    // Implement the JpegPictureCallback
    private final class JpegPictureCallback implements PictureCallback {
        public void onPictureTaken(byte[] rawData, Camera camera) {
            try {
                mJpegData = rawData;
                if (rawData != null) {
                    // try to store the picture on the SD card
                    File rawoutput = new File(mJpegPath);
                    FileOutputStream outStream = new FileOutputStream(rawoutput);
                    outStream.write(rawData);
                    outStream.close();
                    mJpegPictureCallbackResult = true;

                    if (VERBOSE) {
                        Log.v(TAG, ""JpegPictureCallback rawDataLength = "" + rawData.length);
                    }
                } else {
                    mJpegPictureCallbackResult = false;
                }
                mSnapshotDone.open();
                if (VERBOSE) Log.v(TAG, ""Jpeg Picture callback"");
            } catch (IOException e) {
                // no need to fail here; callback worked fine
                Log.w(TAG, ""Error writing picture to sd card."");
            }
        }
    }

    // Implement the ErrorCallback
    private final class TestErrorCallback implements ErrorCallback {
        public void onError(int error, Camera camera) {
            Log.e(TAG, ""Got camera error="" + error);
            mCameraErrorCode = error;
        }
    }

    // parent independent version of TestErrorCallback
    private static final class TestErrorCallbackI implements ErrorCallback {
        private int mCameraErrorCode = NO_ERROR;
        public void onError(int error, Camera camera) {
            Log.e(TAG, ""Got camera error="" + error);
            mCameraErrorCode = error;
        }
    }

    private final class AutoFocusCallback
            implements android.hardware.Camera.AutoFocusCallback {
        public void onAutoFocus(boolean success, Camera camera) {
            mAutoFocusSucceeded = success;
            Log.v(TAG, ""AutoFocusCallback success="" + success);
            mFocusDone.open();
        }
    }

    private final class AutoFocusMoveCallback
            implements android.hardware.Camera.AutoFocusMoveCallback {
        @Override
        public void onAutoFocusMoving(boolean start, Camera camera) {
        }
    }

    private void waitForPreviewDone() {
        if (VERBOSE) Log.v(TAG, ""Wait for preview callback"");
        if (!mPreviewDone.block(WAIT_FOR_COMMAND_TO_COMPLETE)) {
            // timeout could be expected or unexpected. The caller will decide.
            Log.v(TAG, ""waitForPreviewDone: timeout"");
        }
        mPreviewDone.close();
    }

    private boolean waitForFocusDone() {
        boolean result = mFocusDone.block(WAIT_FOR_FOCUS_TO_COMPLETE);
        if (!result) {
            // timeout could be expected or unexpected. The caller will decide.
            Log.v(TAG, ""waitForFocusDone: timeout"");
        }
        mFocusDone.close();
        return result;
    }

    private void waitForSnapshotDone() {
        if (!mSnapshotDone.block(WAIT_FOR_SNAPSHOT_TO_COMPLETE)) {
            // timeout could be expected or unexpected. The caller will decide.
            Log.v(TAG, ""waitForSnapshotDone: timeout"");
        }
        mSnapshotDone.close();
    }

    private void checkPreviewCallback() throws Exception {
        if (VERBOSE) Log.v(TAG, ""check preview callback"");
        mCamera.startPreview();
        waitForPreviewDone();
        mCamera.setPreviewCallback(null);
    }

    /**
     * Start preview and wait for the first preview callback, which indicates the
     * preview becomes active.
     */
    private void blockingStartPreview() {
        mCamera.setPreviewCallback(new SimplePreviewStreamCb(/*Id*/0));
        mCamera.startPreview();
        waitForPreviewDone();
        mCamera.setPreviewCallback(null);
    }

    /*
     * Test case 1: Take a picture and verify all the callback
     * functions are called properly.
     */
    @UiThreadTest"	""	""	"12"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"testParameters"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"public void testParameters() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            testParametersByCamera(id);
        }
    }

    private void testParametersByCamera(int cameraId) throws Exception {
        initializeMessageLooper(cameraId);
        // we can get parameters just by getxxx method due to the private constructor
        Parameters pSet = mCamera.getParameters();
        assertParameters(pSet);
        terminateMessageLooper();
    }

    // Also test Camera.Parameters
    private void assertParameters(Parameters parameters) {
        // Parameters constants
        final int PICTURE_FORMAT = ImageFormat.JPEG;
        final int PREVIEW_FORMAT = ImageFormat.NV21;

        // Before setting Parameters
        final int origPictureFormat = parameters.getPictureFormat();
        final int origPictureWidth = parameters.getPictureSize().width;
        final int origPictureHeight = parameters.getPictureSize().height;
        final int origPreviewFormat = parameters.getPreviewFormat();
        final int origPreviewWidth = parameters.getPreviewSize().width;
        final int origPreviewHeight = parameters.getPreviewSize().height;
        final int origPreviewFrameRate = parameters.getPreviewFrameRate();

        assertTrue(origPictureWidth > 0);
        assertTrue(origPictureHeight > 0);
        assertTrue(origPreviewWidth > 0);
        assertTrue(origPreviewHeight > 0);
        assertTrue(origPreviewFrameRate > 0);

        // The default preview format must be yuv420 (NV21).
        assertEquals(ImageFormat.NV21, origPreviewFormat);

        // The default picture format must be Jpeg.
        assertEquals(ImageFormat.JPEG, origPictureFormat);

        // If camera supports flash, the default flash mode must be off.
        String flashMode = parameters.getFlashMode();
        assertTrue(flashMode == null || flashMode.equals(parameters.FLASH_MODE_OFF));
        String wb = parameters.getWhiteBalance();
        assertTrue(wb == null || wb.equals(parameters.WHITE_BALANCE_AUTO));
        String effect = parameters.getColorEffect();
        assertTrue(effect == null || effect.equals(parameters.EFFECT_NONE));

        // Some parameters must be supported.
        List<Size> previewSizes = parameters.getSupportedPreviewSizes();
        List<Size> pictureSizes = parameters.getSupportedPictureSizes();
        List<Integer> previewFormats = parameters.getSupportedPreviewFormats();
        List<Integer> pictureFormats = parameters.getSupportedPictureFormats();
        List<Integer> frameRates = parameters.getSupportedPreviewFrameRates();
        List<String> focusModes = parameters.getSupportedFocusModes();
        String focusMode = parameters.getFocusMode();
        float focalLength = parameters.getFocalLength();
        float horizontalViewAngle = parameters.getHorizontalViewAngle();
        float verticalViewAngle = parameters.getVerticalViewAngle();
        int jpegQuality = parameters.getJpegQuality();
        int jpegThumnailQuality = parameters.getJpegThumbnailQuality();
        assertTrue(previewSizes != null && previewSizes.size() != 0);
        assertTrue(pictureSizes != null && pictureSizes.size() != 0);
        assertTrue(previewFormats != null && previewFormats.size() >= 2);
        assertTrue(previewFormats.contains(ImageFormat.NV21));
        assertTrue(previewFormats.contains(ImageFormat.YV12));
        assertTrue(pictureFormats != null && pictureFormats.size() != 0);
        assertTrue(frameRates != null && frameRates.size() != 0);
        assertTrue(focusModes != null && focusModes.size() != 0);
        assertNotNull(focusMode);
        // The default focus mode must be auto if it exists.
        if (focusModes.contains(Parameters.FOCUS_MODE_AUTO)) {
            assertEquals(Parameters.FOCUS_MODE_AUTO, focusMode);
        }

        if (mIsExternalCamera) {
            // External camera by default reports -1.0, but don't fail if
            // the HAL implementation somehow chooses to report this information.
            assertTrue(focalLength == -1.0 || focalLength > 0);
            assertTrue(horizontalViewAngle == -1.0 ||
                    (horizontalViewAngle > 0 && horizontalViewAngle <= 360));
            assertTrue(verticalViewAngle == -1.0 ||
                    (verticalViewAngle > 0 && verticalViewAngle <= 360));
        } else {
            assertTrue(focalLength > 0);
            assertTrue(horizontalViewAngle > 0 && horizontalViewAngle <= 360);
            assertTrue(verticalViewAngle > 0 && verticalViewAngle <= 360);
        }

        Size previewSize = previewSizes.get(0);
        Size pictureSize = pictureSizes.get(0);
        assertTrue(jpegQuality >= 1 && jpegQuality <= 100);
        assertTrue(jpegThumnailQuality >= 1 && jpegThumnailQuality <= 100);

        // If a parameter is supported, both getXXX and getSupportedXXX have to
        // be non null.
        if (parameters.getWhiteBalance() != null) {
            assertNotNull(parameters.getSupportedWhiteBalance());
        }
        if (parameters.getSupportedWhiteBalance() != null) {
            assertNotNull(parameters.getWhiteBalance());
        }
        if (parameters.getColorEffect() != null) {
            assertNotNull(parameters.getSupportedColorEffects());
        }
        if (parameters.getSupportedColorEffects() != null) {
            assertNotNull(parameters.getColorEffect());
        }
        if (parameters.getAntibanding() != null) {
            assertNotNull(parameters.getSupportedAntibanding());
        }
        if (parameters.getSupportedAntibanding() != null) {
            assertNotNull(parameters.getAntibanding());
        }
        if (parameters.getSceneMode() != null) {
            assertNotNull(parameters.getSupportedSceneModes());
        }
        if (parameters.getSupportedSceneModes() != null) {
            assertNotNull(parameters.getSceneMode());
        }
        if (parameters.getFlashMode() != null) {
            assertNotNull(parameters.getSupportedFlashModes());
        }
        if (parameters.getSupportedFlashModes() != null) {
            assertNotNull(parameters.getFlashMode());
        }

        // Check if the sizes value contain invalid characters.
        assertNoLetters(parameters.get(""preview-size-values""), ""preview-size-values"");
        assertNoLetters(parameters.get(""picture-size-values""), ""picture-size-values"");
        assertNoLetters(parameters.get(""jpeg-thumbnail-size-values""),
                ""jpeg-thumbnail-size-values"");

        // Set the parameters.
        parameters.setPictureFormat(PICTURE_FORMAT);
        assertEquals(PICTURE_FORMAT, parameters.getPictureFormat());
        parameters.setPictureSize(pictureSize.width, pictureSize.height);
        assertEquals(pictureSize.width, parameters.getPictureSize().width);
        assertEquals(pictureSize.height, parameters.getPictureSize().height);
        parameters.setPreviewFormat(PREVIEW_FORMAT);
        assertEquals(PREVIEW_FORMAT, parameters.getPreviewFormat());
        parameters.setPreviewFrameRate(frameRates.get(0));
        assertEquals(frameRates.get(0).intValue(), parameters.getPreviewFrameRate());
        parameters.setPreviewSize(previewSize.width, previewSize.height);
        assertEquals(previewSize.width, parameters.getPreviewSize().width);
        assertEquals(previewSize.height, parameters.getPreviewSize().height);

        mCamera.setParameters(parameters);
        Parameters paramActual = mCamera.getParameters();

        assertTrue(isValidPixelFormat(paramActual.getPictureFormat()));
        assertEquals(pictureSize.width, paramActual.getPictureSize().width);
        assertEquals(pictureSize.height, paramActual.getPictureSize().height);
        assertTrue(isValidPixelFormat(paramActual.getPreviewFormat()));
        assertEquals(previewSize.width, paramActual.getPreviewSize().width);
        assertEquals(previewSize.height, paramActual.getPreviewSize().height);
        assertTrue(paramActual.getPreviewFrameRate() > 0);

        checkExposureCompensation(parameters);
        checkPreferredPreviewSizeForVideo(parameters);
    }

    private void checkPreferredPreviewSizeForVideo(Parameters parameters) {
        List<Size> videoSizes = parameters.getSupportedVideoSizes();
        Size preferredPreviewSize = parameters.getPreferredPreviewSizeForVideo();

        // If getSupportedVideoSizes() returns null,
        // getPreferredPreviewSizeForVideo() will return null;
        // otherwise, if getSupportedVideoSizes() does not return null,
        // getPreferredPreviewSizeForVideo() will not return null.
        if (videoSizes == null) {
            assertNull(preferredPreviewSize);
        } else {
            assertNotNull(preferredPreviewSize);
        }

        // If getPreferredPreviewSizeForVideo() returns null,
        // getSupportedVideoSizes() will return null;
        // otherwise, if getPreferredPreviewSizeForVideo() does not return null,
        // getSupportedVideoSizes() will not return null.
        if (preferredPreviewSize == null) {
            assertNull(videoSizes);
        } else {
            assertNotNull(videoSizes);
        }

        if (videoSizes != null) {  // implies: preferredPreviewSize != null
            // If getSupportedVideoSizes() does not return null,
            // the returned list will contain at least one size.
            assertTrue(videoSizes.size() > 0);

            // In addition, getPreferredPreviewSizeForVideo() returns a size
            // that is among the supported preview sizes.
            List<Size> previewSizes = parameters.getSupportedPreviewSizes();
            assertNotNull(previewSizes);
            assertTrue(previewSizes.size() > 0);
            assertTrue(previewSizes.contains(preferredPreviewSize));
        }
    }

    private void checkExposureCompensation(Parameters parameters) {
        assertEquals(0, parameters.getExposureCompensation());
        int max = parameters.getMaxExposureCompensation();
        int min = parameters.getMinExposureCompensation();
        float step = parameters.getExposureCompensationStep();
        if (max == 0 && min == 0) {
            assertEquals(0f, step, 0.000001f);
            return;
        }
        assertTrue(step > 0);
        assertTrue(max >= 0);
        assertTrue(min <= 0);
    }

    private boolean isValidPixelFormat(int format) {
        return (format == ImageFormat.RGB_565) || (format == ImageFormat.NV21)
                || (format == ImageFormat.JPEG) || (format == ImageFormat.YUY2);
    }

    @UiThreadTest"	""	""	"12"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"testJpegExif"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"public void testJpegExif() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            initializeMessageLooper(id);
            testJpegExifByCamera(false);
            terminateMessageLooper();
        }
    }

    private void testJpegExifByCamera(boolean recording) throws Exception {
        if (!recording) mCamera.startPreview();
        // Get current time in milliseconds, removing the millisecond part
        long captureStartTime = System.currentTimeMillis() / 1000 * 1000;
        mCamera.takePicture(mShutterCallback, mRawPictureCallback, mJpegPictureCallback);
        waitForSnapshotDone();

        Camera.Parameters parameters = mCamera.getParameters();
        double focalLength = parameters.getFocalLength();

        // Test various exif tags.
        ExifInterface exif = new ExifInterface(mJpegPath);
        StringBuffer failedCause = new StringBuffer(""Jpeg exif test failed:\n"");
        boolean extraExiftestPassed = checkExtraExifTagsSucceeds(failedCause, exif);

        if (VERBOSE) Log.v(TAG, ""Testing exif tag TAG_DATETIME"");
        String datetime = exif.getAttribute(ExifInterface.TAG_DATETIME);
        assertNotNull(datetime);
        assertTrue(datetime.length() == 19); // EXIF spec is ""yyyy:MM:dd HH:mm:ss"".
        // Datetime should be local time.
        SimpleDateFormat exifDateFormat = new SimpleDateFormat(""yyyy:MM:dd HH:mm:ss"");
        try {
            Date exifDateTime = exifDateFormat.parse(datetime);
            long captureFinishTime = exifDateTime.getTime();
            long timeDelta = captureFinishTime - captureStartTime;
            assertTrue(String.format(""Snapshot delay (%d ms) is not in range of [0, %d]"", timeDelta,
                    WAIT_FOR_SNAPSHOT_TO_COMPLETE),
                    timeDelta >= 0 && timeDelta <= WAIT_FOR_SNAPSHOT_TO_COMPLETE);
        } catch (ParseException e) {
            fail(String.format(""Invalid string value on exif tag TAG_DATETIME: %s"", datetime));
        }
        checkGpsDataNull(exif);
        double exifFocalLength = exif.getAttributeDouble(ExifInterface.TAG_FOCAL_LENGTH, -1);
        assertEquals(focalLength, exifFocalLength, 0.001);
        // Test image width and height exif tags. They should match the jpeg.
        assertBitmapAndJpegSizeEqual(mJpegData, exif);

        // Test gps exif tags.
        if (VERBOSE) Log.v(TAG, ""Testing exif GPS tags"");
        testGpsExifValues(parameters, 37.736071, -122.441983, 21, 1199145600,
            ""GPS NETWORK HYBRID ARE ALL FINE."");
        testGpsExifValues(parameters, 0.736071, 0.441983, 1, 1199145601, ""GPS"");
        testGpsExifValues(parameters, -89.736071, -179.441983, 100000, 1199145602, ""NETWORK"");

        // Test gps tags do not exist after calling removeGpsData. Also check if
        // image width and height exif match the jpeg when jpeg rotation is set.
        if (VERBOSE) Log.v(TAG, ""Testing exif GPS tag removal"");
        if (!recording) mCamera.startPreview();
        parameters.removeGpsData();
        parameters.setRotation(90); // For testing image width and height exif.
        mCamera.setParameters(parameters);
        mCamera.takePicture(mShutterCallback, mRawPictureCallback, mJpegPictureCallback);
        waitForSnapshotDone();
        exif = new ExifInterface(mJpegPath);
        checkGpsDataNull(exif);
        assertBitmapAndJpegSizeEqual(mJpegData, exif);
        // Reset the rotation to prevent from affecting other tests.
        parameters.setRotation(0);
        mCamera.setParameters(parameters);
    }

    /**
     * Correctness check of some extra exif tags.
     * <p>
     * Check some extra exif tags without asserting the check failures
     * immediately. When a failure is detected, the failure cause is logged,
     * the rest of the tests are still executed. The caller can assert with the
     * failure cause based on the returned test status.
     * </p>
     *
     * @param logBuf Log failure cause to this StringBuffer if there is
     * any failure.
     * @param exif The exif data associated with a jpeg image being tested.
     * @return true if no test failure is found, false if there is any failure.
     */
    private boolean checkExtraExifTagsSucceeds(StringBuffer logBuf, ExifInterface exif) {
        if (logBuf == null || exif == null) {
            throw new IllegalArgumentException(""failureCause and exif shouldn't be null"");
        }

        if (VERBOSE) Log.v(TAG, ""Testing extra exif tags"");
        boolean allTestsPassed = true;
        boolean passedSoFar = true;
        String failureMsg;

        // TAG_EXPOSURE_TIME
        // ExifInterface API gives exposure time value in the form of float instead of rational
        String exposureTime = exif.getAttribute(ExifInterface.TAG_EXPOSURE_TIME);
        passedSoFar = expectNotNull(""Exif TAG_EXPOSURE_TIME is null!"", logBuf, exposureTime);
        if (passedSoFar) {
            double exposureTimeValue = Double.parseDouble(exposureTime);
            failureMsg = ""Exif exposure time "" + exposureTime + "" should be a positive value"";
            passedSoFar = expectTrue(failureMsg, logBuf, exposureTimeValue > 0);
        }
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_APERTURE
        // ExifInterface API gives aperture value in the form of float instead of rational
        String aperture = exif.getAttribute(ExifInterface.TAG_APERTURE);
        passedSoFar = expectNotNull(""Exif TAG_APERTURE is null!"", logBuf, aperture);
        if (passedSoFar) {
            double apertureValue = Double.parseDouble(aperture);
            passedSoFar = expectTrue(""Exif TAG_APERTURE value "" + aperture + "" should be positive!"",
                    logBuf, apertureValue > 0);
        }
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_FLASH
        String flash = exif.getAttribute(ExifInterface.TAG_FLASH);
        passedSoFar = expectNotNull(""Exif TAG_FLASH is null!"", logBuf, flash);
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_WHITE_BALANCE
        String whiteBalance = exif.getAttribute(ExifInterface.TAG_WHITE_BALANCE);
        passedSoFar = expectNotNull(""Exif TAG_WHITE_BALANCE is null!"", logBuf, whiteBalance);
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_MAKE
        String make = exif.getAttribute(ExifInterface.TAG_MAKE);
        passedSoFar = expectNotNull(""Exif TAG_MAKE is null!"", logBuf, make);
        if (passedSoFar) {
            passedSoFar = expectTrue(""Exif TAG_MODEL value: "" + make
                    + "" should match build manufacturer: "" + Build.MANUFACTURER, logBuf,
                    make.equals(Build.MANUFACTURER));
        }
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_MODEL
        String model = exif.getAttribute(ExifInterface.TAG_MODEL);
        passedSoFar = expectNotNull(""Exif TAG_MODEL is null!"", logBuf, model);
        if (passedSoFar) {
            passedSoFar = expectTrue(""Exif TAG_MODEL value: "" + model
                    + "" should match build manufacturer: "" + Build.MODEL, logBuf,
                    model.equals(Build.MODEL));
        }
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_ISO
        int iso = exif.getAttributeInt(ExifInterface.TAG_ISO, -1);
        passedSoFar = expectTrue(""Exif ISO value "" + iso + "" is invalid"", logBuf, iso > 0);
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_DATETIME_DIGITIZED (a.k.a Create time for digital cameras).
        String digitizedTime = exif.getAttribute(TAG_DATETIME_DIGITIZED);
        passedSoFar = expectNotNull(""Exif TAG_DATETIME_DIGITIZED is null!"", logBuf, digitizedTime);
        if (passedSoFar) {
            String datetime = exif.getAttribute(ExifInterface.TAG_DATETIME);
            passedSoFar = expectNotNull(""Exif TAG_DATETIME is null!"", logBuf, datetime);
            if (passedSoFar) {
                passedSoFar = expectTrue(""dataTime should match digitizedTime"", logBuf,
                        digitizedTime.equals(datetime));
            }
        }
        allTestsPassed = allTestsPassed && passedSoFar;

        /**
         * TAG_SUBSEC_TIME. Since the sub second tag strings are truncated to at
         * most 9 digits in ExifInterface implementation, use getAttributeInt to
         * sanitize it. When the default value -1 is returned, it means that
         * this exif tag either doesn't exist or is a non-numerical invalid
         * string. Same rule applies to the rest of sub second tags.
         */
        int subSecTime = exif.getAttributeInt(TAG_SUBSEC_TIME, -1);
        passedSoFar = expectTrue(
                ""Exif TAG_SUBSEC_TIME value is null or invalid!"", logBuf, subSecTime > 0);
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_SUBSEC_TIME_ORIG
        int subSecTimeOrig = exif.getAttributeInt(TAG_SUBSEC_TIME_ORIG, -1);
        passedSoFar = expectTrue(
                ""Exif TAG_SUBSEC_TIME_ORIG value is null or invalid!"", logBuf, subSecTimeOrig > 0);
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_SUBSEC_TIME_DIG
        int subSecTimeDig = exif.getAttributeInt(TAG_SUBSEC_TIME_DIG, -1);
        passedSoFar = expectTrue(
                ""Exif TAG_SUBSEC_TIME_DIG value is null or invalid!"", logBuf, subSecTimeDig > 0);
        allTestsPassed = allTestsPassed && passedSoFar;

        return allTestsPassed;
    }

    /**
     * Check if object is null and log failure msg.
     *
     * @param msg Failure msg.
     * @param logBuffer StringBuffer to log the failure msg.
     * @param obj Object to test.
     * @return true if object is not null, otherwise return false.
     */
    private boolean expectNotNull(String msg, StringBuffer logBuffer, Object obj)
    {
        if (obj == null) {
            logBuffer.append(msg + ""\n"");
        }
        return (obj != null);
    }

    /**
     * Check if condition is false and log failure msg.
     *
     * @param msg Failure msg.
     * @param logBuffer StringBuffer to log the failure msg.
     * @param condition Condition to test.
     * @return The value of the condition.
     */
    private boolean expectTrue(String msg, StringBuffer logBuffer, boolean condition) {
        if (!condition) {
            logBuffer.append(msg + ""\n"");
        }
        return condition;
    }

    private void assertBitmapAndJpegSizeEqual(byte[] jpegData, ExifInterface exif) {
        int exifWidth = exif.getAttributeInt(ExifInterface.TAG_IMAGE_WIDTH, 0);
        int exifHeight = exif.getAttributeInt(ExifInterface.TAG_IMAGE_LENGTH, 0);
        assertTrue(exifWidth != 0 && exifHeight != 0);
        BitmapFactory.Options bmpOptions = new BitmapFactory.Options();
        bmpOptions.inJustDecodeBounds = true;
        BitmapFactory.decodeByteArray(jpegData, 0, jpegData.length, bmpOptions);
        assertEquals(bmpOptions.outWidth, exifWidth);
        assertEquals(bmpOptions.outHeight, exifHeight);
    }

    private void testGpsExifValues(Parameters parameters, double latitude,
            double longitude, double altitude, long timestamp, String method)
            throws IOException {
        mCamera.startPreview();
        parameters.setGpsLatitude(latitude);
        parameters.setGpsLongitude(longitude);
        parameters.setGpsAltitude(altitude);
        parameters.setGpsTimestamp(timestamp);
        parameters.setGpsProcessingMethod(method);
        mCamera.setParameters(parameters);
        mCamera.takePicture(mShutterCallback, mRawPictureCallback, mJpegPictureCallback);
        waitForSnapshotDone();
        ExifInterface exif = new ExifInterface(mJpegPath);
        assertNotNull(exif.getAttribute(ExifInterface.TAG_GPS_LATITUDE));
        assertNotNull(exif.getAttribute(ExifInterface.TAG_GPS_LONGITUDE));
        assertNotNull(exif.getAttribute(ExifInterface.TAG_GPS_LATITUDE_REF));
        assertNotNull(exif.getAttribute(ExifInterface.TAG_GPS_LONGITUDE_REF));
        assertNotNull(exif.getAttribute(ExifInterface.TAG_GPS_TIMESTAMP));
        assertNotNull(exif.getAttribute(ExifInterface.TAG_GPS_DATESTAMP));
        assertEquals(method, exif.getAttribute(ExifInterface.TAG_GPS_PROCESSING_METHOD));
        float[] latLong = new float[2];
        assertTrue(exif.getLatLong(latLong));
        assertEquals((float)latitude, latLong[0], 0.0001f);
        assertEquals((float)longitude, latLong[1], 0.0001f);
        assertEquals(altitude, exif.getAltitude(-1), 1);
        assertEquals(timestamp, getGpsDateTimeFromJpeg(exif) / 1000);
    }

    private long getGpsDateTimeFromJpeg(ExifInterface exif) {
        String date = exif.getAttribute(ExifInterface.TAG_GPS_DATESTAMP);
        String time = exif.getAttribute(ExifInterface.TAG_GPS_TIMESTAMP);
        if (date == null || time == null) return -1;

        String dateTimeString = date + ' ' + time;
        ParsePosition pos = new ParsePosition(0);
        try {
            SimpleDateFormat formatter = new SimpleDateFormat(""yyyy:MM:dd HH:mm:ss"");
            formatter.setTimeZone(TimeZone.getTimeZone(""UTC""));

            Date datetime = formatter.parse(dateTimeString, pos);
            if (datetime == null) return -1;
            return datetime.getTime();
        } catch (IllegalArgumentException ex) {
            return -1;
        }
    }

    private void checkGpsDataNull(ExifInterface exif) {
        assertNull(exif.getAttribute(ExifInterface.TAG_GPS_LATITUDE));
        assertNull(exif.getAttribute(ExifInterface.TAG_GPS_LONGITUDE));
        assertNull(exif.getAttribute(ExifInterface.TAG_GPS_LATITUDE_REF));
        assertNull(exif.getAttribute(ExifInterface.TAG_GPS_LONGITUDE_REF));
        assertNull(exif.getAttribute(ExifInterface.TAG_GPS_TIMESTAMP));
        assertNull(exif.getAttribute(ExifInterface.TAG_GPS_DATESTAMP));
        assertNull(exif.getAttribute(ExifInterface.TAG_GPS_PROCESSING_METHOD));
    }

    @UiThreadTest"	""	""	"12"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiViewTest"	"testSharedSurfaceYUVImageReaderSwitch"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiViewTest.java"	""	"public void testSharedSurfaceYUVImageReaderSwitch() throws Exception {
        int YUVFormats[] = {ImageFormat.YUV_420_888, ImageFormat.YUV_422_888,
            ImageFormat.YUV_444_888, ImageFormat.YUY2, ImageFormat.YV12,
            ImageFormat.NV16, ImageFormat.NV21};
        for (String cameraId : mCameraIdsUnderTest) {
            try {
                openCamera(cameraId);
                if (getStaticInfo(cameraId).isHardwareLevelLegacy()) {
                    Log.i(TAG, ""Camera "" + cameraId + "" is legacy, skipping"");
                    continue;
                }
                if (!getStaticInfo(cameraId).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + cameraId +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                Size frameSize = null;
                int yuvFormat = -1;
                for (int it : YUVFormats) {
                    Size yuvSizes[] = getStaticInfo(cameraId).getAvailableSizesForFormatChecked(
                            it, StaticMetadata.StreamDirection.Output);
                    if (yuvSizes != null) {
                        frameSize = yuvSizes[0];
                        yuvFormat = it;
                        break;
                    }
                }

                if ((yuvFormat != -1) && (frameSize.getWidth() > 0) &&
                        (frameSize.getHeight() > 0)) {
                    testSharedSurfaceYUVImageReaderSwitch(cameraId, NUM_SURFACE_SWITCHES, yuvFormat,
                            frameSize, /*blockMaxAcquired*/ false);
                    testSharedSurfaceYUVImageReaderSwitch(cameraId, NUM_SURFACE_SWITCHES, yuvFormat,
                            frameSize, /*blockMaxAcquired*/ true);
                } else {
                    Log.i(TAG, ""Camera "" + cameraId +
                            "" does not support YUV outputs, skipping"");
                }
            }
            finally {
                closeCamera(cameraId);
            }
        }
    }

    private void testSharedSurfaceYUVImageReaderSwitch(String cameraId, int switchCount, int format,
            Size frameSize, boolean blockMaxAcquired) throws Exception {

        assertTrue(""YUV_IMG_READER_COUNT should be equal or greater than 2"",
                (YUV_IMG_READER_COUNT >= 2));

        SimpleImageListener imageListeners[] = new SimpleImageListener[YUV_IMG_READER_COUNT];
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        ImageReader imageReaders[] = new ImageReader[YUV_IMG_READER_COUNT];
        Surface readerSurfaces[] = new Surface[YUV_IMG_READER_COUNT];

        for (int i = 0; i < YUV_IMG_READER_COUNT; i++) {
            imageListeners[i] = new SimpleImageListener();
            imageReaders[i] = ImageReader.newInstance(frameSize.getWidth(), frameSize.getHeight(),
                    format, 2);
            imageReaders[i].setOnImageAvailableListener(imageListeners[i], mHandler);
            readerSurfaces[i] = imageReaders[i].getSurface();
        }

        OutputConfiguration outputConfig = new OutputConfiguration(readerSurfaces[0]);
        outputConfig.enableSurfaceSharing();
        List<OutputConfiguration> outputConfigurations = new ArrayList<>();
        outputConfigurations.add(outputConfig);
        if (outputConfig.getMaxSharedSurfaceCount() < YUV_IMG_READER_COUNT) {
            return;
        }

        createSessionWithConfigs(cameraId, outputConfigurations);

        // Test YUV ImageReader surface sharing. The first ImageReader will
        // always be part of the capture request, the rest will switch on each
        // iteration.
        // If 'blockMaxAcquired' is enabled, the first image reader will acquire
        // the maximum possible amount of buffers and also block a few more.
        int maxAcquiredImages = imageReaders[0].getMaxImages();
        int acquiredCount = 0;
        Image[] acquiredImages = new Image[maxAcquiredImages];
        for (int j = 0; j < switchCount; j++) {
            for (int i = 1; i < YUV_IMG_READER_COUNT; i++) {
                outputConfig.addSurface(readerSurfaces[i]);
                updateOutputConfiguration(cameraId, outputConfig);
                CaptureRequest.Builder imageReaderRequestBuilder = getCaptureBuilder(cameraId,
                        CameraDevice.TEMPLATE_PREVIEW);
                imageReaderRequestBuilder.addTarget(readerSurfaces[i]);
                if (blockMaxAcquired) {
                    if (acquiredCount <= (maxAcquiredImages + 1)) {
                        // Camera should be able to handle cases where
                        // one output blocks more buffers than the respective
                        // maximum acquired count.
                        imageReaderRequestBuilder.addTarget(readerSurfaces[0]);
                    }
                } else {
                    imageReaderRequestBuilder.addTarget(readerSurfaces[0]);
                }
                capture(cameraId, imageReaderRequestBuilder.build(), resultListener);
                imageListeners[i].waitForAnyImageAvailable(PREVIEW_TIME_MS);
                Image img = imageReaders[i].acquireLatestImage();
                assertNotNull(""Invalid image acquired!"", img);
                assertNotNull(""Image planes are invalid!"", img.getPlanes());
                img.close();
                if (blockMaxAcquired) {
                    if (acquiredCount < maxAcquiredImages) {
                        imageListeners[0].waitForAnyImageAvailable(PREVIEW_TIME_MS);
                        acquiredImages[acquiredCount] = imageReaders[0].acquireNextImage();
                    }
                    acquiredCount++;
                } else {
                    imageListeners[0].waitForAnyImageAvailable(PREVIEW_TIME_MS);
                    img = imageReaders[0].acquireLatestImage();
                    assertNotNull(""Invalid image acquired!"", img);
                    img.close();
                }
                outputConfig.removeSurface(readerSurfaces[i]);
                updateOutputConfiguration(cameraId, outputConfig);
            }
        }

        for (int i = 0; i < YUV_IMG_READER_COUNT; i++) {
            imageReaders[i].close();
        }
    }

    /*
     * Test the dynamic shared surface limit.
     */"	""	""	"12"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionImageReaderTest"	"testMultiResolutionCaptureCharacteristics"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionImageReaderTest.java"	""	"public void testMultiResolutionCaptureCharacteristics() {
        for (String id : mCameraIdsUnderTest) {
            if (VERBOSE) {
                Log.v(TAG, ""Testing multi-resolution capture characteristics for Camera "" + id);
            }
            StaticMetadata info = mAllStaticInfo.get(id);
            CameraCharacteristics c = info.getCharacteristics();
            StreamConfigurationMap config = c.get(
                    CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            int[] outputFormats = config.getOutputFormats();
            int[] capabilities = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
            boolean isLogicalCamera = CameraTestUtils.contains(capabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_LOGICAL_MULTI_CAMERA);
            boolean isUltraHighResCamera = info.isUltraHighResolutionSensor();
            Set<String> physicalCameraIds = c.getPhysicalCameraIds();

            MultiResolutionStreamConfigurationMap multiResolutionMap = c.get(
                    CameraCharacteristics.SCALER_MULTI_RESOLUTION_STREAM_CONFIGURATION_MAP);
            if (multiResolutionMap == null) {
                Log.i(TAG, ""Camera "" + id + "" doesn't support multi-resolution capture."");
                continue;
            }
            if (VERBOSE) {
                Log.v(TAG, ""MULTI_RESOLUTION_STREAM_CONFIGURATION_MAP: ""
                        + multiResolutionMap.toString());
            }

            int[] multiResolutionOutputFormats = multiResolutionMap.getOutputFormats();
            assertTrue(""Camera "" + id + "" must be a logical multi-camera or ultra high res camera ""
                    + ""to support multi-resolution capture."",
                    isLogicalCamera || isUltraHighResCamera);

            for (int format : multiResolutionOutputFormats) {
                assertTrue(String.format(""Camera %s: multi-resolution output format %d ""
                        + ""isn't a supported format"", id, format),
                        CameraTestUtils.contains(outputFormats, format));

                Collection<MultiResolutionStreamInfo> multiResolutionStreams =
                        multiResolutionMap.getOutputInfo(format);
                assertTrue(String.format(""Camera %s supports %d multi-resolution ""
                        + ""outputInfo, expected at least 2"", id,
                        multiResolutionStreams.size()),
                        multiResolutionStreams.size() >= 2);

                // Make sure that each multi-resolution output stream info has the maximum size
                // for that format.
                for (MultiResolutionStreamInfo streamInfo : multiResolutionStreams) {
                    String physicalCameraId = streamInfo.getPhysicalCameraId();
                    Size streamSize = new Size(streamInfo.getWidth(), streamInfo.getHeight());
                    if (!isLogicalCamera) {
                        assertTrue(""Camera "" + id + "" is ultra high resolution camera, but "" +
                                ""the multi-resolution stream info camera Id  "" + physicalCameraId +
                                "" doesn't match"", physicalCameraId.equals(id));
                    } else {
                        assertTrue(""Camera "" + id + ""'s multi-resolution output info "" +
                                ""physical camera id "" + physicalCameraId + "" isn't valid"",
                                physicalCameraIds.contains(physicalCameraId));
                    }

                    StaticMetadata pInfo = mAllStaticInfo.get(physicalCameraId);
                    CameraCharacteristics pChar = pInfo.getCharacteristics();
                    StreamConfigurationMap pConfig = pChar.get(
                            CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
                    Size[] sizes = pConfig.getOutputSizes(format);
                    assertTrue(String.format(""Camera %s must ""
                            + ""support at least one output size for output ""
                            + ""format %d."", physicalCameraId, format),
                             sizes != null && sizes.length > 0);

                    List<Size> maxSizes = new ArrayList<Size>();
                    maxSizes.add(CameraTestUtils.getMaxSize(sizes));
                    StreamConfigurationMap pMaxResConfig = pChar.get(CameraCharacteristics.
                            SCALER_STREAM_CONFIGURATION_MAP_MAXIMUM_RESOLUTION);
                    if (pMaxResConfig != null) {
                        Size[] maxResSizes = pMaxResConfig.getOutputSizes(format);
                        if (maxResSizes != null && maxResSizes.length > 0) {
                            maxSizes.add(CameraTestUtils.getMaxSize(maxResSizes));
                        }
                    }

                    assertTrue(String.format(""Camera %s's supported multi-resolution""
                           + "" size %s for physical camera %s is not one of the largest ""
                           + ""supported sizes %s for format %d"", id, streamSize,
                           physicalCameraId, maxSizes, format),
                           maxSizes.contains(streamSize));
                }
            }
        }
    }"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionImageReaderTest"	"testMultiResolutionImageReaderJpeg"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionImageReaderTest.java"	""	"public void testMultiResolutionImageReaderJpeg() throws Exception {
        testMultiResolutionImageReaderForFormat(ImageFormat.JPEG, /*repeating*/false);
    }"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionImageReaderTest"	"testMultiResolutionImageReaderFlexibleYuv"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionImageReaderTest.java"	""	"public void testMultiResolutionImageReaderFlexibleYuv() throws Exception {
        testMultiResolutionImageReaderForFormat(ImageFormat.YUV_420_888, /*repeating*/false);
    }"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionImageReaderTest"	"testMultiResolutionImageReaderRaw"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionImageReaderTest.java"	""	"public void testMultiResolutionImageReaderRaw() throws Exception {
        testMultiResolutionImageReaderForFormat(ImageFormat.RAW_SENSOR, /*repeating*/false);
    }"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionImageReaderTest"	"testMultiResolutionImageReaderPrivate"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionImageReaderTest.java"	""	"public void testMultiResolutionImageReaderPrivate() throws Exception {
        testMultiResolutionImageReaderForFormat(ImageFormat.PRIVATE, /*repeating*/false);
    }"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionImageReaderTest"	"testMultiResolutionImageReaderRepeatingJpeg"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionImageReaderTest.java"	""	"public void testMultiResolutionImageReaderRepeatingJpeg() throws Exception {
        testMultiResolutionImageReaderForFormat(ImageFormat.JPEG, /*repeating*/true);
    }"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionImageReaderTest"	"testMultiResolutionImageReaderRepeatingFlexibleYuv"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionImageReaderTest.java"	""	"public void testMultiResolutionImageReaderRepeatingFlexibleYuv() throws Exception {
        testMultiResolutionImageReaderForFormat(ImageFormat.YUV_420_888, /*repeating*/true);
    }"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionImageReaderTest"	"testMultiResolutionImageReaderRepeatingRaw"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionImageReaderTest.java"	""	"public void testMultiResolutionImageReaderRepeatingRaw() throws Exception {
        testMultiResolutionImageReaderForFormat(ImageFormat.RAW_SENSOR, /*repeating*/true);
    }"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionImageReaderTest"	"testMultiResolutionImageReaderRepeatingPrivate"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionImageReaderTest.java"	""	"public void testMultiResolutionImageReaderRepeatingPrivate() throws Exception {
        testMultiResolutionImageReaderForFormat(ImageFormat.PRIVATE, /*repeating*/true);
    }

    /**
     * Test for making sure the mandatory stream combinations work for multi-resolution output.
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionImageReaderTest"	"testMultiResolutionMandatoryStreamCombinationTest"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionImageReaderTest.java"	""	"public void testMultiResolutionMandatoryStreamCombinationTest() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            StaticMetadata info = mAllStaticInfo.get(id);
            CameraCharacteristics c = info.getCharacteristics();
            MandatoryStreamCombination[] combinations = c.get(
                            CameraCharacteristics.SCALER_MANDATORY_STREAM_COMBINATIONS);
            if (combinations == null) {
                Log.i(TAG, ""No mandatory stream combinations for camera: "" + id + "" skip test"");
                continue;
            }
            MultiResolutionStreamConfigurationMap multiResolutionMap = c.get(
                    CameraCharacteristics.SCALER_MULTI_RESOLUTION_STREAM_CONFIGURATION_MAP);
            if (multiResolutionMap == null) {
                Log.i(TAG, ""Camera "" + id + "" doesn't support multi-resolution capture."");
                continue;
            }
            int[] multiResolutionOutputFormats = multiResolutionMap.getOutputFormats();
            if (multiResolutionOutputFormats.length == 0) {
                Log.i(TAG, ""Camera "" + id + "" doesn't support multi-resolution output capture."");
                continue;
            }

            try {
                openDevice(id);
                for (MandatoryStreamCombination combination : combinations) {
                    if (combination.isReprocessable()) {
                        continue;
                    }

                    List<MandatoryStreamCombination.MandatoryStreamInformation> streamsInfo =
                            combination.getStreamsInformation();
                    for (MandatoryStreamCombination.MandatoryStreamInformation mandateInfo :
                            streamsInfo) {
                        boolean supportMultiResOutput = CameraTestUtils.contains(
                                multiResolutionOutputFormats, mandateInfo.getFormat());
                        if (mandateInfo.isMaximumSize() && supportMultiResOutput)  {
                            testMultiResolutionMandatoryStreamCombination(id, info, combination,
                                    multiResolutionMap);
                            break;
                        }
                    }
                }
            } finally {
                closeDevice(id);
            }
        }
    }

    private void testMultiResolutionMandatoryStreamCombination(String cameraId,
            StaticMetadata staticInfo, MandatoryStreamCombination combination,
            MultiResolutionStreamConfigurationMap multiResStreamConfig) throws Exception {
        String log = ""Testing multi-resolution mandatory stream combination: "" +
                combination.getDescription() + "" on camera: "" + cameraId;
        Log.i(TAG, log);

        final int TIMEOUT_FOR_RESULT_MS = 1000;
        final int MIN_RESULT_COUNT = 3;

        // Set up outputs
        List<OutputConfiguration> outputConfigs = new ArrayList<OutputConfiguration>();
        List<Surface> outputSurfaces = new ArrayList<Surface>();
        StreamCombinationTargets targets = new StreamCombinationTargets();

        CameraTestUtils.setupConfigurationTargets(combination.getStreamsInformation(),
                targets, outputConfigs, outputSurfaces, MIN_RESULT_COUNT,
                /*substituteY8*/false, /*substituteHeic*/false, /*physicalCameraId*/null,
                multiResStreamConfig, mHandler);

        boolean haveSession = false;
        try {
            CaptureRequest.Builder requestBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);

            for (Surface s : outputSurfaces) {
                requestBuilder.addTarget(s);
            }

            CameraCaptureSession.CaptureCallback mockCaptureCallback =
                    mock(CameraCaptureSession.CaptureCallback.class);

            checkSessionConfigurationSupported(mCamera, mHandler, outputConfigs,
                    /*inputConfig*/ null, SessionConfiguration.SESSION_REGULAR,
                    true/*defaultSupport*/, String.format(
                    ""Session configuration query for multi-res combination: %s failed"",
                    combination.getDescription()));

            createSessionByConfigs(outputConfigs);
            haveSession = true;
            CaptureRequest request = requestBuilder.build();
            mCameraSession.setRepeatingRequest(request, mockCaptureCallback, mHandler);

            verify(mockCaptureCallback,
                    timeout(TIMEOUT_FOR_RESULT_MS * MIN_RESULT_COUNT).atLeast(MIN_RESULT_COUNT))
                    .onCaptureCompleted(
                        eq(mCameraSession),
                        eq(request),
                        isA(TotalCaptureResult.class));
            verify(mockCaptureCallback, never()).
                    onCaptureFailed(
                        eq(mCameraSession),
                        eq(request),
                        isA(CaptureFailure.class));

        } catch (Throwable e) {
            mCollector.addMessage(
                    String.format(""Mandatory multi-res stream combination: %s failed due: %s"",
                    combination.getDescription(), e.getMessage()));
        }
        if (haveSession) {
            try {
                Log.i(TAG, String.format(
                        ""Done with camera %s, multi-res combination: %s, closing session"",
                        cameraId, combination.getDescription()));
                stopCapture(/*fast*/false);
            } catch (Throwable e) {
                mCollector.addMessage(
                    String.format(""Closing down for multi-res combination: %s failed due to: %s"",
                            combination.getDescription(), e.getMessage()));
            }
        }

        targets.close();
    }

    private void testMultiResolutionImageReaderForFormat(int format, boolean repeating)
            throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                if (VERBOSE) {
                    Log.v(TAG, ""Testing multi-resolution capture for Camera "" + id
                            + "" format "" + format + "" repeating "" + repeating);
                }
                StaticMetadata staticInfo = mAllStaticInfo.get(id);
                CameraCharacteristics c = staticInfo.getCharacteristics();

                // Find the supported multi-resolution output stream info for the specified format
                MultiResolutionStreamConfigurationMap multiResolutionMap = c.get(
                        CameraCharacteristics.SCALER_MULTI_RESOLUTION_STREAM_CONFIGURATION_MAP);
                if (multiResolutionMap == null) {
                    Log.i(TAG, ""Camera "" + id + "" doesn't support multi-resolution image reader."");
                    continue;
                }
                int[] outputFormats = multiResolutionMap.getOutputFormats();
                if (!CameraTestUtils.contains(outputFormats, format)) {
                    Log.i(TAG, ""Camera "" + id + "" doesn't support multi-resolution image reader ""
                            + ""for format "" + format + "" vs "" + Arrays.toString(outputFormats));
                    continue;
                }
                Collection<MultiResolutionStreamInfo> multiResolutionStreams =
                        multiResolutionMap.getOutputInfo(format);

               /* Test the multi-resolution ImageReader at different zoom ratios
                 * to give the camera device best chance to switch between
                 * physical cameras.*/
                List<Float> zoomRatios = CameraTestUtils.getCandidateZoomRatios(staticInfo);

                openDevice(id);
                multiResolutionImageReaderFormatTestByCamera(format,
                        multiResolutionStreams, zoomRatios, repeating);
            } finally {
                closeDevice(id);
            }
        }
    }

    private void multiResolutionImageReaderFormatTestByCamera(int format,
            Collection<MultiResolutionStreamInfo> multiResolutionStreams, List<Float> zoomRatios,
            boolean repeating) throws Exception {
        try {
            int numFrameVerified = repeating ? NUM_FRAME_VERIFIED : 1;

            // Create multi-resolution ImageReader
            mMultiResolutionImageReader = new MultiResolutionImageReader(
                    multiResolutionStreams, format, MAX_NUM_IMAGES);
            mListener = new SimpleMultiResolutionImageReaderListener(
                    mMultiResolutionImageReader, MAX_NUM_IMAGES, repeating);
            mMultiResolutionImageReader.setOnImageAvailableListener(mListener,
                    new HandlerExecutor(mHandler));

            // Create session
            Collection<OutputConfiguration> outputConfigs =
                    OutputConfiguration.createInstancesForMultiResolutionOutput(
                    mMultiResolutionImageReader);
            ArrayList<OutputConfiguration> outputConfigsList = new ArrayList<OutputConfiguration>(
                    outputConfigs);
            createSessionByConfigs(outputConfigsList);

            CaptureRequest.Builder captureBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
            assertNotNull(""Failed to create captureRequest"", captureBuilder);
            captureBuilder.addTarget(mMultiResolutionImageReader.getSurface());

            // Capture images at different zoom ratios
            SimpleCaptureCallback listener = new SimpleCaptureCallback();
            for (Float zoomRatio : zoomRatios) {
                captureBuilder.set(CaptureRequest.CONTROL_ZOOM_RATIO, zoomRatio);
                CaptureRequest request = captureBuilder.build();

                int sequenceId = -1;
                if (repeating) {
                    sequenceId = mCameraSession.setRepeatingRequest(request, listener, mHandler);
                } else {
                    mCameraSession.capture(request, listener, mHandler);
                }

                // Validate  images
                validateImage(format, multiResolutionStreams, numFrameVerified, listener,
                        repeating);

                if (repeating) {
                    mCameraSession.stopRepeating();
                    listener.getCaptureSequenceLastFrameNumber(sequenceId, CAPTURE_TIMEOUT);
                    listener.drain();
                }

                // Return all pending images to the ImageReader as the validateImage may
                // take a while to return and there could be many images pending.
                mMultiResolutionImageReader.flush();
                mListener.reset();
            }
        } finally {
            // Close MultiResolutionImageReader
            if (mMultiResolutionImageReader != null) {
                mMultiResolutionImageReader.close();
            }
            mMultiResolutionImageReader = null;
        }
    }

    private void validateImage(int format, Collection<MultiResolutionStreamInfo> streams,
            int captureCount, SimpleCaptureCallback listener, boolean repeating) throws Exception {
        ImageAndMultiResStreamInfo imgAndStreamInfo;
        final int MAX_RETRY_COUNT = 20;
        int retryCount = 0;
        int numImageVerified = 0;
        while (numImageVerified < captureCount) {
            assertNotNull(""Image listener is null"", mListener);
            imgAndStreamInfo = mListener.getAnyImageAndInfoAvailable(CAPTURE_WAIT_TIMEOUT_MS);
            if (imgAndStreamInfo == null && retryCount < MAX_RETRY_COUNT) {
                // For acquireLatestImage, a null image may be returned.
                retryCount++;
                continue;
            }

            Image img = imgAndStreamInfo.image;
            MultiResolutionStreamInfo streamInfoForImage = imgAndStreamInfo.streamInfo;
            mCollector.expectEquals(String.format(""Output image width %d doesn't match "" +
                    "" the expected width %d"", img.getWidth(), streamInfoForImage.getWidth()),
                    img.getWidth(), streamInfoForImage.getWidth());
            mCollector.expectEquals(String.format(""Output image height %d doesn't match "" +
                    "" the expected height %d"", img.getHeight(), streamInfoForImage.getHeight()),
                    img.getHeight(), streamInfoForImage.getHeight());

            if (format != ImageFormat.PRIVATE) {
                CameraTestUtils.validateImage(img, img.getWidth(), img.getHeight(), format,
                        mDebugFileNameBase);
            } else {
                mCollector.expectEquals(String.format(""Output image format %d doesn't match "" +
                        ""expected format %d"", img.getFormat(), format), format, img.getFormat());
            }

            // Get active physical camera id in the capture result. Only do the correlation
            // between activePhysicalCameraId with image size for single request for simplicity
            // reasons.
            String activePhysicalCameraId = null;
            if (!repeating && mStaticInfo.isActivePhysicalCameraIdSupported()) {
                TotalCaptureResult result = listener.getCaptureResult(
                        WAIT_FOR_RESULT_TIMEOUT_MS, img.getTimestamp());
                activePhysicalCameraId =
                        result.get(CaptureResult.LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID);
                mCollector.expectNotNull(
                        ""Camera's capture result should contain ACTIVE_PHYSICAL_ID"",
                        activePhysicalCameraId);
                mCollector.expectEquals(String.format(""Active physical camera id %s doesn't "" +
                        ""match the expected physical camera id %s for the image"",
                        activePhysicalCameraId, streamInfoForImage.getPhysicalCameraId()),
                        activePhysicalCameraId, streamInfoForImage.getPhysicalCameraId());
            }

            // Make sure the image size is one within streams
            boolean validSize = false;
            for (MultiResolutionStreamInfo streamInfo : streams) {
                if (streamInfoForImage.getPhysicalCameraId().equals(
                        streamInfo.getPhysicalCameraId())
                        && streamInfo.getWidth() == img.getWidth()
                        && streamInfo.getHeight() == img.getHeight()) {
                    validSize = true;
                }
            }
            mCollector.expectTrue(String.format(""Camera's physical camera id + image size "" +
                    ""[%s: %d, %d] must be the supported multi-resolution output streams "" +
                    ""for current physical camera"", streamInfoForImage.getPhysicalCameraId(),
                    img.getWidth(), img.getHeight()), validSize);

            HardwareBuffer hwb = img.getHardwareBuffer();
            assertNotNull(""Unable to retrieve the Image's HardwareBuffer"", hwb);

            img.close();
            numImageVerified++;
        }
    }
}"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.formats.CameraFormatsActivity"	"setPassFailButtonClickListeners"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/formats/CameraFormatsActivity.java"	""	"public void test/*
 *.
 */
package com.android.cts.verifier.camera.formats;

import com.android.cts.verifier.PassFailButtons;
import com.android.cts.verifier.R;

import android.app.AlertDialog;
import android.graphics.Bitmap;
import android.graphics.Color;
import android.graphics.ColorMatrix;
import android.graphics.ColorMatrixColorFilter;
import android.graphics.ImageFormat;
import android.graphics.Matrix;
import android.graphics.SurfaceTexture;
import android.hardware.Camera;
import android.hardware.Camera.CameraInfo;
import android.os.AsyncTask;
import android.os.Bundle;
import android.os.Handler;
import android.util.Log;
import android.util.SparseArray;
import android.view.Menu;
import android.view.MenuItem;
import android.view.View;
import android.view.Surface;
import android.view.TextureView;
import android.widget.AdapterView;
import android.widget.ArrayAdapter;
import android.widget.Button;
import android.widget.ImageButton;
import android.widget.ImageView;
import android.widget.Spinner;
import android.widget.Toast;

import java.io.IOException;
import java.lang.Math;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.Comparator;
import java.util.List;
import java.util.Optional;
import java.util.TreeSet;

/**
 * Tests for manual verification of the CDD-required camera output formats
 * for preview callbacks
 */
public class CameraFormatsActivity extends PassFailButtons.Activity
        implements TextureView.SurfaceTextureListener, Camera.PreviewCallback {

    private static final String TAG = ""CameraFormats"";

    private TextureView mPreviewView;
    private SurfaceTexture mPreviewTexture;
    private int mPreviewTexWidth;
    private int mPreviewTexHeight;
    private int mPreviewRotation;

    private ImageView mFormatView;

    private Spinner mCameraSpinner;
    private Spinner mFormatSpinner;
    private Spinner mResolutionSpinner;

    private int mCurrentCameraId = -1;
    private Camera mCamera;

    private List<Camera.Size> mPreviewSizes;
    private Camera.Size mNextPreviewSize;
    private Camera.Size mPreviewSize;
    private List<Integer> mPreviewFormats;
    private int mNextPreviewFormat;
    private int mPreviewFormat;
    private SparseArray<String> mPreviewFormatNames;

    private ColorMatrixColorFilter mYuv2RgbFilter;

    private Bitmap mCallbackBitmap;
    private int[] mRgbData;
    private int mRgbWidth;
    private int mRgbHeight;

    private static final int STATE_OFF = 0;
    private static final int STATE_PREVIEW = 1;
    private static final int STATE_NO_CALLBACKS = 2;
    private int mState = STATE_OFF;
    private boolean mProcessInProgress = false;
    private boolean mProcessingFirstFrame = false;

    private final TreeSet<CameraCombination> mTestedCombinations = new TreeSet<>(COMPARATOR);
    private final TreeSet<CameraCombination> mUntestedCombinations = new TreeSet<>(COMPARATOR);

    private int mAllCombinationsSize = 0;

    // Menu to show the test progress
    private static final int MENU_ID_PROGRESS = Menu.FIRST + 1;

    private class CameraCombination {
        private final int mCameraIndex;
        private final int mResolutionIndex;
        private final int mFormatIndex;
        private final int mResolutionWidth;
        private final int mResolutionHeight;
        private final String mFormatName;

        private CameraCombination(int cameraIndex, int resolutionIndex, int formatIndex,
            int resolutionWidth, int resolutionHeight, String formatName) {
            this.mCameraIndex = cameraIndex;
            this.mResolutionIndex = resolutionIndex;
            this.mFormatIndex = formatIndex;
            this.mResolutionWidth = resolutionWidth;
            this.mResolutionHeight = resolutionHeight;
            this.mFormatName = formatName;
        }

        @Override
        public String toString() {
            return String.format(""Camera %d, %dx%d, %s"",
                mCameraIndex, mResolutionWidth, mResolutionHeight, mFormatName);
        }
    }

    private static final Comparator<CameraCombination> COMPARATOR =
        Comparator.<CameraCombination, Integer>comparing(c -> c.mCameraIndex)
            .thenComparing(c -> c.mResolutionIndex)
            .thenComparing(c -> c.mFormatIndex);

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        setContentView(R.layout.cf_main);

        mAllCombinationsSize = calcAllCombinationsSize();

        // disable ""Pass"" button until all combinations are tested
        setPassButtonEnabled(false);

        setPassFailButtonClickListeners();
        setInfoResources(R.string.camera_format, R.string.cf_info, -1);

        mPreviewView = (TextureView) findViewById(R.id.preview_view);
        mFormatView = (ImageView) findViewById(R.id.format_view);

        mPreviewView.setSurfaceTextureListener(this);

        int numCameras = Camera.getNumberOfCameras();
        String[] cameraNames = new String[numCameras];
        for (int i = 0; i < numCameras; i++) {
            cameraNames[i] = ""Camera "" + i;
        }
        mCameraSpinner = (Spinner) findViewById(R.id.cameras_selection);
        mCameraSpinner.setAdapter(
            new ArrayAdapter<String>(
                this, R.layout.camera_list_item, cameraNames));
        mCameraSpinner.setOnItemSelectedListener(mCameraSpinnerListener);

        mFormatSpinner = (Spinner) findViewById(R.id.format_selection);
        mFormatSpinner.setOnItemSelectedListener(mFormatSelectedListener);

        mResolutionSpinner = (Spinner) findViewById(R.id.resolution_selection);
        mResolutionSpinner.setOnItemSelectedListener(mResolutionSelectedListener);

        // Must be kept in sync with android.graphics.ImageFormat manually
        mPreviewFormatNames = new SparseArray(7);
        mPreviewFormatNames.append(ImageFormat.JPEG, ""JPEG"");
        mPreviewFormatNames.append(ImageFormat.NV16, ""NV16"");
        mPreviewFormatNames.append(ImageFormat.NV21, ""NV21"");
        mPreviewFormatNames.append(ImageFormat.RGB_565, ""RGB_565"");
        mPreviewFormatNames.append(ImageFormat.UNKNOWN, ""UNKNOWN"");
        mPreviewFormatNames.append(ImageFormat.YUY2, ""YUY2"");
        mPreviewFormatNames.append(ImageFormat.YV12, ""YV12"");

        // Need YUV->RGB conversion in many cases

        ColorMatrix y2r = new ColorMatrix();
        y2r.setYUV2RGB();
        float[] yuvOffset = new float[] {
            1.f, 0.f, 0.f, 0.f, 0.f,
            0.f, 1.f, 0.f, 0.f, -128.f,
            0.f, 0.f, 1.f, 0.f, -128.f,
            0.f, 0.f, 0.f, 1.f, 0.f
        };

        ColorMatrix yOffset = new ColorMatrix(yuvOffset);

        ColorMatrix yTotal = new ColorMatrix();
        yTotal.setConcat(y2r, yOffset);

        mYuv2RgbFilter = new ColorMatrixColorFilter(yTotal);

        Button mNextButton = findViewById(R.id.next_button);
        mNextButton.setOnClickListener(v -> {
                setUntestedCombination();
                startPreview();
        });
    }

    /**
     * Set an untested combination of resolution and format for the current camera.
     * Triggered by next button click.
     */
    private void setUntestedCombination() {
        Optional<CameraCombination> combination = mUntestedCombinations.stream().filter(
            c -> c.mCameraIndex == mCurrentCameraId).findFirst();
        if (!combination.isPresent()) {
            Toast.makeText(this, ""All Camera "" + mCurrentCameraId + "" tests are done."",
                Toast.LENGTH_SHORT).show();
            return;
        }

        // There is untested combination for the current camera, set the next untested combination.
        int mNextResolutionIndex = combination.get().mResolutionIndex;
        int mNextFormatIndex = combination.get().mFormatIndex;

        mNextPreviewSize = mPreviewSizes.get(mNextResolutionIndex);
        mResolutionSpinner.setSelection(mNextResolutionIndex);
        mNextPreviewFormat = mPreviewFormats.get(mNextFormatIndex);
        mFormatSpinner.setSelection(mNextFormatIndex);
    }

    @Override
    public boolean onCreateOptionsMenu(Menu menu) {
        menu.add(Menu.NONE, MENU_ID_PROGRESS, Menu.NONE, ""Current Progress"");
        return super.onCreateOptionsMenu(menu);
    }

    @Override
    public boolean onOptionsItemSelected(MenuItem item) {
        boolean ret = true;
        switch (item.getItemId()) {
            case MENU_ID_PROGRESS:
                showCombinationsDialog();
                ret = true;
                break;
            default:
                ret = super.onOptionsItemSelected(item);
                break;
        }
        return ret;
    }

    private void showCombinationsDialog() {
        AlertDialog.Builder builder =
                new AlertDialog.Builder(CameraFormatsActivity.this);
        builder.setMessage(getTestDetails())
                .setTitle(""Current Progress"")
                .setPositiveButton(""OK"", null);
        builder.show();
    }

    @Override
    public void onResume() {
        super.onResume();

        setUpCamera(mCameraSpinner.getSelectedItemPosition());
    }

    @Override
    public void onPause() {
        super.onPause();

        shutdownCamera();
        mPreviewTexture = null;
    }

    @Override
    public String getTestDetails() {
        StringBuilder reportBuilder = new StringBuilder();
        reportBuilder.append(""Tested combinations:\n"");
        for (CameraCombination combination: mTestedCombinations) {
            reportBuilder.append(combination);
            reportBuilder.append(""\n"");
        }

        reportBuilder.append(""Untested combinations:\n"");
        for (CameraCombination combination: mUntestedCombinations) {
            reportBuilder.append(combination);
            reportBuilder.append(""\n"");
        }
        return reportBuilder.toString();
    }

    public void onSurfaceTextureAvailable(SurfaceTexture surface,
            int width, int height) {
        mPreviewTexture = surface;
        if (mFormatView.getMeasuredWidth() != width
                || mFormatView.getMeasuredHeight() != height) {
            mPreviewTexWidth = mFormatView.getMeasuredWidth();
            mPreviewTexHeight = mFormatView.getMeasuredHeight();
        } else {
            mPreviewTexWidth = width;
            mPreviewTexHeight = height;
        }

        if (mCamera != null) {
            startPreview();
        }
    }

    public void onSurfaceTextureSizeChanged(SurfaceTexture surface, int width, int height) {
        // Ignored, Camera does all the work for us
    }

    public boolean onSurfaceTextureDestroyed(SurfaceTexture surface) {
        return true;
    }

    public void onSurfaceTextureUpdated(SurfaceTexture surface) {
        // Invoked every time there's a new Camera preview frame
    }

    private AdapterView.OnItemSelectedListener mCameraSpinnerListener =
            new AdapterView.OnItemSelectedListener() {
                public void onItemSelected(AdapterView<?> parent,
                        View view, int pos, long id) {
                    if (mCurrentCameraId != pos) {
                        setUpCamera(pos);
                    }
                }

                public void onNothingSelected(AdapterView parent) {

                }

            };

    private AdapterView.OnItemSelectedListener mResolutionSelectedListener =
            new AdapterView.OnItemSelectedListener() {
                public void onItemSelected(AdapterView<?> parent,
                        View view, int position, long id) {
                    if (mPreviewSizes.get(position) != mPreviewSize) {
                        mNextPreviewSize = mPreviewSizes.get(position);
                        startPreview();
                    }
                }

                public void onNothingSelected(AdapterView parent) {

                }

            };


    private AdapterView.OnItemSelectedListener mFormatSelectedListener =
            new AdapterView.OnItemSelectedListener() {
                public void onItemSelected(AdapterView<?> parent,
                        View view, int position, long id) {
                    if (mPreviewFormats.get(position) != mNextPreviewFormat) {
                        mNextPreviewFormat = mPreviewFormats.get(position);
                        startPreview();
                    }
                }

                public void onNothingSelected(AdapterView parent) {

                }

            };

    private void setUpCamera(int id) {
        shutdownCamera();

        mCurrentCameraId = id;
        mCamera = Camera.open(id);
        Camera.Parameters p = mCamera.getParameters();

        // Get preview resolutions

        List<Camera.Size> unsortedSizes = p.getSupportedPreviewSizes();

        class SizeCompare implements Comparator<Camera.Size> {
            public int compare(Camera.Size lhs, Camera.Size rhs) {
                if (lhs.width < rhs.width) return -1;
                if (lhs.width > rhs.width) return 1;
                if (lhs.height < rhs.height) return -1;
                if (lhs.height > rhs.height) return 1;
                return 0;
            }
        }

        SizeCompare s = new SizeCompare();
        TreeSet<Camera.Size> sortedResolutions = new TreeSet<Camera.Size>(s);
        sortedResolutions.addAll(unsortedSizes);

        mPreviewSizes = new ArrayList<Camera.Size>(sortedResolutions);

        String[] availableSizeNames = new String[mPreviewSizes.size()];
        for (int i = 0; i < mPreviewSizes.size(); i++) {
            availableSizeNames[i] =
                    Integer.toString(mPreviewSizes.get(i).width) + "" x "" +
                    Integer.toString(mPreviewSizes.get(i).height);
        }
        mResolutionSpinner.setAdapter(
            new ArrayAdapter<String>(
                this, R.layout.camera_list_item, availableSizeNames));

        // Get preview formats, removing duplicates

        HashSet<Integer> formatSet = new HashSet<>(p.getSupportedPreviewFormats());
        mPreviewFormats = new ArrayList<Integer>(formatSet);

        String[] availableFormatNames = new String[mPreviewFormats.size()];
        for (int i = 0; i < mPreviewFormats.size(); i++) {
            availableFormatNames[i] =
                    mPreviewFormatNames.get(mPreviewFormats.get(i));
        }
        mFormatSpinner.setAdapter(
            new ArrayAdapter<String>(
                this, R.layout.camera_list_item, availableFormatNames));

        // Update untested entries

        for (int resolutionIndex = 0; resolutionIndex < mPreviewSizes.size(); resolutionIndex++) {
            for (int formatIndex = 0; formatIndex < mPreviewFormats.size(); formatIndex++) {
                CameraCombination combination = new CameraCombination(
                    id, resolutionIndex, formatIndex,
                    mPreviewSizes.get(resolutionIndex).width,
                    mPreviewSizes.get(resolutionIndex).height,
                    mPreviewFormatNames.get(mPreviewFormats.get(formatIndex)));

                if (!mTestedCombinations.contains(combination)) {
                    mUntestedCombinations.add(combination);
                }
            }
        }

        // Set initial values

        mNextPreviewSize = mPreviewSizes.get(0);
        mResolutionSpinner.setSelection(0);

        mNextPreviewFormat = mPreviewFormats.get(0);
        mFormatSpinner.setSelection(0);


        // Set up correct display orientation

        CameraInfo info =
            new CameraInfo();
        Camera.getCameraInfo(id, info);
        int rotation = getWindowManager().getDefaultDisplay().getRotation();
        int degrees = 0;
        switch (rotation) {
            case Surface.ROTATION_0: degrees = 0; break;
            case Surface.ROTATION_90: degrees = 90; break;
            case Surface.ROTATION_180: degrees = 180; break;
            case Surface.ROTATION_270: degrees = 270; break;
        }

        if (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {
            mPreviewRotation = (info.orientation + degrees) % 360;
            mPreviewRotation = (360 - mPreviewRotation) % 360;  // compensate the mirror
        } else {  // back-facing
            mPreviewRotation = (info.orientation - degrees + 360) % 360;
        }
        if (mPreviewRotation != 0 && mPreviewRotation != 180) {
            Log.w(TAG,
                ""Display orientation correction is not 0 or 180, as expected!"");
        }

        mCamera.setDisplayOrientation(mPreviewRotation);

        // Start up preview if display is ready

        if (mPreviewTexture != null) {
            startPreview();
        }

    }

    private void shutdownCamera() {
        if (mCamera != null) {
            mCamera.setPreviewCallback(null);
            mCamera.stopPreview();
            mCamera.release();
            mCamera = null;
            mState = STATE_OFF;
        }
    }

    private void startPreview() {
        if (mState != STATE_OFF) {
            // Stop for a while to drain callbacks
            mCamera.setPreviewCallback(null);
            mCamera.stopPreview();
            mState = STATE_OFF;
            Handler h = new Handler();
            Runnable mDelayedPreview = new Runnable() {
                public void run() {
                    startPreview();
                }
            };
            h.postDelayed(mDelayedPreview, 300);
            return;
        }
        mState = STATE_PREVIEW;

        Matrix transform = new Matrix();
        float widthRatio = mNextPreviewSize.width / (float)mPreviewTexWidth;
        float heightRatio = mNextPreviewSize.height / (float)mPreviewTexHeight;

        if (heightRatio < widthRatio) {
            transform.setScale(1, heightRatio/widthRatio);
            transform.postTranslate(0,
                mPreviewTexHeight * (1 - heightRatio/widthRatio)/2);
        } else {
            transform.setScale(widthRatio/heightRatio, 1);
            transform.postTranslate(mPreviewTexWidth * (1 - widthRatio/heightRatio)/2,
            0);
        }

        mPreviewView.setTransform(transform);

        mPreviewFormat = mNextPreviewFormat;
        mPreviewSize   = mNextPreviewSize;

        Camera.Parameters p = mCamera.getParameters();
        p.setPreviewFormat(mPreviewFormat);
        p.setPreviewSize(mPreviewSize.width, mPreviewSize.height);
        mCamera.setParameters(p);

        mCamera.setPreviewCallback(this);
        switch (mPreviewFormat) {
            case ImageFormat.NV16:
            case ImageFormat.NV21:
            case ImageFormat.YUY2:
            case ImageFormat.YV12:
                mFormatView.setColorFilter(mYuv2RgbFilter);
                break;
            default:
                mFormatView.setColorFilter(null);
                break;
        }

        // Filter out currently untestable formats
        switch (mPreviewFormat) {
            case ImageFormat.NV16:
            case ImageFormat.RGB_565:
            case ImageFormat.UNKNOWN:
            case ImageFormat.JPEG:
                AlertDialog.Builder builder =
                        new AlertDialog.Builder(CameraFormatsActivity.this);
                builder.setMessage(""Unsupported format "" +
                        mPreviewFormatNames.get(mPreviewFormat) +
                        ""; consider this combination as pass. "")
                        .setTitle(""Missing test"" )
                        .setNeutralButton(""Back"", null);
                builder.show();
                mState = STATE_NO_CALLBACKS;
                mCamera.setPreviewCallback(null);
                break;
            default:
                // supported
                break;
        }

        mProcessingFirstFrame = true;
        try {
            mCamera.setPreviewTexture(mPreviewTexture);
            mCamera.startPreview();
        } catch (IOException ioe) {
            // Something bad happened
            Log.e(TAG, ""Unable to start up preview"");
        }
    }

    private class ProcessPreviewDataTask extends AsyncTask<byte[], Void, Boolean> {
        protected Boolean doInBackground(byte[]... datas) {
            byte[] data = datas[0];
            try {
                if (mRgbData == null ||
                        mPreviewSize.width != mRgbWidth ||
                        mPreviewSize.height != mRgbHeight) {

                    mRgbData = new int[mPreviewSize.width * mPreviewSize.height * 4];
                    mRgbWidth = mPreviewSize.width;
                    mRgbHeight = mPreviewSize.height;
                }
                switch(mPreviewFormat) {
                    case ImageFormat.NV21:
                        convertFromNV21(data, mRgbData);
                        break;
                    case ImageFormat.YV12:
                        convertFromYV12(data, mRgbData);
                        break;
                    case ImageFormat.YUY2:
                        convertFromYUY2(data, mRgbData);
                        break;
                    case ImageFormat.NV16:
                    case ImageFormat.RGB_565:
                    case ImageFormat.UNKNOWN:
                    case ImageFormat.JPEG:
                    default:
                        convertFromUnknown(data, mRgbData);
                        break;
                }

                if (mCallbackBitmap == null ||
                        mRgbWidth != mCallbackBitmap.getWidth() ||
                        mRgbHeight != mCallbackBitmap.getHeight() ) {
                    mCallbackBitmap =
                            Bitmap.createBitmap(
                                mRgbWidth, mRgbHeight,
                                Bitmap.Config.ARGB_8888);
                }
                mCallbackBitmap.setPixels(mRgbData, 0, mRgbWidth,
                        0, 0, mRgbWidth, mRgbHeight);
            } catch (OutOfMemoryError o) {
                Log.e(TAG, ""Out of memory trying to process preview data"");
                return false;
            }
            return true;
        }

        protected void onPostExecute(Boolean result) {
            if (result) {
                mFormatView.setImageBitmap(mCallbackBitmap);
                if (mProcessingFirstFrame) {
                    mProcessingFirstFrame = false;

                    CameraCombination combination = new CameraCombination(
                        mCurrentCameraId,
                        mResolutionSpinner.getSelectedItemPosition(),
                        mFormatSpinner.getSelectedItemPosition(),
                        mPreviewSizes.get(mResolutionSpinner.getSelectedItemPosition()).width,
                        mPreviewSizes.get(mResolutionSpinner.getSelectedItemPosition()).height,
                        mPreviewFormatNames.get(
                            mPreviewFormats.get(mFormatSpinner.getSelectedItemPosition())));

                    mUntestedCombinations.remove(combination);
                    mTestedCombinations.add(combination);

                    displayToast(combination.toString());

                    if (mTestedCombinations.size() == mAllCombinationsSize) {
                        setPassButtonEnabled(true);
                    }
                }
            }
            mProcessInProgress = false;
        }

    }

    private void setPassButtonEnabled(boolean enabled) {
        ImageButton pass_button = (ImageButton) findViewById(R.id.pass_button);
        pass_button.setEnabled(enabled);
    }

    private int calcAllCombinationsSize() {
        int allCombinationsSize = 0;
        int numCameras = Camera.getNumberOfCameras();

        for (int i = 0; i<numCameras; i++) {
            // must release a Camera object before a new Camera object is created
            shutdownCamera();

            mCamera = Camera.open(i);
            Camera.Parameters p = mCamera.getParameters();

            HashSet<Integer> formatSet = new HashSet<>(p.getSupportedPreviewFormats());

            allCombinationsSize +=
                    p.getSupportedPreviewSizes().size() *   // resolutions
                    formatSet.size();  // unique formats
        }

        return allCombinationsSize;
    }

    private void displayToast(String combination) {
        Toast.makeText(this, ""\"""" + combination + ""\""\n"" + "" has been tested."", Toast.LENGTH_SHORT)
            .show();
    }

    public void onPreviewFrame(byte[] data, Camera camera) {
        if (mProcessInProgress || mState != STATE_PREVIEW) return;

        int expectedBytes;
        switch (mPreviewFormat) {
            case ImageFormat.YV12:
                // YV12 may have stride != width.
                int w = mPreviewSize.width;
                int h = mPreviewSize.height;
                int yStride = (int)Math.ceil(w / 16.0) * 16;
                int uvStride = (int)Math.ceil(yStride / 2 / 16.0) * 16;
                int ySize = yStride * h;
                int uvSize = uvStride * h / 2;
                expectedBytes = ySize + uvSize * 2;
                break;
            case ImageFormat.NV21:
            case ImageFormat.YUY2:
            default:
                expectedBytes = mPreviewSize.width * mPreviewSize.height *
                        ImageFormat.getBitsPerPixel(mPreviewFormat) / 8;
                break;
        }
        if (expectedBytes != data.length) {
            AlertDialog.Builder builder =
                    new AlertDialog.Builder(CameraFormatsActivity.this);
            builder.setMessage(""Mismatched size of buffer! Expected "" +
                    expectedBytes + "", but got "" +
                    data.length + "" bytes instead!"")
                    .setTitle(""Error trying to use format ""
                            + mPreviewFormatNames.get(mPreviewFormat))
                    .setNeutralButton(""Back"", null);

            builder.show();

            mState = STATE_NO_CALLBACKS;
            mCamera.setPreviewCallback(null);
            return;
        }

        mProcessInProgress = true;
        new ProcessPreviewDataTask().execute(data);
    }

    private void convertFromUnknown(byte[] data, int[] rgbData) {
        int w = mPreviewSize.width;
        int h = mPreviewSize.height;
        // RGBA output
        int rgbInc = 1;
        if (mPreviewRotation == 180) {
            rgbInc = -1;
        }
        int index = 0;
        for (int y = 0; y < h; y++) {
            int rgbIndex = y * w;
            if (mPreviewRotation == 180) {
                rgbIndex = w * (h - y) - 1;
            }
            for (int x = 0; x < mPreviewSize.width/3; x++) {
                int r = data[index + 0] & 0xFF;
                int g = data[index + 1] & 0xFF;
                int b = data[index + 2] & 0xFF;
                rgbData[rgbIndex] = Color.rgb(r,g,b);
                rgbIndex += rgbInc;
                index += 3;
            }
        }
    }

    // NV21 is a semi-planar 4:2:0 format, in the order YVU, which means we have:
    // a W x H-size 1-byte-per-pixel Y plane, then
    // a W/2 x H/2-size 2-byte-per-pixel plane, where each pixel has V then U.
    private void convertFromNV21(byte[] data, int rgbData[]) {
        int w = mPreviewSize.width;
        int h = mPreviewSize.height;
        // RGBA output
        int rgbIndex = 0;
        int rgbInc = 1;
        if (mPreviewRotation == 180) {
            rgbIndex = h * w - 1;
            rgbInc = -1;
        }
        int yIndex = 0;
        int uvRowIndex = w*h;
        int uvRowInc = 0;
        for (int y = 0; y < h; y++) {
            int uvInc = 0;
            int vIndex = uvRowIndex;
            int uIndex = uvRowIndex + 1;

            uvRowIndex += uvRowInc * w;
            uvRowInc = (uvRowInc + 1) & 0x1;

            for (int x = 0; x < w; x++) {
                int yv = data[yIndex] & 0xFF;
                int uv = data[uIndex] & 0xFF;
                int vv = data[vIndex] & 0xFF;
                rgbData[rgbIndex] =
                        Color.rgb(yv, uv, vv);

                rgbIndex += rgbInc;
                yIndex += 1;
                uIndex += uvInc;
                vIndex += uvInc;
                uvInc = (uvInc + 2) & 0x2;
            }
        }
    }

    // YV12 is a planar 4:2:0 format, in the order YVU, which means we have:
    // a W x H-size 1-byte-per-pixel Y plane, then
    // a W/2 x H/2-size 1-byte-per-pixel V plane, then
    // a W/2 x H/2-size 1-byte-per-pixel U plane
    // The stride may not be equal to width, since it has to be a multiple of
    // 16 pixels for both the Y and UV planes.
    private void convertFromYV12(byte[] data, int rgbData[]) {
        int w = mPreviewSize.width;
        int h = mPreviewSize.height;
        // RGBA output
        int rgbIndex = 0;
        int rgbInc = 1;
        if (mPreviewRotation == 180) {
            rgbIndex = h * w - 1;
            rgbInc = -1;
        }

        int yStride = (int)Math.ceil(w / 16.0) * 16;
        int uvStride = (int)Math.ceil(yStride/2/16.0) * 16;
        int ySize = yStride * h;
        int uvSize = uvStride * h / 2;

        int uRowIndex = ySize + uvSize;
        int vRowIndex = ySize;

        int uv_w = w/2;
        for (int y = 0; y < h; y++) {
            int yIndex = yStride * y;
            int uIndex = uRowIndex;
            int vIndex = vRowIndex;

            if ( (y & 0x1) == 1) {
                uRowIndex += uvStride;
                vRowIndex += uvStride;
            }

            int uv = 0, vv = 0;
            for (int x = 0; x < w; x++) {
                if ( (x & 0x1)  == 0) {
                    uv = data[uIndex] & 0xFF;
                    vv = data[vIndex] & 0xFF;
                    uIndex++;
                    vIndex++;
                }
                int yv = data[yIndex] & 0xFF;
                rgbData[rgbIndex] =
                        Color.rgb(yv, uv, vv);

                rgbIndex += rgbInc;
                yIndex += 1;
            }
        }
    }

    // YUY2 is an interleaved 4:2:2 format: YU,YV,YU,YV
    private void convertFromYUY2(byte[] data, int[] rgbData) {
        int w = mPreviewSize.width;
        int h = mPreviewSize.height;
        // RGBA output
        int yIndex = 0;
        int uIndex = 1;
        int vIndex = 3;
        int rgbIndex = 0;
        int rgbInc = 1;
        if (mPreviewRotation == 180) {
            rgbIndex = h * w - 1;
            rgbInc = -1;
        }

        for (int y = 0; y < h; y++) {
            for (int x = 0; x < w; x++) {
                int yv = data[yIndex] & 0xFF;
                int uv = data[uIndex] & 0xFF;
                int vv = data[vIndex] & 0xFF;
                rgbData[rgbIndex] = Color.rgb(yv,uv,vv);
                rgbIndex += rgbInc;
                yIndex += 2;
                if ( (x & 0x1) == 1 ) {
                    uIndex += 4;
                    vIndex += 4;
                }
            }
        }
    }

}"	""	""	"cdd 12 resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.PerformanceTest"	"testMultipleCapture"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/PerformanceTest.java"	""	"public void testMultipleCapture() throws Exception {
        double[] avgResultTimes = new double[mTestRule.getCameraIdsUnderTest().length];
        double[] avgDurationMs = new double[mTestRule.getCameraIdsUnderTest().length];

        // A simple CaptureSession StateCallback to handle onCaptureQueueEmpty
        class MultipleCaptureStateCallback extends CameraCaptureSession.StateCallback {
            private ConditionVariable captureQueueEmptyCond = new ConditionVariable();
            private int captureQueueEmptied = 0;

            @Override
            public void onConfigured(CameraCaptureSession session) {
                // Empty implementation
            }

            @Override
            public void onConfigureFailed(CameraCaptureSession session) {
                // Empty implementation
            }

            @Override
            public void onCaptureQueueEmpty(CameraCaptureSession session) {
                captureQueueEmptied++;
                if (VERBOSE) {
                    Log.v(TAG, ""onCaptureQueueEmpty received. captureQueueEmptied = ""
                            + captureQueueEmptied);
                }

                captureQueueEmptyCond.open();
            }

            /* Wait for onCaptureQueueEmpty, return immediately if an onCaptureQueueEmpty was
             * already received, otherwise, wait for one to arrive. */
            public void waitForCaptureQueueEmpty(long timeout) {
                if (captureQueueEmptied > 0) {
                    captureQueueEmptied--;
                    return;
                }

                if (captureQueueEmptyCond.block(timeout)) {
                    captureQueueEmptyCond.close();
                    captureQueueEmptied = 0;
                } else {
                    throw new TimeoutRuntimeException(""Unable to receive onCaptureQueueEmpty after ""
                            + timeout + ""ms"");
                }
            }
        }

        final MultipleCaptureStateCallback sessionListener = new MultipleCaptureStateCallback();

        int counter = 0;
        for (String id : mTestRule.getCameraIdsUnderTest()) {
            // Do NOT move these variables to outer scope
            // They will be passed to DeviceReportLog and their references will be stored
            String streamName = ""test_multiple_capture"";
            mReportLog = new DeviceReportLog(REPORT_LOG_NAME, streamName);
            mReportLog.addValue(""camera_id"", id, ResultType.NEUTRAL, ResultUnit.NONE);
            long[] startTimes = new long[NUM_MAX_IMAGES];
            double[] getResultTimes = new double[NUM_MAX_IMAGES];
            double[] frameDurationMs = new double[NUM_MAX_IMAGES-1];
            try {
                StaticMetadata staticMetadata = mTestRule.getAllStaticInfo().get(id);
                if (!staticMetadata.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" does not support color outputs, skipping"");
                    continue;
                }
                boolean useSessionKeys = isFpsRangeASessionKey(staticMetadata.getCharacteristics());

                mTestRule.openDevice(id);
                for (int i = 0; i < NUM_TEST_LOOPS; i++) {

                    // setup builders and listeners
                    CaptureRequest.Builder previewBuilder =
                            mTestRule.getCamera().createCaptureRequest(
                                    CameraDevice.TEMPLATE_PREVIEW);
                    CaptureRequest.Builder captureBuilder =
                            mTestRule.getCamera().createCaptureRequest(
                                    CameraDevice.TEMPLATE_STILL_CAPTURE);
                    SimpleCaptureCallback previewResultListener =
                            new SimpleCaptureCallback();
                    SimpleTimingResultListener captureResultListener =
                            new SimpleTimingResultListener();
                    SimpleImageReaderListener imageListener =
                            new SimpleImageReaderListener(/*asyncMode*/true, NUM_MAX_IMAGES);

                    Size maxYuvSize = CameraTestUtils.getSortedSizesForFormat(
                            id, mTestRule.getCameraManager(),
                            ImageFormat.YUV_420_888, /*bound*/null).get(0);
                    // Find minimum frame duration for YUV_420_888
                    StreamConfigurationMap config =
                            mTestRule.getStaticInfo().getCharacteristics().get(
                            CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);

                    final long minStillFrameDuration =
                            config.getOutputMinFrameDuration(ImageFormat.YUV_420_888, maxYuvSize);
                    if (minStillFrameDuration > 0) {
                        Range<Integer> targetRange =
                                CameraTestUtils.getSuitableFpsRangeForDuration(id,
                                        minStillFrameDuration, mTestRule.getStaticInfo());
                        previewBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, targetRange);
                        captureBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, targetRange);
                    }

                    prepareCaptureAndStartPreview(previewBuilder, captureBuilder,
                            mTestRule.getOrderedPreviewSizes().get(0), maxYuvSize,
                            ImageFormat.YUV_420_888, previewResultListener,
                            sessionListener, NUM_MAX_IMAGES, imageListener,
                            useSessionKeys);

                    // Converge AE
                    CameraTestUtils.waitForAeStable(previewResultListener,
                            NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY, mTestRule.getStaticInfo(),
                            WAIT_FOR_RESULT_TIMEOUT_MS, NUM_RESULTS_WAIT_TIMEOUT);

                    if (mTestRule.getStaticInfo().isAeLockSupported()) {
                        // Lock AE if possible to improve stability
                        previewBuilder.set(CaptureRequest.CONTROL_AE_LOCK, true);
                        mTestRule.getCameraSession().setRepeatingRequest(previewBuilder.build(),
                                previewResultListener, mTestRule.getHandler());
                        if (mTestRule.getStaticInfo().isHardwareLevelAtLeastLimited()) {
                            // Legacy mode doesn't output AE state
                            CameraTestUtils.waitForResultValue(previewResultListener,
                                    CaptureResult.CONTROL_AE_STATE,
                                    CaptureResult.CONTROL_AE_STATE_LOCKED,
                                    NUM_RESULTS_WAIT_TIMEOUT, WAIT_FOR_RESULT_TIMEOUT_MS);
                        }
                    }

                    // Capture NUM_MAX_IMAGES images based on onCaptureQueueEmpty callback
                    for (int j = 0; j < NUM_MAX_IMAGES; j++) {

                        // Capture an image and get image data
                        startTimes[j] = SystemClock.elapsedRealtime();
                        CaptureRequest request = captureBuilder.build();
                        mTestRule.getCameraSession().capture(
                                request, captureResultListener, mTestRule.getHandler());

                        // Wait for capture queue empty for the current request
                        sessionListener.waitForCaptureQueueEmpty(
                                CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS);
                    }

                    // Acquire the capture result time and frame duration
                    long prevTimestamp = -1;
                    for (int j = 0; j < NUM_MAX_IMAGES; j++) {
                        Pair<CaptureResult, Long> captureResultNTime =
                                captureResultListener.getCaptureResultNTime(
                                        CameraTestUtils.CAPTURE_RESULT_TIMEOUT_MS);

                        getResultTimes[j] +=
                                (double)(captureResultNTime.second - startTimes[j])/NUM_TEST_LOOPS;

                        // Collect inter-frame timestamp
                        long timestamp = captureResultNTime.first.get(
                                CaptureResult.SENSOR_TIMESTAMP);
                        if (prevTimestamp != -1) {
                            frameDurationMs[j-1] +=
                                    (double)(timestamp - prevTimestamp)/(
                                            NUM_TEST_LOOPS * 1000000.0);
                        }
                        prevTimestamp = timestamp;
                    }

                    // simulate real scenario (preview runs a bit)
                    CameraTestUtils.waitForNumResults(previewResultListener, NUM_RESULTS_WAIT,
                            WAIT_FOR_RESULT_TIMEOUT_MS);

                    stopRepeating();
                }

                for (int i = 0; i < getResultTimes.length; i++) {
                    Log.v(TAG, ""Camera "" + id + "" result time["" + i + ""] is "" +
                            getResultTimes[i] + "" ms"");
                }
                for (int i = 0; i < NUM_MAX_IMAGES-1; i++) {
                    Log.v(TAG, ""Camera "" + id + "" frame duration time["" + i + ""] is "" +
                            frameDurationMs[i] + "" ms"");
                }

                mReportLog.addValues(""camera_multiple_capture_result_latency"", getResultTimes,
                        ResultType.LOWER_BETTER, ResultUnit.MS);
                mReportLog.addValues(""camera_multiple_capture_frame_duration"", frameDurationMs,
                        ResultType.LOWER_BETTER, ResultUnit.MS);


                avgResultTimes[counter] = Stat.getAverage(getResultTimes);
                avgDurationMs[counter] = Stat.getAverage(frameDurationMs);
            }
            finally {
                mTestRule.closeDefaultImageReader();
                mTestRule.closeDevice(id);
                closePreviewSurface();
            }
            counter++;
            mReportLog.submit(mInstrumentation);
        }

        // Result will not be reported in CTS report if no summary is printed.
        if (mTestRule.getCameraIdsUnderTest().length != 0) {
            String streamName = ""test_multiple_capture_average"";
            mReportLog = new DeviceReportLog(REPORT_LOG_NAME, streamName);
            mReportLog.setSummary(""camera_multiple_capture_result_average_latency_for_all_cameras"",
                    Stat.getAverage(avgResultTimes), ResultType.LOWER_BETTER, ResultUnit.MS);
            mReportLog.submit(mInstrumentation);
            mReportLog = new DeviceReportLog(REPORT_LOG_NAME, streamName);
            mReportLog.setSummary(""camera_multiple_capture_frame_duration_average_for_all_cameras"",
                    Stat.getAverage(avgDurationMs), ResultType.LOWER_BETTER, ResultUnit.MS);
            mReportLog.submit(mInstrumentation);
        }
    }

    /**
     * Test reprocessing shot-to-shot latency with default NR and edge options, i.e., from the time
     * a reprocess request is issued to the time the reprocess image is returned.
     */"	""	""	"minimum"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionReprocessCaptureTest"	"testMultiResolutionReprocessCharacteristics"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionReprocessCaptureTest.java"	""	"public void testMultiResolutionReprocessCharacteristics() {
        for (String id : mCameraIdsUnderTest) {
            if (VERBOSE) {
                Log.v(TAG, ""Testing multi-resolution reprocess characteristics for Camera "" + id);
            }
            StaticMetadata info = mAllStaticInfo.get(id);
            CameraCharacteristics c = info.getCharacteristics();
            StreamConfigurationMap config = c.get(
                    CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            int[] inputFormats = config.getInputFormats();
            int[] capabilities = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
            boolean isLogicalCamera = CameraTestUtils.contains(capabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_LOGICAL_MULTI_CAMERA);
            boolean isUltraHighResCamera = info.isUltraHighResolutionSensor();
            Set<String> physicalCameraIds = c.getPhysicalCameraIds();

            MultiResolutionStreamConfigurationMap multiResolutionMap = c.get(
                    CameraCharacteristics.SCALER_MULTI_RESOLUTION_STREAM_CONFIGURATION_MAP);
            if (multiResolutionMap == null) {
                Log.i(TAG, ""Camera "" + id + "" doesn't support multi-resolution reprocessing."");
                continue;
            }
            if (VERBOSE) {
                Log.v(TAG, ""MULTI_RESOLUTION_STREAM_CONFIGURATION_MAP: ""
                        + multiResolutionMap.toString());
            }

            // Find multi-resolution input and output formats
            int[] multiResolutionInputFormats = multiResolutionMap.getInputFormats();
            int[] multiResolutionOutputFormats = multiResolutionMap.getOutputFormats();

            assertTrue(""Camera "" + id + "" must be a logical multi-camera or ultra high res camera ""
                    + ""to support multi-resolution reprocessing."",
                    isLogicalCamera || isUltraHighResCamera);

            for (int format : multiResolutionInputFormats) {
                assertTrue(String.format(""Camera %s: multi-resolution input format %d ""
                        + ""isn't a supported format"", id, format),
                        CameraTestUtils.contains(inputFormats, format));

                Collection<MultiResolutionStreamInfo> multiResolutionStreams =
                        multiResolutionMap.getInputInfo(format);
                assertTrue(String.format(""Camera %s supports %d multi-resolution ""
                        + ""input stream info, expected at least 2"", id,
                        multiResolutionStreams.size()),
                        multiResolutionStreams.size() >= 2);

                // Make sure that each multi-resolution input stream info has the maximum size
                // for that format.
                for (MultiResolutionStreamInfo streamInfo : multiResolutionStreams) {
                    String physicalCameraId = streamInfo.getPhysicalCameraId();
                    Size streamSize = new Size(streamInfo.getWidth(), streamInfo.getHeight());
                    if (!isLogicalCamera) {
                        assertTrue(""Camera "" + id + "" is ultra high resolution camera, but ""
                                + ""the multi-resolution reprocessing stream info camera Id ""
                                + physicalCameraId + "" doesn't match"",
                                physicalCameraId.equals(id));
                    } else {
                        assertTrue(""Camera "" + id + ""'s multi-resolution input info ""
                                + ""physical camera id "" + physicalCameraId + "" isn't valid"",
                                physicalCameraIds.contains(physicalCameraId));
                    }

                    StaticMetadata pInfo = mAllStaticInfo.get(physicalCameraId);
                    CameraCharacteristics pChar = pInfo.getCharacteristics();
                    StreamConfigurationMap pConfig = pChar.get(
                            CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
                    Size[] sizes = pConfig.getInputSizes(format);

                    assertTrue(String.format(""Camera %s must ""
                            + ""support at least one input size for multi-resolution input ""
                            + ""format %d."", physicalCameraId, format),
                             sizes != null && sizes.length > 0);

                    List<Size> maxSizes = new ArrayList<Size>();
                    maxSizes.add(CameraTestUtils.getMaxSize(sizes));
                    StreamConfigurationMap pMaxResConfig = pChar.get(CameraCharacteristics.
                            SCALER_STREAM_CONFIGURATION_MAP_MAXIMUM_RESOLUTION);
                    if (pMaxResConfig != null) {
                        Size[] maxResSizes = pMaxResConfig.getInputSizes(format);
                        if (maxResSizes != null && maxResSizes.length > 0) {
                            maxSizes.add(CameraTestUtils.getMaxSize(maxResSizes));
                        }
                    }

                    assertTrue(String.format(""Camera %s's supported multi-resolution""
                           + "" input size %s for physical camera %s is not one of the largest ""
                           + ""supported input sizes %s for format %d"", id, streamSize,
                           physicalCameraId, maxSizes, format), maxSizes.contains(streamSize));
                }
            }

            // YUV reprocessing capabilities check
            if (CameraTestUtils.contains(multiResolutionOutputFormats, ImageFormat.YUV_422_888) &&
                    CameraTestUtils.contains(multiResolutionInputFormats,
                    ImageFormat.YUV_420_888)) {
                assertTrue(""The camera device must have YUV_REPROCESSING capability if it ""
                        + ""supports multi-resolution YUV input and YUV output"",
                        CameraTestUtils.contains(capabilities,
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING));

                assertTrue(""The camera device must supports multi-resolution JPEG output if ""
                        + ""supports multi-resolution YUV input and YUV output"",
                        CameraTestUtils.contains(multiResolutionOutputFormats, ImageFormat.JPEG));
            }

            // OPAQUE reprocessing capabilities check
            if (CameraTestUtils.contains(multiResolutionOutputFormats, ImageFormat.PRIVATE) &&
                    CameraTestUtils.contains(multiResolutionInputFormats, ImageFormat.PRIVATE)) {
                assertTrue(""The camera device must have PRIVATE_REPROCESSING capability if it ""
                        + ""supports multi-resolution PRIVATE input and PRIVATE output"",
                        CameraTestUtils.contains(capabilities,
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING));

                assertTrue(""The camera device must supports multi-resolution JPEG output if ""
                        + ""supports multi-resolution PRIVATE input and PRIVATE output"",
                        CameraTestUtils.contains(multiResolutionOutputFormats, ImageFormat.JPEG));
                assertTrue(""The camera device must supports multi-resolution YUV output if ""
                        + ""supports multi-resolution PRIVATE input and PRIVATE output"",
                        CameraTestUtils.contains(multiResolutionOutputFormats,
                        ImageFormat.YUV_420_888));
            }
        }
    }

    /**
     * Test YUV_420_888 -> YUV_420_888 multi-resolution reprocessing
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionReprocessCaptureTest"	"testMultiResolutionYuvToYuvReprocessing"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionReprocessCaptureTest.java"	""	"public void testMultiResolutionYuvToYuvReprocessing() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            testMultiResolutionReprocessing(id, ImageFormat.YUV_420_888, ImageFormat.YUV_420_888);
        }
    }

    /**
     * Test YUV_420_888 -> JPEG multi-resolution reprocessing
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionReprocessCaptureTest"	"testMultiResolutionYuvToJpegReprocessing"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionReprocessCaptureTest.java"	""	"public void testMultiResolutionYuvToJpegReprocessing() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            testMultiResolutionReprocessing(id, ImageFormat.YUV_420_888, ImageFormat.JPEG);
        }
    }

    /**
     * Test OPAQUE -> YUV_420_888 multi-resolution reprocessing
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionReprocessCaptureTest"	"testMultiResolutionOpaqueToYuvReprocessing"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionReprocessCaptureTest.java"	""	"public void testMultiResolutionOpaqueToYuvReprocessing() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            // Opaque -> YUV_420_888 must be supported.
            testMultiResolutionReprocessing(id, ImageFormat.PRIVATE, ImageFormat.YUV_420_888);
        }
    }

    /**
     * Test OPAQUE -> JPEG multi-resolution reprocessing
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionReprocessCaptureTest"	"testMultiResolutionOpaqueToJpegReprocessing"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionReprocessCaptureTest.java"	""	"public void testMultiResolutionOpaqueToJpegReprocessing() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            // OPAQUE -> JPEG must be supported.
            testMultiResolutionReprocessing(id, ImageFormat.PRIVATE, ImageFormat.JPEG);
        }
    }

    /**
     * Test for making sure the mandatory stream combinations work for multi-resolution
     * reprocessing.
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionReprocessCaptureTest"	"testMultiResolutionMandatoryStreamCombinationTest"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionReprocessCaptureTest.java"	""	"public void testMultiResolutionMandatoryStreamCombinationTest() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            StaticMetadata info = mAllStaticInfo.get(id);
            CameraCharacteristics c = info.getCharacteristics();
            MandatoryStreamCombination[] combinations = c.get(
                            CameraCharacteristics.SCALER_MANDATORY_STREAM_COMBINATIONS);
            if (combinations == null) {
                Log.i(TAG, ""No mandatory stream combinations for camera: "" + id + "" skip test"");
                continue;
            }
            MultiResolutionStreamConfigurationMap multiResolutionMap = c.get(
                    CameraCharacteristics.SCALER_MULTI_RESOLUTION_STREAM_CONFIGURATION_MAP);
            if (multiResolutionMap == null) {
                Log.i(TAG, ""Camera "" + id + "" doesn't support multi-resolution capture."");
                continue;
            }
            int[] multiResolutionInputFormats = multiResolutionMap.getInputFormats();
            int[] multiResolutionOutputFormats = multiResolutionMap.getOutputFormats();
            if (multiResolutionInputFormats.length == 0
                    || multiResolutionOutputFormats.length == 0) {
                Log.i(TAG, ""Camera "" + id + "" doesn't support multi-resolution reprocess ""
                        + ""input/output."");
                continue;
            }

            try {
                openDevice(id);
                for (MandatoryStreamCombination combination : combinations) {
                    if (!combination.isReprocessable()) {
                        continue;
                    }

                    MandatoryStreamCombination.MandatoryStreamInformation firstStreamInfo =
                            combination.getStreamsInformation().get(0);
                    int inputFormat = firstStreamInfo.getFormat();
                    boolean supportMultiResReprocess = firstStreamInfo.isInput() &&
                            CameraTestUtils.contains(multiResolutionOutputFormats, inputFormat) &&
                            CameraTestUtils.contains(multiResolutionInputFormats, inputFormat);
                    if (!supportMultiResReprocess)  {
                        continue;
                    }

                    testMultiResolutionMandatoryStreamCombination(id, info, combination,
                            multiResolutionMap);
                }
            } finally {
                closeDevice(id);
            }
        }
    }

    private void testMultiResolutionMandatoryStreamCombination(String cameraId,
            StaticMetadata staticInfo, MandatoryStreamCombination combination,
            MultiResolutionStreamConfigurationMap multiResStreamConfig) throws Exception {
        String log = ""Testing multi-resolution mandatory stream combination: "" +
                combination.getDescription() + "" on camera: "" + cameraId;
        Log.i(TAG, log);

        final int TIMEOUT_FOR_RESULT_MS = 5000;
        final int NUM_REPROCESS_CAPTURES_PER_CONFIG = 3;

        // Set up outputs
        List<OutputConfiguration> outputConfigs = new ArrayList<OutputConfiguration>();
        List<Surface> outputSurfaces = new ArrayList<Surface>();
        StreamCombinationTargets targets = new StreamCombinationTargets();
        MultiResolutionImageReader inputReader = null;
        ImageWriter inputWriter = null;
        SimpleImageReaderListener inputReaderListener = new SimpleImageReaderListener();
        SimpleCaptureCallback inputCaptureListener = new SimpleCaptureCallback();
        SimpleCaptureCallback reprocessOutputCaptureListener = new SimpleCaptureCallback();

        List<MandatoryStreamInformation> streamInfo = combination.getStreamsInformation();
        assertTrue(""Reprocessable stream combinations should have at least 3 or more streams"",
                    (streamInfo != null) && (streamInfo.size() >= 3));
        assertTrue(""The first mandatory stream information in a reprocessable combination must "" +
                ""always be input"", streamInfo.get(0).isInput());

        int inputFormat = streamInfo.get(0).getFormat();

        CameraTestUtils.setupConfigurationTargets(streamInfo.subList(2, streamInfo.size()),
                targets, outputConfigs, outputSurfaces, NUM_REPROCESS_CAPTURES_PER_CONFIG,
                /*substituteY8*/false, /*substituteHeic*/false, /*physicalCameraId*/null,
                multiResStreamConfig, mHandler);

        Collection<MultiResolutionStreamInfo> multiResInputs =
                multiResStreamConfig.getInputInfo(inputFormat);
        InputConfiguration inputConfig = new InputConfiguration(multiResInputs, inputFormat);

        try {
            // For each config, YUV and JPEG outputs will be tested. (For YUV reprocessing,
            // the YUV ImageReader for input is also used for output.)
            final boolean inputIsYuv = inputConfig.getFormat() == ImageFormat.YUV_420_888;
            final boolean useYuv = inputIsYuv || targets.mYuvTargets.size() > 0 ||
                    targets.mYuvMultiResTargets.size() > 0;
            final int totalNumReprocessCaptures =  NUM_REPROCESS_CAPTURES_PER_CONFIG * (
                    (inputIsYuv ? 1 : 0) + targets.mJpegMultiResTargets.size() +
                    targets.mJpegTargets.size() +
                    (useYuv ? targets.mYuvMultiResTargets.size() + targets.mYuvTargets.size() : 0));

            // It needs 1 input buffer for each reprocess capture + the number of buffers
            // that will be used as outputs.
            inputReader = new MultiResolutionImageReader(multiResInputs, inputFormat,
                    totalNumReprocessCaptures + NUM_REPROCESS_CAPTURES_PER_CONFIG);
            inputReader.setOnImageAvailableListener(
                    inputReaderListener, new HandlerExecutor(mHandler));
            outputConfigs.addAll(
                    OutputConfiguration.createInstancesForMultiResolutionOutput(inputReader));
            outputSurfaces.add(inputReader.getSurface());

            CameraCaptureSession.CaptureCallback mockCaptureCallback =
                    mock(CameraCaptureSession.CaptureCallback.class);

            checkSessionConfigurationSupported(mCamera, mHandler, outputConfigs,
                    inputConfig, SessionConfiguration.SESSION_REGULAR,
                    true/*defaultSupport*/, String.format(
                    ""Session configuration query for multi-res combination: %s failed"",
                    combination.getDescription()));

            // Verify we can create a reprocessable session with the input and all outputs.
            BlockingSessionCallback sessionListener = new BlockingSessionCallback();
            CameraCaptureSession session = configureReprocessableCameraSessionWithConfigurations(
                    mCamera, inputConfig, outputConfigs, sessionListener, mHandler);
            inputWriter = ImageWriter.newInstance(
                    session.getInputSurface(), totalNumReprocessCaptures);

            // Prepare a request for reprocess input
            CaptureRequest.Builder builder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG);
            builder.addTarget(inputReader.getSurface());

            for (int i = 0; i < totalNumReprocessCaptures; i++) {
                session.capture(builder.build(), inputCaptureListener, mHandler);
            }

            List<CaptureRequest> reprocessRequests = new ArrayList<>();
            List<Surface> reprocessOutputs = new ArrayList<>();

            if (inputIsYuv) {
                reprocessOutputs.add(inputReader.getSurface());
            }
            for (MultiResolutionImageReader reader : targets.mJpegMultiResTargets) {
                reprocessOutputs.add(reader.getSurface());
            }
            for (ImageReader reader : targets.mJpegTargets) {
                reprocessOutputs.add(reader.getSurface());
            }
            for (MultiResolutionImageReader reader : targets.mYuvMultiResTargets) {
                reprocessOutputs.add(reader.getSurface());
            }
            for (ImageReader reader : targets.mYuvTargets) {
                reprocessOutputs.add(reader.getSurface());
            }

            for (int i = 0; i < NUM_REPROCESS_CAPTURES_PER_CONFIG; i++) {
                for (Surface output : reprocessOutputs) {
                    TotalCaptureResult result = inputCaptureListener.getTotalCaptureResult(
                            TIMEOUT_FOR_RESULT_MS);
                    Map<String, TotalCaptureResult> physicalResults =
                            result.getPhysicalCameraTotalResults();
                    for (Map.Entry<String, TotalCaptureResult> entry : physicalResults.entrySet()) {
                        String physicalCameraId = entry.getKey();
                        TotalCaptureResult physicalResult = entry.getValue();
                        String activePhysicalId = physicalResult.get(
                                CaptureResult.LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID);
                        mCollector.expectEquals(String.format(
                                ""Physical camera result metadata must contain activePhysicalId "" +
                                ""(%s) matching with physical camera Id (%s)."", activePhysicalId,
                                physicalCameraId), physicalCameraId, activePhysicalId);
                    }

                    String activePhysicalCameraId = result.get(
                            CaptureResult.LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID);
                    if (activePhysicalCameraId != null) {
                        result = physicalResults.get(activePhysicalCameraId);
                    }

                    builder = mCamera.createReprocessCaptureRequest(result);
                    inputWriter.queueInputImage(
                            inputReaderListener.getImage(TIMEOUT_FOR_RESULT_MS));
                    builder.addTarget(output);
                    reprocessRequests.add(builder.build());
                }
            }

            session.captureBurst(reprocessRequests, reprocessOutputCaptureListener, mHandler);

            for (int i = 0; i < reprocessOutputs.size() * NUM_REPROCESS_CAPTURES_PER_CONFIG; i++) {
                TotalCaptureResult result = reprocessOutputCaptureListener.getTotalCaptureResult(
                        TIMEOUT_FOR_RESULT_MS);
            }
        } catch (Throwable e) {
            mCollector.addMessage(
                    String.format(""Mandatory multi-res stream combination: %s failed due: %s"",
                    combination.getDescription(), e.getMessage()));
        } finally {
            inputReaderListener.drain();
            reprocessOutputCaptureListener.drain();
            targets.close();

            if (inputReader != null) {
                inputReader.close();
            }

            if (inputWriter != null) {
                inputWriter.close();
            }
        }
    }

    /**
     * Test multi-resolution reprocessing from the input format to the output format
     */
    private void testMultiResolutionReprocessing(String cameraId, int inputFormat,
            int outputFormat) throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""testMultiResolutionReprocessing: cameraId: "" + cameraId + "" inputFormat: ""
                    + inputFormat + "" outputFormat: "" + outputFormat);
        }

        Collection<MultiResolutionStreamInfo> inputStreamInfo =
                getMultiResReprocessInfo(cameraId, inputFormat, /*input*/ true);
        Collection<MultiResolutionStreamInfo> regularOutputStreamInfo =
                getMultiResReprocessInfo(cameraId, inputFormat, /*input*/ false);
        Collection<MultiResolutionStreamInfo> reprocessOutputStreamInfo =
                getMultiResReprocessInfo(cameraId, outputFormat, /*input*/ false);
        if (inputStreamInfo == null || regularOutputStreamInfo == null ||
                reprocessOutputStreamInfo == null) {
            return;
        }
        assertTrue(""The multi-resolution stream info for format "" + inputFormat
                + "" must be equal between input and output"",
                inputStreamInfo.containsAll(regularOutputStreamInfo)
                && regularOutputStreamInfo.containsAll(inputStreamInfo));

        try {
            openDevice(cameraId);

            testMultiResolutionReprocessWithStreamInfo(cameraId, inputFormat, inputStreamInfo,
                    outputFormat, reprocessOutputStreamInfo);
        } finally {
            closeDevice(cameraId);
        }
    }

    /**
     * Test multi-resolution reprocess with multi-resolution stream info lists for a particular
     * format combination.
     */
    private void testMultiResolutionReprocessWithStreamInfo(String cameraId,
            int inputFormat, Collection<MultiResolutionStreamInfo> inputInfo,
            int outputFormat, Collection<MultiResolutionStreamInfo> outputInfo)
            throws Exception {
        try {
            setupMultiResImageReaders(inputFormat, inputInfo, outputFormat, outputInfo,
                    /*maxImages*/1);
            setupReprocessableSession(inputFormat, inputInfo, outputInfo,
                    /*numImageWriterImages*/1);

            List<Float> zoomRatioList = CameraTestUtils.getCandidateZoomRatios(mStaticInfo);
            for (Float zoomRatio :  zoomRatioList) {
                ImageResultSizeHolder imageResultSizeHolder = null;

                try {
                    imageResultSizeHolder = doMultiResReprocessCapture(zoomRatio);
                    Image reprocessedImage = imageResultSizeHolder.getImage();
                    Size outputSize = imageResultSizeHolder.getExpectedSize();
                    TotalCaptureResult result = imageResultSizeHolder.getTotalCaptureResult();

                    mCollector.expectImageProperties(""testMultiResolutionReprocess"",
                            reprocessedImage, outputFormat, outputSize,
                            result.get(CaptureResult.SENSOR_TIMESTAMP));

                    if (DEBUG) {
                        Log.d(TAG, String.format(""camera %s %d zoom %f out %dx%d %d"",
                                cameraId, inputFormat, zoomRatio,
                                outputSize.getWidth(), outputSize.getHeight(),
                                outputFormat));

                        dumpImage(reprocessedImage,
                                ""/testMultiResolutionReprocess_camera"" + cameraId
                                + ""_"" + mDumpFrameCount);
                        mDumpFrameCount++;
                    }
                } finally {
                    if (imageResultSizeHolder != null) {
                        imageResultSizeHolder.getImage().close();
                    }
                }
            }
        } finally {
            closeReprossibleSession();
            closeMultiResImageReaders();
        }
    }

    /**
     * Set up multi-resolution image readers for regular and reprocess output
     *
     * <p>If the reprocess input format is equal to output format, share one multi-resolution
     * image reader.</p>
     */
    private void setupMultiResImageReaders(int inputFormat,
            Collection<MultiResolutionStreamInfo> inputInfo, int outputFormat,
            Collection<MultiResolutionStreamInfo> outputInfo, int maxImages) {

        mShareOneReader = false;
        // If the regular output and reprocess output have the same format,
        // they can share one MultiResolutionImageReader.
        if (inputFormat == outputFormat) {
            maxImages *= 2;
            mShareOneReader = true;
        }

        // create an MultiResolutionImageReader for the regular capture
        mMultiResImageReader = new MultiResolutionImageReader(inputInfo,
                inputFormat, maxImages);
        mMultiResImageReaderListener = new SimpleMultiResolutionImageReaderListener(
                mMultiResImageReader, 1, /*repeating*/false);
        mMultiResImageReader.setOnImageAvailableListener(mMultiResImageReaderListener,
                new HandlerExecutor(mHandler));

        if (!mShareOneReader) {
            // create an MultiResolutionImageReader for the reprocess capture
            mSecondMultiResImageReader = new MultiResolutionImageReader(
                    outputInfo, outputFormat, maxImages);
            mSecondMultiResImageReaderListener = new SimpleMultiResolutionImageReaderListener(
                    mSecondMultiResImageReader, maxImages, /*repeating*/ false);
            mSecondMultiResImageReader.setOnImageAvailableListener(
                    mSecondMultiResImageReaderListener, new HandlerExecutor(mHandler));
        }
    }

    /**
     * Close two multi-resolution image readers.
     */
    private void closeMultiResImageReaders() {
        mMultiResImageReader.close();
        mMultiResImageReader = null;

        if (!mShareOneReader) {
            mSecondMultiResImageReader.close();
            mSecondMultiResImageReader = null;
        }
    }

    /**
     * Get the MultiResolutionImageReader for reprocess output.
     */
    private MultiResolutionImageReader getOutputMultiResImageReader() {
        if (mShareOneReader) {
            return mMultiResImageReader;
        } else {
            return mSecondMultiResImageReader;
        }
    }

    /**
     * Get the MultiResolutionImageReaderListener for reprocess output.
     */
    private SimpleMultiResolutionImageReaderListener getOutputMultiResImageReaderListener() {
        if (mShareOneReader) {
            return mMultiResImageReaderListener;
        } else {
            return mSecondMultiResImageReaderListener;
        }
    }

    /**
     * Set up a reprocessable session and create an ImageWriter with the session's input surface.
     */
    private void setupReprocessableSession(int inputFormat,
            Collection<MultiResolutionStreamInfo> inputInfo,
            Collection<MultiResolutionStreamInfo> outputInfo,
            int numImageWriterImages) throws Exception {
        // create a reprocessable capture session
        Collection<OutputConfiguration> outConfigs =
                OutputConfiguration.createInstancesForMultiResolutionOutput(
                        mMultiResImageReader);
        ArrayList<OutputConfiguration> outputConfigsList = new ArrayList<OutputConfiguration>(
                outConfigs);

        if (!mShareOneReader) {
            Collection<OutputConfiguration> secondOutputConfigs =
                    OutputConfiguration.createInstancesForMultiResolutionOutput(
                            mSecondMultiResImageReader);
            outputConfigsList.addAll(secondOutputConfigs);
        }

        InputConfiguration inputConfig = new InputConfiguration(inputInfo, inputFormat);
        if (VERBOSE) {
            String inputConfigString = inputConfig.toString();
            Log.v(TAG, ""InputConfiguration: "" + inputConfigString);
        }

        mCameraSessionListener = new BlockingSessionCallback();
        mCameraSession = configureReprocessableCameraSessionWithConfigurations(
                mCamera, inputConfig, outputConfigsList, mCameraSessionListener, mHandler);

        // create an ImageWriter
        mInputSurface = mCameraSession.getInputSurface();
        mImageWriter = ImageWriter.newInstance(mInputSurface,
                numImageWriterImages);

        mImageWriterListener = new SimpleImageWriterListener(mImageWriter);
        mImageWriter.setOnImageReleasedListener(mImageWriterListener, mHandler);
    }

    /**
     * Close the reprocessable session and ImageWriter.
     */
    private void closeReprossibleSession() {
        mInputSurface = null;

        if (mCameraSession != null) {
            mCameraSession.close();
            mCameraSession = null;
        }

        if (mImageWriter != null) {
            mImageWriter.close();
            mImageWriter = null;
        }
    }

    /**
     * Do one multi-resolution reprocess capture for the specified zoom ratio
     */
    private ImageResultSizeHolder doMultiResReprocessCapture(float zoomRatio) throws Exception {
        // submit a regular capture and get the result
        TotalCaptureResult totalResult = submitCaptureRequest(
                zoomRatio, mMultiResImageReader.getSurface(), /*inputResult*/null);
        Map<String, TotalCaptureResult> physicalResults =
                totalResult.getPhysicalCameraTotalResults();

        ImageAndMultiResStreamInfo inputImageAndInfo =
                mMultiResImageReaderListener.getAnyImageAndInfoAvailable(CAPTURE_TIMEOUT_MS);
        assertNotNull(""Failed to capture input image"", inputImageAndInfo);
        Image inputImage = inputImageAndInfo.image;
        MultiResolutionStreamInfo inputStreamInfo = inputImageAndInfo.streamInfo;
        TotalCaptureResult inputSettings =
                physicalResults.get(inputStreamInfo.getPhysicalCameraId());
        assertTrue(""Regular capture's TotalCaptureResult doesn't contain capture result for ""
                + ""physical camera id "" + inputStreamInfo.getPhysicalCameraId(),
                inputSettings != null);

        // Submit a reprocess capture and get the result
        mImageWriter.queueInputImage(inputImage);

        TotalCaptureResult finalResult = submitCaptureRequest(zoomRatio,
                getOutputMultiResImageReader().getSurface(), inputSettings);

        ImageAndMultiResStreamInfo outputImageAndInfo =
                getOutputMultiResImageReaderListener().getAnyImageAndInfoAvailable(
                CAPTURE_TIMEOUT_MS);
        Image outputImage = outputImageAndInfo.image;
        MultiResolutionStreamInfo outputStreamInfo = outputImageAndInfo.streamInfo;

        assertTrue(""The regular output and reprocess output's stream info must be the same"",
                outputStreamInfo.equals(inputStreamInfo));

        ImageResultSizeHolder holder = new ImageResultSizeHolder(outputImageAndInfo.image,
                finalResult, new Size(outputStreamInfo.getWidth(), outputStreamInfo.getHeight()));

        return holder;
    }

    /**
     * Issue a capture request and return the result for a particular zoom ratio.
     *
     * <p>If inputResult is null, it's a regular request. Otherwise, it's a reprocess request.</p>
     */
    private TotalCaptureResult submitCaptureRequest(float zoomRatio,
            Surface output, TotalCaptureResult inputResult) throws Exception {

        SimpleCaptureCallback captureCallback = new SimpleCaptureCallback();

        // Prepare a list of capture requests. Whether it's a regular or reprocess capture request
        // is based on inputResult.
        CaptureRequest.Builder builder;
        boolean isReprocess = (inputResult != null);
        if (isReprocess) {
            builder = mCamera.createReprocessCaptureRequest(inputResult);
        } else {
            builder = mCamera.createCaptureRequest(CAPTURE_TEMPLATE);
            builder.set(CaptureRequest.CONTROL_ZOOM_RATIO, zoomRatio);
        }
        builder.addTarget(output);
        CaptureRequest request = builder.build();
        assertTrue(""Capture request reprocess type "" + request.isReprocess() + "" is wrong."",
            request.isReprocess() == isReprocess);

        mCameraSession.capture(request, captureCallback, mHandler);

        TotalCaptureResult result = captureCallback.getTotalCaptureResultForRequest(
                request, CAPTURE_TIMEOUT_FRAMES);

        // make sure all input surfaces are released.
        if (isReprocess) {
            mImageWriterListener.waitForImageReleased(CAPTURE_TIMEOUT_MS);
        }

        return result;
    }

    private Size getMaxSize(int format, StaticMetadata.StreamDirection direction) {
        Size[] sizes = mStaticInfo.getAvailableSizesForFormatChecked(format, direction);
        return getAscendingOrderSizes(Arrays.asList(sizes), /*ascending*/false).get(0);
    }

    private Collection<MultiResolutionStreamInfo> getMultiResReprocessInfo(String cameraId,
            int format, boolean input) throws Exception {
        StaticMetadata staticInfo = mAllStaticInfo.get(cameraId);
        CameraCharacteristics characteristics = staticInfo.getCharacteristics();
        MultiResolutionStreamConfigurationMap configs = characteristics.get(
                CameraCharacteristics.SCALER_MULTI_RESOLUTION_STREAM_CONFIGURATION_MAP);
        if (configs == null) {
            Log.i(TAG, ""Camera "" + cameraId + "" doesn't support multi-resolution streams"");
            return null;
        }

        String streamType = input ? ""input"" : ""output"";
        int[] formats = input ? configs.getInputFormats() :
                configs.getOutputFormats();
        if (!CameraTestUtils.contains(formats, format)) {
            Log.i(TAG, ""Camera "" + cameraId + "" doesn't support multi-resolution ""
                    + streamType + "" stream for format "" + format + "". Supported formats are ""
                    + Arrays.toString(formats));
            return null;
        }
        Collection<MultiResolutionStreamInfo> streams =
                input ? configs.getInputInfo(format) : configs.getOutputInfo(format);
        mCollector.expectTrue(String.format(""Camera %s supported 0 multi-resolution ""
                + streamType + "" stream info, expected at least 1"", cameraId),
                streams.size() > 0);

        return streams;
    }

    private void dumpImage(Image image, String name) {
        String filename = mDebugFileNameBase + name;
        switch(image.getFormat()) {
            case ImageFormat.JPEG:
                filename += "".jpg"";
                break;
            case ImageFormat.YUV_420_888:
                filename += "".yuv"";
                break;
            default:
                filename += ""."" + image.getFormat();
                break;
        }

        Log.d(TAG, ""dumping an image to "" + filename);
        dumpFile(filename , getDataFromImage(image));
    }

    /**
     * A class that holds an Image, a TotalCaptureResult, and expected image size.
     */
    public static class ImageResultSizeHolder {
        private final Image mImage;
        private final TotalCaptureResult mResult;
        private final Size mExpectedSize;

        public ImageResultSizeHolder(Image image, TotalCaptureResult result, Size expectedSize) {
            mImage = image;
            mResult = result;
            mExpectedSize = expectedSize;
        }

        public Image getImage() {
            return mImage;
        }

        public TotalCaptureResult getTotalCaptureResult() {
            return mResult;
        }

        public Size getExpectedSize() {
            return mExpectedSize;
        }
    }

}"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.camera.cts.api31test.SPerfClassTest"	"getCameraIdList"	"CtsCameraApi31TestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/api31test/src/android/camera/cts/api31test/SPerfClassTest.java"	""	"public void test/*
 *.
 */

package android.camera.cts.api31test;

import static android.hardware.camera2.cts.CameraTestUtils.*;

import android.content.Context;
import android.graphics.ImageFormat;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CaptureFailure;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.cts.CameraTestUtils;
import android.hardware.camera2.cts.CameraTestUtils.SimpleImageReaderListener;
import android.hardware.camera2.cts.helpers.CameraErrorCollector;
import android.hardware.camera2.cts.helpers.StaticMetadata;
import android.hardware.camera2.cts.helpers.StaticMetadata.CheckLevel;
import android.hardware.camera2.params.OutputConfiguration;
import android.hardware.camera2.params.SessionConfiguration;
import android.hardware.camera2.TotalCaptureResult;
import android.media.Image;
import android.media.ImageReader;
import android.os.Handler;
import android.os.HandlerThread;
import android.test.AndroidTestCase;
import android.util.Log;
import android.util.Size;
import android.view.Surface;

import com.android.compatibility.common.util.CddTest;
import com.android.ex.camera2.blocking.BlockingSessionCallback;

import java.util.ArrayList;
import java.util.List;

import org.junit.Test;

import static junit.framework.Assert.*;
import static org.mockito.Mockito.*;

public class SPerfClassTest extends AndroidTestCase {
    private static final String TAG = ""SPerfClassTest"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);

    private static final Size FULLHD = new Size(1920, 1080);
    private static final Size VGA = new Size(640, 480);
    private static final int CONFIGURE_TIMEOUT = 5000; //ms
    private static final int CAPTURE_TIMEOUT = 1500; //ms

    private CameraManager mCameraManager;
    private String[] mCameraIds;
    private Handler mHandler;
    private HandlerThread mHandlerThread;
    private CameraErrorCollector mCollector;

    @Override
    public void setContext(Context context) {
        super.setContext(context);
        mCameraManager = context.getSystemService(CameraManager.class);
        assertNotNull(""Can't connect to camera manager!"", mCameraManager);
    }

    @Override
    protected void setUp() throws Exception {
        super.setUp();

        mCameraIds = mCameraManager.getCameraIdList();
        assertNotNull(""Camera ids shouldn't be null"", mCameraIds);

        mHandlerThread = new HandlerThread(TAG);
        mHandlerThread.start();
        mHandler = new Handler(mHandlerThread.getLooper());

        mCollector = new CameraErrorCollector();
    }

    @Override
    protected void tearDown() throws Exception {
        mHandlerThread.quitSafely();
        mHandler = null;

        try {
            mCollector.verify();
        } catch (Throwable e) {
            // When new Exception(e) is used, exception info will be printed twice.
            throw new Exception(e.getMessage());
        } finally {
            super.tearDown();
        }
    }

    // Verify primary camera devices's supported JPEG sizes are at least 1080p.
    private void testSPerfClassJpegSizesByCamera(String cameraId) throws Exception {
        boolean isPrimaryRear = CameraTestUtils.isPrimaryRearFacingCamera(
                mCameraManager, cameraId);
        boolean isPrimaryFront = CameraTestUtils.isPrimaryFrontFacingCamera(
                mCameraManager, cameraId);
        if (!isPrimaryRear && !isPrimaryFront) {
            return;
        }

        CameraCharacteristics c = mCameraManager.getCameraCharacteristics(cameraId);
        StaticMetadata staticInfo = new StaticMetadata(c, CheckLevel.ASSERT, mCollector);

        Size[] jpegSizes = staticInfo.getJpegOutputSizesChecked();
        assertTrue(""Primary cameras must support JPEG formats"",
                jpegSizes != null && jpegSizes.length > 0);
        for (Size jpegSize : jpegSizes) {
            mCollector.expectTrue(
                    ""Primary camera's JPEG size must be at least 1080p, but is "" +
                    jpegSize,
                    jpegSize.getWidth() >= FULLHD.getWidth() &&
                    jpegSize.getHeight() >= FULLHD.getHeight());
        }

        CameraDevice camera = null;
        ImageReader jpegTarget = null;
        Image image = null;
        try {
            camera = CameraTestUtils.openCamera(mCameraManager, cameraId,
                    /*listener*/null, mHandler);

            List<OutputConfiguration> outputConfigs = new ArrayList<>();
            SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
            jpegTarget = CameraTestUtils.makeImageReader(VGA,
                    ImageFormat.JPEG, 1 /*maxNumImages*/, imageListener, mHandler);
            Surface jpegSurface = jpegTarget.getSurface();
            outputConfigs.add(new OutputConfiguration(jpegSurface));

            // isSessionConfigurationSupported will return true for JPEG sizes smaller
            // than 1080P, due to framework rouding up to closest supported size (1080p).
            SessionConfigSupport sessionConfigSupport = isSessionConfigSupported(
                    camera, mHandler, outputConfigs, /*inputConfig*/ null,
                    SessionConfiguration.SESSION_REGULAR, true/*defaultSupport*/);
            mCollector.expectTrue(""isSessionConfiguration fails with error"",
                    !sessionConfigSupport.error);
            mCollector.expectTrue(""isSessionConfiguration returns false for JPEG < 1080p"",
                    sessionConfigSupport.configSupported);

            // Session creation for JPEG sizes smaller than 1080p will succeed, and the
            // result JPEG image dimension is rounded up to closest supported size (1080p).
            CaptureRequest.Builder request =
                    camera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
            request.addTarget(jpegSurface);


            CameraCaptureSession.StateCallback sessionListener =
                    mock(CameraCaptureSession.StateCallback.class);
            CameraCaptureSession session = configureCameraSessionWithConfig(
                    camera, outputConfigs, sessionListener, mHandler);

            verify(sessionListener, timeout(CONFIGURE_TIMEOUT).atLeastOnce()).
                    onConfigured(any(CameraCaptureSession.class));
            verify(sessionListener, timeout(CONFIGURE_TIMEOUT).atLeastOnce()).
                    onReady(any(CameraCaptureSession.class));
            verify(sessionListener, never()).onConfigureFailed(any(CameraCaptureSession.class));
            verify(sessionListener, never()).onActive(any(CameraCaptureSession.class));
            verify(sessionListener, never()).onClosed(any(CameraCaptureSession.class));

            CameraCaptureSession.CaptureCallback captureListener =
                    mock(CameraCaptureSession.CaptureCallback.class);
            session.capture(request.build(), captureListener, mHandler);

            verify(captureListener, timeout(CAPTURE_TIMEOUT).atLeastOnce()).
                    onCaptureCompleted(any(CameraCaptureSession.class),
                            any(CaptureRequest.class), any(TotalCaptureResult.class));
            verify(captureListener, never()).onCaptureFailed(any(CameraCaptureSession.class),
                    any(CaptureRequest.class), any(CaptureFailure.class));

            image = imageListener.getImage(CAPTURE_TIMEOUT);
            assertNotNull(""Image must be valid"", image);
            assertEquals(""Image format isn't JPEG"", image.getFormat(), ImageFormat.JPEG);

            byte[] data = CameraTestUtils.getDataFromImage(image);
            assertTrue(""Invalid image data"", data != null && data.length > 0);

            CameraTestUtils.validateJpegData(data, FULLHD.getWidth(), FULLHD.getHeight(),
                    null /*filePath*/);
        } finally {
            if (camera != null) {
                camera.close();
            }
            if (jpegTarget != null) {
                jpegTarget.close();
            }
            if (image != null) {
                image.close();
            }
        }
    }

    /**
     * Check camera S Performance class requirement for JPEG sizes.
     */
    @CddTest(requirement=""7.5/H-1-8"")"	""	""	"cdd rear"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.bokeh.CameraBokehActivity"	"setPassFailButtonClickListeners"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/bokeh/CameraBokehActivity.java"	""	"public void test/*
 *.
 */
package com.android.cts.verifier.camera.bokeh;

import com.android.cts.verifier.PassFailButtons;
import com.android.cts.verifier.R;

import com.android.ex.camera2.blocking.BlockingCameraManager;
import com.android.ex.camera2.blocking.BlockingCameraManager.BlockingOpenException;
import com.android.ex.camera2.blocking.BlockingStateCallback;
import com.android.ex.camera2.blocking.BlockingSessionCallback;

import android.app.AlertDialog;
import android.content.res.Configuration;
import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.graphics.Color;
import android.graphics.ColorFilter;
import android.graphics.ColorMatrixColorFilter;
import android.graphics.ImageFormat;
import android.graphics.Matrix;
import android.graphics.RectF;
import android.graphics.SurfaceTexture;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraCharacteristics.Key;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CameraMetadata;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.params.Capability;
import android.hardware.camera2.params.StreamConfigurationMap;
import android.hardware.camera2.TotalCaptureResult;
import android.media.Image;
import android.media.ImageReader;
import android.os.Bundle;
import android.os.Handler;
import android.os.HandlerThread;
import android.util.Log;
import android.util.Size;
import android.util.SparseArray;
import android.view.Menu;
import android.view.MenuItem;
import android.view.View;
import android.view.Surface;
import android.view.TextureView;
import android.widget.AdapterView;
import android.widget.ArrayAdapter;
import android.widget.Button;
import android.widget.ImageButton;
import android.widget.ImageView;
import android.widget.Spinner;
import android.widget.TextView;
import android.widget.Toast;
import android.content.Context;

import java.lang.Math;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Comparator;
import java.util.List;
import java.util.Optional;
import java.util.Set;
import java.util.TreeSet;

/**
 * Tests for manual verification of bokeh modes supported by the camera device.
 */
public class CameraBokehActivity extends PassFailButtons.Activity
        implements TextureView.SurfaceTextureListener,
                   ImageReader.OnImageAvailableListener {

    private static final String TAG = ""CameraBokehActivity"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);
    private static final int SESSION_READY_TIMEOUT_MS = 5000;
    private static final Size FULLHD = new Size(1920, 1080);
    private static final ColorMatrixColorFilter sJFIF_YUVToRGB_Filter =
            new ColorMatrixColorFilter(new float[] {
                        1f,        0f,    1.402f, 0f, -179.456f,
                        1f, -0.34414f, -0.71414f, 0f,   135.46f,
                        1f,    1.772f,        0f, 0f, -226.816f,
                        0f,        0f,        0f, 1f,        0f
                    });

    private TextureView mPreviewView;
    private SurfaceTexture mPreviewTexture;
    private Surface mPreviewSurface;
    private int mPreviewTexWidth, mPreviewTexHeight;

    private ImageView mImageView;
    private ColorFilter mCurrentColorFilter;

    private Spinner mCameraSpinner;
    private TextView mTestLabel;
    private TextView mPreviewLabel;
    private TextView mImageLabel;

    private CameraManager mCameraManager;
    private String[] mCameraIdList;
    private HandlerThread mCameraThread;
    private Handler mCameraHandler;
    private BlockingCameraManager mBlockingCameraManager;
    private CameraCharacteristics mCameraCharacteristics;
    private BlockingStateCallback mCameraListener;

    private BlockingSessionCallback mSessionListener;
    private CaptureRequest.Builder mPreviewRequestBuilder;
    private CaptureRequest mPreviewRequest;
    private CaptureRequest.Builder mStillCaptureRequestBuilder;
    private CaptureRequest mStillCaptureRequest;

    private HashMap<String, ArrayList<Capability>> mTestCases = new HashMap<>();
    private int mCurrentCameraIndex = -1;
    private String mCameraId;
    private CameraCaptureSession mCaptureSession;
    private CameraDevice mCameraDevice;

    SizeComparator mSizeComparator = new SizeComparator();

    private Size mPreviewSize;
    private Size mJpegSize;
    private ImageReader mJpegImageReader;
    private ImageReader mYuvImageReader;

    private SparseArray<String> mModeNames;

    private CameraCombination mNextCombination;
    private Size mMaxBokehStreamingSize;

    private Button mNextButton;

    private final TreeSet<CameraCombination> mTestedCombinations = new TreeSet<>(COMPARATOR);
    private final TreeSet<CameraCombination> mUntestedCombinations = new TreeSet<>(COMPARATOR);
    private final TreeSet<String> mUntestedCameras = new TreeSet<>();

    // Menu to show the test progress
    private static final int MENU_ID_PROGRESS = Menu.FIRST + 1;

    private class CameraCombination {
        private final int mCameraIndex;
        private final int mMode;
        private final Size mPreviewSize;
        private final boolean mIsStillCapture;
        private final String mCameraId;
        private final String mModeName;

        private CameraCombination(int cameraIndex, int mode,
                int streamingWidth, int streamingHeight,
                String cameraId, String modeName,
                boolean isStillCapture) {
            this.mCameraIndex = cameraIndex;
            this.mMode = mode;
            this.mPreviewSize = new Size(streamingWidth, streamingHeight);
            this.mCameraId = cameraId;
            this.mModeName = modeName;
            this.mIsStillCapture = isStillCapture;
        }

        @Override
        public String toString() {
            return String.format(""Camera %s, mode %s, intent %s"",
                    mCameraId, mModeName, mIsStillCapture ? ""PREVIEW"" : ""STILL_CAPTURE"");
        }
    }

    private static final Comparator<CameraCombination> COMPARATOR =
        Comparator.<CameraCombination, Integer>comparing(c -> c.mCameraIndex)
            .thenComparing(c -> c.mMode)
            .thenComparing(c -> c.mIsStillCapture);

    private CameraCaptureSession.CaptureCallback mCaptureCallback =
            new CameraCaptureSession.CaptureCallback() {
        @Override
        public void onCaptureProgressed(CameraCaptureSession session,
                                        CaptureRequest request,
                                        CaptureResult partialResult) {
            // Don't need to do anything here.
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session,
                                       CaptureRequest request,
                                       TotalCaptureResult result) {
            // Don't need to do anything here.
        }
    };

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        setContentView(R.layout.cb_main);

        setPassFailButtonClickListeners();

        mPreviewView = (TextureView) findViewById(R.id.preview_view);
        mImageView = (ImageView) findViewById(R.id.image_view);

        mPreviewView.setSurfaceTextureListener(this);

        mCameraManager = (CameraManager) getSystemService(Context.CAMERA_SERVICE);
        try {
            mCameraIdList = mCameraManager.getCameraIdList();
            for (String id : mCameraIdList) {
                CameraCharacteristics characteristics =
                        mCameraManager.getCameraCharacteristics(id);
                Key<Capability[]> key =
                        CameraCharacteristics.CONTROL_AVAILABLE_EXTENDED_SCENE_MODE_CAPABILITIES;
                Capability[] extendedSceneModeCaps = characteristics.get(key);

                if (extendedSceneModeCaps == null) {
                    continue;
                }

                ArrayList<Capability> nonOffModes = new ArrayList<>();
                for (Capability cap : extendedSceneModeCaps) {
                    int mode = cap.getMode();
                    if (mode == CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE ||
                            mode == CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_CONTINUOUS) {
                        nonOffModes.add(cap);
                    }
                }

                if (nonOffModes.size() > 0) {
                    mUntestedCameras.add(""All combinations for Camera "" + id);
                    mTestCases.put(id, nonOffModes);
                }

            }
        } catch (CameraAccessException e) {
            e.printStackTrace();
        }

        // If no supported bokeh modes, mark the test as pass
        if (mTestCases.size() == 0) {
            setInfoResources(R.string.camera_bokeh_test, R.string.camera_bokeh_no_support, -1);
            setPassButtonEnabled(true);
        } else {
            setInfoResources(R.string.camera_bokeh_test, R.string.camera_bokeh_test_info, -1);
            // disable ""Pass"" button until all combinations are tested
            setPassButtonEnabled(false);
        }

        Set<String> cameraIdSet = mTestCases.keySet();
        String[] cameraNames = new String[cameraIdSet.size()];
        int i = 0;
        for (String id : cameraIdSet) {
            cameraNames[i++] = ""Camera "" + id;
        }
        mCameraSpinner = (Spinner) findViewById(R.id.cameras_selection);
        mCameraSpinner.setAdapter(
            new ArrayAdapter<String>(
                this, R.layout.camera_list_item, cameraNames));
        mCameraSpinner.setOnItemSelectedListener(mCameraSpinnerListener);

        mTestLabel = (TextView) findViewById(R.id.test_label);
        mPreviewLabel = (TextView) findViewById(R.id.preview_label);
        mImageLabel = (TextView) findViewById(R.id.image_label);

        // Must be kept in sync with camera bokeh mode manually
        mModeNames = new SparseArray(2);
        mModeNames.append(
                CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE, ""STILL_CAPTURE"");
        mModeNames.append(
                CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_CONTINUOUS, ""CONTINUOUS"");

        mNextButton = findViewById(R.id.next_button);
        mNextButton.setOnClickListener(v -> {
                if (mNextCombination != null) {
                    mUntestedCombinations.remove(mNextCombination);
                    mTestedCombinations.add(mNextCombination);
                }
                setUntestedCombination();

                if (mNextCombination != null) {
                    if (mNextCombination.mIsStillCapture) {
                        takePicture();
                    } else {
                        if (mCaptureSession != null) {
                            mCaptureSession.close();
                        }
                        startPreview();
                    }
                }
        });

        mBlockingCameraManager = new BlockingCameraManager(mCameraManager);
        mCameraListener = new BlockingStateCallback();
    }

    /**
     * Set an untested combination of resolution and bokeh mode for the current camera.
     * Triggered by next button click.
     */
    private void setUntestedCombination() {
        Optional<CameraCombination> combination = mUntestedCombinations.stream().filter(
            c -> c.mCameraIndex == mCurrentCameraIndex).findFirst();
        if (!combination.isPresent()) {
            Toast.makeText(this, ""All Camera "" + mCurrentCameraIndex + "" tests are done."",
                Toast.LENGTH_SHORT).show();
            mNextCombination = null;

            if (mUntestedCombinations.isEmpty() && mUntestedCameras.isEmpty()) {
                setPassButtonEnabled(true);
            }
            return;
        }

        // There is untested combination for the current camera, set the next untested combination.
        mNextCombination = combination.get();
        int nextMode = mNextCombination.mMode;
        ArrayList<Capability> bokehCaps = mTestCases.get(mCameraId);
        for (Capability cap : bokehCaps) {
            if (cap.getMode() == nextMode) {
                mMaxBokehStreamingSize = cap.getMaxStreamingSize();
            }
        }

        // Update bokeh mode and use case
        String testString = ""Mode: "" + mModeNames.get(mNextCombination.mMode);
        if (mNextCombination.mIsStillCapture) {
            testString += ""\nIntent: Capture"";
        } else {
            testString += ""\nIntent: Preview"";
        }
        testString += ""\n\nPress Next if the bokeh effect works as intended"";
        mTestLabel.setText(testString);

        // Update preview view and image view bokeh expectation
        boolean previewIsBokehCompatible =
                mSizeComparator.compare(mNextCombination.mPreviewSize, mMaxBokehStreamingSize) <= 0;
        String previewLabel = ""Normal preview"";
        if (previewIsBokehCompatible || mNextCombination.mIsStillCapture) {
            previewLabel += "" with bokeh"";
        }
        mPreviewLabel.setText(previewLabel);

        String imageLabel;
        if (mNextCombination.mIsStillCapture) {
            imageLabel = ""JPEG with bokeh"";
        } else {
            imageLabel = ""YUV"";
            if (previewIsBokehCompatible) {
                imageLabel += "" with bokeh"";
            }
        }
        mImageLabel.setText(imageLabel);
    }

    @Override
    public boolean onCreateOptionsMenu(Menu menu) {
        menu.add(Menu.NONE, MENU_ID_PROGRESS, Menu.NONE, ""Current Progress"");
        return super.onCreateOptionsMenu(menu);
    }

    @Override
    public boolean onOptionsItemSelected(MenuItem item) {
        boolean ret = true;
        switch (item.getItemId()) {
            case MENU_ID_PROGRESS:
                showCombinationsDialog();
                ret = true;
                break;
            default:
                ret = super.onOptionsItemSelected(item);
                break;
        }
        return ret;
    }

    private void showCombinationsDialog() {
        AlertDialog.Builder builder =
                new AlertDialog.Builder(CameraBokehActivity.this);
        builder.setMessage(getTestDetails())
                .setTitle(""Current Progress"")
                .setPositiveButton(""OK"", null);
        builder.show();
    }

    @Override
    public void onResume() {
        super.onResume();

        startBackgroundThread();

        int cameraIndex = mCameraSpinner.getSelectedItemPosition();
        if (cameraIndex >= 0) {
            setUpCamera(mCameraSpinner.getSelectedItemPosition());
        }
    }

    @Override
    public void onPause() {
        shutdownCamera();
        stopBackgroundThread();

        super.onPause();
    }

    @Override
    public String getTestDetails() {
        StringBuilder reportBuilder = new StringBuilder();
        reportBuilder.append(""Tested combinations:\n"");
        for (CameraCombination combination: mTestedCombinations) {
            reportBuilder.append(combination);
            reportBuilder.append(""\n"");
        }

        reportBuilder.append(""Untested cameras:\n"");
        for (String untestedCamera : mUntestedCameras) {
            reportBuilder.append(untestedCamera);
            reportBuilder.append(""\n"");
        }
        reportBuilder.append(""Untested combinations:\n"");
        for (CameraCombination combination: mUntestedCombinations) {
            reportBuilder.append(combination);
            reportBuilder.append(""\n"");
        }
        return reportBuilder.toString();
    }

    @Override
    public void onSurfaceTextureAvailable(SurfaceTexture surfaceTexture,
            int width, int height) {
        mPreviewTexture = surfaceTexture;
        mPreviewTexWidth = width;
        mPreviewTexHeight = height;

        mPreviewSurface = new Surface(mPreviewTexture);

        if (mCameraDevice != null) {
            startPreview();
        }
    }

    @Override
    public void onSurfaceTextureSizeChanged(SurfaceTexture surface, int width, int height) {
        // Ignored, Camera does all the work for us
        if (VERBOSE) {
            Log.v(TAG, ""onSurfaceTextureSizeChanged: "" + width + "" x "" + height);
        }
    }

    @Override
    public boolean onSurfaceTextureDestroyed(SurfaceTexture surface) {
        mPreviewTexture = null;
        return true;
    }

    @Override
    public void onSurfaceTextureUpdated(SurfaceTexture surface) {
        // Invoked every time there's a new Camera preview frame
    }

    @Override
    public void onImageAvailable(ImageReader reader) {
        Image img = null;
        try {
            img = reader.acquireNextImage();
            if (img == null) {
                Log.d(TAG, ""Invalid image!"");
                return;
            }
            final int format = img.getFormat();

            Size configuredSize = (format == ImageFormat.YUV_420_888 ? mPreviewSize : mJpegSize);
            Bitmap imgBitmap = null;
            if (format == ImageFormat.YUV_420_888) {
                ByteBuffer yBuffer = img.getPlanes()[0].getBuffer();
                ByteBuffer uBuffer = img.getPlanes()[1].getBuffer();
                ByteBuffer vBuffer = img.getPlanes()[2].getBuffer();
                yBuffer.rewind();
                uBuffer.rewind();
                vBuffer.rewind();
                int w = configuredSize.getWidth();
                int h = configuredSize.getHeight();
                int stride = img.getPlanes()[0].getRowStride();
                int uStride = img.getPlanes()[1].getRowStride();
                int vStride = img.getPlanes()[2].getRowStride();
                int uPStride = img.getPlanes()[1].getPixelStride();
                int vPStride = img.getPlanes()[2].getPixelStride();
                byte[] row = new byte[configuredSize.getWidth()];
                byte[] uRow = new byte[(configuredSize.getWidth()/2-1)*uPStride + 1];
                byte[] vRow = new byte[(configuredSize.getWidth()/2-1)*vPStride + 1];
                int[] imgArray = new int[w * h];
                for (int y = 0, j = 0, rowStart = 0, uRowStart = 0, vRowStart = 0; y < h;
                     y++, rowStart += stride) {
                    yBuffer.position(rowStart);
                    yBuffer.get(row);
                    if (y % 2 == 0) {
                        uBuffer.position(uRowStart);
                        uBuffer.get(uRow);
                        vBuffer.position(vRowStart);
                        vBuffer.get(vRow);
                        uRowStart += uStride;
                        vRowStart += vStride;
                    }
                    for (int x = 0, i = 0; x < w; x++) {
                        int yval = row[i] & 0xFF;
                        int uval = uRow[i/2 * uPStride] & 0xFF;
                        int vval = vRow[i/2 * vPStride] & 0xFF;
                        // Write YUV directly; the ImageView color filter will convert to RGB for us.
                        imgArray[j] = Color.rgb(yval, uval, vval);
                        i++;
                        j++;
                    }
                }
                img.close();
                imgBitmap = Bitmap.createBitmap(imgArray, w, h, Bitmap.Config.ARGB_8888);
            } else if (format == ImageFormat.JPEG) {
                ByteBuffer jpegBuffer = img.getPlanes()[0].getBuffer();
                jpegBuffer.rewind();
                byte[] jpegData = new byte[jpegBuffer.limit()];
                jpegBuffer.get(jpegData);
                imgBitmap = BitmapFactory.decodeByteArray(jpegData, 0, jpegData.length);
                img.close();
            } else {
                Log.i(TAG, ""Unsupported image format: "" + format);
            }
            if (imgBitmap != null) {
                final Bitmap bitmap = imgBitmap;
                runOnUiThread(new Runnable() {
                    @Override
                    public void run() {
                        if (format == ImageFormat.YUV_420_888 && (mCurrentColorFilter == null ||
                                !mCurrentColorFilter.equals(sJFIF_YUVToRGB_Filter))) {
                            mCurrentColorFilter = sJFIF_YUVToRGB_Filter;
                            mImageView.setColorFilter(mCurrentColorFilter);
                        } else if (format == ImageFormat.JPEG && mCurrentColorFilter != null &&
                                mCurrentColorFilter.equals(sJFIF_YUVToRGB_Filter)) {
                            mCurrentColorFilter = null;
                            mImageView.clearColorFilter();
                        }
                        mImageView.setImageBitmap(bitmap);
                    }
                });
            }
        } catch (java.lang.IllegalStateException e) {
            // Swallow exceptions
            e.printStackTrace();
        } finally {
            if (img != null) {
                img.close();
            }
        }
    }

    private AdapterView.OnItemSelectedListener mCameraSpinnerListener =
            new AdapterView.OnItemSelectedListener() {
                public void onItemSelected(AdapterView<?> parent,
                        View view, int pos, long id) {
                    if (mCurrentCameraIndex != pos) {
                        setUpCamera(pos);
                    }
                }

                public void onNothingSelected(AdapterView parent) {
                }
            };

    private class SizeComparator implements Comparator<Size> {
        @Override
        public int compare(Size lhs, Size rhs) {
            long lha = lhs.getWidth() * lhs.getHeight();
            long rha = rhs.getWidth() * rhs.getHeight();
            if (lha == rha) {
                lha = lhs.getWidth();
                rha = rhs.getWidth();
            }
            return (lha < rha) ? -1 : (lha > rha ? 1 : 0);
        }
    }

    private void setUpCamera(int index) {
        shutdownCamera();

        mCurrentCameraIndex = index;
        mCameraId = mCameraIdList[index];
        try {
            mCameraCharacteristics = mCameraManager.getCameraCharacteristics(mCameraId);
            mCameraDevice = mBlockingCameraManager.openCamera(mCameraId,
                    mCameraListener, mCameraHandler);
        } catch (CameraAccessException e) {
            e.printStackTrace();
        } catch (BlockingOpenException e) {
            e.printStackTrace();
        }

        // Update untested cameras
        mUntestedCameras.remove(""All combinations for Camera "" + mCameraId);

        StreamConfigurationMap config =
                mCameraCharacteristics.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        Size[] jpegSizes = config.getOutputSizes(ImageFormat.JPEG);
        Arrays.sort(jpegSizes, mSizeComparator);
        mJpegSize = jpegSizes[jpegSizes.length-1];

        Size[] yuvSizes = config.getOutputSizes(ImageFormat.YUV_420_888);
        Arrays.sort(yuvSizes, mSizeComparator);
        Size maxYuvSize = yuvSizes[yuvSizes.length-1];
        if (mSizeComparator.compare(maxYuvSize, FULLHD) > 1) {
            maxYuvSize = FULLHD;
        }

        // Update untested entries
        ArrayList<Capability> currentTestCase = mTestCases.get(mCameraId);
        for (Capability bokehCap : currentTestCase) {
            Size maxStreamingSize = bokehCap.getMaxStreamingSize();
            Size previewSize;
            if ((maxStreamingSize.getWidth() == 0 && maxStreamingSize.getHeight() == 0) ||
                    (mSizeComparator.compare(maxStreamingSize, maxYuvSize) > 0)) {
                previewSize = maxYuvSize;
            } else {
                previewSize = maxStreamingSize;
            }

            CameraCombination combination = new CameraCombination(
                    index, bokehCap.getMode(), previewSize.getWidth(),
                    previewSize.getHeight(), mCameraId,
                    mModeNames.get(bokehCap.getMode()),
                    /*isStillCapture*/false);

            if (!mTestedCombinations.contains(combination)) {
                mUntestedCombinations.add(combination);
            }

            // For BOKEH_MODE_STILL_CAPTURE, add 2 combinations: one streaming, one still capture.
            if (bokehCap.getMode() ==
                    CaptureRequest.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE) {
                CameraCombination combination2 = new CameraCombination(
                        index, bokehCap.getMode(), previewSize.getWidth(),
                        previewSize.getHeight(), mCameraId,
                        mModeNames.get(bokehCap.getMode()),
                        /*isStillCapture*/true);

                if (!mTestedCombinations.contains(combination2)) {
                    mUntestedCombinations.add(combination2);
                }
            }
        }

        mJpegImageReader = ImageReader.newInstance(
                mJpegSize.getWidth(), mJpegSize.getHeight(), ImageFormat.JPEG, 1);
        mJpegImageReader.setOnImageAvailableListener(this, mCameraHandler);

        setUntestedCombination();

        if (mPreviewTexture != null) {
            startPreview();
        }
    }

    private void shutdownCamera() {
        if (null != mCaptureSession) {
            mCaptureSession.close();
            mCaptureSession = null;
        }
        if (null != mCameraDevice) {
            mCameraDevice.close();
            mCameraDevice = null;
        }
        if (null != mJpegImageReader) {
            mJpegImageReader.close();
            mJpegImageReader = null;
        }
        if (null != mYuvImageReader) {
            mYuvImageReader.close();
            mYuvImageReader = null;
        }
    }

    private void configurePreviewTextureTransform() {
        int rotation = getWindowManager().getDefaultDisplay().getRotation();
        Configuration config = getResources().getConfiguration();
        int degrees = 0;
        switch (rotation) {
            case Surface.ROTATION_0: degrees = 0; break;
            case Surface.ROTATION_90: degrees = 90; break;
            case Surface.ROTATION_180: degrees = 180; break;
            case Surface.ROTATION_270: degrees = 270; break;
        }
        Matrix matrix = mPreviewView.getTransform(null);
        int deviceOrientation = Configuration.ORIENTATION_PORTRAIT;
        if ((degrees % 180 == 0 && config.orientation == Configuration.ORIENTATION_LANDSCAPE) ||
                (degrees % 180 == 90 && config.orientation == Configuration.ORIENTATION_PORTRAIT)) {
            deviceOrientation = Configuration.ORIENTATION_LANDSCAPE;
        }
        int effectiveWidth = mPreviewSize.getWidth();
        int effectiveHeight = mPreviewSize.getHeight();
        if (deviceOrientation == Configuration.ORIENTATION_PORTRAIT) {
            int temp = effectiveWidth;
            effectiveWidth = effectiveHeight;
            effectiveHeight = temp;
        }

        RectF viewRect = new RectF(0, 0, mPreviewTexWidth, mPreviewTexHeight);
        RectF bufferRect = new RectF(0, 0, effectiveWidth, effectiveHeight);
        float centerX = viewRect.centerX();
        float centerY = viewRect.centerY();
        bufferRect.offset(centerX - bufferRect.centerX(),
                centerY - bufferRect.centerY());

        matrix.setRectToRect(viewRect, bufferRect, Matrix.ScaleToFit.FILL);

        matrix.postRotate((360 - degrees) % 360, centerX, centerY);
        if ((degrees % 180) == 90) {
            int temp = effectiveWidth;
            effectiveWidth = effectiveHeight;
            effectiveHeight = temp;
        }
        // Scale to fit view, avoiding any crop
        float scale = Math.min(mPreviewTexWidth / (float) effectiveWidth,
                mPreviewTexHeight / (float) effectiveHeight);
        matrix.postScale(scale, scale, centerX, centerY);

        mPreviewView.setTransform(matrix);
    }
    /**
     * Starts a background thread and its {@link Handler}.
     */
    private void startBackgroundThread() {
        mCameraThread = new HandlerThread(""CameraBokehBackground"");
        mCameraThread.start();
        mCameraHandler = new Handler(mCameraThread.getLooper());
    }

    /**
     * Stops the background thread and its {@link Handler}.
     */
    private void stopBackgroundThread() {
        mCameraThread.quitSafely();
        try {
            mCameraThread.join();
            mCameraThread = null;
            mCameraHandler = null;
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }

    private void startPreview() {
        try {
            if (mPreviewSize == null || !mPreviewSize.equals(mNextCombination.mPreviewSize)) {
                mPreviewSize = mNextCombination.mPreviewSize;

                mYuvImageReader = ImageReader.newInstance(mPreviewSize.getWidth(),
                        mPreviewSize.getHeight(), ImageFormat.YUV_420_888, 1);
                mYuvImageReader.setOnImageAvailableListener(this, mCameraHandler);
            };

            mPreviewTexture.setDefaultBufferSize(mPreviewSize.getWidth(), mPreviewSize.getHeight());
            mPreviewRequestBuilder =
                    mCameraDevice.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
            mPreviewRequestBuilder.addTarget(mPreviewSurface);
            mPreviewRequestBuilder.addTarget(mYuvImageReader.getSurface());

            mStillCaptureRequestBuilder =
                    mCameraDevice.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
            mStillCaptureRequestBuilder.addTarget(mPreviewSurface);
            mStillCaptureRequestBuilder.addTarget(mJpegImageReader.getSurface());

            mSessionListener = new BlockingSessionCallback();
            List<Surface> outputSurfaces = new ArrayList<Surface>(/*capacity*/3);
            outputSurfaces.add(mPreviewSurface);
            outputSurfaces.add(mYuvImageReader.getSurface());
            outputSurfaces.add(mJpegImageReader.getSurface());
            mCameraDevice.createCaptureSession(outputSurfaces, mSessionListener, mCameraHandler);
            mCaptureSession = mSessionListener.waitAndGetSession(/*timeoutMs*/3000);

            configurePreviewTextureTransform();

            /* Set bokeh mode and start streaming */
            int bokehMode = mNextCombination.mMode;
            mPreviewRequestBuilder.set(CaptureRequest.CONTROL_EXTENDED_SCENE_MODE, bokehMode);
            mStillCaptureRequestBuilder.set(CaptureRequest.CONTROL_EXTENDED_SCENE_MODE, bokehMode);
            mPreviewRequest = mPreviewRequestBuilder.build();
            mStillCaptureRequest = mStillCaptureRequestBuilder.build();

            mCaptureSession.setRepeatingRequest(mPreviewRequest, mCaptureCallback, mCameraHandler);
        } catch (CameraAccessException e) {
            e.printStackTrace();
        }
    }

    private void takePicture() {
        try {
            mCaptureSession.stopRepeating();
            mSessionListener.getStateWaiter().waitForState(
                    BlockingSessionCallback.SESSION_READY, SESSION_READY_TIMEOUT_MS);

            mCaptureSession.capture(mStillCaptureRequest, mCaptureCallback, mCameraHandler);
        } catch (CameraAccessException e) {
            e.printStackTrace();
        }
    }

    private void setPassButtonEnabled(boolean enabled) {
        ImageButton pass_button = (ImageButton) findViewById(R.id.pass_button);
        pass_button.setEnabled(enabled);
    }
}"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.LogicalCameraDeviceTest"	"testDefaultFov"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/LogicalCameraDeviceTest.java"	""	"@CddTest(requirement=""7.5.4/C-1-1"")
    public void testDefaultFov() throws Exception {
        final double MIN_FOV = 50;
        final double MAX_FOV = 90;
        if (!isHandheldDevice()) {
            return;
        }
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.i(TAG, ""Testing Camera "" + id);

                StaticMetadata staticInfo = mAllStaticInfo.get(id);
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" does not support color outputs, skipping"");
                    continue;
                }

                if (!staticInfo.isLogicalMultiCamera()) {
                    Log.i(TAG, ""Camera "" + id + "" is not a logical multi-camera, skipping"");
                    continue;
                }

                SizeF physicalSize = staticInfo.getCharacteristics().get(
                        CameraCharacteristics.SENSOR_INFO_PHYSICAL_SIZE);
                double physicalDiag = Math.sqrt(Math.pow(physicalSize.getWidth(), 2)
                        + Math.pow(physicalSize.getHeight(), 2));
                Rect activeArraySize = staticInfo.getCharacteristics().get(
                        CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE);

                openDevice(id);
                for (int template : sTemplates) {
                    try {
                        CaptureRequest.Builder requestBuilder =
                                mCamera.createCaptureRequest(template);
                        Float requestFocalLength = requestBuilder.get(
                                CaptureRequest.LENS_FOCAL_LENGTH);
                        assertNotNull(""LENS_FOCAL_LENGTH must not be null"", requestFocalLength);

                        Float requestZoomRatio = requestBuilder.get(
                                CaptureRequest.CONTROL_ZOOM_RATIO);
                        assertNotNull(""CONTROL_ZOOM_RATIO must not be null"", requestZoomRatio);
                        Rect requestCropRegion = requestBuilder.get(
                                CaptureRequest.SCALER_CROP_REGION);
                        assertNotNull(""SCALER_CROP_REGION must not be null"", requestCropRegion);
                        float totalZoomRatio = Math.min(
                                1.0f * activeArraySize.width() / requestCropRegion.width(),
                                1.0f * activeArraySize.height() / requestCropRegion.height()) *
                                requestZoomRatio;

                        double fov = 2 *
                                Math.toDegrees(Math.atan2(physicalDiag/(2 * totalZoomRatio),
                                requestFocalLength));

                        Log.v(TAG, ""Camera "" +  id + "" template "" + template +
                                ""'s default FOV is "" + fov);
                        mCollector.expectInRange(""Camera "" +  id + "" template "" + template +
                                ""'s default FOV must fall between [50, 90] degrees"",
                                fov, MIN_FOV, MAX_FOV);
                    } catch (IllegalArgumentException e) {
                        if (template == CameraDevice.TEMPLATE_MANUAL &&
                                !staticInfo.isCapabilitySupported(CameraCharacteristics.
                                REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
                            // OK
                        } else if (template == CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG &&
                                !staticInfo.isCapabilitySupported(CameraCharacteristics.
                                REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING)) {
                            // OK.
                        } else {
                            throw e; // rethrow
                        }
                    }
                }
            } finally {
                closeDevice();
            }
        }
    }

    /**
     * Find a common preview size that's supported by both the logical camera and
     * two of the underlying physical cameras.
     */
    private Size findCommonPreviewSize(String cameraId,
            List<String> dualPhysicalCameraIds) throws Exception {

        Set<String> physicalCameraIds =
                mStaticInfo.getCharacteristics().getPhysicalCameraIds();
        assertTrue(""Logical camera must contain at least 2 physical camera ids"",
                physicalCameraIds.size() >= 2);

        List<Size> previewSizes = getSupportedPreviewSizes(
                cameraId, mCameraManager, PREVIEW_SIZE_BOUND);
        HashMap<String, List<Size>> physicalPreviewSizesMap = new HashMap<String, List<Size>>();
        HashMap<String, StreamConfigurationMap> physicalConfigs = new HashMap<>();
        for (String physicalCameraId : physicalCameraIds) {
            CameraCharacteristics properties =
                    mCameraManager.getCameraCharacteristics(physicalCameraId);
            assertNotNull(""Can't get camera characteristics!"", properties);
            if (!mAllStaticInfo.get(physicalCameraId).isColorOutputSupported()) {
                // No color output support, skip.
                continue;
            }
            StreamConfigurationMap configMap =
                properties.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            physicalConfigs.put(physicalCameraId, configMap);
            physicalPreviewSizesMap.put(physicalCameraId,
                    getSupportedPreviewSizes(physicalCameraId, mCameraManager, PREVIEW_SIZE_BOUND));
        }

        // Find display size from window service.
        Context context = mActivityRule.getActivity().getApplicationContext();
        WindowManager windowManager =
                (WindowManager) context.getSystemService(Context.WINDOW_SERVICE);
        Display display = windowManager.getDefaultDisplay();

        int displayWidth = display.getWidth();
        int displayHeight = display.getHeight();

        if (displayHeight > displayWidth) {
            displayHeight = displayWidth;
            displayWidth = display.getHeight();
        }

        StreamConfigurationMap config = mStaticInfo.getCharacteristics().get(
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        for (Size previewSize : previewSizes) {
            dualPhysicalCameraIds.clear();
            // Skip preview sizes larger than screen size
            if (previewSize.getWidth() > displayWidth ||
                    previewSize.getHeight() > displayHeight) {
                continue;
            }

            final long minFrameDuration = config.getOutputMinFrameDuration(
                   ImageFormat.YUV_420_888, previewSize);

            ArrayList<String> supportedPhysicalCameras = new ArrayList<String>();
            for (String physicalCameraId : physicalCameraIds) {
                List<Size> physicalPreviewSizes = physicalPreviewSizesMap.get(physicalCameraId);
                if (physicalPreviewSizes != null && physicalPreviewSizes.contains(previewSize)) {
                   long minDurationPhysical =
                           physicalConfigs.get(physicalCameraId).getOutputMinFrameDuration(
                           ImageFormat.YUV_420_888, previewSize);
                   if (minDurationPhysical <= minFrameDuration) {
                        dualPhysicalCameraIds.add(physicalCameraId);
                        if (dualPhysicalCameraIds.size() == 2) {
                            return previewSize;
                        }
                   }
                }
            }
        }
        return null;
    }

    /**
     * Validate that physical cameras are not cropping too much.
     *
     * This is to make sure physical processed streams have at least the same field of view as
     * the logical stream, or the maximum field of view of the physical camera, whichever is
     * smaller.
     *
     * Note that the FOV is calculated in the directio of sensor width.
     */
    private void validatePhysicalCamerasFov(TotalCaptureResult totalCaptureResult,
            List<String> physicalCameraIds) {
        Rect cropRegion = totalCaptureResult.get(CaptureResult.SCALER_CROP_REGION);
        Float focalLength = totalCaptureResult.get(CaptureResult.LENS_FOCAL_LENGTH);
        Float zoomRatio = totalCaptureResult.get(CaptureResult.CONTROL_ZOOM_RATIO);
        Rect activeArraySize = mStaticInfo.getActiveArraySizeChecked();
        SizeF sensorSize = mStaticInfo.getValueFromKeyNonNull(
                CameraCharacteristics.SENSOR_INFO_PHYSICAL_SIZE);

        // Assume subject distance >> focal length, and subject distance >> camera baseline.
        double fov = 2 * Math.toDegrees(Math.atan2(sensorSize.getWidth() * cropRegion.width() /
                (2 * zoomRatio * activeArraySize.width()),  focalLength));

        Map<String, CaptureResult> physicalResultsDual =
                    totalCaptureResult.getPhysicalCameraResults();
        for (String physicalId : physicalCameraIds) {
            StaticMetadata physicalStaticInfo = mAllStaticInfo.get(physicalId);
            CaptureResult physicalResult = physicalResultsDual.get(physicalId);
            Rect physicalCropRegion = physicalResult.get(CaptureResult.SCALER_CROP_REGION);
            Float physicalFocalLength = physicalResult.get(CaptureResult.LENS_FOCAL_LENGTH);
            Float physicalZoomRatio = physicalResult.get(CaptureResult.CONTROL_ZOOM_RATIO);
            Rect physicalActiveArraySize = physicalStaticInfo.getActiveArraySizeChecked();
            SizeF physicalSensorSize = mStaticInfo.getValueFromKeyNonNull(
                    CameraCharacteristics.SENSOR_INFO_PHYSICAL_SIZE);

            // Physical result metadata's ZOOM_RATIO is 1.0f.
            assertTrue(""Physical result metadata ZOOM_RATIO should be 1.0f, but is "" +
                    physicalZoomRatio, Math.abs(physicalZoomRatio - 1.0f) < ZOOM_RATIO_THRESHOLD);

            double physicalFov = 2 * Math.toDegrees(Math.atan2(
                    physicalSensorSize.getWidth() * physicalCropRegion.width() /
                    (2 * physicalZoomRatio * physicalActiveArraySize.width()), physicalFocalLength));

            double maxPhysicalFov = 2 * Math.toDegrees(Math.atan2(physicalSensorSize.getWidth() / 2,
                    physicalFocalLength));
            double expectedPhysicalFov = Math.min(maxPhysicalFov, fov);

            if (VERBOSE) {
                Log.v(TAG, ""Logical camera Fov: "" + fov + "", maxPhyiscalFov: "" + maxPhysicalFov +
                        "", physicalFov: "" + physicalFov);
            }
            assertTrue(""Physical stream FOV (Field of view) should be greater or equal to""
                    + "" min(logical stream FOV, max physical stream FOV). Physical FOV: ""
                    + physicalFov + "" vs min("" + fov + "", "" + maxPhysicalFov,
                    physicalFov - expectedPhysicalFov > -FOV_THRESHOLD);
        }
    }

    /**
     * Test physical camera YUV streaming within a particular logical camera.
     *
     * Use 2 YUV streams with PREVIEW or smaller size.
     */
    private void testBasicPhysicalStreamingForCamera(String logicalCameraId,
            List<String> physicalCameraIds, Size previewSize) throws Exception {
        List<OutputConfiguration> outputConfigs = new ArrayList<>();
        List<ImageReader> imageReaders = new ArrayList<>();

        // Add 1 logical YUV stream
        ImageReader logicalTarget = CameraTestUtils.makeImageReader(previewSize,
                ImageFormat.YUV_420_888, MAX_IMAGE_COUNT,
                new ImageDropperListener(), mHandler);
        imageReaders.add(logicalTarget);
        outputConfigs.add(new OutputConfiguration(logicalTarget.getSurface()));

        // Add physical YUV streams
        if (physicalCameraIds.size() != 2) {
            throw new IllegalArgumentException(""phyiscalCameraIds must contain 2 camera ids"");
        }
        List<ImageReader> physicalTargets = new ArrayList<>();
        for (String physicalCameraId : physicalCameraIds) {
            ImageReader physicalTarget = CameraTestUtils.makeImageReader(previewSize,
                    ImageFormat.YUV_420_888, MAX_IMAGE_COUNT,
                    new ImageDropperListener(), mHandler);
            OutputConfiguration config = new OutputConfiguration(physicalTarget.getSurface());
            config.setPhysicalCameraId(physicalCameraId);
            outputConfigs.add(config);
            physicalTargets.add(physicalTarget);
        }

        SessionConfigSupport sessionConfigSupport = isSessionConfigSupported(
                mCamera, mHandler, outputConfigs, /*inputConfig*/ null,
                SessionConfiguration.SESSION_REGULAR, false/*defaultSupport*/);
        assertTrue(""Session configuration query for logical camera failed with error"",
                !sessionConfigSupport.error);
        if (!sessionConfigSupport.callSupported) {
            return;
        }

        mSessionListener = new BlockingSessionCallback();
        mSessionWaiter = mSessionListener.getStateWaiter();
        mSession = configureCameraSessionWithConfig(mCamera, outputConfigs,
                mSessionListener, mHandler);
        if (!sessionConfigSupport.configSupported) {
            mSessionWaiter.waitForState(BlockingSessionCallback.SESSION_CONFIGURE_FAILED,
                    SESSION_CONFIGURE_TIMEOUT_MS);
            return;
        }

        // Stream logical YUV stream and note down the FPS
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        requestBuilder.addTarget(logicalTarget.getSurface());

        SimpleCaptureCallback simpleResultListener =
                new SimpleCaptureCallback();
        StreamConfigurationMap config = mStaticInfo.getCharacteristics().get(
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        final long minFrameDuration = config.getOutputMinFrameDuration(
                ImageFormat.YUV_420_888, previewSize);
        if (minFrameDuration > 0) {
            Range<Integer> targetRange = getSuitableFpsRangeForDuration(logicalCameraId,
                    minFrameDuration);
            requestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, targetRange);
        }
        mSession.setRepeatingRequest(requestBuilder.build(),
                simpleResultListener, mHandler);

        // Converge AE
        waitForAeStable(simpleResultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

        if (mStaticInfo.isAeLockSupported()) {
            // Lock AE if supported.
            requestBuilder.set(CaptureRequest.CONTROL_AE_LOCK, true);
            mSession.setRepeatingRequest(requestBuilder.build(), simpleResultListener,
                    mHandler);
            waitForResultValue(simpleResultListener, CaptureResult.CONTROL_AE_STATE,
                    CaptureResult.CONTROL_AE_STATE_LOCKED, NUM_RESULTS_WAIT_TIMEOUT);
        }

        // Verify results
        CaptureResultTest.validateCaptureResult(mCollector, simpleResultListener,
                mStaticInfo, mAllStaticInfo, null/*requestedPhysicalIds*/,
                requestBuilder, NUM_FRAMES_CHECKED);

        // Collect timestamps for one logical stream only.
        long prevTimestamp = -1;
        long[] logicalTimestamps = new long[NUM_FRAMES_CHECKED];
        for (int i = 0; i < NUM_FRAMES_CHECKED; i++) {
            TotalCaptureResult totalCaptureResult =
                    simpleResultListener.getTotalCaptureResult(
                    CameraTestUtils.CAPTURE_RESULT_TIMEOUT_MS);
            logicalTimestamps[i] = totalCaptureResult.get(CaptureResult.SENSOR_TIMESTAMP);
        }

        double logicalAvgDurationMs = (logicalTimestamps[NUM_FRAMES_CHECKED-1] -
                logicalTimestamps[0])/(NS_PER_MS*(NUM_FRAMES_CHECKED-1));

        // Request one logical stream and one physical stream
        simpleResultListener = new SimpleCaptureCallback();
        requestBuilder.addTarget(physicalTargets.get(1).getSurface());
        mSession.setRepeatingRequest(requestBuilder.build(), simpleResultListener,
                mHandler);

        // Verify results for physical streams request.
        CaptureResultTest.validateCaptureResult(mCollector, simpleResultListener,
                mStaticInfo, mAllStaticInfo, physicalCameraIds.subList(1, 2), requestBuilder,
                NUM_FRAMES_CHECKED);


        // Start requesting on both logical and physical streams
        SimpleCaptureCallback simpleResultListenerDual =
                new SimpleCaptureCallback();
        for (ImageReader physicalTarget : physicalTargets) {
            requestBuilder.addTarget(physicalTarget.getSurface());
        }
        mSession.setRepeatingRequest(requestBuilder.build(), simpleResultListenerDual,
                mHandler);

        // Verify results for physical streams request.
        CaptureResultTest.validateCaptureResult(mCollector, simpleResultListenerDual,
                mStaticInfo, mAllStaticInfo, physicalCameraIds, requestBuilder,
                NUM_FRAMES_CHECKED);

        // Acquire the timestamps of the physical camera.
        long[] logicalTimestamps2 = new long[NUM_FRAMES_CHECKED];
        long [][] physicalTimestamps = new long[physicalTargets.size()][];
        for (int i = 0; i < physicalTargets.size(); i++) {
            physicalTimestamps[i] = new long[NUM_FRAMES_CHECKED];
        }
        for (int i = 0; i < NUM_FRAMES_CHECKED; i++) {
            TotalCaptureResult totalCaptureResultDual =
                    simpleResultListenerDual.getTotalCaptureResult(
                    CameraTestUtils.CAPTURE_RESULT_TIMEOUT_MS);
            logicalTimestamps2[i] = totalCaptureResultDual.get(CaptureResult.SENSOR_TIMESTAMP);

            int index = 0;
            Map<String, CaptureResult> physicalResultsDual =
                    totalCaptureResultDual.getPhysicalCameraResults();
            for (String physicalId : physicalCameraIds) {
                assertTrue(""Physical capture result camera ID must match the right camera"",
                        physicalResultsDual.get(physicalId).getCameraId().equals(physicalId));
                if (physicalResultsDual.containsKey(physicalId)) {
                    physicalTimestamps[index][i] = physicalResultsDual.get(physicalId).get(
                        CaptureResult.SENSOR_TIMESTAMP);
                } else {
                    physicalTimestamps[index][i] = -1;
                }
                index++;
            }
        }

        // Check both logical and physical streams' crop region, and make sure their FOVs
        // are similar.
        TotalCaptureResult totalCaptureResult =
                simpleResultListenerDual.getTotalCaptureResult(
                CameraTestUtils.CAPTURE_RESULT_TIMEOUT_MS);
        validatePhysicalCamerasFov(totalCaptureResult, physicalCameraIds);

        // Check timestamp monolithity for individual camera and across cameras
        for (int i = 0; i < NUM_FRAMES_CHECKED-1; i++) {
            assertTrue(""Logical camera timestamp must monolithically increase"",
                    logicalTimestamps2[i] < logicalTimestamps2[i+1]);
        }
        for (int i = 0; i < physicalCameraIds.size(); i++) {
            for (int j = 0 ; j < NUM_FRAMES_CHECKED-1; j++) {
                if (physicalTimestamps[i][j] != -1 && physicalTimestamps[i][j+1] != -1) {
                    assertTrue(""Physical camera timestamp must monolithically increase"",
                            physicalTimestamps[i][j] < physicalTimestamps[i][j+1]);
                }
                if (j > 0 && physicalTimestamps[i][j] != -1) {
                    assertTrue(""Physical camera's timestamp N must be greater than logical "" +
                            ""camera's timestamp N-1"",
                            physicalTimestamps[i][j] > logicalTimestamps[j-1]);
                }
                if (physicalTimestamps[i][j] != -1) {
                    assertTrue(""Physical camera's timestamp N must be less than logical camera's "" +
                            ""timestamp N+1"", physicalTimestamps[i][j] > logicalTimestamps[j+1]);
                }
            }
        }

        double logicalAvgDurationMs2 = (logicalTimestamps2[NUM_FRAMES_CHECKED-1] -
                logicalTimestamps2[0])/(NS_PER_MS*(NUM_FRAMES_CHECKED-1));
        // Check CALIBRATED synchronization between physical cameras
        Integer syncType = mStaticInfo.getCharacteristics().get(
                CameraCharacteristics.LOGICAL_MULTI_CAMERA_SENSOR_SYNC_TYPE);
        double fpsRatio = (logicalAvgDurationMs2 - logicalAvgDurationMs)/logicalAvgDurationMs;
        if (syncType == CameraCharacteristics.LOGICAL_MULTI_CAMERA_SENSOR_SYNC_TYPE_CALIBRATED) {
            // Check framerate doesn't slow down with physical streams
            mCollector.expectTrue(
                    ""The average frame duration with concurrent physical streams is"" +
                    logicalAvgDurationMs2 + "" ms vs "" + logicalAvgDurationMs +
                    "" ms for logical streams only"", fpsRatio <= FRAME_DURATION_THRESHOLD);

            long maxTimestampDelta = 0;
            for (int i = 0; i < NUM_FRAMES_CHECKED; i++) {
                long delta = Math.abs(physicalTimestamps[0][i] - physicalTimestamps[1][i]);
                if (delta > maxTimestampDelta) {
                    maxTimestampDelta = delta;
                }
            }

            Log.i(TAG, ""Maximum difference between physical camera timestamps: ""
                    + maxTimestampDelta);

            // The maximum timestamp difference should not be larger than the threshold.
            mCollector.expectTrue(
                    ""The maximum timestamp deltas between the physical cameras ""
                    + maxTimestampDelta + "" is larger than "" + MAX_TIMESTAMP_DIFFERENCE_THRESHOLD,
                    maxTimestampDelta <= MAX_TIMESTAMP_DIFFERENCE_THRESHOLD);
        } else {
            // Do not enforce fps check for APPROXIMATE synced device.
            if (fpsRatio > FRAME_DURATION_THRESHOLD) {
                Log.w(TAG, ""The average frame duration with concurrent physical streams is"" +
                        logicalAvgDurationMs2 + "" ms vs "" + logicalAvgDurationMs +
                        "" ms for logical streams only"");
            }
        }

        if (VERBOSE) {
            while (simpleResultListenerDual.hasMoreFailures()) {
                ArrayList<CaptureFailure> failures =
                    simpleResultListenerDual.getCaptureFailures(/*maxNumFailures*/ 1);
                for (CaptureFailure failure : failures) {
                    String physicalCameraId = failure.getPhysicalCameraId();
                    if (physicalCameraId != null) {
                        Log.v(TAG, ""Capture result failure for physical camera id: "" +
                                physicalCameraId);
                    }
                }
            }
        }

        // Stop preview
        if (mSession != null) {
            mSession.close();
        }
    }

    /**
     * The CDD defines a handheld device as one that has a battery and a screen size between
     * 2.5 and 8 inches.
     */
    private boolean isHandheldDevice() throws Exception {
        double screenInches = getScreenSizeInInches();
        return deviceHasBattery() && screenInches >= 2.5 && screenInches <= 8.0;
    }

    private boolean deviceHasBattery() {
        final Intent batteryInfo = mContext.registerReceiver(null,
                new IntentFilter(Intent.ACTION_BATTERY_CHANGED));
        return batteryInfo != null && batteryInfo.getBooleanExtra(BatteryManager.EXTRA_PRESENT, true);
    }

    private double getScreenSizeInInches() {
        DisplayMetrics dm = new DisplayMetrics();
        mWindowManager.getDefaultDisplay().getMetrics(dm);
        double widthInInchesSquared = Math.pow(dm.widthPixels/dm.xdpi,2);
        double heightInInchesSquared = Math.pow(dm.heightPixels/dm.ydpi,2);
        return Math.sqrt(widthInInchesSquared + heightInInchesSquared);
    }
}"	""	""	"cdd"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.StillCaptureTest"	"testAePrecaptureTriggerCancelJpegCapture"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/StillCaptureTest.java"	""	"public void testAePrecaptureTriggerCancelJpegCapture() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.i(TAG, ""Testing AE precapture cancel for jpeg capture for Camera "" + id);

                StaticMetadata staticInfo = mAllStaticInfo.get(id);
                // Legacy device doesn't support AE precapture trigger
                if (staticInfo.isHardwareLevelLegacy()) {
                    Log.i(TAG, ""Skipping AE precapture trigger cancel test on legacy devices"");
                    continue;
                }
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" does not support color outputs, skipping"");
                    continue;
                }
                openDevice(id);
                takePictureTestByCamera(/*aeRegions*/null, /*awbRegions*/null, /*afRegions*/null,
                        /*addAeTriggerCancel*/true, /*allocateBitmap*/false,
                        /*previewRequest*/null, /*stillRequest*/null);
            } finally {
                closeDevice();
                closeImageReader();
            }
        }
    }

    /**
     * Test allocate some bitmaps while taking picture.
     * <p>
     * Per android CDD (5.0 and newer), android devices should support allocation of at least 3
     * bitmaps equal to the size of the images produced by the largest resolution camera sensor on
     * the devices.
     * </p>
     */"	""	""	"cdd resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.StillCaptureTest"	"testFocalLengths"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/StillCaptureTest.java"	""	"public void testFocalLengths() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                StaticMetadata staticInfo = mAllStaticInfo.get(id);
                if (staticInfo.isHardwareLevelLegacy()) {
                    Log.i(TAG, ""Camera "" + id + "" is legacy, skipping"");
                    continue;
                }
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" does not support color outputs, skipping"");
                    continue;
                }
                if (staticInfo.isExternalCamera()) {
                    Log.i(TAG, ""Camera "" + id + "" is external, skipping"");
                    continue;
                }
                openDevice(id);
                focalLengthTestByCamera();
            } finally {
                closeDevice();
                closeImageReader();
            }
        }
    }

    private void focalLengthTestByCamera() throws Exception {
        float[] focalLengths = mStaticInfo.getAvailableFocalLengthsChecked();
        int numStillCaptures = focalLengths.length;

        Size maxStillSz = mOrderedStillSizes.get(0);
        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
        CaptureRequest.Builder previewRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder stillRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        Size thumbnailSize = new Size(0, 0);
        Location sTestLocation = new Location(LocationManager.GPS_PROVIDER);
        sTestLocation.setTime(1199145600000L);
        sTestLocation.setLatitude(37.736071);
        sTestLocation.setLongitude(-122.441983);
        sTestLocation.setAltitude(21.0);
        ExifTestData exifTestData = new ExifTestData(
                /* gpsLocation */ sTestLocation,
                /* orientation */ 0,
                /* jpgQuality */ (byte) 80,
                /* thumbnailQuality */ (byte) 75);
        setJpegKeys(stillRequest, exifTestData, thumbnailSize, mCollector);
        CaptureResult result;

        // Set the max number of images to number of focal lengths supported
        prepareStillCaptureAndStartPreview(previewRequest, stillRequest, maxPreviewSz,
                maxStillSz, resultListener, focalLengths.length, imageListener, false /*isHeic*/);

        for(float focalLength : focalLengths) {

            previewRequest.set(CaptureRequest.LENS_FOCAL_LENGTH, focalLength);
            mSession.setRepeatingRequest(previewRequest.build(), resultListener, mHandler);
            waitForSettingsApplied(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
            waitForResultValue(resultListener, CaptureResult.LENS_STATE,
                    CaptureResult.LENS_STATE_STATIONARY, NUM_RESULTS_WAIT_TIMEOUT);
            result = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            mCollector.expectEquals(""Focal length in preview result and request should be the same"",
                    previewRequest.get(CaptureRequest.LENS_FOCAL_LENGTH),
                    result.get(CaptureResult.LENS_FOCAL_LENGTH));

            stillRequest.set(CaptureRequest.LENS_FOCAL_LENGTH, focalLength);
            CaptureRequest request = stillRequest.build();
            resultListener = new SimpleCaptureCallback();
            mSession.capture(request, resultListener, mHandler);
            result = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            mCollector.expectEquals(
                    ""Focal length in still capture result and request should be the same"",
                    stillRequest.get(CaptureRequest.LENS_FOCAL_LENGTH),
                    result.get(CaptureResult.LENS_FOCAL_LENGTH));

            Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);

            validateJpegCapture(image, maxStillSz);
            verifyJpegKeys(image, result, maxStillSz, thumbnailSize, exifTestData,
                    mStaticInfo, mCollector, mDebugFileNameBase, ImageFormat.JPEG);
        }
    }


    /**
     * Start preview,take a picture and test preview is still running after snapshot
     */
    private void previewPersistenceTestByCamera() throws Exception {
        Size maxStillSz = mOrderedStillSizes.get(0);
        Size maxPreviewSz = mOrderedPreviewSizes.get(0);

        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleCaptureCallback stillResultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
        CaptureRequest.Builder previewRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder stillRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        prepareStillCaptureAndStartPreview(previewRequest, stillRequest, maxPreviewSz,
                maxStillSz, resultListener, imageListener, false /*isHeic*/);

        // make sure preview is actually running
        waitForNumResults(resultListener, NUM_FRAMES_WAITED);

        // take a picture
        CaptureRequest request = stillRequest.build();
        mSession.capture(request, stillResultListener, mHandler);
        stillResultListener.getCaptureResultForRequest(request,
                WAIT_FOR_RESULT_TIMEOUT_MS);

        // validate image
        Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
        validateJpegCapture(image, maxStillSz);

        // make sure preview is still running after still capture
        waitForNumResults(resultListener, NUM_FRAMES_WAITED);

        stopPreview();

        // Free image resources
        image.close();
        closeImageReader();
        return;
    }

    /**
     * Take a picture for a given set of 3A regions for a particular camera.
     * <p>
     * Before take a still capture, it triggers an auto focus and lock it first,
     * then wait for AWB to converge and lock it, then trigger a precapture
     * metering sequence and wait for AE converged. After capture is received, the
     * capture result and image are validated.
     * </p>
     *
     * @param aeRegions AE regions for this capture
     * @param awbRegions AWB regions for this capture
     * @param afRegions AF regions for this capture
     */
    private void takePictureTestByCamera(
            MeteringRectangle[] aeRegions, MeteringRectangle[] awbRegions,
            MeteringRectangle[] afRegions) throws Exception {
        takePictureTestByCamera(aeRegions, awbRegions, afRegions,
                /*addAeTriggerCancel*/false, /*allocateBitmap*/false,
                /*previewRequest*/null, /*stillRequest*/null);
    }

    /**
     * Take a picture for a given set of 3A regions for a particular camera.
     * <p>
     * Before take a still capture, it triggers an auto focus and lock it first,
     * then wait for AWB to converge and lock it, then trigger a precapture
     * metering sequence and wait for AE converged. After capture is received, the
     * capture result and image are validated. If {@code addAeTriggerCancel} is true,
     * a precapture trigger cancel will be inserted between two adjacent triggers, which
     * should effective cancel the first trigger.
     * </p>
     *
     * @param aeRegions AE regions for this capture
     * @param awbRegions AWB regions for this capture
     * @param afRegions AF regions for this capture
     * @param addAeTriggerCancel If a AE precapture trigger cancel is sent after the trigger.
     * @param allocateBitmap If a set of bitmaps are allocated during the test for memory test.
     * @param previewRequest The preview request builder to use, or null to use the default
     * @param stillRequest The still capture request to use, or null to use the default
     */
    private void takePictureTestByCamera(
            MeteringRectangle[] aeRegions, MeteringRectangle[] awbRegions,
            MeteringRectangle[] afRegions, boolean addAeTriggerCancel, boolean allocateBitmap,
            CaptureRequest.Builder previewRequest, CaptureRequest.Builder stillRequest)
                    throws Exception {

        boolean hasFocuser = mStaticInfo.hasFocuser();

        Size maxStillSz = mOrderedStillSizes.get(0);
        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        CaptureResult result;
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
        if (previewRequest == null) {
            previewRequest = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        }
        if (stillRequest == null) {
            stillRequest = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        }
        prepareStillCaptureAndStartPreview(previewRequest, stillRequest, maxPreviewSz,
                maxStillSz, resultListener, imageListener, false /*isHeic*/);

        // Set AE mode to ON_AUTO_FLASH if flash is available.
        if (mStaticInfo.hasFlash()) {
            previewRequest.set(CaptureRequest.CONTROL_AE_MODE,
                    CaptureRequest.CONTROL_AE_MODE_ON_AUTO_FLASH);
            stillRequest.set(CaptureRequest.CONTROL_AE_MODE,
                    CaptureRequest.CONTROL_AE_MODE_ON_AUTO_FLASH);
        }

        Camera2Focuser focuser = null;
        /**
         * Step 1: trigger an auto focus run, and wait for AF locked.
         */
        boolean canSetAfRegion = hasFocuser && (afRegions != null) &&
                isRegionsSupportedFor3A(MAX_REGIONS_AF_INDEX);
        if (hasFocuser) {
            SimpleAutoFocusListener afListener = new SimpleAutoFocusListener();
            focuser = new Camera2Focuser(mCamera, mSession, mPreviewSurface, afListener,
                    mStaticInfo.getCharacteristics(), mHandler);
            if (canSetAfRegion) {
                previewRequest.set(CaptureRequest.CONTROL_AF_REGIONS, afRegions);
                stillRequest.set(CaptureRequest.CONTROL_AF_REGIONS, afRegions);
            }
            focuser.startAutoFocus(afRegions);
            afListener.waitForAutoFocusDone(WAIT_FOR_FOCUS_DONE_TIMEOUT_MS);
        }

        /**
         * Have to get the current AF mode to be used for other 3A repeating
         * request, otherwise, the new AF mode in AE/AWB request could be
         * different with existing repeating requests being sent by focuser,
         * then it could make AF unlocked too early. Beside that, for still
         * capture, AF mode must not be different with the one in current
         * repeating request, otherwise, the still capture itself would trigger
         * an AF mode change, and the AF lock would be lost for this capture.
         */
        int currentAfMode = CaptureRequest.CONTROL_AF_MODE_OFF;
        if (hasFocuser) {
            currentAfMode = focuser.getCurrentAfMode();
        }
        previewRequest.set(CaptureRequest.CONTROL_AF_MODE, currentAfMode);
        stillRequest.set(CaptureRequest.CONTROL_AF_MODE, currentAfMode);

        /**
         * Step 2: AF is already locked, wait for AWB converged, then lock it.
         */
        resultListener = new SimpleCaptureCallback();
        boolean canSetAwbRegion =
                (awbRegions != null) && isRegionsSupportedFor3A(MAX_REGIONS_AWB_INDEX);
        if (canSetAwbRegion) {
            previewRequest.set(CaptureRequest.CONTROL_AWB_REGIONS, awbRegions);
            stillRequest.set(CaptureRequest.CONTROL_AWB_REGIONS, awbRegions);
        }
        mSession.setRepeatingRequest(previewRequest.build(), resultListener, mHandler);
        if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
            waitForResultValue(resultListener, CaptureResult.CONTROL_AWB_STATE,
                    CaptureResult.CONTROL_AWB_STATE_CONVERGED, NUM_RESULTS_WAIT_TIMEOUT);
        } else {
            // LEGACY Devices don't have the AWB_STATE reported in results, so just wait
            waitForSettingsApplied(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
        }
        boolean canSetAwbLock = mStaticInfo.isAwbLockSupported();
        if (canSetAwbLock) {
            previewRequest.set(CaptureRequest.CONTROL_AWB_LOCK, true);
        }
        mSession.setRepeatingRequest(previewRequest.build(), resultListener, mHandler);
        // Validate the next result immediately for region and mode.
        result = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
        mCollector.expectEquals(""AWB mode in result and request should be same"",
                previewRequest.get(CaptureRequest.CONTROL_AWB_MODE),
                result.get(CaptureResult.CONTROL_AWB_MODE));
        if (canSetAwbRegion) {
            MeteringRectangle[] resultAwbRegions =
                    getValueNotNull(result, CaptureResult.CONTROL_AWB_REGIONS);
            mCollector.expectEquals(""AWB regions in result and request should be same"",
                    awbRegions, resultAwbRegions);
        }

        /**
         * Step 3: trigger an AE precapture metering sequence and wait for AE converged.
         */
        resultListener = new SimpleCaptureCallback();
        boolean canSetAeRegion =
                (aeRegions != null) && isRegionsSupportedFor3A(MAX_REGIONS_AE_INDEX);
        if (canSetAeRegion) {
            previewRequest.set(CaptureRequest.CONTROL_AE_REGIONS, aeRegions);
            stillRequest.set(CaptureRequest.CONTROL_AE_REGIONS, aeRegions);
        }
        mSession.setRepeatingRequest(previewRequest.build(), resultListener, mHandler);
        previewRequest.set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,
                CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_START);
        mSession.capture(previewRequest.build(), resultListener, mHandler);
        if (addAeTriggerCancel) {
            // Cancel the current precapture trigger, then send another trigger.
            // The camera device should behave as if the first trigger is not sent.
            // Wait one request to make the trigger start doing something before cancel.
            waitForNumResults(resultListener, /*numResultsWait*/ 1);
            previewRequest.set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,
                    CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_CANCEL);
            mSession.capture(previewRequest.build(), resultListener, mHandler);
            waitForResultValue(resultListener, CaptureResult.CONTROL_AE_PRECAPTURE_TRIGGER,
                    CaptureResult.CONTROL_AE_PRECAPTURE_TRIGGER_CANCEL,
                    NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
            // Issue another trigger
            previewRequest.set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,
                    CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_START);
            mSession.capture(previewRequest.build(), resultListener, mHandler);
        }
        waitForAeStable(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

        // Validate the next result immediately for region and mode.
        result = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
        mCollector.expectEquals(""AE mode in result and request should be same"",
                previewRequest.get(CaptureRequest.CONTROL_AE_MODE),
                result.get(CaptureResult.CONTROL_AE_MODE));
        if (canSetAeRegion) {
            MeteringRectangle[] resultAeRegions =
                    getValueNotNull(result, CaptureResult.CONTROL_AE_REGIONS);

            mCollector.expectMeteringRegionsAreSimilar(
                    ""AE regions in result and request should be similar"",
                    aeRegions,
                    resultAeRegions,
                    METERING_REGION_ERROR_PERCENT_DELTA);
        }

        /**
         * Step 4: take a picture when all 3A are in good state.
         */
        resultListener = new SimpleCaptureCallback();
        CaptureRequest request = stillRequest.build();
        mSession.capture(request, resultListener, mHandler);
        // Validate the next result immediately for region and mode.
        result = resultListener.getCaptureResultForRequest(request, WAIT_FOR_RESULT_TIMEOUT_MS);
        mCollector.expectEquals(""AF mode in result and request should be same"",
                stillRequest.get(CaptureRequest.CONTROL_AF_MODE),
                result.get(CaptureResult.CONTROL_AF_MODE));
        if (canSetAfRegion) {
            MeteringRectangle[] resultAfRegions =
                    getValueNotNull(result, CaptureResult.CONTROL_AF_REGIONS);
            mCollector.expectMeteringRegionsAreSimilar(
                    ""AF regions in result and request should be similar"",
                    afRegions,
                    resultAfRegions,
                    METERING_REGION_ERROR_PERCENT_DELTA);
        }

        if (hasFocuser) {
            // Unlock auto focus.
            focuser.cancelAutoFocus();
        }

        // validate image
        Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
        validateJpegCapture(image, maxStillSz);
        // Test if the system can allocate 3 bitmap successfully, per android CDD camera memory
        // requirements added by CDD 5.0
        if (allocateBitmap) {
            Bitmap bm[] = new Bitmap[MAX_ALLOCATED_BITMAPS];
            for (int i = 0; i < MAX_ALLOCATED_BITMAPS; i++) {
                bm[i] = Bitmap.createBitmap(
                        maxStillSz.getWidth(), maxStillSz.getHeight(), Config.ARGB_8888);
                assertNotNull(""Created bitmap #"" + i + "" shouldn't be null"", bm[i]);
            }
        }

        // Free image resources
        image.close();

        stopPreview();
    }

    /**
     * Test touch region for focus by camera.
     */
    private void touchForFocusTestByCamera() throws Exception {
        SimpleCaptureCallback listener = new SimpleCaptureCallback();
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        startPreview(requestBuilder, maxPreviewSz, listener);

        SimpleAutoFocusListener afListener = new SimpleAutoFocusListener();
        Camera2Focuser focuser = new Camera2Focuser(mCamera, mSession, mPreviewSurface, afListener,
                mStaticInfo.getCharacteristics(), mHandler);
        ArrayList<MeteringRectangle[]> testAfRegions = get3ARegionTestCasesForCamera();

        for (MeteringRectangle[] afRegions : testAfRegions) {
            focuser.touchForAutoFocus(afRegions);
            afListener.waitForAutoFocusDone(WAIT_FOR_FOCUS_DONE_TIMEOUT_MS);
            focuser.cancelAutoFocus();
        }
    }

    private void previewStillCombinationTestByCamera() throws Exception {
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();

        Size QCIF = new Size(176, 144);
        Size FULL_HD = new Size(1920, 1080);
        for (Size stillSz : mOrderedStillSizes)
            for (Size previewSz : mOrderedPreviewSizes) {
                if (VERBOSE) {
                    Log.v(TAG, ""Testing JPEG capture size "" + stillSz.toString()
                            + "" with preview size "" + previewSz.toString() + "" for camera ""
                            + mCamera.getId());
                }

                // Skip testing QCIF + >FullHD combinations
                if (stillSz.equals(QCIF) &&
                        ((previewSz.getWidth() > FULL_HD.getWidth()) ||
                         (previewSz.getHeight() > FULL_HD.getHeight()))) {
                    continue;
                }

                if (previewSz.equals(QCIF) &&
                        ((stillSz.getWidth() > FULL_HD.getWidth()) ||
                         (stillSz.getHeight() > FULL_HD.getHeight()))) {
                    continue;
                }

                CaptureRequest.Builder previewRequest =
                        mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
                CaptureRequest.Builder stillRequest =
                        mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
                prepareStillCaptureAndStartPreview(previewRequest, stillRequest, previewSz,
                        stillSz, resultListener, imageListener, false /*isHeic*/);
                mSession.capture(stillRequest.build(), resultListener, mHandler);
                Image image = imageListener.getImage((mStaticInfo.isHardwareLevelLegacy()) ?
                        RELAXED_CAPTURE_IMAGE_TIMEOUT_MS : CAPTURE_IMAGE_TIMEOUT_MS);
                validateJpegCapture(image, stillSz);

                // Free image resources
                image.close();

                // stopPreview must be called here to make sure next time a preview stream
                // is created with new size.
                stopPreview();
                // Drain the results after each combination. Depending on the device the results
                // can be relatively big and could accumulate fairly quickly after many iterations.
                resultListener.drain();
            }
    }

    /**
     * Basic raw capture test for each camera.
     */
    private void rawCaptureTestByCamera(CaptureRequest.Builder stillRequest) throws Exception {
        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        Size size = mStaticInfo.getRawDimensChecked();

        // Prepare raw capture and start preview.
        CaptureRequest.Builder previewBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder rawBuilder = (stillRequest != null) ? stillRequest :
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
        prepareRawCaptureAndStartPreview(previewBuilder, rawBuilder, maxPreviewSz, size,
                resultListener, imageListener);

        if (VERBOSE) {
            Log.v(TAG, ""Testing Raw capture with size "" + size.toString()
                    + "", preview size "" + maxPreviewSz);
        }

        CaptureRequest rawRequest = rawBuilder.build();
        mSession.capture(rawRequest, resultListener, mHandler);

        Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
        validateRaw16Image(image, size);
        if (DEBUG) {
            byte[] rawBuffer = getDataFromImage(image);
            String rawFileName = mDebugFileNameBase + ""/test"" + ""_"" + size.toString() + ""_cam"" +
                    mCamera.getId() + "".raw16"";
            Log.d(TAG, ""Dump raw file into "" + rawFileName);
            dumpFile(rawFileName, rawBuffer);
        }

        // Free image resources
        image.close();

        stopPreview();
    }

    private void fullRawCaptureTestByCamera(CaptureRequest.Builder stillRequest) throws Exception {
        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        Size maxStillSz = mOrderedStillSizes.get(0);

        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener jpegListener = new SimpleImageReaderListener();
        SimpleImageReaderListener rawListener = new SimpleImageReaderListener();

        Size size = mStaticInfo.getRawDimensChecked();

        if (VERBOSE) {
            Log.v(TAG, ""Testing multi capture with size "" + size.toString()
                    + "", preview size "" + maxPreviewSz);
        }

        // Prepare raw capture and start preview.
        CaptureRequest.Builder previewBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder multiBuilder = (stillRequest != null) ? stillRequest :
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);

        ImageReader rawReader = null;
        ImageReader jpegReader = null;

        try {
            // Create ImageReaders.
            rawReader = makeImageReader(size,
                    ImageFormat.RAW_SENSOR, MAX_READER_IMAGES, rawListener, mHandler);
            jpegReader = makeImageReader(maxStillSz,
                    ImageFormat.JPEG, MAX_READER_IMAGES, jpegListener, mHandler);
            updatePreviewSurface(maxPreviewSz);

            // Configure output streams with preview and jpeg streams.
            List<Surface> outputSurfaces = new ArrayList<Surface>();
            outputSurfaces.add(rawReader.getSurface());
            outputSurfaces.add(jpegReader.getSurface());
            outputSurfaces.add(mPreviewSurface);
            mSessionListener = new BlockingSessionCallback();
            mSession = configureCameraSession(mCamera, outputSurfaces,
                    mSessionListener, mHandler);

            // Configure the requests.
            previewBuilder.addTarget(mPreviewSurface);
            multiBuilder.addTarget(mPreviewSurface);
            multiBuilder.addTarget(rawReader.getSurface());
            multiBuilder.addTarget(jpegReader.getSurface());

            // Start preview.
            mSession.setRepeatingRequest(previewBuilder.build(), null, mHandler);

            // Poor man's 3A, wait 2 seconds for AE/AF (if any) to settle.
            // TODO: Do proper 3A trigger and lock (see testTakePictureTest).
            Thread.sleep(3000);

            multiBuilder.set(CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE,
                    CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE_ON);
            CaptureRequest multiRequest = multiBuilder.build();

            mSession.capture(multiRequest, resultListener, mHandler);

            CaptureResult result = resultListener.getCaptureResultForRequest(multiRequest,
                    NUM_RESULTS_WAIT_TIMEOUT);
            Image jpegImage = jpegListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
            basicValidateBlobImage(jpegImage, maxStillSz, ImageFormat.JPEG);
            Image rawImage = rawListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
            validateRaw16Image(rawImage, size);
            verifyRawCaptureResult(multiRequest, result);


            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            try (DngCreator dngCreator = new DngCreator(mStaticInfo.getCharacteristics(), result)) {
                dngCreator.writeImage(outputStream, rawImage);
            }

            if (DEBUG) {
                byte[] rawBuffer = outputStream.toByteArray();
                String rawFileName = mDebugFileNameBase + ""/raw16_"" + TAG + size.toString() +
                        ""_cam_"" + mCamera.getId() + "".dng"";
                Log.d(TAG, ""Dump raw file into "" + rawFileName);
                dumpFile(rawFileName, rawBuffer);

                byte[] jpegBuffer = getDataFromImage(jpegImage);
                String jpegFileName = mDebugFileNameBase + ""/jpeg_"" + TAG + size.toString() +
                        ""_cam_"" + mCamera.getId() + "".jpg"";
                Log.d(TAG, ""Dump jpeg file into "" + rawFileName);
                dumpFile(jpegFileName, jpegBuffer);
            }

            stopPreview();
        } finally {
            CameraTestUtils.closeImageReader(rawReader);
            CameraTestUtils.closeImageReader(jpegReader);
            rawReader = null;
            jpegReader = null;
        }
    }

    /**
     * Validate that raw {@link CaptureResult}.
     *
     * @param rawRequest a {@link CaptureRequest} use to capture a RAW16 image.
     * @param rawResult the {@link CaptureResult} corresponding to the given request.
     */
    private void verifyRawCaptureResult(CaptureRequest rawRequest, CaptureResult rawResult) {
        assertNotNull(rawRequest);
        assertNotNull(rawResult);

        if (!mStaticInfo.isMonochromeCamera()) {
            Rational[] empty = new Rational[] { Rational.ZERO, Rational.ZERO, Rational.ZERO};
            Rational[] neutralColorPoint = mCollector.expectKeyValueNotNull(""NeutralColorPoint"",
                    rawResult, CaptureResult.SENSOR_NEUTRAL_COLOR_POINT);
            if (neutralColorPoint != null) {
                mCollector.expectEquals(""NeutralColorPoint length"", empty.length,
                        neutralColorPoint.length);
                mCollector.expectNotEquals(""NeutralColorPoint cannot be all zeroes, "", empty,
                        neutralColorPoint);
                mCollector.expectValuesGreaterOrEqual(""NeutralColorPoint"", neutralColorPoint,
                        Rational.ZERO);
            }

            mCollector.expectKeyValueGreaterOrEqual(rawResult,
                    CaptureResult.SENSOR_GREEN_SPLIT, 0.0f);
        }

        Pair<Double, Double>[] noiseProfile = mCollector.expectKeyValueNotNull(""NoiseProfile"",
                rawResult, CaptureResult.SENSOR_NOISE_PROFILE);
        if (noiseProfile != null) {
            int cfa = mStaticInfo.getCFAChecked();
            int numCfaChannels = 0;
            switch (cfa) {
                case CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_RGGB:
                case CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_GRBG:
                case CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_GBRG:
                case CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_BGGR:
                    numCfaChannels = 4;
                    break;
                case CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_MONO:
                case CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_NIR:
                    numCfaChannels = 1;
                    break;
                default:
                    Assert.fail(""Invalid color filter arrangement "" + cfa);
                    break;
            }
            mCollector.expectEquals(""NoiseProfile length"", noiseProfile.length, numCfaChannels);
            for (Pair<Double, Double> p : noiseProfile) {
                mCollector.expectTrue(""NoiseProfile coefficients "" + p +
                        "" must have: S > 0, O >= 0"", p.first > 0 && p.second >= 0);
            }
        }

        Integer hotPixelMode = mCollector.expectKeyValueNotNull(""HotPixelMode"", rawResult,
                CaptureResult.HOT_PIXEL_MODE);
        Boolean hotPixelMapMode = mCollector.expectKeyValueNotNull(""HotPixelMapMode"", rawResult,
                CaptureResult.STATISTICS_HOT_PIXEL_MAP_MODE);
        Point[] hotPixelMap = rawResult.get(CaptureResult.STATISTICS_HOT_PIXEL_MAP);

        Size pixelArraySize = mStaticInfo.getPixelArraySizeChecked();
        boolean[] availableHotPixelMapModes = mStaticInfo.getValueFromKeyNonNull(
                        CameraCharacteristics.STATISTICS_INFO_AVAILABLE_HOT_PIXEL_MAP_MODES);

        if (hotPixelMode != null) {
            Integer requestMode = mCollector.expectKeyValueNotNull(rawRequest,
                    CaptureRequest.HOT_PIXEL_MODE);
            if (requestMode != null) {
                mCollector.expectKeyValueEquals(rawResult, CaptureResult.HOT_PIXEL_MODE,
                        requestMode);
            }
        }

        if (hotPixelMapMode != null) {
            Boolean requestMapMode = mCollector.expectKeyValueNotNull(rawRequest,
                    CaptureRequest.STATISTICS_HOT_PIXEL_MAP_MODE);
            if (requestMapMode != null) {
                mCollector.expectKeyValueEquals(rawResult,
                        CaptureResult.STATISTICS_HOT_PIXEL_MAP_MODE, requestMapMode);
            }

            if (!hotPixelMapMode) {
                mCollector.expectTrue(""HotPixelMap must be empty"", hotPixelMap == null ||
                        hotPixelMap.length == 0);
            } else {
                mCollector.expectTrue(""HotPixelMap must not be empty"", hotPixelMap != null);
                mCollector.expectNotNull(""AvailableHotPixelMapModes must not be null"",
                        availableHotPixelMapModes);
                if (availableHotPixelMapModes != null) {
                    mCollector.expectContains(""HotPixelMapMode"", availableHotPixelMapModes, true);
                }

                int height = pixelArraySize.getHeight();
                int width = pixelArraySize.getWidth();
                for (Point p : hotPixelMap) {
                    mCollector.expectTrue(""Hotpixel "" + p + "" must be in pixelArray "" +
                            pixelArraySize, p.x >= 0 && p.x < width && p.y >= 0 && p.y < height);
                }
            }
        }
        // TODO: profileHueSatMap, and profileToneCurve aren't supported yet.

    }

    /**
     * Issue a still capture and validate the exif information.
     * <p>
     * TODO: Differentiate full and limited device, some of the checks rely on
     * per frame control and synchronization, most of them don't.
     * </p>
     */
    private void stillExifTestByCamera(int format, Size stillSize) throws Exception {
        assertTrue(format == ImageFormat.JPEG || format == ImageFormat.HEIC);
        boolean isHeic = (format == ImageFormat.HEIC);

        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        if (VERBOSE) {
            Log.v(TAG, ""Testing exif with size "" + stillSize.toString()
                    + "", preview size "" + maxPreviewSz);
        }

        // prepare capture and start preview.
        CaptureRequest.Builder previewBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder stillBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
        prepareStillCaptureAndStartPreview(previewBuilder, stillBuilder, maxPreviewSz, stillSize,
                resultListener, imageListener, isHeic);

        // Set the jpeg keys, then issue a capture
        Size[] thumbnailSizes = mStaticInfo.getAvailableThumbnailSizesChecked();
        Size maxThumbnailSize = thumbnailSizes[thumbnailSizes.length - 1];
        Size[] testThumbnailSizes = new Size[EXIF_TEST_DATA.length];
        Arrays.fill(testThumbnailSizes, maxThumbnailSize);
        // Make sure thumbnail size (0, 0) is covered.
        testThumbnailSizes[0] = new Size(0, 0);

        for (int i = 0; i < EXIF_TEST_DATA.length; i++) {
            setJpegKeys(stillBuilder, EXIF_TEST_DATA[i], testThumbnailSizes[i], mCollector);

            // Capture a jpeg/heic image.
            CaptureRequest request = stillBuilder.build();
            mSession.capture(request, resultListener, mHandler);
            CaptureResult stillResult =
                    resultListener.getCaptureResultForRequest(request, NUM_RESULTS_WAIT_TIMEOUT);
            Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);

            verifyJpegKeys(image, stillResult, stillSize, testThumbnailSizes[i], EXIF_TEST_DATA[i],
                    mStaticInfo, mCollector, mDebugFileNameBase, format);

            // Free image resources
            image.close();
        }
    }

    /**
     * Issue a still capture and validate the dynamic depth output.
     */
    private void stillDynamicDepthTestByCamera(int format, Size stillSize) throws Exception {
        assertTrue(format == ImageFormat.DEPTH_JPEG);

        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        if (VERBOSE) {
            Log.v(TAG, ""Testing dynamic depth with size "" + stillSize.toString()
                    + "", preview size "" + maxPreviewSz);
        }

        // prepare capture and start preview.
        CaptureRequest.Builder previewBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder stillBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
        prepareCaptureAndStartPreview(previewBuilder, stillBuilder, maxPreviewSz, stillSize,
                ImageFormat.DEPTH_JPEG, resultListener, /*sessionListener*/null,
                MAX_READER_IMAGES, imageListener);

        // Capture a few dynamic depth images and check whether they are valid jpegs.
        for (int i = 0; i < MAX_READER_IMAGES; i++) {
            CaptureRequest request = stillBuilder.build();
            mSession.capture(request, resultListener, mHandler);
            CaptureResult stillResult =
                resultListener.getCaptureResultForRequest(request, NUM_RESULTS_WAIT_TIMEOUT);
            Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
            assertNotNull(""Unable to acquire next image"", image);
            CameraTestUtils.validateImage(image, stillSize.getWidth(), stillSize.getHeight(),
                    format, null /*filePath*/);

            // Free image resources
            image.close();
        }
    }

    private void aeCompensationTestByCamera() throws Exception {
        Range<Integer> compensationRange = mStaticInfo.getAeCompensationRangeChecked();
        // Skip the test if exposure compensation is not supported.
        if (compensationRange.equals(Range.create(0, 0))) {
            return;
        }

        Rational step = mStaticInfo.getAeCompensationStepChecked();
        float stepF = (float) step.getNumerator() / step.getDenominator();
        int stepsPerEv = (int) Math.round(1.0 / stepF);
        int numSteps = (compensationRange.getUpper() - compensationRange.getLower()) / stepsPerEv;

        Size maxStillSz = mOrderedStillSizes.get(0);
        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
        CaptureRequest.Builder previewRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder stillRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        boolean canSetAeLock = mStaticInfo.isAeLockSupported();
        boolean canReadSensorSettings = mStaticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS);

        if (canSetAeLock) {
            stillRequest.set(CaptureRequest.CONTROL_AE_LOCK, true);
        }

        CaptureResult normalResult;
        CaptureResult compensatedResult;

        boolean canReadExposureValueRange = mStaticInfo.areKeysAvailable(
                CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE,
                CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE);
        boolean canVerifyExposureValue = canReadSensorSettings && canReadExposureValueRange;
        long minExposureValue = -1;
        long maxExposureValuePreview = -1;
        long maxExposureValueStill = -1;
        if (canReadExposureValueRange) {
            // Minimum exposure settings is mostly static while maximum exposure setting depends on
            // frame rate range which in term depends on capture request.
            minExposureValue = mStaticInfo.getSensitivityMinimumOrDefault() *
                    mStaticInfo.getExposureMinimumOrDefault() / 1000;
            long maxSensitivity = mStaticInfo.getSensitivityMaximumOrDefault();
            long maxExposureTimeUs = mStaticInfo.getExposureMaximumOrDefault() / 1000;
            maxExposureValuePreview = getMaxExposureValue(previewRequest, maxExposureTimeUs,
                    maxSensitivity);
            maxExposureValueStill = getMaxExposureValue(stillRequest, maxExposureTimeUs,
                    maxSensitivity);
        }

        // Set the max number of images to be same as the burst count, as the verification
        // could be much slower than producing rate, and we don't want to starve producer.
        prepareStillCaptureAndStartPreview(previewRequest, stillRequest, maxPreviewSz,
                maxStillSz, resultListener, numSteps, imageListener, false /*isHeic*/);

        for (int i = 0; i <= numSteps; i++) {
            int exposureCompensation = i * stepsPerEv + compensationRange.getLower();
            double expectedRatio = Math.pow(2.0, exposureCompensation / stepsPerEv);

            // Wait for AE to be stabilized before capture: CONVERGED or FLASH_REQUIRED.
            waitForAeStable(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
            normalResult = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);

            long normalExposureValue = -1;
            if (canVerifyExposureValue) {
                // get and check if current exposure value is valid
                normalExposureValue = getExposureValue(normalResult);
                mCollector.expectInRange(""Exposure setting out of bound"", normalExposureValue,
                        minExposureValue, maxExposureValuePreview);

                // Only run the test if expectedExposureValue is within valid range
                long expectedExposureValue = (long) (normalExposureValue * expectedRatio);
                if (expectedExposureValue < minExposureValue ||
                    expectedExposureValue > maxExposureValueStill) {
                    continue;
                }
                Log.v(TAG, ""Expect ratio: "" + expectedRatio +
                        "" normalExposureValue: "" + normalExposureValue +
                        "" expectedExposureValue: "" + expectedExposureValue +
                        "" minExposureValue: "" + minExposureValue +
                        "" maxExposureValuePreview: "" + maxExposureValuePreview +
                        "" maxExposureValueStill: "" + maxExposureValueStill);
            }

            // Now issue exposure compensation and wait for AE locked. AE could take a few
            // frames to go back to locked state
            previewRequest.set(CaptureRequest.CONTROL_AE_EXPOSURE_COMPENSATION,
                    exposureCompensation);
            if (canSetAeLock) {
                previewRequest.set(CaptureRequest.CONTROL_AE_LOCK, true);
            }
            mSession.setRepeatingRequest(previewRequest.build(), resultListener, mHandler);
            if (canSetAeLock) {
                waitForAeLocked(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
            } else {
                waitForSettingsApplied(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
            }

            // Issue still capture
            if (VERBOSE) {
                Log.v(TAG, ""Verifying capture result for ae compensation value ""
                        + exposureCompensation);
            }

            stillRequest.set(CaptureRequest.CONTROL_AE_EXPOSURE_COMPENSATION, exposureCompensation);
            CaptureRequest request = stillRequest.build();
            mSession.capture(request, resultListener, mHandler);

            compensatedResult = resultListener.getCaptureResultForRequest(
                    request, WAIT_FOR_RESULT_TIMEOUT_MS);

            if (canVerifyExposureValue) {
                // Verify the exposure value compensates as requested
                long compensatedExposureValue = getExposureValue(compensatedResult);
                mCollector.expectInRange(""Exposure setting out of bound"", compensatedExposureValue,
                        minExposureValue, maxExposureValueStill);
                double observedRatio = (double) compensatedExposureValue / normalExposureValue;
                double error = observedRatio / expectedRatio;
                String errorString = String.format(
                        ""Exposure compensation ratio exceeds error tolerence:"" +
                        "" expected(%f) observed(%f)."" +
                        "" Normal exposure time %d us, sensitivity %d."" +
                        "" Compensated exposure time %d us, sensitivity %d"",
                        expectedRatio, observedRatio,
                        (int) (getValueNotNull(
                                normalResult, CaptureResult.SENSOR_EXPOSURE_TIME) / 1000),
                        getValueNotNull(normalResult, CaptureResult.SENSOR_SENSITIVITY),
                        (int) (getValueNotNull(
                                compensatedResult, CaptureResult.SENSOR_EXPOSURE_TIME) / 1000),
                        getValueNotNull(compensatedResult, CaptureResult.SENSOR_SENSITIVITY));
                mCollector.expectInRange(errorString, error,
                        1.0 - AE_COMPENSATION_ERROR_TOLERANCE,
                        1.0 + AE_COMPENSATION_ERROR_TOLERANCE);
            }

            mCollector.expectEquals(""Exposure compensation result should match requested value."",
                    exposureCompensation,
                    compensatedResult.get(CaptureResult.CONTROL_AE_EXPOSURE_COMPENSATION));
            if (canSetAeLock) {
                mCollector.expectTrue(""Exposure lock should be set"",
                        compensatedResult.get(CaptureResult.CONTROL_AE_LOCK));
            }

            Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
            validateJpegCapture(image, maxStillSz);
            image.close();

            // Recover AE compensation and lock
            previewRequest.set(CaptureRequest.CONTROL_AE_EXPOSURE_COMPENSATION, 0);
            if (canSetAeLock) {
                previewRequest.set(CaptureRequest.CONTROL_AE_LOCK, false);
            }
            mSession.setRepeatingRequest(previewRequest.build(), resultListener, mHandler);
        }
    }

    private long getExposureValue(CaptureResult result) throws Exception {
        int expTimeUs = (int) (getValueNotNull(result, CaptureResult.SENSOR_EXPOSURE_TIME) / 1000);
        int sensitivity = getValueNotNull(result, CaptureResult.SENSOR_SENSITIVITY);
        Integer postRawSensitivity = result.get(CaptureResult.CONTROL_POST_RAW_SENSITIVITY_BOOST);
        if (postRawSensitivity != null) {
            return (long) sensitivity * postRawSensitivity / 100 * expTimeUs;
        }
        return (long) sensitivity * expTimeUs;
    }

    private long getMaxExposureValue(CaptureRequest.Builder request, long maxExposureTimeUs,
                long maxSensitivity)  throws Exception {
        Range<Integer> fpsRange = request.get(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE);
        long maxFrameDurationUs = Math.round(1000000.0 / fpsRange.getLower());
        long currentMaxExposureTimeUs = Math.min(maxFrameDurationUs, maxExposureTimeUs);
        return currentMaxExposureTimeUs * maxSensitivity;
    }


    //----------------------------------------------------------------
    //---------Below are common functions for all tests.--------------
    //----------------------------------------------------------------
    /**
     * Validate standard raw (RAW16) capture image.
     *
     * @param image The raw16 format image captured
     * @param rawSize The expected raw size
     */
    private static void validateRaw16Image(Image image, Size rawSize) {
        CameraTestUtils.validateImage(image, rawSize.getWidth(), rawSize.getHeight(),
                ImageFormat.RAW_SENSOR, /*filePath*/null);
    }

    /**
     * Validate JPEG capture image object correctness and test.
     * <p>
     * In addition to image object correctness, this function also does the decoding
     * test, which is slower.
     * </p>
     *
     * @param image The JPEG image to be verified.
     * @param jpegSize The JPEG capture size to be verified against.
     */
    private static void validateJpegCapture(Image image, Size jpegSize) {
        CameraTestUtils.validateImage(image, jpegSize.getWidth(), jpegSize.getHeight(),
                ImageFormat.JPEG, /*filePath*/null);
    }

    private static class SimpleAutoFocusListener implements Camera2Focuser.AutoFocusListener {
        final ConditionVariable focusDone = new ConditionVariable();
        @Override
        public void onAutoFocusLocked(boolean success) {
            focusDone.open();
        }

        public void waitForAutoFocusDone(long timeoutMs) {
            if (focusDone.block(timeoutMs)) {
                focusDone.close();
            } else {
                throw new TimeoutRuntimeException(""Wait for auto focus done timed out after ""
                        + timeoutMs + ""ms"");
            }
        }
    }

    /**
     * Get 5 3A region test cases, each with one square region in it.
     * The first one is at center, the other four are at corners of
     * active array rectangle.
     *
     * @return array of test 3A regions
     */
    private ArrayList<MeteringRectangle[]> get3ARegionTestCasesForCamera() {
        final int TEST_3A_REGION_NUM = 5;
        final int DEFAULT_REGION_WEIGHT = 30;
        final int DEFAULT_REGION_SCALE_RATIO = 8;
        ArrayList<MeteringRectangle[]> testCases =
                new ArrayList<MeteringRectangle[]>(TEST_3A_REGION_NUM);
        final Rect activeArraySize = mStaticInfo.getActiveArraySizeChecked();
        int regionWidth = activeArraySize.width() / DEFAULT_REGION_SCALE_RATIO - 1;
        int regionHeight = activeArraySize.height() / DEFAULT_REGION_SCALE_RATIO - 1;
        int centerX = activeArraySize.width() / 2;
        int centerY = activeArraySize.height() / 2;
        int bottomRightX = activeArraySize.width() - 1;
        int bottomRightY = activeArraySize.height() - 1;

        // Center region
        testCases.add(
                new MeteringRectangle[] {
                    new MeteringRectangle(
                            centerX - regionWidth / 2,  // x
                            centerY - regionHeight / 2, // y
                            regionWidth,                // width
                            regionHeight,               // height
                            DEFAULT_REGION_WEIGHT)});

        // Upper left corner
        testCases.add(
                new MeteringRectangle[] {
                    new MeteringRectangle(
                            0,                // x
                            0,                // y
                            regionWidth,      // width
                            regionHeight,     // height
                            DEFAULT_REGION_WEIGHT)});

        // Upper right corner
        testCases.add(
                new MeteringRectangle[] {
                    new MeteringRectangle(
                            bottomRightX - regionWidth, // x
                            0,                          // y
                            regionWidth,                // width
                            regionHeight,               // height
                            DEFAULT_REGION_WEIGHT)});

        // Bottom left corner
        testCases.add(
                new MeteringRectangle[] {
                    new MeteringRectangle(
                            0,                           // x
                            bottomRightY - regionHeight, // y
                            regionWidth,                 // width
                            regionHeight,                // height
                            DEFAULT_REGION_WEIGHT)});

        // Bottom right corner
        testCases.add(
                new MeteringRectangle[] {
                    new MeteringRectangle(
                            bottomRightX - regionWidth,  // x
                            bottomRightY - regionHeight, // y
                            regionWidth,                 // width
                            regionHeight,                // height
                            DEFAULT_REGION_WEIGHT)});

        if (VERBOSE) {
            StringBuilder sb = new StringBuilder();
            for (MeteringRectangle[] mr : testCases) {
                sb.append(""{"");
                sb.append(Arrays.toString(mr));
                sb.append(""}, "");
            }
            if (sb.length() > 1)
                sb.setLength(sb.length() - 2); // Remove the redundant comma and space at the end
            Log.v(TAG, ""Generated test regions are: "" + sb.toString());
        }

        return testCases;
    }

    private boolean isRegionsSupportedFor3A(int index) {
        int maxRegions = 0;
        switch (index) {
            case MAX_REGIONS_AE_INDEX:
                maxRegions = mStaticInfo.getAeMaxRegionsChecked();
                break;
            case MAX_REGIONS_AWB_INDEX:
                maxRegions = mStaticInfo.getAwbMaxRegionsChecked();
                break;
            case  MAX_REGIONS_AF_INDEX:
                maxRegions = mStaticInfo.getAfMaxRegionsChecked();
                break;
            default:
                throw new IllegalArgumentException(""Unknown algorithm index"");
        }
        boolean isRegionsSupported = maxRegions > 0;
        if (index == MAX_REGIONS_AF_INDEX && isRegionsSupported) {
            mCollector.expectTrue(
                    ""Device reports non-zero max AF region count for a camera without focuser!"",
                    mStaticInfo.hasFocuser());
            isRegionsSupported = isRegionsSupported && mStaticInfo.hasFocuser();
        }

        return isRegionsSupported;
    }
}"	""	""	"cdd minimum 12"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraManagerTest"	"testCameraManagerGetDeviceIdList"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CameraManagerTest.java"	""	"public void testCameraManagerGetDeviceIdList() throws Exception {

        String[] ids = mCameraIdsUnderTest;
        if (VERBOSE) Log.v(TAG, ""CameraManager ids: "" + Arrays.toString(ids));

        /**
         * Test: that if there is at least one reported id, then the system must have
         * the FEATURE_CAMERA_ANY feature.
         */
        assertTrue(""System camera feature and camera id list don't match"",
                ids.length == 0 ||
                mPackageManager.hasSystemFeature(PackageManager.FEATURE_CAMERA_ANY));

        /**
         * Test: that if the device has front or rear facing cameras, then there
         * must be matched system features.
         */
        boolean externalCameraConnected = false;
        Map<String, Integer> lensFacingMap = new HashMap<String, Integer>();
        for (int i = 0; i < ids.length; i++) {
            CameraCharacteristics props = mCameraManager.getCameraCharacteristics(ids[i]);
            assertNotNull(""Can't get camera characteristics for camera "" + ids[i], props);
            Integer lensFacing = props.get(CameraCharacteristics.LENS_FACING);
            lensFacingMap.put(ids[i], lensFacing);
            assertNotNull(""Can't get lens facing info"", lensFacing);
            if (lensFacing == CameraCharacteristics.LENS_FACING_FRONT) {
                assertTrue(""System doesn't have front camera feature"",
                        mPackageManager.hasSystemFeature(PackageManager.FEATURE_CAMERA_FRONT));
            } else if (lensFacing == CameraCharacteristics.LENS_FACING_BACK) {
                assertTrue(""System doesn't have back camera feature"",
                        mPackageManager.hasSystemFeature(PackageManager.FEATURE_CAMERA));
            } else if (lensFacing == CameraCharacteristics.LENS_FACING_EXTERNAL) {
                externalCameraConnected = true;
                assertTrue(""System doesn't have external camera feature"",
                        mPackageManager.hasSystemFeature(PackageManager.FEATURE_CAMERA_EXTERNAL));
            } else {
                fail(""Unknown camera lens facing "" + lensFacing.toString());
            }
        }

        // Test an external camera is connected if FEATURE_CAMERA_EXTERNAL is advertised
        if (!mAdoptShellPerm &&
                mPackageManager.hasSystemFeature(PackageManager.FEATURE_CAMERA_EXTERNAL)) {
            assertTrue(""External camera is not connected on device with FEATURE_CAMERA_EXTERNAL"",
                    externalCameraConnected);
        }

        /**
         * Test: that if there is one camera device, then the system must have some
         * specific features.
         */
        assertTrue(""Missing system feature: FEATURE_CAMERA_ANY"",
               ids.length == 0
            || mPackageManager.hasSystemFeature(PackageManager.FEATURE_CAMERA_ANY));
        assertTrue(""Missing system feature: FEATURE_CAMERA, FEATURE_CAMERA_FRONT or FEATURE_CAMERA_EXTERNAL"",
               ids.length == 0
            || mPackageManager.hasSystemFeature(PackageManager.FEATURE_CAMERA)
            || mPackageManager.hasSystemFeature(PackageManager.FEATURE_CAMERA_FRONT)
            || mPackageManager.hasSystemFeature(PackageManager.FEATURE_CAMERA_EXTERNAL));

        boolean frontBackAdvertised =
                mPackageManager.hasSystemFeature(PackageManager.FEATURE_CAMERA_CONCURRENT);

        boolean frontBackCombinationFound = false;
        // Go through all combinations and see that at least one combination has a front + back
        // camera.
        for (Set<String> cameraIdCombination : mConcurrentCameraIdCombinations) {
            boolean frontFacingFound = false, backFacingFound = false;
            for (String cameraId : cameraIdCombination) {
                Integer lensFacing = lensFacingMap.get(cameraId);
                if (lensFacing == CameraCharacteristics.LENS_FACING_FRONT) {
                    frontFacingFound = true;
                } else if (lensFacing == CameraCharacteristics.LENS_FACING_BACK) {
                    backFacingFound = true;
                }
                if (frontFacingFound && backFacingFound) {
                    frontBackCombinationFound = true;
                    break;
                }
            }
            if (frontBackCombinationFound) {
                break;
            }
        }

        if(mCameraIdsUnderTest.length > 0) {
            assertTrue(""System camera feature FEATURE_CAMERA_CONCURRENT = "" + frontBackAdvertised +
                    "" and device actually having a front back combination which can operate "" +
                    ""concurrently = "" + frontBackCombinationFound +  "" do not match"",
                    frontBackAdvertised == frontBackCombinationFound);
        }
    }

    // Test: that properties can be queried from each device, without exceptions."	""	""	"rear"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraTestUtils"	"ImageDropperListener"	""	"/home/gpoor/cts-12-source/cts/tests/camera/utils/src/android/hardware/camera2/cts/CameraTestUtils.java"	""	"public void test/*
 *.
 */

package android.hardware.camera2.cts;

import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.graphics.ImageFormat;
import android.graphics.PointF;
import android.graphics.Rect;
import android.graphics.SurfaceTexture;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraConstrainedHighSpeedCaptureSession;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CameraMetadata;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CaptureFailure;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.MultiResolutionImageReader;
import android.hardware.camera2.cts.helpers.CameraErrorCollector;
import android.hardware.camera2.cts.helpers.StaticMetadata;
import android.hardware.camera2.params.InputConfiguration;
import android.hardware.camera2.TotalCaptureResult;
import android.hardware.cts.helpers.CameraUtils;
import android.hardware.camera2.params.MeteringRectangle;
import android.hardware.camera2.params.MandatoryStreamCombination;
import android.hardware.camera2.params.MandatoryStreamCombination.MandatoryStreamInformation;
import android.hardware.camera2.params.MultiResolutionStreamConfigurationMap;
import android.hardware.camera2.params.MultiResolutionStreamInfo;
import android.hardware.camera2.params.OutputConfiguration;
import android.hardware.camera2.params.SessionConfiguration;
import android.hardware.camera2.params.StreamConfigurationMap;
import android.location.Location;
import android.location.LocationManager;
import android.media.ExifInterface;
import android.media.Image;
import android.media.ImageReader;
import android.media.ImageWriter;
import android.media.Image.Plane;
import android.os.Build;
import android.os.ConditionVariable;
import android.os.Handler;
import android.util.Log;
import android.util.Pair;
import android.util.Size;
import android.util.Range;
import android.view.Display;
import android.view.Surface;
import android.view.WindowManager;

import com.android.ex.camera2.blocking.BlockingCameraManager;
import com.android.ex.camera2.blocking.BlockingCameraManager.BlockingOpenException;
import com.android.ex.camera2.blocking.BlockingSessionCallback;
import com.android.ex.camera2.blocking.BlockingStateCallback;
import com.android.ex.camera2.exceptions.TimeoutRuntimeException;

import junit.framework.Assert;

import org.mockito.Mockito;

import java.io.FileOutputStream;
import java.io.IOException;
import java.lang.reflect.Array;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.Comparator;
import java.util.Date;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.Executor;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.Semaphore;
import java.util.concurrent.TimeUnit;
import java.text.ParseException;
import java.text.SimpleDateFormat;

/**
 * A package private utility class for wrapping up the camera2 cts test common utility functions
 */
public class CameraTestUtils extends Assert {
    private static final String TAG = ""CameraTestUtils"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);
    private static final boolean DEBUG = Log.isLoggable(TAG, Log.DEBUG);
    public static final Size SIZE_BOUND_720P = new Size(1280, 720);
    public static final Size SIZE_BOUND_1080P = new Size(1920, 1088);
    public static final Size SIZE_BOUND_2K = new Size(2048, 1088);
    public static final Size SIZE_BOUND_QHD = new Size(2560, 1440);
    public static final Size SIZE_BOUND_2160P = new Size(3840, 2160);
    // Only test the preview size that is no larger than 1080p.
    public static final Size PREVIEW_SIZE_BOUND = SIZE_BOUND_1080P;
    // Default timeouts for reaching various states
    public static final int CAMERA_OPEN_TIMEOUT_MS = 3000;
    public static final int CAMERA_CLOSE_TIMEOUT_MS = 3000;
    public static final int CAMERA_IDLE_TIMEOUT_MS = 3000;
    public static final int CAMERA_ACTIVE_TIMEOUT_MS = 1000;
    public static final int CAMERA_BUSY_TIMEOUT_MS = 1000;
    public static final int CAMERA_UNCONFIGURED_TIMEOUT_MS = 1000;
    public static final int CAMERA_CONFIGURE_TIMEOUT_MS = 3000;
    public static final int CAPTURE_RESULT_TIMEOUT_MS = 3000;
    public static final int CAPTURE_IMAGE_TIMEOUT_MS = 3000;

    public static final int SESSION_CONFIGURE_TIMEOUT_MS = 3000;
    public static final int SESSION_CLOSE_TIMEOUT_MS = 3000;
    public static final int SESSION_READY_TIMEOUT_MS = 5000;
    public static final int SESSION_ACTIVE_TIMEOUT_MS = 1000;

    public static final int MAX_READER_IMAGES = 5;

    // Compensate for the loss of ""sensitivity"" and ""sensitivityBoost""
    public static final int MAX_ISO_MISMATCH = 3;

    public static final String OFFLINE_CAMERA_ID = ""offline_camera_id"";
    public static final String REPORT_LOG_NAME = ""CtsCameraTestCases"";

    private static final int EXIF_DATETIME_LENGTH = 19;
    private static final int EXIF_DATETIME_ERROR_MARGIN_SEC = 60;
    private static final float EXIF_FOCAL_LENGTH_ERROR_MARGIN = 0.001f;
    private static final float EXIF_EXPOSURE_TIME_ERROR_MARGIN_RATIO = 0.05f;
    private static final float EXIF_EXPOSURE_TIME_MIN_ERROR_MARGIN_SEC = 0.002f;
    private static final float EXIF_APERTURE_ERROR_MARGIN = 0.001f;

    private static final float ZOOM_RATIO_THRESHOLD = 0.01f;

    private static final Location sTestLocation0 = new Location(LocationManager.GPS_PROVIDER);
    private static final Location sTestLocation1 = new Location(LocationManager.GPS_PROVIDER);
    private static final Location sTestLocation2 = new Location(LocationManager.NETWORK_PROVIDER);

    static {
        sTestLocation0.setTime(1199145600000L);
        sTestLocation0.setLatitude(37.736071);
        sTestLocation0.setLongitude(-122.441983);
        sTestLocation0.setAltitude(21.0);

        sTestLocation1.setTime(1199145601000L);
        sTestLocation1.setLatitude(0.736071);
        sTestLocation1.setLongitude(0.441983);
        sTestLocation1.setAltitude(1.0);

        sTestLocation2.setTime(1199145602000L);
        sTestLocation2.setLatitude(-89.736071);
        sTestLocation2.setLongitude(-179.441983);
        sTestLocation2.setAltitude(100000.0);
    }

    // Exif test data vectors.
    public static final ExifTestData[] EXIF_TEST_DATA = {
            new ExifTestData(
                    /*gpsLocation*/ sTestLocation0,
                    /* orientation */90,
                    /* jpgQuality */(byte) 80,
                    /* thumbQuality */(byte) 75),
            new ExifTestData(
                    /*gpsLocation*/ sTestLocation1,
                    /* orientation */180,
                    /* jpgQuality */(byte) 90,
                    /* thumbQuality */(byte) 85),
            new ExifTestData(
                    /*gpsLocation*/ sTestLocation2,
                    /* orientation */270,
                    /* jpgQuality */(byte) 100,
                    /* thumbQuality */(byte) 100)
    };

    /**
     * Create an {@link android.media.ImageReader} object and get the surface.
     *
     * @param size The size of this ImageReader to be created.
     * @param format The format of this ImageReader to be created
     * @param maxNumImages The max number of images that can be acquired simultaneously.
     * @param listener The listener used by this ImageReader to notify callbacks.
     * @param handler The handler to use for any listener callbacks.
     */
    public static ImageReader makeImageReader(Size size, int format, int maxNumImages,
            ImageReader.OnImageAvailableListener listener, Handler handler) {
        ImageReader reader;
        reader = ImageReader.newInstance(size.getWidth(), size.getHeight(), format,
                maxNumImages);
        reader.setOnImageAvailableListener(listener, handler);
        if (VERBOSE) Log.v(TAG, ""Created ImageReader size "" + size);
        return reader;
    }

    /**
     * Create an ImageWriter and hook up the ImageListener.
     *
     * @param inputSurface The input surface of the ImageWriter.
     * @param maxImages The max number of Images that can be dequeued simultaneously.
     * @param listener The listener used by this ImageWriter to notify callbacks
     * @param handler The handler to post listener callbacks.
     * @return ImageWriter object created.
     */
    public static ImageWriter makeImageWriter(
            Surface inputSurface, int maxImages,
            ImageWriter.OnImageReleasedListener listener, Handler handler) {
        ImageWriter writer = ImageWriter.newInstance(inputSurface, maxImages);
        writer.setOnImageReleasedListener(listener, handler);
        return writer;
    }

    /**
     * Utility class to store the targets for mandatory stream combination test.
     */
    public static class StreamCombinationTargets {
        public List<SurfaceTexture> mPrivTargets = new ArrayList<>();
        public List<ImageReader> mJpegTargets = new ArrayList<>();
        public List<ImageReader> mYuvTargets = new ArrayList<>();
        public List<ImageReader> mY8Targets = new ArrayList<>();
        public List<ImageReader> mRawTargets = new ArrayList<>();
        public List<ImageReader> mHeicTargets = new ArrayList<>();
        public List<ImageReader> mDepth16Targets = new ArrayList<>();

        public List<MultiResolutionImageReader> mPrivMultiResTargets = new ArrayList<>();
        public List<MultiResolutionImageReader> mJpegMultiResTargets = new ArrayList<>();
        public List<MultiResolutionImageReader> mYuvMultiResTargets = new ArrayList<>();
        public List<MultiResolutionImageReader> mRawMultiResTargets = new ArrayList<>();

        public void close() {
            for (SurfaceTexture target : mPrivTargets) {
                target.release();
            }
            for (ImageReader target : mJpegTargets) {
                target.close();
            }
            for (ImageReader target : mYuvTargets) {
                target.close();
            }
            for (ImageReader target : mY8Targets) {
                target.close();
            }
            for (ImageReader target : mRawTargets) {
                target.close();
            }
            for (ImageReader target : mHeicTargets) {
                target.close();
            }
            for (ImageReader target : mDepth16Targets) {
                target.close();
            }

            for (MultiResolutionImageReader target : mPrivMultiResTargets) {
                target.close();
            }
            for (MultiResolutionImageReader target : mJpegMultiResTargets) {
                target.close();
            }
            for (MultiResolutionImageReader target : mYuvMultiResTargets) {
                target.close();
            }
            for (MultiResolutionImageReader target : mRawMultiResTargets) {
                target.close();
            }
        }
    }

    private static void configureTarget(StreamCombinationTargets targets,
            List<OutputConfiguration> outputConfigs, List<Surface> outputSurfaces,
            int format, Size targetSize, int numBuffers, String overridePhysicalCameraId,
            MultiResolutionStreamConfigurationMap multiResStreamConfig,
            boolean createMultiResiStreamConfig, ImageDropperListener listener, Handler handler) {
        if (createMultiResiStreamConfig) {
            Collection<MultiResolutionStreamInfo> multiResolutionStreams =
                    multiResStreamConfig.getOutputInfo(format);
            MultiResolutionImageReader multiResReader = new MultiResolutionImageReader(
                    multiResolutionStreams, format, numBuffers);
            multiResReader.setOnImageAvailableListener(listener, new HandlerExecutor(handler));
            Collection<OutputConfiguration> configs =
                    OutputConfiguration.createInstancesForMultiResolutionOutput(multiResReader);
            outputConfigs.addAll(configs);
            outputSurfaces.add(multiResReader.getSurface());
            switch (format) {
                case ImageFormat.PRIVATE:
                    targets.mPrivMultiResTargets.add(multiResReader);
                    break;
                case ImageFormat.JPEG:
                    targets.mJpegMultiResTargets.add(multiResReader);
                    break;
                case ImageFormat.YUV_420_888:
                    targets.mYuvMultiResTargets.add(multiResReader);
                    break;
                case ImageFormat.RAW_SENSOR:
                    targets.mRawMultiResTargets.add(multiResReader);
                    break;
                default:
                    fail(""Unknown/Unsupported output format "" + format);
            }
        } else {
            if (format == ImageFormat.PRIVATE) {
                SurfaceTexture target = new SurfaceTexture(/*random int*/1);
                target.setDefaultBufferSize(targetSize.getWidth(), targetSize.getHeight());
                OutputConfiguration config = new OutputConfiguration(new Surface(target));
                if (overridePhysicalCameraId != null) {
                    config.setPhysicalCameraId(overridePhysicalCameraId);
                }
                outputConfigs.add(config);
                outputSurfaces.add(config.getSurface());
                targets.mPrivTargets.add(target);
            } else {
                ImageReader target = ImageReader.newInstance(targetSize.getWidth(),
                        targetSize.getHeight(), format, numBuffers);
                target.setOnImageAvailableListener(listener, handler);
                OutputConfiguration config = new OutputConfiguration(target.getSurface());
                if (overridePhysicalCameraId != null) {
                    config.setPhysicalCameraId(overridePhysicalCameraId);
                }
                outputConfigs.add(config);
                outputSurfaces.add(config.getSurface());

                switch (format) {
                    case ImageFormat.JPEG:
                      targets.mJpegTargets.add(target);
                      break;
                    case ImageFormat.YUV_420_888:
                      targets.mYuvTargets.add(target);
                      break;
                    case ImageFormat.Y8:
                      targets.mY8Targets.add(target);
                      break;
                    case ImageFormat.RAW_SENSOR:
                      targets.mRawTargets.add(target);
                      break;
                    case ImageFormat.HEIC:
                      targets.mHeicTargets.add(target);
                      break;
                    case ImageFormat.DEPTH16:
                      targets.mDepth16Targets.add(target);
                      break;
                    default:
                      fail(""Unknown/Unsupported output format "" + format);
                }
            }
        }
    }

    public static void setupConfigurationTargets(List<MandatoryStreamInformation> streamsInfo,
            StreamCombinationTargets targets,
            List<OutputConfiguration> outputConfigs,
            List<Surface> outputSurfaces, int numBuffers,
            boolean substituteY8, boolean substituteHeic, String overridenPhysicalCameraId,
            MultiResolutionStreamConfigurationMap multiResStreamConfig, Handler handler) {
            List<Surface> uhSurfaces = new ArrayList<Surface>();
        setupConfigurationTargets(streamsInfo, targets, outputConfigs, outputSurfaces, uhSurfaces,
            numBuffers, substituteY8, substituteHeic, overridenPhysicalCameraId,
            multiResStreamConfig, handler);
    }

    public static void setupConfigurationTargets(List<MandatoryStreamInformation> streamsInfo,
            StreamCombinationTargets targets,
            List<OutputConfiguration> outputConfigs,
            List<Surface> outputSurfaces, List<Surface> uhSurfaces, int numBuffers,
            boolean substituteY8, boolean substituteHeic, String overridePhysicalCameraId,
            MultiResolutionStreamConfigurationMap multiResStreamConfig, Handler handler) {

        ImageDropperListener imageDropperListener = new ImageDropperListener();
        List<Surface> chosenSurfaces;
        for (MandatoryStreamInformation streamInfo : streamsInfo) {
            if (streamInfo.isInput()) {
                continue;
            }
            chosenSurfaces = outputSurfaces;
            if (streamInfo.isUltraHighResolution()) {
                chosenSurfaces = uhSurfaces;
            }
            int format = streamInfo.getFormat();
            if (substituteY8 && (format == ImageFormat.YUV_420_888)) {
                format = ImageFormat.Y8;
            } else if (substituteHeic && (format == ImageFormat.JPEG)) {
                format = ImageFormat.HEIC;
            }
            Size[] availableSizes = new Size[streamInfo.getAvailableSizes().size()];
            availableSizes = streamInfo.getAvailableSizes().toArray(availableSizes);
            Size targetSize = CameraTestUtils.getMaxSize(availableSizes);
            boolean createMultiResReader =
                    (multiResStreamConfig != null &&
                     !multiResStreamConfig.getOutputInfo(format).isEmpty() &&
                     streamInfo.isMaximumSize());
            switch (format) {
                case ImageFormat.PRIVATE:
                case ImageFormat.JPEG:
                case ImageFormat.YUV_420_888:
                case ImageFormat.Y8:
                case ImageFormat.HEIC:
                case ImageFormat.DEPTH16:
                {
                    configureTarget(targets, outputConfigs, chosenSurfaces, format,
                            targetSize, numBuffers, overridePhysicalCameraId, multiResStreamConfig,
                            createMultiResReader, imageDropperListener, handler);
                    break;
                }
                case ImageFormat.RAW_SENSOR: {
                    // targetSize could be null in the logical camera case where only
                    // physical camera supports RAW stream.
                    if (targetSize != null) {
                        configureTarget(targets, outputConfigs, chosenSurfaces, format,
                                targetSize, numBuffers, overridePhysicalCameraId,
                                multiResStreamConfig, createMultiResReader, imageDropperListener,
                                handler);
                    }
                    break;
                }
                default:
                    fail(""Unknown output format "" + format);
            }
        }
    }

    /**
     * Close pending images and clean up an {@link android.media.ImageReader} object.
     * @param reader an {@link android.media.ImageReader} to close.
     */
    public static void closeImageReader(ImageReader reader) {
        if (reader != null) {
            reader.close();
        }
    }

    /**
     * Close the pending images then close current active {@link ImageReader} objects.
     */
    public static void closeImageReaders(ImageReader[] readers) {
        if ((readers != null) && (readers.length > 0)) {
            for (ImageReader reader : readers) {
                CameraTestUtils.closeImageReader(reader);
            }
        }
    }

    /**
     * Close pending images and clean up an {@link android.media.ImageWriter} object.
     * @param writer an {@link android.media.ImageWriter} to close.
     */
    public static void closeImageWriter(ImageWriter writer) {
        if (writer != null) {
            writer.close();
        }
    }

    /**
     * Dummy listener that release the image immediately once it is available.
     *
     * <p>
     * It can be used for the case where we don't care the image data at all.
     * </p>
     */
    public static class ImageDropperListener implements ImageReader.OnImageAvailableListener {
        @Override
        public synchronized void onImageAvailable(ImageReader reader) {
            Image image = null;
            try {
                image = reader.acquireNextImage();
            } finally {
                if (image != null) {
                    image.close();
                    mImagesDropped++;
                }
            }
        }

        public synchronized int getImageCount() {
            return mImagesDropped;
        }

        public synchronized void resetImageCount() {
            mImagesDropped = 0;
        }

        private int mImagesDropped = 0;
    }

    /**
     * Image listener that release the image immediately after validating the image
     */
    public static class ImageVerifierListener implements ImageReader.OnImageAvailableListener {
        private Size mSize;
        private int mFormat;
        // Whether the parent ImageReader is valid or not. If the parent ImageReader
        // is destroyed, the acquired Image may become invalid.
        private boolean mReaderIsValid;

        public ImageVerifierListener(Size sz, int format) {
            mSize = sz;
            mFormat = format;
            mReaderIsValid = true;
        }

        public synchronized void onReaderDestroyed() {
            mReaderIsValid = false;
        }

        @Override
        public synchronized void onImageAvailable(ImageReader reader) {
            Image image = null;
            try {
                image = reader.acquireNextImage();
            } finally {
                if (image != null) {
                    // Should only do some quick validity checks in callback, as the ImageReader
                    // could be closed asynchronously, which will close all images acquired from
                    // this ImageReader.
                    checkImage(image, mSize.getWidth(), mSize.getHeight(), mFormat);
                    // checkAndroidImageFormat calls into underlying Image object, which could
                    // become invalid if the ImageReader is destroyed.
                    if (mReaderIsValid) {
                        checkAndroidImageFormat(image);
                    }
                    image.close();
                }
            }
        }
    }

    public static class SimpleImageReaderListener
            implements ImageReader.OnImageAvailableListener {
        private final LinkedBlockingQueue<Image> mQueue =
                new LinkedBlockingQueue<Image>();
        // Indicate whether this listener will drop images or not,
        // when the queued images reaches the reader maxImages
        private final boolean mAsyncMode;
        // maxImages held by the queue in async mode.
        private final int mMaxImages;

        /**
         * Create a synchronous SimpleImageReaderListener that queues the images
         * automatically when they are available, no image will be dropped. If
         * the caller doesn't call getImage(), the producer will eventually run
         * into buffer starvation.
         */
        public SimpleImageReaderListener() {
            mAsyncMode = false;
            mMaxImages = 0;
        }

        /**
         * Create a synchronous/asynchronous SimpleImageReaderListener that
         * queues the images automatically when they are available. For
         * asynchronous listener, image will be dropped if the queued images
         * reach to maxImages queued. If the caller doesn't call getImage(), the
         * producer will not be blocked. For synchronous listener, no image will
         * be dropped. If the caller doesn't call getImage(), the producer will
         * eventually run into buffer starvation.
         *
         * @param asyncMode If the listener is operating at asynchronous mode.
         * @param maxImages The max number of images held by this listener.
         */
        /**
         *
         * @param asyncMode
         */
        public SimpleImageReaderListener(boolean asyncMode, int maxImages) {
            mAsyncMode = asyncMode;
            mMaxImages = maxImages;
        }

        @Override
        public void onImageAvailable(ImageReader reader) {
            try {
                Image imge = reader.acquireNextImage();
                if (imge == null) {
                    return;
                }
                mQueue.put(imge);
                if (mAsyncMode && mQueue.size() >= mMaxImages) {
                    Image img = mQueue.poll();
                    img.close();
                }
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onImageAvailable"");
            }
        }

        /**
         * Get an image from the image reader.
         *
         * @param timeout Timeout value for the wait.
         * @return The image from the image reader.
         */
        public Image getImage(long timeout) throws InterruptedException {
            Image image = mQueue.poll(timeout, TimeUnit.MILLISECONDS);
            assertNotNull(""Wait for an image timed out in "" + timeout + ""ms"", image);
            return image;
        }

        /**
         * Drain the pending images held by this listener currently.
         *
         */
        public void drain() {
            while (!mQueue.isEmpty()) {
                Image image = mQueue.poll();
                assertNotNull(""Unable to get an image"", image);
                image.close();
            }
        }
    }

    public static class SimpleImageWriterListener implements ImageWriter.OnImageReleasedListener {
        private final Semaphore mImageReleasedSema = new Semaphore(0);
        private final ImageWriter mWriter;
        @Override
        public void onImageReleased(ImageWriter writer) {
            if (writer != mWriter) {
                return;
            }

            if (VERBOSE) {
                Log.v(TAG, ""Input image is released"");
            }
            mImageReleasedSema.release();
        }

        public SimpleImageWriterListener(ImageWriter writer) {
            if (writer == null) {
                throw new IllegalArgumentException(""writer cannot be null"");
            }
            mWriter = writer;
        }

        public void waitForImageReleased(long timeoutMs) throws InterruptedException {
            if (!mImageReleasedSema.tryAcquire(timeoutMs, TimeUnit.MILLISECONDS)) {
                fail(""wait for image available timed out after "" + timeoutMs + ""ms"");
            }
        }
    }

    public static class ImageAndMultiResStreamInfo {
        public final Image image;
        public final MultiResolutionStreamInfo streamInfo;

        public ImageAndMultiResStreamInfo(Image image, MultiResolutionStreamInfo streamInfo) {
            this.image = image;
            this.streamInfo = streamInfo;
        }
    }

    public static class SimpleMultiResolutionImageReaderListener
            implements ImageReader.OnImageAvailableListener {
        public SimpleMultiResolutionImageReaderListener(MultiResolutionImageReader owner,
                int maxBuffers, boolean acquireLatest) {
            mOwner = owner;
            mMaxBuffers = maxBuffers;
            mAcquireLatest = acquireLatest;
        }

        @Override
        public void onImageAvailable(ImageReader reader) {
            if (VERBOSE) Log.v(TAG, ""new image available"");

            if (mAcquireLatest) {
                mLastReader = reader;
                mImageAvailable.open();
            } else {
                if (mQueue.size() < mMaxBuffers) {
                    Image image = reader.acquireNextImage();
                    MultiResolutionStreamInfo multiResStreamInfo =
                            mOwner.getStreamInfoForImageReader(reader);
                    mQueue.offer(new ImageAndMultiResStreamInfo(image, multiResStreamInfo));
                }
            }
        }

        public ImageAndMultiResStreamInfo getAnyImageAndInfoAvailable(long timeoutMs)
                throws Exception {
            if (mAcquireLatest) {
                Image image = null;
                if (mImageAvailable.block(timeoutMs)) {
                    if (mLastReader != null) {
                        image = mLastReader.acquireLatestImage();
                        if (VERBOSE) Log.v(TAG, ""acquireLatestImage"");
                    } else {
                        fail(""invalid image reader"");
                    }
                    mImageAvailable.close();
                } else {
                    fail(""wait for image available time out after "" + timeoutMs + ""ms"");
                }
                return new ImageAndMultiResStreamInfo(image,
                        mOwner.getStreamInfoForImageReader(mLastReader));
            } else {
                ImageAndMultiResStreamInfo imageAndInfo = mQueue.poll(timeoutMs,
                        java.util.concurrent.TimeUnit.MILLISECONDS);
                if (imageAndInfo == null) {
                    fail(""wait for image available timed out after "" + timeoutMs + ""ms"");
                }
                return imageAndInfo;
            }
        }

        public void reset() {
            while (!mQueue.isEmpty()) {
                ImageAndMultiResStreamInfo imageAndInfo = mQueue.poll();
                assertNotNull(""Acquired image is not valid"", imageAndInfo.image);
                imageAndInfo.image.close();
            }
            mImageAvailable.close();
            mLastReader = null;
        }

        private LinkedBlockingQueue<ImageAndMultiResStreamInfo> mQueue =
                new LinkedBlockingQueue<ImageAndMultiResStreamInfo>();
        private final MultiResolutionImageReader mOwner;
        private final int mMaxBuffers;
        private final boolean mAcquireLatest;
        private ConditionVariable mImageAvailable = new ConditionVariable();
        private ImageReader mLastReader = null;
    }

    public static class SimpleCaptureCallback extends CameraCaptureSession.CaptureCallback {
        private final LinkedBlockingQueue<TotalCaptureResult> mQueue =
                new LinkedBlockingQueue<TotalCaptureResult>();
        private final LinkedBlockingQueue<CaptureFailure> mFailureQueue =
                new LinkedBlockingQueue<>();
        // (Surface, framenumber) pair for lost buffers
        private final LinkedBlockingQueue<Pair<Surface, Long>> mBufferLostQueue =
                new LinkedBlockingQueue<>();
        private final LinkedBlockingQueue<Integer> mAbortQueue =
                new LinkedBlockingQueue<>();
        // Pair<CaptureRequest, Long> is a pair of capture request and timestamp.
        private final LinkedBlockingQueue<Pair<CaptureRequest, Long>> mCaptureStartQueue =
                new LinkedBlockingQueue<>();
        // Pair<Int, Long> is a pair of sequence id and frame number
        private final LinkedBlockingQueue<Pair<Integer, Long>> mCaptureSequenceCompletedQueue =
                new LinkedBlockingQueue<>();

        private AtomicLong mNumFramesArrived = new AtomicLong(0);

        @Override
        public void onCaptureStarted(CameraCaptureSession session, CaptureRequest request,
                long timestamp, long frameNumber) {
            try {
                mCaptureStartQueue.put(new Pair(request, timestamp));
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureStarted"");
            }
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                TotalCaptureResult result) {
            try {
                mNumFramesArrived.incrementAndGet();
                mQueue.put(result);
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureCompleted"");
            }
        }

        @Override
        public void onCaptureFailed(CameraCaptureSession session, CaptureRequest request,
                CaptureFailure failure) {
            try {
                mFailureQueue.put(failure);
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureFailed"");
            }
        }

        @Override
        public void onCaptureSequenceAborted(CameraCaptureSession session, int sequenceId) {
            try {
                mAbortQueue.put(sequenceId);
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureAborted"");
            }
        }

        @Override
        public void onCaptureSequenceCompleted(CameraCaptureSession session, int sequenceId,
                long frameNumber) {
            try {
                mCaptureSequenceCompletedQueue.put(new Pair(sequenceId, frameNumber));
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureSequenceCompleted"");
            }
        }

        @Override
        public void onCaptureBufferLost(CameraCaptureSession session,
                CaptureRequest request, Surface target, long frameNumber) {
            try {
                mBufferLostQueue.put(new Pair<>(target, frameNumber));
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureBufferLost"");
            }
        }

        public long getTotalNumFrames() {
            return mNumFramesArrived.get();
        }

        public CaptureResult getCaptureResult(long timeout) {
            return getTotalCaptureResult(timeout);
        }

        public TotalCaptureResult getCaptureResult(long timeout, long timestamp) {
            try {
                long currentTs = -1L;
                TotalCaptureResult result;
                while (true) {
                    result = mQueue.poll(timeout, TimeUnit.MILLISECONDS);
                    if (result == null) {
                        throw new RuntimeException(
                                ""Wait for a capture result timed out in "" + timeout + ""ms"");
                    }
                    currentTs = result.get(CaptureResult.SENSOR_TIMESTAMP);
                    if (currentTs == timestamp) {
                        return result;
                    }
                }

            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }
        }

        public TotalCaptureResult getTotalCaptureResult(long timeout) {
            try {
                TotalCaptureResult result = mQueue.poll(timeout, TimeUnit.MILLISECONDS);
                assertNotNull(""Wait for a capture result timed out in "" + timeout + ""ms"", result);
                return result;
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }
        }

        /**
         * Get the {@link #CaptureResult capture result} for a given
         * {@link #CaptureRequest capture request}.
         *
         * @param myRequest The {@link #CaptureRequest capture request} whose
         *            corresponding {@link #CaptureResult capture result} was
         *            being waited for
         * @param numResultsWait Number of frames to wait for the capture result
         *            before timeout.
         * @throws TimeoutRuntimeException If more than numResultsWait results are
         *            seen before the result matching myRequest arrives, or each
         *            individual wait for result times out after
         *            {@value #CAPTURE_RESULT_TIMEOUT_MS}ms.
         */
        public CaptureResult getCaptureResultForRequest(CaptureRequest myRequest,
                int numResultsWait) {
            return getTotalCaptureResultForRequest(myRequest, numResultsWait);
        }

        /**
         * Get the {@link #TotalCaptureResult total capture result} for a given
         * {@link #CaptureRequest capture request}.
         *
         * @param myRequest The {@link #CaptureRequest capture request} whose
         *            corresponding {@link #TotalCaptureResult capture result} was
         *            being waited for
         * @param numResultsWait Number of frames to wait for the capture result
         *            before timeout.
         * @throws TimeoutRuntimeException If more than numResultsWait results are
         *            seen before the result matching myRequest arrives, or each
         *            individual wait for result times out after
         *            {@value #CAPTURE_RESULT_TIMEOUT_MS}ms.
         */
        public TotalCaptureResult getTotalCaptureResultForRequest(CaptureRequest myRequest,
                int numResultsWait) {
            ArrayList<CaptureRequest> captureRequests = new ArrayList<>(1);
            captureRequests.add(myRequest);
            return getTotalCaptureResultsForRequests(captureRequests, numResultsWait)[0];
        }

        /**
         * Get an array of {@link #TotalCaptureResult total capture results} for a given list of
         * {@link #CaptureRequest capture requests}. This can be used when the order of results
         * may not the same as the order of requests.
         *
         * @param captureRequests The list of {@link #CaptureRequest capture requests} whose
         *            corresponding {@link #TotalCaptureResult capture results} are
         *            being waited for.
         * @param numResultsWait Number of frames to wait for the capture results
         *            before timeout.
         * @throws TimeoutRuntimeException If more than numResultsWait results are
         *            seen before all the results matching captureRequests arrives.
         */
        public TotalCaptureResult[] getTotalCaptureResultsForRequests(
                List<CaptureRequest> captureRequests, int numResultsWait) {
            if (numResultsWait < 0) {
                throw new IllegalArgumentException(""numResultsWait must be no less than 0"");
            }
            if (captureRequests == null || captureRequests.size() == 0) {
                throw new IllegalArgumentException(""captureRequests must have at least 1 request."");
            }

            // Create a request -> a list of result indices map that it will wait for.
            HashMap<CaptureRequest, ArrayList<Integer>> remainingResultIndicesMap = new HashMap<>();
            for (int i = 0; i < captureRequests.size(); i++) {
                CaptureRequest request = captureRequests.get(i);
                ArrayList<Integer> indices = remainingResultIndicesMap.get(request);
                if (indices == null) {
                    indices = new ArrayList<>();
                    remainingResultIndicesMap.put(request, indices);
                }
                indices.add(i);
            }

            TotalCaptureResult[] results = new TotalCaptureResult[captureRequests.size()];
            int i = 0;
            do {
                TotalCaptureResult result = getTotalCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);
                CaptureRequest request = result.getRequest();
                ArrayList<Integer> indices = remainingResultIndicesMap.get(request);
                if (indices != null) {
                    results[indices.get(0)] = result;
                    indices.remove(0);

                    // Remove the entry if all results for this request has been fulfilled.
                    if (indices.isEmpty()) {
                        remainingResultIndicesMap.remove(request);
                    }
                }

                if (remainingResultIndicesMap.isEmpty()) {
                    return results;
                }
            } while (i++ < numResultsWait);

            throw new TimeoutRuntimeException(""Unable to get the expected capture result after ""
                    + ""waiting for "" + numResultsWait + "" results"");
        }

        /**
         * Get an array list of {@link #CaptureFailure capture failure} with maxNumFailures entries
         * at most. If it times out before maxNumFailures failures are received, return the failures
         * received so far.
         *
         * @param maxNumFailures The maximal number of failures to return. If it times out before
         *                       the maximal number of failures are received, return the received
         *                       failures so far.
         * @throws UnsupportedOperationException If an error happens while waiting on the failure.
         */
        public ArrayList<CaptureFailure> getCaptureFailures(long maxNumFailures) {
            ArrayList<CaptureFailure> failures = new ArrayList<>();
            try {
                for (int i = 0; i < maxNumFailures; i++) {
                    CaptureFailure failure = mFailureQueue.poll(CAPTURE_RESULT_TIMEOUT_MS,
                            TimeUnit.MILLISECONDS);
                    if (failure == null) {
                        // If waiting on a failure times out, return the failures so far.
                        break;
                    }
                    failures.add(failure);
                }
            }  catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }

            return failures;
        }

        /**
         * Get an array list of lost buffers with maxNumLost entries at most.
         * If it times out before maxNumLost buffer lost callbacks are received, return the
         * lost callbacks received so far.
         *
         * @param maxNumLost The maximal number of buffer lost failures to return. If it times out
         *                   before the maximal number of failures are received, return the received
         *                   buffer lost failures so far.
         * @throws UnsupportedOperationException If an error happens while waiting on the failure.
         */
        public ArrayList<Pair<Surface, Long>> getLostBuffers(long maxNumLost) {
            ArrayList<Pair<Surface, Long>> failures = new ArrayList<>();
            try {
                for (int i = 0; i < maxNumLost; i++) {
                    Pair<Surface, Long> failure = mBufferLostQueue.poll(CAPTURE_RESULT_TIMEOUT_MS,
                            TimeUnit.MILLISECONDS);
                    if (failure == null) {
                        // If waiting on a failure times out, return the failures so far.
                        break;
                    }
                    failures.add(failure);
                }
            }  catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }

            return failures;
        }

        /**
         * Get an array list of aborted capture sequence ids with maxNumAborts entries
         * at most. If it times out before maxNumAborts are received, return the aborted sequences
         * received so far.
         *
         * @param maxNumAborts The maximal number of aborted sequences to return. If it times out
         *                     before the maximal number of aborts are received, return the received
         *                     failed sequences so far.
         * @throws UnsupportedOperationException If an error happens while waiting on the failed
         *                                       sequences.
         */
        public ArrayList<Integer> geAbortedSequences(long maxNumAborts) {
            ArrayList<Integer> abortList = new ArrayList<>();
            try {
                for (int i = 0; i < maxNumAborts; i++) {
                    Integer abortSequence = mAbortQueue.poll(CAPTURE_RESULT_TIMEOUT_MS,
                            TimeUnit.MILLISECONDS);
                    if (abortSequence == null) {
                        break;
                    }
                    abortList.add(abortSequence);
                }
            }  catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }

            return abortList;
        }

        /**
         * Wait until the capture start of a request and expected timestamp arrives or it times
         * out after a number of capture starts.
         *
         * @param request The request for the capture start to wait for.
         * @param timestamp The timestamp for the capture start to wait for.
         * @param numCaptureStartsWait The number of capture start events to wait for before timing
         *                             out.
         */
        public void waitForCaptureStart(CaptureRequest request, Long timestamp,
                int numCaptureStartsWait) throws Exception {
            Pair<CaptureRequest, Long> expectedShutter = new Pair<>(request, timestamp);

            int i = 0;
            do {
                Pair<CaptureRequest, Long> shutter = mCaptureStartQueue.poll(
                        CAPTURE_RESULT_TIMEOUT_MS, TimeUnit.MILLISECONDS);

                if (shutter == null) {
                    throw new TimeoutRuntimeException(""Unable to get any more capture start "" +
                            ""event after waiting for "" + CAPTURE_RESULT_TIMEOUT_MS + "" ms."");
                } else if (expectedShutter.equals(shutter)) {
                    return;
                }

            } while (i++ < numCaptureStartsWait);

            throw new TimeoutRuntimeException(""Unable to get the expected capture start "" +
                    ""event after waiting for "" + numCaptureStartsWait + "" capture starts"");
        }

        /**
         * Wait until it receives capture sequence completed callback for a given squence ID.
         *
         * @param sequenceId The sequence ID of the capture sequence completed callback to wait for.
         * @param timeoutMs Time to wait for each capture sequence complete callback before
         *                  timing out.
         */
        public long getCaptureSequenceLastFrameNumber(int sequenceId, long timeoutMs) {
            try {
                while (true) {
                    Pair<Integer, Long> completedSequence =
                            mCaptureSequenceCompletedQueue.poll(timeoutMs, TimeUnit.MILLISECONDS);
                    assertNotNull(""Wait for a capture sequence completed timed out in "" +
                            timeoutMs + ""ms"", completedSequence);

                    if (completedSequence.first.equals(sequenceId)) {
                        return completedSequence.second.longValue();
                    }
                }
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }
        }

        public boolean hasMoreResults()
        {
            return !mQueue.isEmpty();
        }

        public boolean hasMoreFailures()
        {
            return !mFailureQueue.isEmpty();
        }

        public int getNumLostBuffers()
        {
            return mBufferLostQueue.size();
        }

        public boolean hasMoreAbortedSequences()
        {
            return !mAbortQueue.isEmpty();
        }

        public void drain() {
            mQueue.clear();
            mNumFramesArrived.getAndSet(0);
            mFailureQueue.clear();
            mBufferLostQueue.clear();
            mCaptureStartQueue.clear();
            mAbortQueue.clear();
        }
    }

    public static boolean hasCapability(CameraCharacteristics characteristics, int capability) {
        int [] capabilities =
                characteristics.get(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
        for (int c : capabilities) {
            if (c == capability) {
                return true;
            }
        }
        return false;
    }

    public static boolean isSystemCamera(CameraManager manager, String cameraId)
            throws CameraAccessException {
        CameraCharacteristics characteristics = manager.getCameraCharacteristics(cameraId);
        return hasCapability(characteristics,
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_SYSTEM_CAMERA);
    }

    public static String[] getCameraIdListForTesting(CameraManager manager,
            boolean getSystemCameras)
            throws CameraAccessException {
        String [] ids = manager.getCameraIdListNoLazy();
        List<String> idsForTesting = new ArrayList<String>();
        for (String id : ids) {
            boolean isSystemCamera = isSystemCamera(manager, id);
            if (getSystemCameras == isSystemCamera) {
                idsForTesting.add(id);
            }
        }
        return idsForTesting.toArray(new String[idsForTesting.size()]);
    }

    public static Set<Set<String>> getConcurrentCameraIds(CameraManager manager,
            boolean getSystemCameras)
            throws CameraAccessException {
        Set<String> cameraIds = new HashSet<String>(Arrays.asList(getCameraIdListForTesting(manager, getSystemCameras)));
        Set<Set<String>> combinations =  manager.getConcurrentCameraIds();
        Set<Set<String>> correctComb = new HashSet<Set<String>>();
        for (Set<String> comb : combinations) {
            Set<String> filteredIds = new HashSet<String>();
            for (String id : comb) {
                if (cameraIds.contains(id)) {
                    filteredIds.add(id);
                }
            }
            if (filteredIds.isEmpty()) {
                continue;
            }
            correctComb.add(filteredIds);
        }
        return correctComb;
    }

    /**
     * Block until the camera is opened.
     *
     * <p>Don't use this to test #onDisconnected/#onError since this will throw
     * an AssertionError if it fails to open the camera device.</p>
     *
     * @return CameraDevice opened camera device
     *
     * @throws IllegalArgumentException
     *            If the handler is null, or if the handler's looper is current.
     * @throws CameraAccessException
     *            If open fails immediately.
     * @throws BlockingOpenException
     *            If open fails after blocking for some amount of time.
     * @throws TimeoutRuntimeException
     *            If opening times out. Typically unrecoverable.
     */
    public static CameraDevice openCamera(CameraManager manager, String cameraId,
            CameraDevice.StateCallback listener, Handler handler) throws CameraAccessException,
            BlockingOpenException {

        /**
         * Although camera2 API allows 'null' Handler (it will just use the current
         * thread's Looper), this is not what we want for CTS.
         *
         * In CTS the default looper is used only to process events in between test runs,
         * so anything sent there would not be executed inside a test and the test would fail.
         *
         * In this case, BlockingCameraManager#openCamera performs the check for us.
         */
        return (new BlockingCameraManager(manager)).openCamera(cameraId, listener, handler);
    }


    /**
     * Block until the camera is opened.
     *
     * <p>Don't use this to test #onDisconnected/#onError since this will throw
     * an AssertionError if it fails to open the camera device.</p>
     *
     * @throws IllegalArgumentException
     *            If the handler is null, or if the handler's looper is current.
     * @throws CameraAccessException
     *            If open fails immediately.
     * @throws BlockingOpenException
     *            If open fails after blocking for some amount of time.
     * @throws TimeoutRuntimeException
     *            If opening times out. Typically unrecoverable.
     */
    public static CameraDevice openCamera(CameraManager manager, String cameraId, Handler handler)
            throws CameraAccessException,
            BlockingOpenException {
        return openCamera(manager, cameraId, /*listener*/null, handler);
    }

    /**
     * Configure a new camera session with output surfaces and type.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputSurfaces The surface list that used for camera output.
     * @param listener The callback CameraDevice will notify when capture results are available.
     */
    public static CameraCaptureSession configureCameraSession(CameraDevice camera,
            List<Surface> outputSurfaces, boolean isHighSpeed,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);
        if (isHighSpeed) {
            camera.createConstrainedHighSpeedCaptureSession(outputSurfaces,
                    sessionListener, handler);
        } else {
            camera.createCaptureSession(outputSurfaces, sessionListener, handler);
        }
        CameraCaptureSession session =
                sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertFalse(""Camera session should not be a reprocessable session"",
                session.isReprocessable());
        String sessionType = isHighSpeed ? ""High Speed"" : ""Normal"";
        assertTrue(""Capture session type must be "" + sessionType,
                isHighSpeed ==
                CameraConstrainedHighSpeedCaptureSession.class.isAssignableFrom(session.getClass()));

        return session;
    }

    /**
     * Build a new constrained camera session with output surfaces, type and recording session
     * parameters.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputSurfaces The surface list that used for camera output.
     * @param listener The callback CameraDevice will notify when capture results are available.
     * @param initialRequest Initial request settings to use as session parameters.
     */
    public static CameraCaptureSession buildConstrainedCameraSession(CameraDevice camera,
            List<Surface> outputSurfaces, CameraCaptureSession.StateCallback listener,
            Handler handler, CaptureRequest initialRequest) throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);

        List<OutputConfiguration> outConfigurations = new ArrayList<>(outputSurfaces.size());
        for (Surface surface : outputSurfaces) {
            outConfigurations.add(new OutputConfiguration(surface));
        }
        SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_HIGH_SPEED, outConfigurations,
                new HandlerExecutor(handler), sessionListener);
        sessionConfig.setSessionParameters(initialRequest);
        camera.createCaptureSession(sessionConfig);

        CameraCaptureSession session =
                sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertFalse(""Camera session should not be a reprocessable session"",
                session.isReprocessable());
        assertTrue(""Capture session type must be High Speed"",
                CameraConstrainedHighSpeedCaptureSession.class.isAssignableFrom(
                        session.getClass()));

        return session;
    }

    /**
     * Configure a new camera session with output configurations.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputs The OutputConfiguration list that is used for camera output.
     * @param listener The callback CameraDevice will notify when capture results are available.
     */
    public static CameraCaptureSession configureCameraSessionWithConfig(CameraDevice camera,
            List<OutputConfiguration> outputs,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);
        camera.createCaptureSessionByOutputConfigurations(outputs, sessionListener, handler);
        CameraCaptureSession session =
                sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertFalse(""Camera session should not be a reprocessable session"",
                session.isReprocessable());
        return session;
    }

    /**
     * Try configure a new camera session with output configurations.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputs The OutputConfiguration list that is used for camera output.
     * @param initialRequest The session parameters passed in during stream configuration
     * @param listener The callback CameraDevice will notify when capture results are available.
     */
    public static CameraCaptureSession tryConfigureCameraSessionWithConfig(CameraDevice camera,
            List<OutputConfiguration> outputs, CaptureRequest initialRequest,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);
        SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_REGULAR, outputs, new HandlerExecutor(handler),
                sessionListener);
        sessionConfig.setSessionParameters(initialRequest);
        camera.createCaptureSession(sessionConfig);

        Integer[] sessionStates = {BlockingSessionCallback.SESSION_READY,
                                   BlockingSessionCallback.SESSION_CONFIGURE_FAILED};
        int state = sessionListener.getStateWaiter().waitForAnyOfStates(
                Arrays.asList(sessionStates), SESSION_CONFIGURE_TIMEOUT_MS);

        CameraCaptureSession session = null;
        if (state == BlockingSessionCallback.SESSION_READY) {
            session = sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
            assertFalse(""Camera session should not be a reprocessable session"",
                    session.isReprocessable());
        }
        return session;
    }

    /**
     * Configure a new camera session with output surfaces and initial session parameters.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputSurfaces The surface list that used for camera output.
     * @param listener The callback CameraDevice will notify when session is available.
     * @param handler The handler used to notify callbacks.
     * @param initialRequest Initial request settings to use as session parameters.
     */
    public static CameraCaptureSession configureCameraSessionWithParameters(CameraDevice camera,
            List<Surface> outputSurfaces, BlockingSessionCallback listener,
            Handler handler, CaptureRequest initialRequest) throws CameraAccessException {
        List<OutputConfiguration> outConfigurations = new ArrayList<>(outputSurfaces.size());
        for (Surface surface : outputSurfaces) {
            outConfigurations.add(new OutputConfiguration(surface));
        }
        SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_REGULAR, outConfigurations,
                new HandlerExecutor(handler), listener);
        sessionConfig.setSessionParameters(initialRequest);
        camera.createCaptureSession(sessionConfig);

        CameraCaptureSession session = listener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertFalse(""Camera session should not be a reprocessable session"",
                session.isReprocessable());
        assertFalse(""Capture session type must be regular"",
                CameraConstrainedHighSpeedCaptureSession.class.isAssignableFrom(
                        session.getClass()));

        return session;
    }

    /**
     * Configure a new camera session with output surfaces.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputSurfaces The surface list that used for camera output.
     * @param listener The callback CameraDevice will notify when capture results are available.
     */
    public static CameraCaptureSession configureCameraSession(CameraDevice camera,
            List<Surface> outputSurfaces,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {

        return configureCameraSession(camera, outputSurfaces, /*isHighSpeed*/false,
                listener, handler);
    }

    public static CameraCaptureSession configureReprocessableCameraSession(CameraDevice camera,
            InputConfiguration inputConfiguration, List<Surface> outputSurfaces,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {
        List<OutputConfiguration> outputConfigs = new ArrayList<OutputConfiguration>();
        for (Surface surface : outputSurfaces) {
            outputConfigs.add(new OutputConfiguration(surface));
        }
        CameraCaptureSession session = configureReprocessableCameraSessionWithConfigurations(
                camera, inputConfiguration, outputConfigs, listener, handler);

        return session;
    }

    public static CameraCaptureSession configureReprocessableCameraSessionWithConfigurations(
            CameraDevice camera, InputConfiguration inputConfiguration,
            List<OutputConfiguration> outputConfigs, CameraCaptureSession.StateCallback listener,
            Handler handler) throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);
        SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_REGULAR, outputConfigs, new HandlerExecutor(handler),
                sessionListener);
        sessionConfig.setInputConfiguration(inputConfiguration);
        camera.createCaptureSession(sessionConfig);

        Integer[] sessionStates = {BlockingSessionCallback.SESSION_READY,
                                   BlockingSessionCallback.SESSION_CONFIGURE_FAILED};
        int state = sessionListener.getStateWaiter().waitForAnyOfStates(
                Arrays.asList(sessionStates), SESSION_CONFIGURE_TIMEOUT_MS);

        assertTrue(""Creating a reprocessable session failed."",
                state == BlockingSessionCallback.SESSION_READY);
        CameraCaptureSession session =
                sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertTrue(""Camera session should be a reprocessable session"", session.isReprocessable());

        return session;
    }

    /**
     * Create a reprocessable camera session with input and output configurations.
     *
     * @param camera The CameraDevice to be configured.
     * @param inputConfiguration The input configuration used to create this session.
     * @param outputs The output configurations used to create this session.
     * @param listener The callback CameraDevice will notify when capture results are available.
     * @param handler The handler used to notify callbacks.
     * @return The session ready to use.
     * @throws CameraAccessException
     */
    public static CameraCaptureSession configureReprocCameraSessionWithConfig(CameraDevice camera,
            InputConfiguration inputConfiguration, List<OutputConfiguration> outputs,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);
        camera.createReprocessableCaptureSessionByConfigurations(inputConfiguration, outputs,
                sessionListener, handler);

        Integer[] sessionStates = {BlockingSessionCallback.SESSION_READY,
                                   BlockingSessionCallback.SESSION_CONFIGURE_FAILED};
        int state = sessionListener.getStateWaiter().waitForAnyOfStates(
                Arrays.asList(sessionStates), SESSION_CONFIGURE_TIMEOUT_MS);

        assertTrue(""Creating a reprocessable session failed."",
                state == BlockingSessionCallback.SESSION_READY);

        CameraCaptureSession session =
                sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertTrue(""Camera session should be a reprocessable session"", session.isReprocessable());

        return session;
    }

    public static <T> void assertArrayNotEmpty(T arr, String message) {
        assertTrue(message, arr != null && Array.getLength(arr) > 0);
    }

    /**
     * Check if the format is a legal YUV format camera supported.
     */
    public static void checkYuvFormat(int format) {
        if ((format != ImageFormat.YUV_420_888) &&
                (format != ImageFormat.NV21) &&
                (format != ImageFormat.YV12)) {
            fail(""Wrong formats: "" + format);
        }
    }

    /**
     * Check if image size and format match given size and format.
     */
    public static void checkImage(Image image, int width, int height, int format) {
        // Image reader will wrap YV12/NV21 image by YUV_420_888
        if (format == ImageFormat.NV21 || format == ImageFormat.YV12) {
            format = ImageFormat.YUV_420_888;
        }
        assertNotNull(""Input image is invalid"", image);
        assertEquals(""Format doesn't match"", format, image.getFormat());
        assertEquals(""Width doesn't match"", width, image.getWidth());
        assertEquals(""Height doesn't match"", height, image.getHeight());
    }

    /**
     * <p>Read data from all planes of an Image into a contiguous unpadded, unpacked
     * 1-D linear byte array, such that it can be write into disk, or accessed by
     * software conveniently. It supports YUV_420_888/NV21/YV12 and JPEG input
     * Image format.</p>
     *
     * <p>For YUV_420_888/NV21/YV12/Y8/Y16, it returns a byte array that contains
     * the Y plane data first, followed by U(Cb), V(Cr) planes if there is any
     * (xstride = width, ystride = height for chroma and luma components).</p>
     *
     * <p>For JPEG, it returns a 1-D byte array contains a complete JPEG image.</p>
     *
     * <p>For YUV P010, it returns a byte array that contains Y plane first, followed
     * by the interleaved U(Cb)/V(Cr) plane.</p>
     */
    public static byte[] getDataFromImage(Image image) {
        assertNotNull(""Invalid image:"", image);
        int format = image.getFormat();
        int width = image.getWidth();
        int height = image.getHeight();
        int rowStride, pixelStride;
        byte[] data = null;

        // Read image data
        Plane[] planes = image.getPlanes();
        assertTrue(""Fail to get image planes"", planes != null && planes.length > 0);

        // Check image validity
        checkAndroidImageFormat(image);

        ByteBuffer buffer = null;
        // JPEG doesn't have pixelstride and rowstride, treat it as 1D buffer.
        // Same goes for DEPTH_POINT_CLOUD, RAW_PRIVATE, DEPTH_JPEG, and HEIC
        if (format == ImageFormat.JPEG || format == ImageFormat.DEPTH_POINT_CLOUD ||
                format == ImageFormat.RAW_PRIVATE || format == ImageFormat.DEPTH_JPEG ||
                format == ImageFormat.HEIC) {
            buffer = planes[0].getBuffer();
            assertNotNull(""Fail to get jpeg/depth/heic ByteBuffer"", buffer);
            data = new byte[buffer.remaining()];
            buffer.get(data);
            buffer.rewind();
            return data;
        } else if (format == ImageFormat.YCBCR_P010) {
            // P010 samples are stored within 16 bit values
            int offset = 0;
            int bytesPerPixelRounded = (ImageFormat.getBitsPerPixel(format) + 7) / 8;
            data = new byte[width * height * bytesPerPixelRounded];
            assertTrue(""Unexpected number of planes, expected "" + 3 + "" actual "" + planes.length,
                    planes.length == 3);
            for (int i = 0; i < 2; i++) {
                buffer = planes[i].getBuffer();
                assertNotNull(""Fail to get bytebuffer from plane"", buffer);
                buffer.rewind();
                rowStride = planes[i].getRowStride();
                if (VERBOSE) {
                    Log.v(TAG, ""rowStride "" + rowStride);
                    Log.v(TAG, ""width "" + width);
                    Log.v(TAG, ""height "" + height);
                }
                int h = (i == 0) ? height : height / 2;
                for (int row = 0; row < h; row++) {
                    int length = rowStride;
                    buffer.get(data, offset, length);
                    offset += length;
                }
                if (VERBOSE) Log.v(TAG, ""Finished reading data from plane "" + i);
                buffer.rewind();
            }
            return data;
        }

        int offset = 0;
        data = new byte[width * height * ImageFormat.getBitsPerPixel(format) / 8];
        int maxRowSize = planes[0].getRowStride();
        for (int i = 0; i < planes.length; i++) {
            if (maxRowSize < planes[i].getRowStride()) {
                maxRowSize = planes[i].getRowStride();
            }
        }
        byte[] rowData = new byte[maxRowSize];
        if(VERBOSE) Log.v(TAG, ""get data from "" + planes.length + "" planes"");
        for (int i = 0; i < planes.length; i++) {
            buffer = planes[i].getBuffer();
            assertNotNull(""Fail to get bytebuffer from plane"", buffer);
            buffer.rewind();
            rowStride = planes[i].getRowStride();
            pixelStride = planes[i].getPixelStride();
            assertTrue(""pixel stride "" + pixelStride + "" is invalid"", pixelStride > 0);
            if (VERBOSE) {
                Log.v(TAG, ""pixelStride "" + pixelStride);
                Log.v(TAG, ""rowStride "" + rowStride);
                Log.v(TAG, ""width "" + width);
                Log.v(TAG, ""height "" + height);
            }
            // For multi-planar yuv images, assuming yuv420 with 2x2 chroma subsampling.
            int w = (i == 0) ? width : width / 2;
            int h = (i == 0) ? height : height / 2;
            assertTrue(""rowStride "" + rowStride + "" should be >= width "" + w , rowStride >= w);
            for (int row = 0; row < h; row++) {
                int bytesPerPixel = ImageFormat.getBitsPerPixel(format) / 8;
                int length;
                if (pixelStride == bytesPerPixel) {
                    // Special case: optimized read of the entire row
                    length = w * bytesPerPixel;
                    buffer.get(data, offset, length);
                    offset += length;
                } else {
                    // Generic case: should work for any pixelStride but slower.
                    // Use intermediate buffer to avoid read byte-by-byte from
                    // DirectByteBuffer, which is very bad for performance
                    length = (w - 1) * pixelStride + bytesPerPixel;
                    buffer.get(rowData, 0, length);
                    for (int col = 0; col < w; col++) {
                        data[offset++] = rowData[col * pixelStride];
                    }
                }
                // Advance buffer the remainder of the row stride
                if (row < h - 1) {
                    buffer.position(buffer.position() + rowStride - length);
                }
            }
            if (VERBOSE) Log.v(TAG, ""Finished reading data from plane "" + i);
            buffer.rewind();
        }
        return data;
    }

    /**
     * <p>Check android image format validity for an image, only support below formats:</p>
     *
     * <p>YUV_420_888/NV21/YV12, can add more for future</p>
     */
    public static void checkAndroidImageFormat(Image image) {
        int format = image.getFormat();
        Plane[] planes = image.getPlanes();
        switch (format) {
            case ImageFormat.YUV_420_888:
            case ImageFormat.NV21:
            case ImageFormat.YV12:
            case ImageFormat.YCBCR_P010:
                assertEquals(""YUV420 format Images should have 3 planes"", 3, planes.length);
                break;
            case ImageFormat.JPEG:
            case ImageFormat.RAW_SENSOR:
            case ImageFormat.RAW_PRIVATE:
            case ImageFormat.DEPTH16:
            case ImageFormat.DEPTH_POINT_CLOUD:
            case ImageFormat.DEPTH_JPEG:
            case ImageFormat.Y8:
            case ImageFormat.HEIC:
                assertEquals(""JPEG/RAW/depth/Y8 Images should have one plane"", 1, planes.length);
                break;
            default:
                fail(""Unsupported Image Format: "" + format);
        }
    }

    public static void dumpFile(String fileName, Bitmap data) {
        FileOutputStream outStream;
        try {
            Log.v(TAG, ""output will be saved as "" + fileName);
            outStream = new FileOutputStream(fileName);
        } catch (IOException ioe) {
            throw new RuntimeException(""Unable to create debug output file "" + fileName, ioe);
        }

        try {
            data.compress(Bitmap.CompressFormat.JPEG, /*quality*/90, outStream);
            outStream.close();
        } catch (IOException ioe) {
            throw new RuntimeException(""failed writing data to file "" + fileName, ioe);
        }
    }

    public static void dumpFile(String fileName, byte[] data) {
        FileOutputStream outStream;
        try {
            Log.v(TAG, ""output will be saved as "" + fileName);
            outStream = new FileOutputStream(fileName);
        } catch (IOException ioe) {
            throw new RuntimeException(""Unable to create debug output file "" + fileName, ioe);
        }

        try {
            outStream.write(data);
            outStream.close();
        } catch (IOException ioe) {
            throw new RuntimeException(""failed writing data to file "" + fileName, ioe);
        }
    }

    /**
     * Get the available output sizes for the user-defined {@code format}.
     *
     * <p>Note that implementation-defined/hidden formats are not supported.</p>
     */
    public static Size[] getSupportedSizeForFormat(int format, String cameraId,
            CameraManager cameraManager) throws CameraAccessException {
        CameraCharacteristics properties = cameraManager.getCameraCharacteristics(cameraId);
        assertNotNull(""Can't get camera characteristics!"", properties);
        if (VERBOSE) {
            Log.v(TAG, ""get camera characteristics for camera: "" + cameraId);
        }
        StreamConfigurationMap configMap =
                properties.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        Size[] availableSizes = configMap.getOutputSizes(format);
        assertArrayNotEmpty(availableSizes, ""availableSizes should not be empty for format: ""
                + format);
        Size[] highResAvailableSizes = configMap.getHighResolutionOutputSizes(format);
        if (highResAvailableSizes != null && highResAvailableSizes.length > 0) {
            Size[] allSizes = new Size[availableSizes.length + highResAvailableSizes.length];
            System.arraycopy(availableSizes, 0, allSizes, 0,
                    availableSizes.length);
            System.arraycopy(highResAvailableSizes, 0, allSizes, availableSizes.length,
                    highResAvailableSizes.length);
            availableSizes = allSizes;
        }
        if (VERBOSE) Log.v(TAG, ""Supported sizes are: "" + Arrays.deepToString(availableSizes));
        return availableSizes;
    }

    /**
     * Get the available output sizes for the given class.
     *
     */
    public static Size[] getSupportedSizeForClass(Class klass, String cameraId,
            CameraManager cameraManager) throws CameraAccessException {
        CameraCharacteristics properties = cameraManager.getCameraCharacteristics(cameraId);
        assertNotNull(""Can't get camera characteristics!"", properties);
        if (VERBOSE) {
            Log.v(TAG, ""get camera characteristics for camera: "" + cameraId);
        }
        StreamConfigurationMap configMap =
                properties.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        Size[] availableSizes = configMap.getOutputSizes(klass);
        assertArrayNotEmpty(availableSizes, ""availableSizes should not be empty for class: ""
                + klass);
        Size[] highResAvailableSizes = configMap.getHighResolutionOutputSizes(ImageFormat.PRIVATE);
        if (highResAvailableSizes != null && highResAvailableSizes.length > 0) {
            Size[] allSizes = new Size[availableSizes.length + highResAvailableSizes.length];
            System.arraycopy(availableSizes, 0, allSizes, 0,
                    availableSizes.length);
            System.arraycopy(highResAvailableSizes, 0, allSizes, availableSizes.length,
                    highResAvailableSizes.length);
            availableSizes = allSizes;
        }
        if (VERBOSE) Log.v(TAG, ""Supported sizes are: "" + Arrays.deepToString(availableSizes));
        return availableSizes;
    }

    /**
     * Size comparator that compares the number of pixels it covers.
     *
     * <p>If two the areas of two sizes are same, compare the widths.</p>
     */
    public static class SizeComparator implements Comparator<Size> {
        @Override
        public int compare(Size lhs, Size rhs) {
            return CameraUtils
                    .compareSizes(lhs.getWidth(), lhs.getHeight(), rhs.getWidth(), rhs.getHeight());
        }
    }

    /**
     * Get sorted size list in descending order. Remove the sizes larger than
     * the bound. If the bound is null, don't do the size bound filtering.
     */
    static public List<Size> getSupportedPreviewSizes(String cameraId,
            CameraManager cameraManager, Size bound) throws CameraAccessException {

        Size[] rawSizes = getSupportedSizeForClass(android.view.SurfaceHolder.class, cameraId,
                cameraManager);
        assertArrayNotEmpty(rawSizes,
                ""Available sizes for SurfaceHolder class should not be empty"");
        if (VERBOSE) {
            Log.v(TAG, ""Supported sizes are: "" + Arrays.deepToString(rawSizes));
        }

        if (bound == null) {
            return getAscendingOrderSizes(Arrays.asList(rawSizes), /*ascending*/false);
        }

        List<Size> sizes = new ArrayList<Size>();
        for (Size sz: rawSizes) {
            if (sz.getWidth() <= bound.getWidth() && sz.getHeight() <= bound.getHeight()) {
                sizes.add(sz);
            }
        }
        return getAscendingOrderSizes(sizes, /*ascending*/false);
    }

    /**
     * Get a sorted list of sizes from a given size list.
     *
     * <p>
     * The size is compare by area it covers, if the areas are same, then
     * compare the widths.
     * </p>
     *
     * @param sizeList The input size list to be sorted
     * @param ascending True if the order is ascending, otherwise descending order
     * @return The ordered list of sizes
     */
    static public List<Size> getAscendingOrderSizes(final List<Size> sizeList, boolean ascending) {
        if (sizeList == null) {
            throw new IllegalArgumentException(""sizeList shouldn't be null"");
        }

        Comparator<Size> comparator = new SizeComparator();
        List<Size> sortedSizes = new ArrayList<Size>();
        sortedSizes.addAll(sizeList);
        Collections.sort(sortedSizes, comparator);
        if (!ascending) {
            Collections.reverse(sortedSizes);
        }

        return sortedSizes;
    }

    /**
     * Get sorted (descending order) size list for given format. Remove the sizes larger than
     * the bound. If the bound is null, don't do the size bound filtering.
     */
    static public List<Size> getSortedSizesForFormat(String cameraId,
            CameraManager cameraManager, int format, Size bound) throws CameraAccessException {
        Comparator<Size> comparator = new SizeComparator();
        Size[] sizes = getSupportedSizeForFormat(format, cameraId, cameraManager);
        List<Size> sortedSizes = null;
        if (bound != null) {
            sortedSizes = new ArrayList<Size>(/*capacity*/1);
            for (Size sz : sizes) {
                if (comparator.compare(sz, bound) <= 0) {
                    sortedSizes.add(sz);
                }
            }
        } else {
            sortedSizes = Arrays.asList(sizes);
        }
        assertTrue(""Supported size list should have at least one element"",
                sortedSizes.size() > 0);

        Collections.sort(sortedSizes, comparator);
        // Make it in descending order.
        Collections.reverse(sortedSizes);
        return sortedSizes;
    }

    /**
     * Get supported video size list for a given camera device.
     *
     * <p>
     * Filter out the sizes that are larger than the bound. If the bound is
     * null, don't do the size bound filtering.
     * </p>
     */
    static public List<Size> getSupportedVideoSizes(String cameraId,
            CameraManager cameraManager, Size bound) throws CameraAccessException {

        Size[] rawSizes = getSupportedSizeForClass(android.media.MediaRecorder.class,
                cameraId, cameraManager);
        assertArrayNotEmpty(rawSizes,
                ""Available sizes for MediaRecorder class should not be empty"");
        if (VERBOSE) {
            Log.v(TAG, ""Supported sizes are: "" + Arrays.deepToString(rawSizes));
        }

        if (bound == null) {
            return getAscendingOrderSizes(Arrays.asList(rawSizes), /*ascending*/false);
        }

        List<Size> sizes = new ArrayList<Size>();
        for (Size sz: rawSizes) {
            if (sz.getWidth() <= bound.getWidth() && sz.getHeight() <= bound.getHeight()) {
                sizes.add(sz);
            }
        }
        return getAscendingOrderSizes(sizes, /*ascending*/false);
    }

    /**
     * Get supported video size list (descending order) for a given camera device.
     *
     * <p>
     * Filter out the sizes that are larger than the bound. If the bound is
     * null, don't do the size bound filtering.
     * </p>
     */
    static public List<Size> getSupportedStillSizes(String cameraId,
            CameraManager cameraManager, Size bound) throws CameraAccessException {
        return getSortedSizesForFormat(cameraId, cameraManager, ImageFormat.JPEG, bound);
    }

    static public List<Size> getSupportedHeicSizes(String cameraId,
            CameraManager cameraManager, Size bound) throws CameraAccessException {
        return getSortedSizesForFormat(cameraId, cameraManager, ImageFormat.HEIC, bound);
    }

    static public Size getMinPreviewSize(String cameraId, CameraManager cameraManager)
            throws CameraAccessException {
        List<Size> sizes = getSupportedPreviewSizes(cameraId, cameraManager, null);
        return sizes.get(sizes.size() - 1);
    }

    /**
     * Get max supported preview size for a camera device.
     */
    static public Size getMaxPreviewSize(String cameraId, CameraManager cameraManager)
            throws CameraAccessException {
        return getMaxPreviewSize(cameraId, cameraManager, /*bound*/null);
    }

    /**
     * Get max preview size for a camera device in the supported sizes that are no larger
     * than the bound.
     */
    static public Size getMaxPreviewSize(String cameraId, CameraManager cameraManager, Size bound)
            throws CameraAccessException {
        List<Size> sizes = getSupportedPreviewSizes(cameraId, cameraManager, bound);
        return sizes.get(0);
    }

    /**
     * Get max depth size for a camera device.
     */
    static public Size getMaxDepthSize(String cameraId, CameraManager cameraManager)
            throws CameraAccessException {
        List<Size> sizes = getSortedSizesForFormat(cameraId, cameraManager, ImageFormat.DEPTH16,
                /*bound*/ null);
        return sizes.get(0);
    }

    /**
     * Get the largest size by area.
     *
     * @param sizes an array of sizes, must have at least 1 element
     *
     * @return Largest Size
     *
     * @throws IllegalArgumentException if sizes was null or had 0 elements
     */
    public static Size getMaxSize(Size... sizes) {
        if (sizes == null || sizes.length == 0) {
            throw new IllegalArgumentException(""sizes was empty"");
        }

        Size sz = sizes[0];
        for (Size size : sizes) {
            if (size.getWidth() * size.getHeight() > sz.getWidth() * sz.getHeight()) {
                sz = size;
            }
        }

        return sz;
    }

    /**
     * Get the largest size by area within (less than) bound
     *
     * @param sizes an array of sizes, must have at least 1 element
     *
     * @return Largest Size. Null if no such size exists within bound.
     *
     * @throws IllegalArgumentException if sizes was null or had 0 elements, or bound is invalid.
     */
    public static Size getMaxSizeWithBound(Size[] sizes, int bound) {
        if (sizes == null || sizes.length == 0) {
            throw new IllegalArgumentException(""sizes was empty"");
        }
        if (bound <= 0) {
            throw new IllegalArgumentException(""bound is invalid"");
        }

        Size sz = null;
        for (Size size : sizes) {
            if (size.getWidth() * size.getHeight() >= bound) {
                continue;
            }

            if (sz == null ||
                    size.getWidth() * size.getHeight() > sz.getWidth() * sz.getHeight()) {
                sz = size;
            }
        }

        return sz;
    }

    /**
     * Returns true if the given {@code array} contains the given element.
     *
     * @param array {@code array} to check for {@code elem}
     * @param elem {@code elem} to test for
     * @return {@code true} if the given element is contained
     */
    public static boolean contains(int[] array, int elem) {
        if (array == null) return false;
        for (int i = 0; i < array.length; i++) {
            if (elem == array[i]) return true;
        }
        return false;
    }

    /**
     * Get object array from byte array.
     *
     * @param array Input byte array to be converted
     * @return Byte object array converted from input byte array
     */
    public static Byte[] toObject(byte[] array) {
        return convertPrimitiveArrayToObjectArray(array, Byte.class);
    }

    /**
     * Get object array from int array.
     *
     * @param array Input int array to be converted
     * @return Integer object array converted from input int array
     */
    public static Integer[] toObject(int[] array) {
        return convertPrimitiveArrayToObjectArray(array, Integer.class);
    }

    /**
     * Get object array from float array.
     *
     * @param array Input float array to be converted
     * @return Float object array converted from input float array
     */
    public static Float[] toObject(float[] array) {
        return convertPrimitiveArrayToObjectArray(array, Float.class);
    }

    /**
     * Get object array from double array.
     *
     * @param array Input double array to be converted
     * @return Double object array converted from input double array
     */
    public static Double[] toObject(double[] array) {
        return convertPrimitiveArrayToObjectArray(array, Double.class);
    }

    /**
     * Convert a primitive input array into its object array version (e.g. from int[] to Integer[]).
     *
     * @param array Input array object
     * @param wrapperClass The boxed class it converts to
     * @return Boxed version of primitive array
     */
    private static <T> T[] convertPrimitiveArrayToObjectArray(final Object array,
            final Class<T> wrapperClass) {
        // getLength does the null check and isArray check already.
        int arrayLength = Array.getLength(array);
        if (arrayLength == 0) {
            throw new IllegalArgumentException(""Input array shouldn't be empty"");
        }

        @SuppressWarnings(""unchecked"")
        final T[] result = (T[]) Array.newInstance(wrapperClass, arrayLength);
        for (int i = 0; i < arrayLength; i++) {
            Array.set(result, i, Array.get(array, i));
        }
        return result;
    }

    /**
     * Validate image based on format and size.
     *
     * @param image The image to be validated.
     * @param width The image width.
     * @param height The image height.
     * @param format The image format.
     * @param filePath The debug dump file path, null if don't want to dump to
     *            file.
     * @throws UnsupportedOperationException if calling with an unknown format
     */
    public static void validateImage(Image image, int width, int height, int format,
            String filePath) {
        checkImage(image, width, height, format);

        /**
         * TODO: validate timestamp:
         * 1. capture result timestamp against the image timestamp (need
         * consider frame drops)
         * 2. timestamps should be monotonically increasing for different requests
         */
        if(VERBOSE) Log.v(TAG, ""validating Image"");
        byte[] data = getDataFromImage(image);
        assertTrue(""Invalid image data"", data != null && data.length > 0);

        switch (format) {
            // Clients must be able to process and handle depth jpeg images like any other
            // regular jpeg.
            case ImageFormat.DEPTH_JPEG:
            case ImageFormat.JPEG:
                validateJpegData(data, width, height, filePath);
                break;
            case ImageFormat.YCBCR_P010:
                validateP010Data(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.YUV_420_888:
            case ImageFormat.YV12:
                validateYuvData(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.RAW_SENSOR:
                validateRaw16Data(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.DEPTH16:
                validateDepth16Data(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.DEPTH_POINT_CLOUD:
                validateDepthPointCloudData(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.RAW_PRIVATE:
                validateRawPrivateData(data, width, height, image.getTimestamp(), filePath);
                break;
            case ImageFormat.Y8:
                validateY8Data(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.HEIC:
                validateHeicData(data, width, height, filePath);
                break;
            default:
                throw new UnsupportedOperationException(""Unsupported format for validation: ""
                        + format);
        }
    }

    public static class HandlerExecutor implements Executor {
        private final Handler mHandler;

        public HandlerExecutor(Handler handler) {
            assertNotNull(""handler must be valid"", handler);
            mHandler = handler;
        }

        @Override
        public void execute(Runnable runCmd) {
            mHandler.post(runCmd);
        }
    }

    /**
     * Provide a mock for {@link CameraDevice.StateCallback}.
     *
     * <p>Only useful because mockito can't mock {@link CameraDevice.StateCallback} which is an
     * abstract class.</p>
     *
     * <p>
     * Use this instead of other classes when needing to verify interactions, since
     * trying to spy on {@link BlockingStateCallback} (or others) will cause unnecessary extra
     * interactions which will cause false test failures.
     * </p>
     *
     */
    public static class MockStateCallback extends CameraDevice.StateCallback {

        @Override
        public void onOpened(CameraDevice camera) {
        }

        @Override
        public void onDisconnected(CameraDevice camera) {
        }

        @Override
        public void onError(CameraDevice camera, int error) {
        }

        private MockStateCallback() {}

        /**
         * Create a Mockito-ready mocked StateCallback.
         */
        public static MockStateCallback mock() {
            return Mockito.spy(new MockStateCallback());
        }
    }

    public static void validateJpegData(byte[] jpegData, int width, int height, String filePath) {
        BitmapFactory.Options bmpOptions = new BitmapFactory.Options();
        // DecodeBound mode: only parse the frame header to get width/height.
        // it doesn't decode the pixel.
        bmpOptions.inJustDecodeBounds = true;
        BitmapFactory.decodeByteArray(jpegData, 0, jpegData.length, bmpOptions);
        assertEquals(width, bmpOptions.outWidth);
        assertEquals(height, bmpOptions.outHeight);

        // Pixel decoding mode: decode whole image. check if the image data
        // is decodable here.
        assertNotNull(""Decoding jpeg failed"",
                BitmapFactory.decodeByteArray(jpegData, 0, jpegData.length));
        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + "".jpeg"";
            dumpFile(fileName, jpegData);
        }
    }

    private static void validateYuvData(byte[] yuvData, int width, int height, int format,
            long ts, String filePath) {
        checkYuvFormat(format);
        if (VERBOSE) Log.v(TAG, ""Validating YUV data"");
        int expectedSize = width * height * ImageFormat.getBitsPerPixel(format) / 8;
        assertEquals(""Yuv data doesn't match"", expectedSize, yuvData.length);

        // TODO: Can add data validation for test pattern.

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".yuv"";
            dumpFile(fileName, yuvData);
        }
    }

    private static void validateP010Data(byte[] p010Data, int width, int height, int format,
            long ts, String filePath) {
        if (VERBOSE) Log.v(TAG, ""Validating P010 data"");
        // The P010 10 bit samples are stored in two bytes so the size needs to be adjusted
        // accordingly.
        int bytesPerPixelRounded = (ImageFormat.getBitsPerPixel(format) + 7) / 8;
        int expectedSize = width * height * bytesPerPixelRounded;
        assertEquals(""P010 data doesn't match"", expectedSize, p010Data.length);

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".p010"";
            dumpFile(fileName, p010Data);
        }
    }
    private static void validateRaw16Data(byte[] rawData, int width, int height, int format,
            long ts, String filePath) {
        if (VERBOSE) Log.v(TAG, ""Validating raw data"");
        int expectedSize = width * height * ImageFormat.getBitsPerPixel(format) / 8;
        assertEquals(""Raw data doesn't match"", expectedSize, rawData.length);

        // TODO: Can add data validation for test pattern.

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".raw16"";
            dumpFile(fileName, rawData);
        }

        return;
    }

    private static void validateY8Data(byte[] rawData, int width, int height, int format,
            long ts, String filePath) {
        if (VERBOSE) Log.v(TAG, ""Validating Y8 data"");
        int expectedSize = width * height * ImageFormat.getBitsPerPixel(format) / 8;
        assertEquals(""Y8 data doesn't match"", expectedSize, rawData.length);

        // TODO: Can add data validation for test pattern.

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".y8"";
            dumpFile(fileName, rawData);
        }

        return;
    }

    private static void validateRawPrivateData(byte[] rawData, int width, int height,
            long ts, String filePath) {
        if (VERBOSE) Log.v(TAG, ""Validating private raw data"");
        // Expect each RAW pixel should occupy at least one byte and no more than 30 bytes
        int expectedSizeMin = width * height;
        int expectedSizeMax = width * height * 30;

        assertTrue(""Opaque RAW size "" + rawData.length + ""out of normal bound ["" +
                expectedSizeMin + "","" + expectedSizeMax + ""]"",
                expectedSizeMin <= rawData.length && rawData.length <= expectedSizeMax);

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".rawPriv"";
            dumpFile(fileName, rawData);
        }

        return;
    }

    private static void validateDepth16Data(byte[] depthData, int width, int height, int format,
            long ts, String filePath) {

        if (VERBOSE) Log.v(TAG, ""Validating depth16 data"");
        int expectedSize = width * height * ImageFormat.getBitsPerPixel(format) / 8;
        assertEquals(""Depth data doesn't match"", expectedSize, depthData.length);


        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".depth16"";
            dumpFile(fileName, depthData);
        }

        return;

    }

    private static void validateDepthPointCloudData(byte[] depthData, int width, int height, int format,
            long ts, String filePath) {

        if (VERBOSE) Log.v(TAG, ""Validating depth point cloud data"");

        // Can't validate size since it is variable

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".depth_point_cloud"";
            dumpFile(fileName, depthData);
        }

        return;

    }

    private static void validateHeicData(byte[] heicData, int width, int height, String filePath) {
        BitmapFactory.Options bmpOptions = new BitmapFactory.Options();
        // DecodeBound mode: only parse the frame header to get width/height.
        // it doesn't decode the pixel.
        bmpOptions.inJustDecodeBounds = true;
        BitmapFactory.decodeByteArray(heicData, 0, heicData.length, bmpOptions);
        assertEquals(width, bmpOptions.outWidth);
        assertEquals(height, bmpOptions.outHeight);

        // Pixel decoding mode: decode whole image. check if the image data
        // is decodable here.
        assertNotNull(""Decoding heic failed"",
                BitmapFactory.decodeByteArray(heicData, 0, heicData.length));
        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + "".heic"";
            dumpFile(fileName, heicData);
        }
    }

    public static <T> T getValueNotNull(CaptureResult result, CaptureResult.Key<T> key) {
        if (result == null) {
            throw new IllegalArgumentException(""Result must not be null"");
        }

        T value = result.get(key);
        assertNotNull(""Value of Key "" + key.getName() + ""shouldn't be null"", value);
        return value;
    }

    public static <T> T getValueNotNull(CameraCharacteristics characteristics,
            CameraCharacteristics.Key<T> key) {
        if (characteristics == null) {
            throw new IllegalArgumentException(""Camera characteristics must not be null"");
        }

        T value = characteristics.get(key);
        assertNotNull(""Value of Key "" + key.getName() + ""shouldn't be null"", value);
        return value;
    }

    /**
     * Get a crop region for a given zoom factor and center position.
     * <p>
     * The center position is normalized position in range of [0, 1.0], where
     * (0, 0) represents top left corner, (1.0. 1.0) represents bottom right
     * corner. The center position could limit the effective minimal zoom
     * factor, for example, if the center position is (0.75, 0.75), the
     * effective minimal zoom position becomes 2.0. If the requested zoom factor
     * is smaller than 2.0, a crop region with 2.0 zoom factor will be returned.
     * </p>
     * <p>
     * The aspect ratio of the crop region is maintained the same as the aspect
     * ratio of active array.
     * </p>
     *
     * @param zoomFactor The zoom factor to generate the crop region, it must be
     *            >= 1.0
     * @param center The normalized zoom center point that is in the range of [0, 1].
     * @param maxZoom The max zoom factor supported by this device.
     * @param activeArray The active array size of this device.
     * @return crop region for the given normalized center and zoom factor.
     */
    public static Rect getCropRegionForZoom(float zoomFactor, final PointF center,
            final float maxZoom, final Rect activeArray) {
        if (zoomFactor < 1.0) {
            throw new IllegalArgumentException(""zoom factor "" + zoomFactor + "" should be >= 1.0"");
        }
        if (center.x > 1.0 || center.x < 0) {
            throw new IllegalArgumentException(""center.x "" + center.x
                    + "" should be in range of [0, 1.0]"");
        }
        if (center.y > 1.0 || center.y < 0) {
            throw new IllegalArgumentException(""center.y "" + center.y
                    + "" should be in range of [0, 1.0]"");
        }
        if (maxZoom < 1.0) {
            throw new IllegalArgumentException(""max zoom factor "" + maxZoom + "" should be >= 1.0"");
        }
        if (activeArray == null) {
            throw new IllegalArgumentException(""activeArray must not be null"");
        }

        float minCenterLength = Math.min(Math.min(center.x, 1.0f - center.x),
                Math.min(center.y, 1.0f - center.y));
        float minEffectiveZoom =  0.5f / minCenterLength;
        if (minEffectiveZoom > maxZoom) {
            throw new IllegalArgumentException(""Requested center "" + center.toString() +
                    "" has minimal zoomable factor "" + minEffectiveZoom + "", which exceeds max""
                            + "" zoom factor "" + maxZoom);
        }

        if (zoomFactor < minEffectiveZoom) {
            Log.w(TAG, ""Requested zoomFactor "" + zoomFactor + "" < minimal zoomable factor ""
                    + minEffectiveZoom + "". It will be overwritten by "" + minEffectiveZoom);
            zoomFactor = minEffectiveZoom;
        }

        int cropCenterX = (int)(activeArray.width() * center.x);
        int cropCenterY = (int)(activeArray.height() * center.y);
        int cropWidth = (int) (activeArray.width() / zoomFactor);
        int cropHeight = (int) (activeArray.height() / zoomFactor);

        return new Rect(
                /*left*/cropCenterX - cropWidth / 2,
                /*top*/cropCenterY - cropHeight / 2,
                /*right*/ cropCenterX + cropWidth / 2,
                /*bottom*/cropCenterY + cropHeight / 2);
    }

    /**
     * Get AeAvailableTargetFpsRanges and sort them in descending order by max fps
     *
     * @param staticInfo camera static metadata
     * @return AeAvailableTargetFpsRanges in descending order by max fps
     */
    public static Range<Integer>[] getDescendingTargetFpsRanges(StaticMetadata staticInfo) {
        Range<Integer>[] fpsRanges = staticInfo.getAeAvailableTargetFpsRangesChecked();
        Arrays.sort(fpsRanges, new Comparator<Range<Integer>>() {
            public int compare(Range<Integer> r1, Range<Integer> r2) {
                return r2.getUpper() - r1.getUpper();
            }
        });
        return fpsRanges;
    }

    /**
     * Get AeAvailableTargetFpsRanges with max fps not exceeding 30
     *
     * @param staticInfo camera static metadata
     * @return AeAvailableTargetFpsRanges with max fps not exceeding 30
     */
    public static List<Range<Integer>> getTargetFpsRangesUpTo30(StaticMetadata staticInfo) {
        Range<Integer>[] fpsRanges = staticInfo.getAeAvailableTargetFpsRangesChecked();
        ArrayList<Range<Integer>> fpsRangesUpTo30 = new ArrayList<Range<Integer>>();
        for (Range<Integer> fpsRange : fpsRanges) {
            if (fpsRange.getUpper() <= 30) {
                fpsRangesUpTo30.add(fpsRange);
            }
        }
        return fpsRangesUpTo30;
    }

    /**
     * Get AeAvailableTargetFpsRanges with max fps greater than 30
     *
     * @param staticInfo camera static metadata
     * @return AeAvailableTargetFpsRanges with max fps greater than 30
     */
    public static List<Range<Integer>> getTargetFpsRangesGreaterThan30(StaticMetadata staticInfo) {
        Range<Integer>[] fpsRanges = staticInfo.getAeAvailableTargetFpsRangesChecked();
        ArrayList<Range<Integer>> fpsRangesGreaterThan30 = new ArrayList<Range<Integer>>();
        for (Range<Integer> fpsRange : fpsRanges) {
            if (fpsRange.getUpper() > 30) {
                fpsRangesGreaterThan30.add(fpsRange);
            }
        }
        return fpsRangesGreaterThan30;
    }

    /**
     * Calculate output 3A region from the intersection of input 3A region and cropped region.
     *
     * @param requestRegions The input 3A regions
     * @param cropRect The cropped region
     * @return expected 3A regions output in capture result
     */
    public static MeteringRectangle[] getExpectedOutputRegion(
            MeteringRectangle[] requestRegions, Rect cropRect){
        MeteringRectangle[] resultRegions = new MeteringRectangle[requestRegions.length];
        for (int i = 0; i < requestRegions.length; i++) {
            Rect requestRect = requestRegions[i].getRect();
            Rect resultRect = new Rect();
            boolean intersect = resultRect.setIntersect(requestRect, cropRect);
            resultRegions[i] = new MeteringRectangle(
                    resultRect,
                    intersect ? requestRegions[i].getMeteringWeight() : 0);
        }
        return resultRegions;
    }

    /**
     * Copy source image data to destination image.
     *
     * @param src The source image to be copied from.
     * @param dst The destination image to be copied to.
     * @throws IllegalArgumentException If the source and destination images have
     *             different format, size, or one of the images is not copyable.
     */
    public static void imageCopy(Image src, Image dst) {
        if (src == null || dst == null) {
            throw new IllegalArgumentException(""Images should be non-null"");
        }
        if (src.getFormat() != dst.getFormat()) {
            throw new IllegalArgumentException(""Src and dst images should have the same format"");
        }
        if (src.getFormat() == ImageFormat.PRIVATE ||
                dst.getFormat() == ImageFormat.PRIVATE) {
            throw new IllegalArgumentException(""PRIVATE format images are not copyable"");
        }

        Size srcSize = new Size(src.getWidth(), src.getHeight());
        Size dstSize = new Size(dst.getWidth(), dst.getHeight());
        if (!srcSize.equals(dstSize)) {
            throw new IllegalArgumentException(""source image size "" + srcSize + "" is different""
                    + "" with "" + ""destination image size "" + dstSize);
        }

        // TODO: check the owner of the dst image, it must be from ImageWriter, other source may
        // not be writable. Maybe we should add an isWritable() method in image class.

        Plane[] srcPlanes = src.getPlanes();
        Plane[] dstPlanes = dst.getPlanes();
        ByteBuffer srcBuffer = null;
        ByteBuffer dstBuffer = null;
        for (int i = 0; i < srcPlanes.length; i++) {
            srcBuffer = srcPlanes[i].getBuffer();
            dstBuffer = dstPlanes[i].getBuffer();
            int srcPos = srcBuffer.position();
            srcBuffer.rewind();
            dstBuffer.rewind();
            int srcRowStride = srcPlanes[i].getRowStride();
            int dstRowStride = dstPlanes[i].getRowStride();
            int srcPixStride = srcPlanes[i].getPixelStride();
            int dstPixStride = dstPlanes[i].getPixelStride();

            if (srcPixStride > 2 || dstPixStride > 2) {
                throw new IllegalArgumentException(""source pixel stride "" + srcPixStride +
                        "" with destination pixel stride "" + dstPixStride +
                        "" is not supported"");
            }

            if (srcRowStride == dstRowStride && srcPixStride == dstPixStride &&
                    srcPixStride == 1) {
                // Fast path, just copy the content in the byteBuffer all together.
                dstBuffer.put(srcBuffer);
            } else {
                Size effectivePlaneSize = getEffectivePlaneSizeForImage(src, i);
                int srcRowByteCount = srcRowStride;
                int dstRowByteCount = dstRowStride;
                byte[] srcDataRow = new byte[Math.max(srcRowStride, dstRowStride)];

                if (srcPixStride == dstPixStride && srcPixStride == 1) {
                    // Row by row copy case
                    for (int row = 0; row < effectivePlaneSize.getHeight(); row++) {
                        if (row == effectivePlaneSize.getHeight() - 1) {
                            // Special case for interleaved planes: need handle the last row
                            // carefully to avoid memory corruption. Check if we have enough bytes
                            // to copy.
                            srcRowByteCount = Math.min(srcRowByteCount, srcBuffer.remaining());
                            dstRowByteCount = Math.min(dstRowByteCount, dstBuffer.remaining());
                        }
                        srcBuffer.get(srcDataRow, /*offset*/0, srcRowByteCount);
                        dstBuffer.put(srcDataRow, /*offset*/0, dstRowByteCount);
                    }
                } else {
                    // Row by row per pixel copy case
                    byte[] dstDataRow = new byte[dstRowByteCount];
                    for (int row = 0; row < effectivePlaneSize.getHeight(); row++) {
                        if (row == effectivePlaneSize.getHeight() - 1) {
                            // Special case for interleaved planes: need handle the last row
                            // carefully to avoid memory corruption. Check if we have enough bytes
                            // to copy.
                            int remainingBytes = srcBuffer.remaining();
                            if (srcRowByteCount > remainingBytes) {
                                srcRowByteCount = remainingBytes;
                            }
                            remainingBytes = dstBuffer.remaining();
                            if (dstRowByteCount > remainingBytes) {
                                dstRowByteCount = remainingBytes;
                            }
                        }
                        srcBuffer.get(srcDataRow, /*offset*/0, srcRowByteCount);
                        int pos = dstBuffer.position();
                        dstBuffer.get(dstDataRow, /*offset*/0, dstRowByteCount);
                        dstBuffer.position(pos);
                        for (int x = 0; x < effectivePlaneSize.getWidth(); x++) {
                            dstDataRow[x * dstPixStride] = srcDataRow[x * srcPixStride];
                        }
                        dstBuffer.put(dstDataRow, /*offset*/0, dstRowByteCount);
                    }
                }
            }
            srcBuffer.position(srcPos);
            dstBuffer.rewind();
        }
    }

    private static Size getEffectivePlaneSizeForImage(Image image, int planeIdx) {
        switch (image.getFormat()) {
            case ImageFormat.YUV_420_888:
                if (planeIdx == 0) {
                    return new Size(image.getWidth(), image.getHeight());
                } else {
                    return new Size(image.getWidth() / 2, image.getHeight() / 2);
                }
            case ImageFormat.JPEG:
            case ImageFormat.RAW_SENSOR:
            case ImageFormat.RAW10:
            case ImageFormat.RAW12:
            case ImageFormat.DEPTH16:
                return new Size(image.getWidth(), image.getHeight());
            case ImageFormat.PRIVATE:
                return new Size(0, 0);
            default:
                throw new UnsupportedOperationException(
                        String.format(""Invalid image format %d"", image.getFormat()));
        }
    }

    /**
     * <p>
     * Checks whether the two images are strongly equal.
     * </p>
     * <p>
     * Two images are strongly equal if and only if the data, formats, sizes,
     * and timestamps are same. For {@link ImageFormat#PRIVATE PRIVATE} format
     * images, the image data is not not accessible thus the data comparison is
     * effectively skipped as the number of planes is zero.
     * </p>
     * <p>
     * Note that this method compares the pixel data even outside of the crop
     * region, which may not be necessary for general use case.
     * </p>
     *
     * @param lhsImg First image to be compared with.
     * @param rhsImg Second image to be compared with.
     * @return true if the two images are equal, false otherwise.
     * @throws IllegalArgumentException If either of image is null.
     */
    public static boolean isImageStronglyEqual(Image lhsImg, Image rhsImg) {
        if (lhsImg == null || rhsImg == null) {
            throw new IllegalArgumentException(""Images should be non-null"");
        }

        if (lhsImg.getFormat() != rhsImg.getFormat()) {
            Log.i(TAG, ""lhsImg format "" + lhsImg.getFormat() + "" is different with rhsImg format ""
                    + rhsImg.getFormat());
            return false;
        }

        if (lhsImg.getWidth() != rhsImg.getWidth()) {
            Log.i(TAG, ""lhsImg width "" + lhsImg.getWidth() + "" is different with rhsImg width ""
                    + rhsImg.getWidth());
            return false;
        }

        if (lhsImg.getHeight() != rhsImg.getHeight()) {
            Log.i(TAG, ""lhsImg height "" + lhsImg.getHeight() + "" is different with rhsImg height ""
                    + rhsImg.getHeight());
            return false;
        }

        if (lhsImg.getTimestamp() != rhsImg.getTimestamp()) {
            Log.i(TAG, ""lhsImg timestamp "" + lhsImg.getTimestamp()
                    + "" is different with rhsImg timestamp "" + rhsImg.getTimestamp());
            return false;
        }

        if (!lhsImg.getCropRect().equals(rhsImg.getCropRect())) {
            Log.i(TAG, ""lhsImg crop rect "" + lhsImg.getCropRect()
                    + "" is different with rhsImg crop rect "" + rhsImg.getCropRect());
            return false;
        }

        // Compare data inside of the image.
        Plane[] lhsPlanes = lhsImg.getPlanes();
        Plane[] rhsPlanes = rhsImg.getPlanes();
        ByteBuffer lhsBuffer = null;
        ByteBuffer rhsBuffer = null;
        for (int i = 0; i < lhsPlanes.length; i++) {
            lhsBuffer = lhsPlanes[i].getBuffer();
            rhsBuffer = rhsPlanes[i].getBuffer();
            lhsBuffer.rewind();
            rhsBuffer.rewind();
            // Special case for YUV420_888 buffer with different layout or
            // potentially differently interleaved U/V planes.
            if (lhsImg.getFormat() == ImageFormat.YUV_420_888 &&
                    (lhsPlanes[i].getPixelStride() != rhsPlanes[i].getPixelStride() ||
                     lhsPlanes[i].getRowStride() != rhsPlanes[i].getRowStride() ||
                     (lhsPlanes[i].getPixelStride() != 1))) {
                int width = getEffectivePlaneSizeForImage(lhsImg, i).getWidth();
                int height = getEffectivePlaneSizeForImage(lhsImg, i).getHeight();
                int rowSizeL = lhsPlanes[i].getRowStride();
                int rowSizeR = rhsPlanes[i].getRowStride();
                byte[] lhsRow = new byte[rowSizeL];
                byte[] rhsRow = new byte[rowSizeR];
                int pixStrideL = lhsPlanes[i].getPixelStride();
                int pixStrideR = rhsPlanes[i].getPixelStride();
                for (int r = 0; r < height; r++) {
                    if (r == height -1) {
                        rowSizeL = lhsBuffer.remaining();
                        rowSizeR = rhsBuffer.remaining();
                    }
                    lhsBuffer.get(lhsRow, /*offset*/0, rowSizeL);
                    rhsBuffer.get(rhsRow, /*offset*/0, rowSizeR);
                    for (int c = 0; c < width; c++) {
                        if (lhsRow[c * pixStrideL] != rhsRow[c * pixStrideR]) {
                            Log.i(TAG, String.format(
                                    ""byte buffers for plane %d row %d col %d don't match."",
                                    i, r, c));
                            return false;
                        }
                    }
                }
            } else {
                // Compare entire buffer directly
                if (!lhsBuffer.equals(rhsBuffer)) {
                    Log.i(TAG, ""byte buffers for plane "" +  i + "" don't match."");
                    return false;
                }
            }
        }

        return true;
    }

    /**
     * Set jpeg related keys in a capture request builder.
     *
     * @param builder The capture request builder to set the keys inl
     * @param exifData The exif data to set.
     * @param thumbnailSize The thumbnail size to set.
     * @param collector The camera error collector to collect errors.
     */
    public static void setJpegKeys(CaptureRequest.Builder builder, ExifTestData exifData,
            Size thumbnailSize, CameraErrorCollector collector) {
        builder.set(CaptureRequest.JPEG_THUMBNAIL_SIZE, thumbnailSize);
        builder.set(CaptureRequest.JPEG_GPS_LOCATION, exifData.gpsLocation);
        builder.set(CaptureRequest.JPEG_ORIENTATION, exifData.jpegOrientation);
        builder.set(CaptureRequest.JPEG_QUALITY, exifData.jpegQuality);
        builder.set(CaptureRequest.JPEG_THUMBNAIL_QUALITY,
                exifData.thumbnailQuality);

        // Validate request set and get.
        collector.expectEquals(""JPEG thumbnail size request set and get should match"",
                thumbnailSize, builder.get(CaptureRequest.JPEG_THUMBNAIL_SIZE));
        collector.expectTrue(""GPS locations request set and get should match."",
                areGpsFieldsEqual(exifData.gpsLocation,
                builder.get(CaptureRequest.JPEG_GPS_LOCATION)));
        collector.expectEquals(""JPEG orientation request set and get should match"",
                exifData.jpegOrientation,
                builder.get(CaptureRequest.JPEG_ORIENTATION));
        collector.expectEquals(""JPEG quality request set and get should match"",
                exifData.jpegQuality, builder.get(CaptureRequest.JPEG_QUALITY));
        collector.expectEquals(""JPEG thumbnail quality request set and get should match"",
                exifData.thumbnailQuality,
                builder.get(CaptureRequest.JPEG_THUMBNAIL_QUALITY));
    }

    /**
     * Simple validation of JPEG"	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 12 resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ImageWriterTest"	"testWriterFormatOverride"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ImageWriterTest.java"	""	"public void testWriterFormatOverride() throws Exception {
        int[] TEXTURE_TEST_FORMATS = {ImageFormat.YV12, ImageFormat.YUV_420_888};
        SurfaceTexture texture = new SurfaceTexture(/*random int*/1);
        texture.setDefaultBufferSize(640, 480);
        Surface surface = new Surface(texture);

        // Make sure that the default newInstance is still valid.
        ImageWriter defaultWriter = ImageWriter.newInstance(surface, MAX_NUM_IMAGES);
        Image defaultImage = defaultWriter.dequeueInputImage();
        defaultWriter.close();

        for (int format : TEXTURE_TEST_FORMATS) {
            // Override default buffer format of Surface texture to test format
            ImageWriter writer = ImageWriter.newInstance(surface, MAX_NUM_IMAGES, format);
            Image image = writer.dequeueInputImage();
            Log.i(TAG, ""testing format "" + format + "", got input image format "" +
                    image.getFormat());
            assertTrue(image.getFormat() == format);
            writer.close();
        }
    }

    private void readerWriterFormatTestByCamera(int format, boolean altFactoryMethod)
            throws Exception {
        List<Size> sizes = getSortedSizesForFormat(mCamera.getId(), mCameraManager, format, null);
        Size maxSize = sizes.get(0);
        if (VERBOSE) {
            Log.v(TAG, ""Testing size "" + maxSize);
        }

        // Create ImageReader for camera output.
        SimpleImageReaderListener listenerForCamera  = new SimpleImageReaderListener();
        if (altFactoryMethod) {
            createDefaultImageReader(maxSize, format, MAX_NUM_IMAGES,
                    HardwareBuffer.USAGE_CPU_READ_OFTEN, listenerForCamera);
        } else {
            createDefaultImageReader(maxSize, format, MAX_NUM_IMAGES, listenerForCamera);
        }

        if (VERBOSE) {
            Log.v(TAG, ""Created camera output ImageReader"");
        }

        // Create ImageReader for ImageWriter output
        SimpleImageReaderListener listenerForWriter  = new SimpleImageReaderListener();
        if (altFactoryMethod) {
            mReaderForWriter = createImageReader(
                    maxSize, format, MAX_NUM_IMAGES,
                    HardwareBuffer.USAGE_CPU_READ_OFTEN, listenerForWriter);
        } else {
            mReaderForWriter = createImageReader(
                    maxSize, format, MAX_NUM_IMAGES, listenerForWriter);
        }

        if (VERBOSE) {
            Log.v(TAG, ""Created ImageWriter output ImageReader"");
        }

        // Create ImageWriter
        Surface surface = mReaderForWriter.getSurface();
        assertNotNull(""Surface from ImageReader shouldn't be null"", surface);
        if (altFactoryMethod) {
            mWriter = ImageWriter.newInstance(surface, MAX_NUM_IMAGES, format);
        } else {
            mWriter = ImageWriter.newInstance(surface, MAX_NUM_IMAGES);
        }
        SimpleImageWriterListener writerImageListener = new SimpleImageWriterListener(mWriter);
        mWriter.setOnImageReleasedListener(writerImageListener, mHandler);

        // Start capture: capture 2 images.
        List<Surface> outputSurfaces = new ArrayList<Surface>();
        outputSurfaces.add(mReader.getSurface());
        CaptureRequest.Builder requestBuilder = prepareCaptureRequestForSurfaces(outputSurfaces,
                CameraDevice.TEMPLATE_PREVIEW);
        SimpleCaptureCallback captureListener = new SimpleCaptureCallback();
        // Capture 1st image.
        startCapture(requestBuilder.build(), /*repeating*/false, captureListener, mHandler);
        // Capture 2nd image.
        startCapture(requestBuilder.build(), /*repeating*/false, captureListener, mHandler);
        if (VERBOSE) {
            Log.v(TAG, ""Submitted 2 captures"");
        }

        // Image from the first ImageReader.
        Image cameraImage = null;
        // ImageWriter input image.
        Image inputImage = null;
        // Image from the second ImageReader.
        Image outputImage = null;
        assertTrue(""ImageWriter max images should be "" + MAX_NUM_IMAGES,
                mWriter.getMaxImages() == MAX_NUM_IMAGES);
        if (format == CAMERA_PRIVATE_FORMAT) {
            assertTrue(""First ImageReader format should be PRIVATE"",
                    mReader.getImageFormat() == CAMERA_PRIVATE_FORMAT);
            assertTrue(""Second ImageReader should be PRIVATE"",
                    mReaderForWriter.getImageFormat() == CAMERA_PRIVATE_FORMAT);
            assertTrue(""Format of first ImageReader should be PRIVATE"",
                    mReader.getImageFormat() == CAMERA_PRIVATE_FORMAT);
            assertTrue("" Format of second ImageReader should be PRIVATE"",
                    mReaderForWriter.getImageFormat() == CAMERA_PRIVATE_FORMAT);
            assertTrue("" Format of ImageWriter should be PRIVATE"",
                    mWriter.getFormat() == CAMERA_PRIVATE_FORMAT);

            // Validate 2 images
            validateOpaqueImages(maxSize, listenerForCamera, listenerForWriter, captureListener,
                    /*numImages*/2, writerImageListener);
        } else {
            // Test case 1: Explicit data copy, only applicable for explicit formats.

            // Get 1st image from first ImageReader, and copy the data to ImageWrtier input image
            cameraImage = listenerForCamera.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
            inputImage = mWriter.dequeueInputImage();
            inputImage.setTimestamp(cameraImage.getTimestamp());
            if (VERBOSE) {
                Log.v(TAG, ""Image is being copied"");
            }
            imageCopy(cameraImage, inputImage);
            if (VERBOSE) {
                Log.v(TAG, ""Image copy is done"");
            }
            mCollector.expectTrue(
                    ""ImageWriter 1st input image should match camera 1st output image"",
                    isImageStronglyEqual(inputImage, cameraImage));

            if (DEBUG) {
                String inputFileName = mDebugFileNameBase + ""/"" + maxSize + ""_image1_input.yuv"";
                dumpFile(inputFileName, getDataFromImage(inputImage));
            }

            // Image should be closed after queueInputImage call
            Plane closedPlane = inputImage.getPlanes()[0];
            ByteBuffer closedBuffer = closedPlane.getBuffer();
            mWriter.queueInputImage(inputImage);
            imageInvalidAccessTestAfterClose(inputImage, closedPlane, closedBuffer);

            outputImage = listenerForWriter.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
            mCollector.expectTrue(""ImageWriter 1st output image should match 1st camera image"",
                    isImageStronglyEqual(cameraImage, outputImage));
            if (DEBUG) {
                String img1FileName = mDebugFileNameBase + ""/"" + maxSize + ""_image1_camera.yuv"";
                String outputImg1FileName = mDebugFileNameBase + ""/"" + maxSize
                        + ""_image1_output.yuv"";
                dumpFile(img1FileName, getDataFromImage(cameraImage));
                dumpFile(outputImg1FileName, getDataFromImage(outputImage));
            }
            // No need to close inputImage, as it is sent to the surface after queueInputImage;
            cameraImage.close();
            outputImage.close();

            // Make sure ImageWriter listener callback is fired.
            writerImageListener.waitForImageReleased(CAPTURE_IMAGE_TIMEOUT_MS);

            // Test case 2: Directly inject the image into ImageWriter: works for all formats.

            // Get 2nd image and queue it directly to ImageWrier
            cameraImage = listenerForCamera.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
            // make a copy of image1 data, as it will be closed after queueInputImage;
            byte[] img1Data = getDataFromImage(cameraImage);
            if (DEBUG) {
                String img2FileName = mDebugFileNameBase + ""/"" + maxSize + ""_image2_camera.yuv"";
                dumpFile(img2FileName, img1Data);
            }

            // Image should be closed after queueInputImage call
            closedPlane = cameraImage.getPlanes()[0];
            closedBuffer = closedPlane.getBuffer();
            mWriter.queueInputImage(cameraImage);
            imageInvalidAccessTestAfterClose(cameraImage, closedPlane, closedBuffer);

            outputImage = listenerForWriter.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
            byte[] outputImageData = getDataFromImage(outputImage);

            mCollector.expectTrue(""ImageWriter 2nd output image should match camera ""
                    + ""2nd output image"", Arrays.equals(img1Data, outputImageData));

            if (DEBUG) {
                String outputImgFileName = mDebugFileNameBase + ""/"" + maxSize +
                        ""_image2_output.yuv"";
                dumpFile(outputImgFileName, outputImageData);
            }
            // No need to close inputImage, as it is sent to the surface after queueInputImage;
            outputImage.close();

            // Make sure ImageWriter listener callback is fired.
            writerImageListener.waitForImageReleased(CAPTURE_IMAGE_TIMEOUT_MS);
        }

        stopCapture(/*fast*/false);
        mReader.close();
        mReader = null;
        mReaderForWriter.close();
        mReaderForWriter = null;
        mWriter.close();
        mWriter = null;
    }

    private void validateOpaqueImages(Size maxSize, SimpleImageReaderListener listenerForCamera,
            SimpleImageReaderListener listenerForWriter, SimpleCaptureCallback captureListener,
            int numImages, SimpleImageWriterListener writerListener) throws Exception {
        Image cameraImage;
        Image outputImage;
        for (int i = 0; i < numImages; i++) {
            cameraImage = listenerForCamera.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
            CaptureResult result = captureListener.getCaptureResult(CAPTURE_IMAGE_TIMEOUT_MS);
            validateOpaqueImage(cameraImage, ""Opaque image "" + i + ""from camera: "", maxSize,
                    result);
            mWriter.queueInputImage(cameraImage);
            // Image should be closed after queueInputImage
            imageInvalidAccessTestAfterClose(cameraImage,
                    /*closedPlane*/null, /*closedBuffer*/null);
            outputImage = listenerForWriter.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
            validateOpaqueImage(outputImage, ""First Opaque image output by ImageWriter: "",
                    maxSize, result);
            outputImage.close();
            writerListener.waitForImageReleased(CAPTURE_IMAGE_TIMEOUT_MS);
        }
    }

    private void validateOpaqueImage(Image image, String msg, Size imageSize,
            CaptureResult result) {
        assertNotNull(""Opaque image Capture result should not be null"", result != null);
        mCollector.expectImageProperties(msg + ""Opaque "", image, CAMERA_PRIVATE_FORMAT,
                imageSize, result.get(CaptureResult.SENSOR_TIMESTAMP));
        mCollector.expectTrue(msg + ""Opaque image number planes should be zero"",
                image.getPlanes().length == 0);
    }
}"	""	""	"12"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraExtensionCharacteristicsTest"	"getCameraListener"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CameraExtensionCharacteristicsTest.java"	""	"/*
 *.
 */
package android.hardware.camera2.cts;

import android.content.Context;
import android.graphics.ImageFormat;
import android.graphics.SurfaceTexture;
import android.hardware.camera2.CameraExtensionCharacteristics;
import android.hardware.camera2.cts.helpers.StaticMetadata;
import android.hardware.camera2.cts.testcases.Camera2AndroidTestRule;
import android.renderscript.Allocation;
import android.util.Log;
import android.util.Range;
import android.util.Size;

import androidx.test.InstrumentationRegistry;

import com.android.compatibility.common.util.PropertyUtil;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

import static org.junit.Assert.*;

import org.junit.Rule;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.junit.runners.JUnit4;

@RunWith(JUnit4.class)
public class CameraExtensionCharacteristicsTest {
    private static final String TAG = ""CameraExtensionManagerTest"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);
    private static final List<Integer> EXTENSIONS = Arrays.asList(
            CameraExtensionCharacteristics.EXTENSION_AUTOMATIC,
            CameraExtensionCharacteristics.EXTENSION_BEAUTY,
            CameraExtensionCharacteristics.EXTENSION_BOKEH,
            CameraExtensionCharacteristics.EXTENSION_HDR,
            CameraExtensionCharacteristics.EXTENSION_NIGHT);

    private final Context mContext = InstrumentationRegistry.getTargetContext();

    @Rule
    public final Camera2AndroidTestRule mTestRule = new Camera2AndroidTestRule(mContext);

    private void openDevice(String cameraId) throws Exception {
        mTestRule.setCamera(CameraTestUtils.openCamera(
                mTestRule.getCameraManager(), cameraId,
                mTestRule.getCameraListener(), mTestRule.getHandler()));
        mTestRule.getCollector().setCameraId(cameraId);
        mTestRule.setStaticInfo(new StaticMetadata(
                mTestRule.getCameraManager().getCameraCharacteristics(cameraId),
                StaticMetadata.CheckLevel.ASSERT, /*collector*/null));
    }

    private <T> void verifySupportedExtension(CameraExtensionCharacteristics chars, String cameraId,
            Integer extension, Class<T> klass) {
        List<Size> availableSizes = chars.getExtensionSupportedSizes(extension, klass);
        assertTrue(String.format(""Supported extension %d on camera id: %s doesn't "" +
                        ""include any valid resolutions!"", extension, cameraId),
                (availableSizes != null) && (!availableSizes.isEmpty()));
    }

    private <T> void verifySupportedSizes(CameraExtensionCharacteristics chars, String cameraId,
            Integer extension, Class<T> klass) throws Exception {
        verifySupportedExtension(chars, cameraId, extension, klass);
        try {
            openDevice(cameraId);
            List<Size> extensionSizes = chars.getExtensionSupportedSizes(extension, klass);
            List<Size> cameraSizes = Arrays.asList(
                    mTestRule.getStaticInfo().getAvailableSizesForFormatChecked(ImageFormat.PRIVATE,
                            StaticMetadata.StreamDirection.Output));
            for (Size extensionSize : extensionSizes) {
                assertTrue(String.format(""Supported extension %d on camera id: %s advertises "" +
                                "" resolution %s unsupported by camera"", extension, cameraId,
                        extensionSize), cameraSizes.contains(extensionSize));
            }
        } finally {
            mTestRule.closeDevice(cameraId);
        }
    }

    private void verifySupportedSizes(CameraExtensionCharacteristics chars, String cameraId,
            Integer extension, int format) throws Exception {
        List<Size> extensionSizes = chars.getExtensionSupportedSizes(extension, format);
        assertFalse(String.format(""No available sizes for extension %d on camera id: %s "" +
                ""using format: %x"", extension, cameraId, format), extensionSizes.isEmpty());
        try {
            openDevice(cameraId);
            List<Size> cameraSizes = Arrays.asList(
                    mTestRule.getStaticInfo().getAvailableSizesForFormatChecked(format,
                            StaticMetadata.StreamDirection.Output));
            for (Size extensionSize : extensionSizes) {
                assertTrue(String.format(""Supported extension %d on camera id: %s advertises "" +
                                "" resolution %s unsupported by camera"", extension, cameraId,
                        extensionSize), cameraSizes.contains(extensionSize));
            }
        } finally {
            mTestRule.closeDevice(cameraId);
        }
    }

    private <T> void verifyUnsupportedExtension(CameraExtensionCharacteristics chars,
            Integer extension, Class<T> klass) {
        try {
            chars.getExtensionSupportedSizes(extension, klass);
            fail(""should get IllegalArgumentException due to unsupported extension"");
        } catch (IllegalArgumentException e) {
            // Expected
        }
    }"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraExtensionCharacteristicsTest"	"testIllegalArguments"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CameraExtensionCharacteristicsTest.java"	""	"public void testIllegalArguments() throws Exception {
        try {
            mTestRule.getCameraManager().getCameraExtensionCharacteristics(""InvalidCameraId!"");
            fail(""should get IllegalArgumentException due to invalid camera id"");
        } catch (IllegalArgumentException e) {
            // Expected
        }

        for (String id : mTestRule.getCameraIdsUnderTest()) {
            CameraExtensionCharacteristics extensionChars =
                    mTestRule.getCameraManager().getCameraExtensionCharacteristics(id);
            List<Integer> supportedExtensions = extensionChars.getSupportedExtensions();
            for (Integer extension : supportedExtensions) {
                try {
                    extensionChars.getExtensionSupportedSizes(extension, ImageFormat.UNKNOWN);
                    fail(""should get IllegalArgumentException due to invalid pixel format"");
                } catch (IllegalArgumentException e) {
                    // Expected
                }

                try {
                    List<Size> ret = extensionChars.getExtensionSupportedSizes(extension,
                            Allocation.class);
                    assertTrue(""should get empty resolution list for unsupported "" +
                            ""surface type"", ret.isEmpty());
                } catch (IllegalArgumentException e) {
                    fail(""should not get IllegalArgumentException due to unsupported surface "" +
                            ""type"");
                }
            }
        }
    }"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CaptureResultTest"	"testResultTimestamps"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CaptureResultTest.java"	""	"public void testResultTimestamps() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            ImageReader previewReader = null;
            ImageReader jpegReader = null;

            CaptureResult resultForNdk = null;

            SimpleImageReaderListener jpegListener = new SimpleImageReaderListener();
            SimpleImageReaderListener prevListener = new SimpleImageReaderListener();
            try {
                if (!mAllStaticInfo.get(id).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" does not support color outputs, skipping"");
                    continue;
                }

                openDevice(id);
                CaptureRequest.Builder previewBuilder =
                        mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
                CaptureRequest.Builder multiBuilder =
                        mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);

                // Create image reader and surface.
                Size previewSize = mOrderedPreviewSizes.get(0);
                Size jpegSize = mOrderedStillSizes.get(0);

                // Create ImageReaders.
                previewReader = makeImageReader(previewSize, ImageFormat.YUV_420_888,
                        MAX_NUM_IMAGES, prevListener, mHandler);
                jpegReader = makeImageReader(jpegSize, ImageFormat.JPEG,
                        MAX_NUM_IMAGES, jpegListener, mHandler);

                // Configure output streams with preview and jpeg streams.
                List<Surface> outputSurfaces = new ArrayList<>(Arrays.asList(
                        previewReader.getSurface(), jpegReader.getSurface()));

                SessionListener mockSessionListener = getMockSessionListener();

                CameraCaptureSession session = configureAndVerifySession(mockSessionListener,
                        mCamera, outputSurfaces, mHandler);

                // Configure the requests.
                previewBuilder.addTarget(previewReader.getSurface());
                multiBuilder.addTarget(previewReader.getSurface());
                multiBuilder.addTarget(jpegReader.getSurface());

                if (mStaticInfo.isEnableZslSupported()) {
                    // Turn off ZSL to ensure timestamps are increasing
                    previewBuilder.set(CaptureRequest.CONTROL_ENABLE_ZSL, false);
                    multiBuilder.set(CaptureRequest.CONTROL_ENABLE_ZSL, false);
                }

                CaptureCallback mockCaptureCallback = getMockCaptureListener();

                // Capture targeting only preview
                Pair<TotalCaptureResult, Long> result = captureAndVerifyResult(mockCaptureCallback,
                        session, previewBuilder.build(), mHandler);

                // Check if all timestamps are the same
                Image prevImage = prevListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                validateTimestamps(""Result 1"", result.first,
                        prevImage, result.second);
                prevImage.close();

                // Capture targeting both jpeg and preview
                Pair<TotalCaptureResult, Long> result2 = captureAndVerifyResult(mockCaptureCallback,
                        session, multiBuilder.build(), mHandler);

                // Check if all timestamps are the same
                prevImage = prevListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                Image jpegImage = jpegListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                validateTimestamps(""Result 2 Preview"", result2.first,
                        prevImage, result2.second);
                validateTimestamps(""Result 2 Jpeg"", result2.first,
                        jpegImage, result2.second);
                prevImage.close();
                jpegImage.close();

                // Check if timestamps are increasing
                mCollector.expectGreater(""Timestamps must be increasing."", result.second,
                        result2.second);

                // Capture two preview frames
                long startTime = SystemClock.elapsedRealtimeNanos();
                Pair<TotalCaptureResult, Long> result3 = captureAndVerifyResult(mockCaptureCallback,
                        session, previewBuilder.build(), mHandler);
                Pair<TotalCaptureResult, Long> result4 = captureAndVerifyResult(mockCaptureCallback,
                        session, previewBuilder.build(), mHandler);
                long clockDiff = SystemClock.elapsedRealtimeNanos() - startTime;
                long resultDiff = result4.second - result3.second;

                // Check if all timestamps are the same
                prevImage = prevListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                validateTimestamps(""Result 3"", result3.first,
                        prevImage, result3.second);
                prevImage.close();
                prevImage = prevListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                validateTimestamps(""Result 4"", result4.first,
                        prevImage, result4.second);
                prevImage.close();

                // Check that the timestamps monotonically increase at a reasonable rate
                mCollector.expectGreaterOrEqual(""Timestamps increase faster than system clock."",
                        resultDiff, clockDiff);
                mCollector.expectGreater(""Timestamps must be increasing."", result3.second,
                        result4.second);

                resultForNdk = result.first;
            } finally {
                closeDevice(id);
                closeImageReader(previewReader);
                closeImageReader(jpegReader);
            }

            mCollector.expectTrue(
                ""validateACameraMetadataFromCameraMetadataCriticalTagsNative failed"",
                validateACameraMetadataFromCameraMetadataCriticalTagsNative(resultForNdk,
                        resultForNdk.get(CaptureResult.SENSOR_TIMESTAMP)));

            long timestamp = resultForNdk.get(CaptureResult.SENSOR_TIMESTAMP);
            mCollector.expectTrue(
                ""stashACameraMetadataFromCameraMetadataNative failed"",
                stashACameraMetadataFromCameraMetadataNative(resultForNdk));

            // Try to drop the Java side object here
            resultForNdk = null;
            int[] block = null;
            final int count = 9;
            for (int i = 0; i < count + 1; i++) {
                block = new int[1000000];
                block[1000 + i] = i;

                Runtime.getRuntime().gc();
                Runtime.getRuntime().runFinalization();

                mCollector.expectTrue(""This should never fail"", block[1000 + i] == i);
            }
            mCollector.expectTrue(
                ""validateStashedACameraMetadataFromCameraMetadataNative failed"",
                validateStashedACameraMetadataFromCameraMetadataNative(timestamp));
            mCollector.expectTrue(""This should never fail"", block[1000 + count] == count);
        }
    }

    private void validateTimestamps(String msg, TotalCaptureResult result, Image resultImage,
                                    long captureTime) {
        mCollector.expectKeyValueEquals(result, CaptureResult.SENSOR_TIMESTAMP, captureTime);
        mCollector.expectEquals(msg + "": Capture timestamp must be same as resultImage timestamp"",
                resultImage.getTimestamp(), captureTime);
    }

    public static void validateCaptureResult(CameraErrorCollector errorCollector,
            SimpleCaptureCallback captureListener, StaticMetadata staticInfo,
            Map<String, StaticMetadata> allStaticInfo, List<String> requestedPhysicalIds,
            CaptureRequest.Builder requestBuilder, int numFramesVerified) throws Exception {
        // List that includes all public keys from CaptureResult
        List<CaptureResult.Key<?>> allKeys = getAllCaptureResultKeys();
        // Get the waived keys for current camera device
        List<CaptureResult.Key<?>> waiverKeys = getWaiverKeysForCamera(staticInfo);
        if (requestedPhysicalIds == null) {
            requestedPhysicalIds = new ArrayList<String>();
        }

        HashMap<String, List<CaptureResult.Key<?>>> physicalWaiverKeys = new HashMap<>();
        for (String physicalId : requestedPhysicalIds) {
            StaticMetadata physicalStaticInfo = allStaticInfo.get(physicalId);
            physicalWaiverKeys.put(physicalId, getWaiverKeysForCamera(physicalStaticInfo));
        }

        TotalCaptureResult result = null;
        // List of (frameNumber, physical camera Id) pairs
        ArrayList<Pair<Long, String>> droppedPhysicalResults = new ArrayList<>();
        for (int i = 0; i < numFramesVerified; i++) {
            result = captureListener.getTotalCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);

            Map<String, CaptureResult> physicalCaptureResults = result.getPhysicalCameraResults();
            ArrayList<String> droppedIds = new ArrayList<String>(requestedPhysicalIds);
            droppedIds.removeAll(physicalCaptureResults.keySet());
            for (String droppedId : droppedIds) {
                droppedPhysicalResults.add(
                        new Pair<Long, String>(result.getFrameNumber(), droppedId));
            }

            validateOneCaptureResult(errorCollector, staticInfo, waiverKeys, allKeys,
                    requestBuilder, result, null/*cameraId*/, i);
            for (String physicalId : physicalCaptureResults.keySet()) {
                StaticMetadata physicalStaticInfo = allStaticInfo.get(physicalId);
                validateOneCaptureResult(errorCollector, physicalStaticInfo,
                        physicalWaiverKeys.get(physicalId),
                        allKeys, null/*requestBuilder*/, physicalCaptureResults.get(physicalId),
                        physicalId, i);
            }
        }

        // Verify that all dropped physical camera results are notified via capture failure.
        while (captureListener.hasMoreFailures()) {
            ArrayList<CaptureFailure> failures =
                    captureListener.getCaptureFailures(/*maxNumFailures*/ 1);
            for (CaptureFailure failure : failures) {
                String failedPhysicalId = failure.getPhysicalCameraId();
                Long failedFrameNumber = failure.getFrameNumber();
                if (failedPhysicalId != null) {
                    droppedPhysicalResults.removeIf(
                            n -> n.equals(
                            new Pair<Long, String>(failedFrameNumber, failedPhysicalId)));
                }
            }
        }
        errorCollector.expectTrue(""Not all dropped results for physical cameras are notified"",
                droppedPhysicalResults.isEmpty());
    }

    private static void validateOneCaptureResult(CameraErrorCollector errorCollector,
            StaticMetadata staticInfo, List<CaptureResult.Key<?>> skippedKeys,
            List<CaptureResult.Key<?>> allKeys,
            CaptureRequest.Builder requestBuilder, CaptureResult result, String cameraId,
            int resultCount) throws Exception {
        String failMsg = ""Failed capture result "" + resultCount + "" test"";
        String cameraIdString = "" "";
        if (cameraId != null) {
            cameraIdString += ""for physical camera "" + cameraId;
        }
        boolean verifyMatchRequest = (requestBuilder != null);
        for (CaptureResult.Key<?> key : allKeys) {
            if (!skippedKeys.contains(key)) {
                /**
                 * Check the critical tags here.
                 * TODO: Can use the same key for request and result when request/result
                 * becomes symmetric (b/14059883). Then below check can be wrapped into
                 * a generic function.
                 */
                String msg = failMsg + cameraIdString + ""for key "" + key.getName();
                if (verifyMatchRequest) {
                    if (key.equals(CaptureResult.CONTROL_AE_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.CONTROL_AE_MODE),
                                result.get(CaptureResult.CONTROL_AE_MODE));
                    } else if (key.equals(CaptureResult.CONTROL_AF_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.CONTROL_AF_MODE),
                                result.get(CaptureResult.CONTROL_AF_MODE));
                    } else if (key.equals(CaptureResult.CONTROL_AWB_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.CONTROL_AWB_MODE),
                                result.get(CaptureResult.CONTROL_AWB_MODE));
                    } else if (key.equals(CaptureResult.CONTROL_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.CONTROL_MODE),
                                result.get(CaptureResult.CONTROL_MODE));
                    } else if (key.equals(CaptureResult.STATISTICS_FACE_DETECT_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.STATISTICS_FACE_DETECT_MODE),
                                result.get(CaptureResult.STATISTICS_FACE_DETECT_MODE));
                    } else if (key.equals(CaptureResult.NOISE_REDUCTION_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.NOISE_REDUCTION_MODE),
                                result.get(CaptureResult.NOISE_REDUCTION_MODE));
                    } else if (key.equals(CaptureResult.NOISE_REDUCTION_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.NOISE_REDUCTION_MODE),
                                result.get(CaptureResult.NOISE_REDUCTION_MODE));
                    } else if (key.equals(CaptureResult.REQUEST_PIPELINE_DEPTH)) {

                    } else if (key.equals(CaptureResult.STATISTICS_OIS_DATA_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.STATISTICS_OIS_DATA_MODE),
                                result.get(CaptureResult.STATISTICS_OIS_DATA_MODE));
                    } else if (key.equals(CaptureResult.DISTORTION_CORRECTION_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.DISTORTION_CORRECTION_MODE),
                                result.get(CaptureResult.DISTORTION_CORRECTION_MODE));
                    } else if (key.equals(CaptureResult.SENSOR_DYNAMIC_BLACK_LEVEL)) {
                        float[] blackLevel = errorCollector.expectKeyValueNotNull(
                                result, CaptureResult.SENSOR_DYNAMIC_BLACK_LEVEL);
                        if (blackLevel != null && staticInfo.isMonochromeCamera()) {
                            errorCollector.expectEquals(
                                    ""Monochrome camera dynamic blacklevel must be 2x2"",
                                    blackLevel.length, 4);
                            for (int index = 1; index < blackLevel.length; index++) {
                                errorCollector.expectEquals(
                                    ""Monochrome camera 2x2 channels blacklevel value must be the same."",
                                    blackLevel[index], blackLevel[0]);
                            }
                        }
                    } else {
                        // Only do non-null check for the rest of keys.
                        errorCollector.expectKeyValueNotNull(failMsg, result, key);
                    }
                } else {
                    // Only do non-null check for the rest of keys.
                    errorCollector.expectKeyValueNotNull(failMsg, result, key);
                }
            } else {
                // These keys should always be null
                if (key.equals(CaptureResult.CONTROL_AE_REGIONS)) {
                    errorCollector.expectNull(
                            ""Capture result contains AE regions but aeMaxRegions is 0""
                            + cameraIdString,
                            result.get(CaptureResult.CONTROL_AE_REGIONS));
                } else if (key.equals(CaptureResult.CONTROL_AWB_REGIONS)) {
                    errorCollector.expectNull(
                            ""Capture result contains AWB regions but awbMaxRegions is 0""
                            + cameraIdString,
                            result.get(CaptureResult.CONTROL_AWB_REGIONS));
                } else if (key.equals(CaptureResult.CONTROL_AF_REGIONS)) {
                    errorCollector.expectNull(
                            ""Capture result contains AF regions but afMaxRegions is 0""
                            + cameraIdString,
                            result.get(CaptureResult.CONTROL_AF_REGIONS));
                }
            }
        }
    }

    /*
     * Add waiver keys per camera device hardware level and capability.
     *
     * Must be called after camera device is opened.
     */
    private static List<CaptureResult.Key<?>> getWaiverKeysForCamera(StaticMetadata staticInfo) {
        List<CaptureResult.Key<?>> waiverKeys = new ArrayList<>();

        // Global waiver keys
        waiverKeys.add(CaptureResult.JPEG_GPS_LOCATION);
        waiverKeys.add(CaptureResult.JPEG_ORIENTATION);
        waiverKeys.add(CaptureResult.JPEG_QUALITY);
        waiverKeys.add(CaptureResult.JPEG_THUMBNAIL_QUALITY);
        waiverKeys.add(CaptureResult.JPEG_THUMBNAIL_SIZE);

        if (!staticInfo.isUltraHighResolutionSensor()) {
            waiverKeys.add(CaptureResult.SENSOR_PIXEL_MODE);
            waiverKeys.add(CaptureResult.SENSOR_RAW_BINNING_FACTOR_USED);
        }

        // Keys only present when corresponding control is on are being
        // verified in its own functional test
        // Only present in certain tonemap mode. Test in CaptureRequestTest.
        waiverKeys.add(CaptureResult.TONEMAP_CURVE);
        waiverKeys.add(CaptureResult.TONEMAP_GAMMA);
        waiverKeys.add(CaptureResult.TONEMAP_PRESET_CURVE);
        // Only present when test pattern mode is SOLID_COLOR.
        // TODO: verify this key in test pattern test later
        waiverKeys.add(CaptureResult.SENSOR_TEST_PATTERN_DATA);
        // Only present when STATISTICS_LENS_SHADING_MAP_MODE is ON
        waiverKeys.add(CaptureResult.STATISTICS_LENS_SHADING_CORRECTION_MAP);
        // Only present when STATISTICS_INFO_AVAILABLE_HOT_PIXEL_MAP_MODES is ON
        waiverKeys.add(CaptureResult.STATISTICS_HOT_PIXEL_MAP);
        // Only present when face detection is on
        waiverKeys.add(CaptureResult.STATISTICS_FACES);
        // Only present in reprocessing capture result.
        waiverKeys.add(CaptureResult.REPROCESS_EFFECTIVE_EXPOSURE_FACTOR);

        // LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID not required if key is not supported.
        if (!staticInfo.isLogicalMultiCamera() ||
                !staticInfo.isActivePhysicalCameraIdSupported()) {
            waiverKeys.add(CaptureResult.LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID);
        }

        //Keys not required if RAW is not supported
        if (!staticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
            waiverKeys.add(CaptureResult.SENSOR_NEUTRAL_COLOR_POINT);
            waiverKeys.add(CaptureResult.SENSOR_GREEN_SPLIT);
            waiverKeys.add(CaptureResult.SENSOR_NOISE_PROFILE);
        } else if (staticInfo.isMonochromeCamera()) {
            waiverKeys.add(CaptureResult.SENSOR_NEUTRAL_COLOR_POINT);
            waiverKeys.add(CaptureResult.SENSOR_GREEN_SPLIT);
        }

        boolean calibrationReported = staticInfo.areKeysAvailable(
                CameraCharacteristics.LENS_POSE_ROTATION,
                CameraCharacteristics.LENS_POSE_TRANSLATION,
                CameraCharacteristics.LENS_INTRINSIC_CALIBRATION);

        // If any of distortion coefficients is reported in CameraCharacteristics, HAL must
        // also report (one of) them in CaptureResult
        boolean distortionReported = 
                staticInfo.areKeysAvailable(
                        CameraCharacteristics.LENS_RADIAL_DISTORTION) || 
                staticInfo.areKeysAvailable(
                        CameraCharacteristics.LENS_DISTORTION);

        //Keys for lens distortion correction
        boolean distortionCorrectionSupported = staticInfo.isDistortionCorrectionSupported();
        if (!distortionCorrectionSupported) {
            waiverKeys.add(CaptureResult.DISTORTION_CORRECTION_MODE);
        }

        boolean mustReportDistortion = true;
        // These keys must present on either DEPTH or distortion correction devices
        if (!staticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_DEPTH_OUTPUT) &&
                !distortionCorrectionSupported &&
                !distortionReported) {
            mustReportDistortion = false;
            waiverKeys.add(CaptureResult.LENS_RADIAL_DISTORTION);
            waiverKeys.add(CaptureResult.LENS_DISTORTION);
        } else {
            // Radial distortion doesn't need to be present for new devices, or old devices that
            // opt in the new lens distortion tag.
            CameraCharacteristics c = staticInfo.getCharacteristics();
            if (Build.VERSION.DEVICE_INITIAL_SDK_INT > Build.VERSION_CODES.O_MR1 ||
                    c.get(CameraCharacteristics.LENS_DISTORTION) != null) {
                waiverKeys.add(CaptureResult.LENS_RADIAL_DISTORTION);
            }
        }

        // Calibration keys must exist for
        //   - DEPTH capable devices
        //   - Devices that reports calibration keys in static metadata
        //   - Devices that reports lens distortion keys in static metadata
        if (!staticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_DEPTH_OUTPUT) &&
                !calibrationReported && !mustReportDistortion) {
            waiverKeys.add(CaptureResult.LENS_POSE_ROTATION);
            waiverKeys.add(CaptureResult.LENS_POSE_TRANSLATION);
            waiverKeys.add(CaptureResult.LENS_INTRINSIC_CALIBRATION);
        }

        // Waived if RAW output is not supported
        int[] outputFormats = staticInfo.getAvailableFormats(
                StaticMetadata.StreamDirection.Output);
        boolean supportRaw = false;
        for (int format : outputFormats) {
            if (format == ImageFormat.RAW_SENSOR || format == ImageFormat.RAW10 ||
                    format == ImageFormat.RAW12 || format == ImageFormat.RAW_PRIVATE) {
                supportRaw = true;
                break;
            }
        }
        if (!supportRaw) {
            waiverKeys.add(CaptureResult.CONTROL_POST_RAW_SENSITIVITY_BOOST);
        }

        // Waived if MONOCHROME capability
        if (staticInfo.isMonochromeCamera()) {
            waiverKeys.add(CaptureResult.COLOR_CORRECTION_MODE);
            waiverKeys.add(CaptureResult.COLOR_CORRECTION_TRANSFORM);
            waiverKeys.add(CaptureResult.COLOR_CORRECTION_GAINS);
        }

        if (staticInfo.getAeMaxRegionsChecked() == 0) {
            waiverKeys.add(CaptureResult.CONTROL_AE_REGIONS);
        }
        if (staticInfo.getAwbMaxRegionsChecked() == 0) {
            waiverKeys.add(CaptureResult.CONTROL_AWB_REGIONS);
        }
        if (staticInfo.getAfMaxRegionsChecked() == 0) {
            waiverKeys.add(CaptureResult.CONTROL_AF_REGIONS);
        }

        // Keys for dynamic black/white levels
        if (!staticInfo.isOpticalBlackRegionSupported()) {
            waiverKeys.add(CaptureResult.SENSOR_DYNAMIC_BLACK_LEVEL);
            waiverKeys.add(CaptureResult.SENSOR_DYNAMIC_WHITE_LEVEL);
        }

        if (!staticInfo.isEnableZslSupported()) {
            waiverKeys.add(CaptureResult.CONTROL_ENABLE_ZSL);
        }

        if (!staticInfo.isAfSceneChangeSupported()) {
            waiverKeys.add(CaptureResult.CONTROL_AF_SCENE_CHANGE);
        }

        if (!staticInfo.isOisDataModeSupported()) {
            waiverKeys.add(CaptureResult.STATISTICS_OIS_DATA_MODE);
            waiverKeys.add(CaptureResult.STATISTICS_OIS_SAMPLES);
        }

        if (staticInfo.getAvailableExtendedSceneModeCapsChecked().length == 0) {
            waiverKeys.add(CaptureResult.CONTROL_EXTENDED_SCENE_MODE);
        }

        if (!staticInfo.isRotateAndCropSupported()) {
            waiverKeys.add(CaptureResult.SCALER_ROTATE_AND_CROP);
        }

        if (staticInfo.isHardwareLevelAtLeastFull()) {
            return waiverKeys;
        }

        /*
         * Hardware Level = LIMITED or LEGACY
         */
        // Key not present if certain control is not supported
        if (!staticInfo.isColorCorrectionSupported()) {
            waiverKeys.add(CaptureResult.COLOR_CORRECTION_GAINS);
            waiverKeys.add(CaptureResult.COLOR_CORRECTION_MODE);
            waiverKeys.add(CaptureResult.COLOR_CORRECTION_TRANSFORM);
        }

        if (!staticInfo.isManualColorAberrationControlSupported()) {
            waiverKeys.add(CaptureResult.COLOR_CORRECTION_ABERRATION_MODE);
        }

        if (!staticInfo.isManualToneMapSupported()) {
            waiverKeys.add(CaptureResult.TONEMAP_MODE);
        }

        if (!staticInfo.isEdgeModeControlSupported()) {
            waiverKeys.add(CaptureResult.EDGE_MODE);
        }

        if (!staticInfo.isHotPixelMapModeControlSupported()) {
            waiverKeys.add(CaptureResult.HOT_PIXEL_MODE);
        }

        if (!staticInfo.isNoiseReductionModeControlSupported()) {
            waiverKeys.add(CaptureResult.NOISE_REDUCTION_MODE);
        }

        if (!staticInfo.isManualLensShadingMapSupported()) {
            waiverKeys.add(CaptureResult.SHADING_MODE);
        }

        //Keys not required if neither MANUAL_SENSOR nor READ_SENSOR_SETTINGS is supported
        if (!staticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR) &&
            !staticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS)) {
            waiverKeys.add(CaptureResult.SENSOR_EXPOSURE_TIME);
            waiverKeys.add(CaptureResult.SENSOR_SENSITIVITY);
            waiverKeys.add(CaptureResult.LENS_FOCUS_DISTANCE);
            waiverKeys.add(CaptureResult.LENS_APERTURE);
        }

        if (!staticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
            waiverKeys.add(CaptureResult.SENSOR_FRAME_DURATION);
            waiverKeys.add(CaptureResult.BLACK_LEVEL_LOCK);
            waiverKeys.add(CaptureResult.LENS_FOCUS_RANGE);
            waiverKeys.add(CaptureResult.LENS_STATE);
            waiverKeys.add(CaptureResult.LENS_FILTER_DENSITY);
        }

        if (staticInfo.isHardwareLevelLimited() && staticInfo.isColorOutputSupported()) {
            return waiverKeys;
        }

        /*
         * Hardware Level = EXTERNAL
         */
        if (staticInfo.isExternalCamera()) {
            waiverKeys.add(CaptureResult.LENS_FOCAL_LENGTH);
            waiverKeys.add(CaptureResult.SENSOR_TEST_PATTERN_MODE);
            waiverKeys.add(CaptureResult.SENSOR_ROLLING_SHUTTER_SKEW);
        }

        if (staticInfo.isExternalCamera() && staticInfo.isColorOutputSupported()) {
            return waiverKeys;
        }

        /*
         * Hardware Level = LEGACY or no regular output is supported
         */
        waiverKeys.add(CaptureResult.CONTROL_AE_PRECAPTURE_TRIGGER);
        waiverKeys.add(CaptureResult.CONTROL_AE_STATE);
        waiverKeys.add(CaptureResult.CONTROL_AWB_STATE);
        waiverKeys.add(CaptureResult.FLASH_STATE);
        waiverKeys.add(CaptureResult.LENS_OPTICAL_STABILIZATION_MODE);
        waiverKeys.add(CaptureResult.SENSOR_ROLLING_SHUTTER_SKEW);
        waiverKeys.add(CaptureResult.STATISTICS_LENS_SHADING_MAP_MODE);
        waiverKeys.add(CaptureResult.STATISTICS_SCENE_FLICKER);
        waiverKeys.add(CaptureResult.STATISTICS_HOT_PIXEL_MAP_MODE);
        waiverKeys.add(CaptureResult.CONTROL_AE_TARGET_FPS_RANGE);
        waiverKeys.add(CaptureResult.CONTROL_AF_TRIGGER);

        if (staticInfo.isHardwareLevelLegacy()) {
            return waiverKeys;
        }

        /*
         * Regular output not supported, only depth, waive color-output-related keys
         */
        waiverKeys.add(CaptureResult.CONTROL_SCENE_MODE);
        waiverKeys.add(CaptureResult.CONTROL_EFFECT_MODE);
        waiverKeys.add(CaptureResult.CONTROL_VIDEO_STABILIZATION_MODE);
        waiverKeys.add(CaptureResult.SENSOR_TEST_PATTERN_MODE);
        waiverKeys.add(CaptureResult.NOISE_REDUCTION_MODE);
        waiverKeys.add(CaptureResult.COLOR_CORRECTION_ABERRATION_MODE);
        waiverKeys.add(CaptureResult.CONTROL_AE_ANTIBANDING_MODE);
        waiverKeys.add(CaptureResult.CONTROL_AE_EXPOSURE_COMPENSATION);
        waiverKeys.add(CaptureResult.CONTROL_AE_LOCK);
        waiverKeys.add(CaptureResult.CONTROL_AE_MODE);
        waiverKeys.add(CaptureResult.CONTROL_AF_MODE);
        waiverKeys.add(CaptureResult.CONTROL_AWB_MODE);
        waiverKeys.add(CaptureResult.CONTROL_AWB_LOCK);
        waiverKeys.add(CaptureResult.CONTROL_ZOOM_RATIO);
        waiverKeys.add(CaptureResult.STATISTICS_FACE_DETECT_MODE);
        waiverKeys.add(CaptureResult.FLASH_MODE);
        waiverKeys.add(CaptureResult.SCALER_CROP_REGION);
        waiverKeys.add(CaptureResult.SCALER_ROTATE_AND_CROP);

        return waiverKeys;
    }

    /**
     * A capture listener implementation for collecting both partial and total results.
     *
     * <p> This is not a full-blown class and has some implicit assumptions. The class groups
     * capture results by capture request, so the user must guarantee each request this listener
     * is listening is unique. This class is not thread safe, so don't attach an instance object
     * with multiple handlers.</p>
     * */
    private static class TotalAndPartialResultListener
            extends CameraCaptureSession.CaptureCallback {
        static final int ERROR_DUPLICATED_REQUEST = 1 << 0;
        static final int ERROR_WRONG_CALLBACK_ORDER = 1 << 1;

        private final LinkedBlockingQueue<Pair<TotalCaptureResult, List<CaptureResult>> > mQueue =
                new LinkedBlockingQueue<>();
        private final HashMap<CaptureRequest, List<CaptureResult>> mPartialResultsMap =
                new HashMap<CaptureRequest, List<CaptureResult>>();
        private final HashSet<CaptureRequest> completedRequests = new HashSet<>();
        private int errorCode = 0;

        @Override
        public void onCaptureStarted(
            CameraCaptureSession session, CaptureRequest request, long timestamp, long frameNumber)
        {
            checkCallbackOrder(request);
            createMapEntryIfNecessary(request);
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                TotalCaptureResult result) {
            try {
                List<CaptureResult> partialResultsList = mPartialResultsMap.get(request);
                if (partialResultsList == null) {
                    Log.w(TAG, ""onCaptureCompleted: unknown request"");
                }
                mQueue.put(new Pair<TotalCaptureResult, List<CaptureResult>>(
                        result, partialResultsList));
                mPartialResultsMap.remove(request);
                boolean newEntryAdded = completedRequests.add(request);
                if (!newEntryAdded) {
                    Integer frame = (Integer) request.getTag();
                    Log.e(TAG, ""Frame "" + frame + ""ERROR_DUPLICATED_REQUEST"");
                    errorCode |= ERROR_DUPLICATED_REQUEST;
                }
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureCompleted"");
            }
        }

        @Override
        public void onCaptureProgressed(CameraCaptureSession session, CaptureRequest request,
                CaptureResult partialResult) {
            createMapEntryIfNecessary(request);
            List<CaptureResult> partialResultsList = mPartialResultsMap.get(request);
            partialResultsList.add(partialResult);
        }

        private void createMapEntryIfNecessary(CaptureRequest request) {
            if (!mPartialResultsMap.containsKey(request)) {
                // create a new entry in the map
                mPartialResultsMap.put(request, new ArrayList<CaptureResult>());
            }
        }

        private void checkCallbackOrder(CaptureRequest request) {
            if (completedRequests.contains(request)) {
                Integer frame = (Integer) request.getTag();
                Log.e(TAG, ""Frame "" + frame + ""ERROR_WRONG_CALLBACK_ORDER"");
                errorCode |= ERROR_WRONG_CALLBACK_ORDER;
            }
        }

        public Pair<TotalCaptureResult, List<CaptureResult>> getCaptureResultPairs(long timeout) {
            try {
                Pair<TotalCaptureResult, List<CaptureResult>> result =
                        mQueue.poll(timeout, TimeUnit.MILLISECONDS);
                assertNotNull(""Wait for a capture result timed out in "" + timeout + ""ms"", result);
                return result;
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }
        }

        public int getErrorCode() {
            return errorCode;
        }
    }

    // Returns true if `result` has timestamp `sensorTimestamp` when queried from the NDK via
    // ACameraMetadata_fromCameraMetadata().
    private static native boolean validateACameraMetadataFromCameraMetadataCriticalTagsNative(
        CaptureResult result, long sensorTimestamp);

    // First stash a native ACameraMetadata created from a capture result, then compare the stored value
    // to the passed-in timestamp.
    private static native boolean stashACameraMetadataFromCameraMetadataNative(CaptureResult result);
    private static native boolean validateStashedACameraMetadataFromCameraMetadataNative(long timestamp);

    /**
     * TODO: Use CameraCharacteristics.getAvailableCaptureResultKeys() once we can filter out
     * @hide keys.
     *
     */

    /*@O~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~
     * The key entries below this point are generated from metadata
     * definitions in /system/media/camera/docs. Do not modify by hand or
     * modify the comment blocks at the start or end.
     *~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~*/

    private static List<CaptureResult.Key<?>> getAllCaptureResultKeys() {
        ArrayList<CaptureResult.Key<?>> resultKeys = new ArrayList<CaptureResult.Key<?>>();
        resultKeys.add(CaptureResult.COLOR_CORRECTION_MODE);
        resultKeys.add(CaptureResult.COLOR_CORRECTION_TRANSFORM);
        resultKeys.add(CaptureResult.COLOR_CORRECTION_GAINS);
        resultKeys.add(CaptureResult.COLOR_CORRECTION_ABERRATION_MODE);
        resultKeys.add(CaptureResult.CONTROL_AE_ANTIBANDING_MODE);
        resultKeys.add(CaptureResult.CONTROL_AE_EXPOSURE_COMPENSATION);
        resultKeys.add(CaptureResult.CONTROL_AE_LOCK);
        resultKeys.add(CaptureResult.CONTROL_AE_MODE);
        resultKeys.add(CaptureResult.CONTROL_AE_REGIONS);
        resultKeys.add(CaptureResult.CONTROL_AE_TARGET_FPS_RANGE);
        resultKeys.add(CaptureResult.CONTROL_AE_PRECAPTURE_TRIGGER);
        resultKeys.add(CaptureResult.CONTROL_AF_MODE);
        resultKeys.add(CaptureResult.CONTROL_AF_REGIONS);
        resultKeys.add(CaptureResult.CONTROL_AF_TRIGGER);
        resultKeys.add(CaptureResult.CONTROL_AWB_LOCK);
        resultKeys.add(CaptureResult.CONTROL_AWB_MODE);
        resultKeys.add(CaptureResult.CONTROL_AWB_REGIONS);
        resultKeys.add(CaptureResult.CONTROL_CAPTURE_INTENT);
        resultKeys.add(CaptureResult.CONTROL_EFFECT_MODE);
        resultKeys.add(CaptureResult.CONTROL_MODE);
        resultKeys.add(CaptureResult.CONTROL_SCENE_MODE);
        resultKeys.add(CaptureResult.CONTROL_VIDEO_STABILIZATION_MODE);
        resultKeys.add(CaptureResult.CONTROL_AE_STATE);
        resultKeys.add(CaptureResult.CONTROL_AF_STATE);
        resultKeys.add(CaptureResult.CONTROL_AWB_STATE);
        resultKeys.add(CaptureResult.CONTROL_POST_RAW_SENSITIVITY_BOOST);
        resultKeys.add(CaptureResult.CONTROL_ENABLE_ZSL);
        resultKeys.add(CaptureResult.CONTROL_AF_SCENE_CHANGE);
        resultKeys.add(CaptureResult.CONTROL_EXTENDED_SCENE_MODE);
        resultKeys.add(CaptureResult.CONTROL_ZOOM_RATIO);
        resultKeys.add(CaptureResult.EDGE_MODE);
        resultKeys.add(CaptureResult.FLASH_MODE);
        resultKeys.add(CaptureResult.FLASH_STATE);
        resultKeys.add(CaptureResult.HOT_PIXEL_MODE);
        resultKeys.add(CaptureResult.JPEG_GPS_LOCATION);
        resultKeys.add(CaptureResult.JPEG_ORIENTATION);
        resultKeys.add(CaptureResult.JPEG_QUALITY);
        resultKeys.add(CaptureResult.JPEG_THUMBNAIL_QUALITY);
        resultKeys.add(CaptureResult.JPEG_THUMBNAIL_SIZE);
        resultKeys.add(CaptureResult.LENS_APERTURE);
        resultKeys.add(CaptureResult.LENS_FILTER_DENSITY);
        resultKeys.add(CaptureResult.LENS_FOCAL_LENGTH);
        resultKeys.add(CaptureResult.LENS_FOCUS_DISTANCE);
        resultKeys.add(CaptureResult.LENS_OPTICAL_STABILIZATION_MODE);
        resultKeys.add(CaptureResult.LENS_POSE_ROTATION);
        resultKeys.add(CaptureResult.LENS_POSE_TRANSLATION);
        resultKeys.add(CaptureResult.LENS_FOCUS_RANGE);
        resultKeys.add(CaptureResult.LENS_STATE);
        resultKeys.add(CaptureResult.LENS_INTRINSIC_CALIBRATION);
        resultKeys.add(CaptureResult.LENS_RADIAL_DISTORTION);
        resultKeys.add(CaptureResult.LENS_DISTORTION);
        resultKeys.add(CaptureResult.NOISE_REDUCTION_MODE);
        resultKeys.add(CaptureResult.REQUEST_PIPELINE_DEPTH);
        resultKeys.add(CaptureResult.SCALER_CROP_REGION);
        resultKeys.add(CaptureResult.SCALER_ROTATE_AND_CROP);
        resultKeys.add(CaptureResult.SENSOR_EXPOSURE_TIME);
        resultKeys.add(CaptureResult.SENSOR_FRAME_DURATION);
        resultKeys.add(CaptureResult.SENSOR_SENSITIVITY);
        resultKeys.add(CaptureResult.SENSOR_TIMESTAMP);
        resultKeys.add(CaptureResult.SENSOR_NEUTRAL_COLOR_POINT);
        resultKeys.add(CaptureResult.SENSOR_NOISE_PROFILE);
        resultKeys.add(CaptureResult.SENSOR_GREEN_SPLIT);
        resultKeys.add(CaptureResult.SENSOR_TEST_PATTERN_DATA);
        resultKeys.add(CaptureResult.SENSOR_TEST_PATTERN_MODE);
        resultKeys.add(CaptureResult.SENSOR_ROLLING_SHUTTER_SKEW);
        resultKeys.add(CaptureResult.SENSOR_DYNAMIC_BLACK_LEVEL);
        resultKeys.add(CaptureResult.SENSOR_DYNAMIC_WHITE_LEVEL);
        resultKeys.add(CaptureResult.SENSOR_PIXEL_MODE);
        resultKeys.add(CaptureResult.SENSOR_RAW_BINNING_FACTOR_USED);
        resultKeys.add(CaptureResult.SHADING_MODE);
        resultKeys.add(CaptureResult.STATISTICS_FACE_DETECT_MODE);
        resultKeys.add(CaptureResult.STATISTICS_HOT_PIXEL_MAP_MODE);
        resultKeys.add(CaptureResult.STATISTICS_FACES);
        resultKeys.add(CaptureResult.STATISTICS_LENS_SHADING_CORRECTION_MAP);
        resultKeys.add(CaptureResult.STATISTICS_SCENE_FLICKER);
        resultKeys.add(CaptureResult.STATISTICS_HOT_PIXEL_MAP);
        resultKeys.add(CaptureResult.STATISTICS_LENS_SHADING_MAP_MODE);
        resultKeys.add(CaptureResult.STATISTICS_OIS_DATA_MODE);
        resultKeys.add(CaptureResult.STATISTICS_OIS_SAMPLES);
        resultKeys.add(CaptureResult.TONEMAP_CURVE);
        resultKeys.add(CaptureResult.TONEMAP_MODE);
        resultKeys.add(CaptureResult.TONEMAP_GAMMA);
        resultKeys.add(CaptureResult.TONEMAP_PRESET_CURVE);
        resultKeys.add(CaptureResult.BLACK_LEVEL_LOCK);
        resultKeys.add(CaptureResult.REPROCESS_EFFECTIVE_EXPOSURE_FACTOR);
        resultKeys.add(CaptureResult.LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID);
        resultKeys.add(CaptureResult.DISTORTION_CORRECTION_MODE);

        return resultKeys;
    }

    /*~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~
     * End generated code
     *~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~O@*/
}"	""	""	"12 resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.SurfaceViewPreviewTest"	"testDeferredSurfaces"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/SurfaceViewPreviewTest.java"	""	"public void testDeferredSurfaces() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                StaticMetadata staticInfo = mAllStaticInfo.get(mCameraIdsUnderTest[i]);
                if (staticInfo.isHardwareLevelLegacy()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] + "" is legacy, skipping"");
                    continue;
                }
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }

                openDevice(mCameraIdsUnderTest[i]);
                testDeferredSurfacesByCamera(mCameraIdsUnderTest[i]);
            }
            finally {
                closeDevice();
            }
        }
    }

    private void testDeferredSurfacesByCamera(String cameraId) throws Exception {
        Size maxPreviewSize = m1080pBoundedOrderedPreviewSizes.get(0);

        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();

        // Create a SurfaceTexture for a second output
        SurfaceTexture sharedOutputTexture = new SurfaceTexture(/*random texture ID*/ 5);
        sharedOutputTexture.setDefaultBufferSize(maxPreviewSize.getWidth(),
                maxPreviewSize.getHeight());
        Surface sharedOutputSurface1 = new Surface(sharedOutputTexture);

        class TextureAvailableListener implements SurfaceTexture.OnFrameAvailableListener {
            @Override
            public void onFrameAvailable(SurfaceTexture t) {
                mGotFrame = true;
            }
            public boolean gotFrame() { return mGotFrame; }

            private volatile boolean mGotFrame = false;
        }
        TextureAvailableListener textureAvailableListener = new TextureAvailableListener();

        sharedOutputTexture.setOnFrameAvailableListener(textureAvailableListener, mHandler);

        updatePreviewSurface(maxPreviewSize);

        // Create deferred outputs for surface view and surface texture
        OutputConfiguration surfaceViewOutput = new OutputConfiguration(maxPreviewSize,
                SurfaceHolder.class);
        OutputConfiguration surfaceTextureOutput = new OutputConfiguration(maxPreviewSize,
                SurfaceTexture.class);

        List<OutputConfiguration> outputSurfaces = new ArrayList<>();
        outputSurfaces.add(surfaceViewOutput);
        outputSurfaces.add(surfaceTextureOutput);

        // Create non-deferred ImageReader output (JPEG for LIMITED-level compatibility)
        ImageDropperListener imageListener = new ImageDropperListener();
        createImageReader(mOrderedStillSizes.get(0), ImageFormat.JPEG, /*maxImages*/ 3,
                imageListener);
        OutputConfiguration jpegOutput =
                new OutputConfiguration(OutputConfiguration.SURFACE_GROUP_ID_NONE, mReaderSurface);
        outputSurfaces.add(jpegOutput);

        // Confirm that other surface types aren't supported for OutputConfiguration
        Class[] unsupportedClasses =
                {android.media.ImageReader.class, android.media.MediaCodec.class,
                 android.renderscript.Allocation.class, android.media.MediaRecorder.class};

        for (Class klass : unsupportedClasses) {
            try {
                OutputConfiguration bad = new OutputConfiguration(maxPreviewSize, klass);
                fail(""OutputConfiguration allowed use of unsupported class "" + klass);
            } catch (IllegalArgumentException e) {
                // expected
            }
        }

        // Confirm that zero surface size isn't supported for OutputConfiguration
        Size[] sizeZeros = { new Size(0, 0), new Size(1, 0), new Size(0, 1) };
        for (Size size : sizeZeros) {
            try {
                OutputConfiguration bad = new OutputConfiguration(size, SurfaceHolder.class);
                fail(""OutputConfiguration allowed use of zero surfaceSize"");
            } catch (IllegalArgumentException e) {
                //expected
            }
        }

        // Check whether session configuration is supported
        CameraTestUtils.checkSessionConfigurationSupported(mCamera, mHandler, outputSurfaces,
                /*inputConfig*/ null, SessionConfiguration.SESSION_REGULAR,
                /*defaultSupport*/ true, ""Deferred session configuration query failed"");

        // Create session

        BlockingSessionCallback sessionListener =
                new BlockingSessionCallback();

        mSession = configureCameraSessionWithConfig(mCamera, outputSurfaces, sessionListener,
                mHandler);
        sessionListener.getStateWaiter().waitForState(BlockingSessionCallback.SESSION_READY,
                SESSION_CONFIGURE_TIMEOUT_MS);

        // Submit JPEG requests

        CaptureRequest.Builder request = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        request.addTarget(mReaderSurface);

        final int SOME_FRAMES = 10;
        for (int i = 0; i < SOME_FRAMES; i++) {
            mSession.capture(request.build(), resultListener, mHandler);
        }

        // Wait to get some frames out to ensure we can operate just the one expected surface
        waitForNumResults(resultListener, SOME_FRAMES);
        assertTrue(""No images received"", imageListener.getImageCount() > 0);

        // Ensure we can't use the deferred surfaces yet
        request.addTarget(sharedOutputSurface1);
        try {
            mSession.capture(request.build(), resultListener, mHandler);
            fail(""Should have received IAE for trying to use a deferred target "" +
                    ""that's not yet configured"");
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Add deferred surfaces to their configurations
        surfaceViewOutput.addSurface(mPreviewSurface);
        surfaceTextureOutput.addSurface(sharedOutputSurface1);

        // Verify bad inputs to addSurface
        try {
            surfaceViewOutput.addSurface(null);
            fail(""No error from setting a null deferred surface"");
        } catch (NullPointerException e) {
            // expected
        }
        try {
            surfaceViewOutput.addSurface(mPreviewSurface);
            fail(""Shouldn't be able to set deferred surface twice"");
        } catch (IllegalStateException e) {
            // expected
        }

        // Add first deferred surface to session
        List<OutputConfiguration> deferredSurfaces = new ArrayList<>();
        deferredSurfaces.add(surfaceTextureOutput);

        mSession.finalizeOutputConfigurations(deferredSurfaces);

        // Try a second time, this should error

        try {
            mSession.finalizeOutputConfigurations(deferredSurfaces);
            fail(""Should have received ISE for trying to finish a deferred output twice"");
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Use new deferred surface for a bit
        imageListener.resetImageCount();
        for (int i = 0; i < SOME_FRAMES; i++) {
            mSession.capture(request.build(), resultListener, mHandler);
        }
        waitForNumResults(resultListener, SOME_FRAMES);
        assertTrue(""No images received"", imageListener.getImageCount() > 0);
        assertTrue(""No texture update received"", textureAvailableListener.gotFrame());

        // Ensure we can't use the last deferred surface yet
        request.addTarget(mPreviewSurface);
        try {
            mSession.capture(request.build(), resultListener, mHandler);
            fail(""Should have received IAE for trying to use a deferred target that's"" +
                    "" not yet configured"");
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Add final deferred surface
        deferredSurfaces.clear();
        deferredSurfaces.add(surfaceViewOutput);

        mSession.finalizeOutputConfigurations(deferredSurfaces);

        // Use final deferred surface for a bit
        imageListener.resetImageCount();
        for (int i = 0; i < SOME_FRAMES; i++) {
            mSession.capture(request.build(), resultListener, mHandler);
        }
        waitForNumResults(resultListener, SOME_FRAMES);
        assertTrue(""No images received"", imageListener.getImageCount() > 0);
        // Can't check GL output since we don't have a context to call updateTexImage on, and
        // the callback only fires once per updateTexImage call.
        // And there's no way to verify data is going to a SurfaceView

        // Check for invalid output configurations being handed to a session
        OutputConfiguration badConfig =
                new OutputConfiguration(maxPreviewSize, SurfaceTexture.class);
        deferredSurfaces.clear();
        try {
            mSession.finalizeOutputConfigurations(deferredSurfaces);
            fail(""No error for empty list passed to finalizeOutputConfigurations"");
        } catch (IllegalArgumentException e) {
            // expected
        }

        deferredSurfaces.add(badConfig);
        try {
            mSession.finalizeOutputConfigurations(deferredSurfaces);
            fail(""No error for invalid output config being passed to finalizeOutputConfigurations"");
        } catch (IllegalArgumentException e) {
            // expected
        }

    }

    /**
     * Measure the inter-frame interval based on SENSOR_TIMESTAMP for frameCount frames from the
     * provided capture listener.  If prevTimestamp is positive, it is used for the first interval
     * calculation; otherwise, the first result is used to establish the starting time.
     *
     * Returns the mean interval in the first pair entry, and the largest interval in the second
     * pair entry
     */
    Pair<Long, Long> measureMeanFrameInterval(SimpleCaptureCallback resultListener, int frameCount,
            long prevTimestamp) throws Exception {
        long summedIntervals = 0;
        long maxInterval = 0;
        int measurementCount = frameCount - ((prevTimestamp > 0) ? 0 : 1);

        for (int i = 0; i < frameCount; i++) {
            CaptureResult result = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            long timestamp = result.get(CaptureResult.SENSOR_TIMESTAMP);
            if (prevTimestamp > 0) {
                long interval = timestamp - prevTimestamp;
                if (interval > maxInterval) maxInterval = interval;
                summedIntervals += interval;
            }
            prevTimestamp = timestamp;
        }
        return new Pair<Long, Long>(summedIntervals / measurementCount, maxInterval);
    }


    /**
     * Test preview fps range for all supported ranges. The exposure time are frame duration are
     * validated.
     */
    private void previewFpsRangeTestByCamera() throws Exception {
        Size maxPreviewSz;
        Range<Integer>[] fpsRanges = getDescendingTargetFpsRanges(mStaticInfo);
        boolean antiBandingOffIsSupported = mStaticInfo.isAntiBandingOffModeSupported();
        Range<Integer> fpsRange;
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();

        for (int i = 0; i < fpsRanges.length; i += 1) {
            fpsRange = fpsRanges[i];
            if (mStaticInfo.isHardwareLevelLegacy()) {
                // Legacy devices don't report minimum frame duration for preview sizes. The FPS
                // range should be valid for any supported preview size.
                maxPreviewSz = mOrderedPreviewSizes.get(0);
            } else {
                maxPreviewSz = getMaxPreviewSizeForFpsRange(fpsRange);
            }

            requestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
            // Turn off auto antibanding to avoid exposure time and frame duration interference
            // from antibanding algorithm.
            if (antiBandingOffIsSupported) {
                requestBuilder.set(CaptureRequest.CONTROL_AE_ANTIBANDING_MODE,
                        CaptureRequest.CONTROL_AE_ANTIBANDING_MODE_OFF);
            } else {
                // The device doesn't implement the OFF mode, test continues. It need make sure
                // that the antibanding algorithm doesn't interfere with the fps range control.
                Log.i(TAG, ""OFF antibanding mode is not supported, the camera device output must"" +
                        "" satisfy the specified fps range regardless of its current antibanding"" +
                        "" mode"");
            }

            startPreview(requestBuilder, maxPreviewSz, resultListener);
            resultListener = new SimpleCaptureCallback();
            mSession.setRepeatingRequest(requestBuilder.build(), resultListener, mHandler);

            waitForSettingsApplied(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

            verifyPreviewTargetFpsRange(resultListener, NUM_FRAMES_VERIFIED, fpsRange,
                    maxPreviewSz);
            stopPreview();
            resultListener.drain();
        }
    }

    private void verifyPreviewTargetFpsRange(SimpleCaptureCallback resultListener,
            int numFramesVerified, Range<Integer> fpsRange, Size previewSz) {
        CaptureResult result = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
        List<Integer> capabilities = mStaticInfo.getAvailableCapabilitiesChecked();

        if (capabilities.contains(CaptureRequest.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
            long frameDuration = getValueNotNull(result, CaptureResult.SENSOR_FRAME_DURATION);
            long[] frameDurationRange =
                    new long[]{(long) (1e9 / fpsRange.getUpper()), (long) (1e9 / fpsRange.getLower())};
            mCollector.expectInRange(
                    ""Frame duration must be in the range of "" + Arrays.toString(frameDurationRange),
                    frameDuration, (long) (frameDurationRange[0] * (1 - FRAME_DURATION_ERROR_MARGIN)),
                    (long) (frameDurationRange[1] * (1 + FRAME_DURATION_ERROR_MARGIN)));
            long expTime = getValueNotNull(result, CaptureResult.SENSOR_EXPOSURE_TIME);
            mCollector.expectTrue(String.format(""Exposure time %d must be no larger than frame""
                    + ""duration %d"", expTime, frameDuration), expTime <= frameDuration);

            Long minFrameDuration = mMinPreviewFrameDurationMap.get(previewSz);
            boolean findDuration = mCollector.expectTrue(""Unable to find minFrameDuration for size ""
                    + previewSz.toString(), minFrameDuration != null);
            if (findDuration) {
                mCollector.expectTrue(""Frame duration "" + frameDuration + "" must be no smaller than""
                        + "" minFrameDuration "" + minFrameDuration, frameDuration >= minFrameDuration);
            }
        } else {
            Log.i(TAG, ""verifyPreviewTargetFpsRange - MANUAL_SENSOR control is not supported,"" +
                    "" skipping duration and exposure time check."");
        }
    }

    /**
     * Test all supported preview sizes for a camera device
     *
     * @throws Exception
     */
    private void previewTestByCamera() throws Exception {
        List<Size> previewSizes = getSupportedPreviewSizes(
                mCamera.getId(), mCameraManager, PREVIEW_SIZE_BOUND);

        for (final Size sz : previewSizes) {
            if (VERBOSE) {
                Log.v(TAG, ""Testing camera preview size: "" + sz.toString());
            }

            // TODO: vary the different settings like crop region to cover more cases.
            CaptureRequest.Builder requestBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
            CaptureCallback mockCaptureCallback =
                    mock(CameraCaptureSession.CaptureCallback.class);

            startPreview(requestBuilder, sz, mockCaptureCallback);
            verifyCaptureResults(mSession, mockCaptureCallback, NUM_FRAMES_VERIFIED,
                    NUM_FRAMES_VERIFIED * FRAME_TIMEOUT_MS);
            stopPreview();
        }
    }

    private void previewTestPatternTestByCamera() throws Exception {
        Size maxPreviewSize = mOrderedPreviewSizes.get(0);
        int[] testPatternModes = mStaticInfo.getAvailableTestPatternModesChecked();
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureCallback mockCaptureCallback;

        final int[] TEST_PATTERN_DATA = {0, 0xFFFFFFFF, 0xFFFFFFFF, 0}; // G:100%, RB:0.
        for (int mode : testPatternModes) {
            if (VERBOSE) {
                Log.v(TAG, ""Test pattern mode: "" + mode);
            }
            requestBuilder.set(CaptureRequest.SENSOR_TEST_PATTERN_MODE, mode);
            if (mode == CaptureRequest.SENSOR_TEST_PATTERN_MODE_SOLID_COLOR) {
                // Assign color pattern to SENSOR_TEST_PATTERN_MODE_DATA
                requestBuilder.set(CaptureRequest.SENSOR_TEST_PATTERN_DATA, TEST_PATTERN_DATA);
            }
            mockCaptureCallback = mock(CaptureCallback.class);
            startPreview(requestBuilder, maxPreviewSize, mockCaptureCallback);
            verifyCaptureResults(mSession, mockCaptureCallback, NUM_TEST_PATTERN_FRAMES_VERIFIED,
                    NUM_TEST_PATTERN_FRAMES_VERIFIED * FRAME_TIMEOUT_MS);
        }

        stopPreview();
    }

    private void surfaceSetTestByCamera(String cameraId) throws Exception {
        final int MAX_SURFACE_GROUP_ID = 10;
        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        Size yuvSizeBound = maxPreviewSz; // Default case: legacy device
        if (mStaticInfo.isHardwareLevelLimited()) {
            yuvSizeBound = mOrderedVideoSizes.get(0);
        } else if (mStaticInfo.isHardwareLevelAtLeastFull()) {
            yuvSizeBound = null;
        }
        Size maxYuvSize = getSupportedPreviewSizes(cameraId, mCameraManager, yuvSizeBound).get(0);

        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        ImageDropperListener imageListener = new ImageDropperListener();

        updatePreviewSurface(maxPreviewSz);
        createImageReader(maxYuvSize, ImageFormat.YUV_420_888, MAX_READER_IMAGES, imageListener);
        List<OutputConfiguration> outputConfigs = new ArrayList<OutputConfiguration>();
        OutputConfiguration previewConfig = new OutputConfiguration(mPreviewSurface);
        OutputConfiguration yuvConfig = new OutputConfiguration(mReaderSurface);
        assertEquals(OutputConfiguration.SURFACE_GROUP_ID_NONE, previewConfig.getSurfaceGroupId());
        assertEquals(OutputConfiguration.SURFACE_GROUP_ID_NONE, yuvConfig.getSurfaceGroupId());
        assertEquals(mPreviewSurface, previewConfig.getSurface());
        assertEquals(mReaderSurface, yuvConfig.getSurface());
        outputConfigs.add(previewConfig);
        outputConfigs.add(yuvConfig);
        requestBuilder.addTarget(mPreviewSurface);
        requestBuilder.addTarget(mReaderSurface);

        // Test different stream set ID.
        for (int surfaceGroupId = OutputConfiguration.SURFACE_GROUP_ID_NONE;
                surfaceGroupId < MAX_SURFACE_GROUP_ID; surfaceGroupId++) {
            if (VERBOSE) {
                Log.v(TAG, ""test preview with surface group id: "");
            }

            previewConfig = new OutputConfiguration(surfaceGroupId, mPreviewSurface);
            yuvConfig = new OutputConfiguration(surfaceGroupId, mReaderSurface);
            outputConfigs.clear();
            outputConfigs.add(previewConfig);
            outputConfigs.add(yuvConfig);

            for (OutputConfiguration config : outputConfigs) {
                assertEquals(surfaceGroupId, config.getSurfaceGroupId());
            }

            CameraCaptureSession.StateCallback mockSessionListener =
                    mock(CameraCaptureSession.StateCallback.class);

            mSession = configureCameraSessionWithConfig(mCamera, outputConfigs,
                    mockSessionListener, mHandler);


            mSession.prepare(mPreviewSurface);
            verify(mockSessionListener,
                    timeout(PREPARE_TIMEOUT_MS).times(1)).
                    onSurfacePrepared(eq(mSession), eq(mPreviewSurface));

            mSession.prepare(mReaderSurface);
            verify(mockSessionListener,
                    timeout(PREPARE_TIMEOUT_MS).times(1)).
                    onSurfacePrepared(eq(mSession), eq(mReaderSurface));

            CaptureRequest request = requestBuilder.build();
            CaptureCallback mockCaptureCallback =
                    mock(CameraCaptureSession.CaptureCallback.class);
            mSession.setRepeatingRequest(request, mockCaptureCallback, mHandler);
            verifyCaptureResults(mSession, mockCaptureCallback, NUM_FRAMES_VERIFIED,
                    NUM_FRAMES_VERIFIED * FRAME_TIMEOUT_MS);
        }
    }

    private class IsCaptureResultValid implements ArgumentMatcher<TotalCaptureResult> {
        @Override
        public boolean matches(TotalCaptureResult obj) {
            TotalCaptureResult result = obj;
            Long timeStamp = result.get(CaptureResult.SENSOR_TIMESTAMP);
            if (timeStamp != null && timeStamp.longValue() > 0L) {
                return true;
            }
            return false;
        }
    }

    private void verifyCaptureResults(
            CameraCaptureSession session,
            CaptureCallback mockListener,
            int expectResultCount,
            int timeOutMs) {
        // Should receive expected number of onCaptureStarted callbacks.
        ArgumentCaptor<Long> timestamps = ArgumentCaptor.forClass(Long.class);
        ArgumentCaptor<Long> frameNumbers = ArgumentCaptor.forClass(Long.class);
        verify(mockListener,
                timeout(timeOutMs).atLeast(expectResultCount))
                        .onCaptureStarted(
                                eq(session),
                                isA(CaptureRequest.class),
                                timestamps.capture(),
                                frameNumbers.capture());

        // Validate timestamps: all timestamps should be larger than 0 and monotonically increase.
        long timestamp = 0;
        for (Long nextTimestamp : timestamps.getAllValues()) {
            assertNotNull(""Next timestamp is null!"", nextTimestamp);
            assertTrue(""Captures are out of order"", timestamp < nextTimestamp);
            timestamp = nextTimestamp;
        }

        // Validate framenumbers: all framenumbers should be consecutive and positive
        long frameNumber = -1;
        for (Long nextFrameNumber : frameNumbers.getAllValues()) {
            assertNotNull(""Next frame number is null!"", nextFrameNumber);
            assertTrue(""Captures are out of order"",
                    (frameNumber == -1) || (frameNumber + 1 == nextFrameNumber));
            frameNumber = nextFrameNumber;
        }

        // Should receive expected number of capture results.
        verify(mockListener,
                timeout(timeOutMs).atLeast(expectResultCount))
                        .onCaptureCompleted(
                                eq(session),
                                isA(CaptureRequest.class),
                                argThat(new IsCaptureResultValid()));

        // Should not receive any capture failed callbacks.
        verify(mockListener, never())
                        .onCaptureFailed(
                                eq(session),
                                isA(CaptureRequest.class),
                                isA(CaptureFailure.class));
    }

}"	""	""	"minimum"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.DngCreatorTest"	"testSingleImageBasic"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/DngCreatorTest.java"	""	"public void testSingleImageBasic() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            String deviceId = mCameraIdsUnderTest[i];
            ImageReader captureReader = null;
            FileOutputStream fileStream = null;
            ByteArrayOutputStream outputStream = null;
            try {
                if (!mAllStaticInfo.get(deviceId).isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                    Log.i(TAG, ""RAW capability is not supported in camera "" + mCameraIdsUnderTest[i] +
                            "". Skip the test."");
                    continue;
                }

                openDevice(deviceId);
                Size activeArraySize = mStaticInfo.getRawDimensChecked();

                // Create capture image reader
                CameraTestUtils.SimpleImageReaderListener captureListener
                        = new CameraTestUtils.SimpleImageReaderListener();
                captureReader = createImageReader(activeArraySize, ImageFormat.RAW_SENSOR, 2,
                        captureListener);
                Pair<Image, CaptureResult> resultPair = captureSingleRawShot(activeArraySize,
                        /*waitForAe*/false, captureReader, captureListener);
                CameraCharacteristics characteristics = mStaticInfo.getCharacteristics();

                // Test simple writeImage, no header checks
                DngCreator dngCreator = new DngCreator(characteristics, resultPair.second);
                outputStream = new ByteArrayOutputStream();
                dngCreator.writeImage(outputStream, resultPair.first);

                if (VERBOSE) {
                    // Write DNG to file
                    String dngFilePath = mDebugFileNameBase + ""/camera_basic_"" + deviceId + ""_"" +
                            DEBUG_DNG_FILE;
                    // Write out captured DNG file for the first camera device if setprop is enabled
                    fileStream = new FileOutputStream(dngFilePath);
                    fileStream.write(outputStream.toByteArray());
                    fileStream.flush();
                    fileStream.close();
                    Log.v(TAG, ""Test DNG file for camera "" + deviceId + "" saved to "" + dngFilePath);
                }
                assertTrue(""Generated DNG file does not pass validation"",
                        validateDngNative(outputStream.toByteArray()));
            } finally {
                closeDevice(deviceId);
                closeImageReader(captureReader);

                if (outputStream != null) {
                    outputStream.close();
                }

                if (fileStream != null) {
                    fileStream.close();
                }
            }
        }
    }

    /**
     * Test basic maximum resolution raw capture and DNG saving functionality for each of the
     * available ultra high resolution cameras.
     *
     * <p>
     * For ultra high resolution each camera, capture a single RAW16 image at the first capture size
     * reported for the maximum resolution raw format on that device, and save that image as a DNG
     * file. No further validation is done.
     * </p>
     *
     * <p>
     * Note: Enabling adb shell setprop log.tag.DngCreatorTest VERBOSE will also cause the
     * raw image captured for the first reported camera device to be saved to an output file.
     * </p>
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.DngCreatorTest"	"testSingleImageBasicMaximumResolution"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/DngCreatorTest.java"	""	"public void testSingleImageBasicMaximumResolution() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            String deviceId = mCameraIdsUnderTest[i];
            ImageReader captureReader = null;
            ImageReader reprocessCaptureReader = null;
            FileOutputStream fileStream = null;
            ByteArrayOutputStream outputStream = null;
            try {
                // All ultra high resolution sensors must necessarily support RAW
                if (!mAllStaticInfo.get(deviceId).isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR)) {
                    Log.i(TAG, ""ULTRA_HIGH_RESOLUTION_SENSOR capability is not supported in "" +
                            "" camera "" + mCameraIdsUnderTest[i] + "". Skip the test."");
                    continue;
                }

                openDevice(deviceId);
                Size activeArraySize = mStaticInfo.getRawDimensChecked(/*maxResolution*/true);

                // Create capture image reader
                CameraTestUtils.SimpleImageReaderListener captureReaderListener
                        = new CameraTestUtils.SimpleImageReaderListener();
                CameraTestUtils.SimpleImageReaderListener reprocessReaderListener
                        = new CameraTestUtils.SimpleImageReaderListener();

                captureReader = createImageReader(activeArraySize, ImageFormat.RAW_SENSOR, 2,
                        captureReaderListener);

                reprocessCaptureReader = createImageReader(activeArraySize, ImageFormat.RAW_SENSOR,
                        2, reprocessReaderListener);
                Pair<Image, CaptureResult> resultPair = null;
                if (mAllStaticInfo.get(deviceId).isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_REMOSAIC_REPROCESSING)) {
                    resultPair =
                            captureReprocessedRawShot(activeArraySize, captureReader,
                                    reprocessCaptureReader, captureReaderListener,
                                    reprocessReaderListener, /*waitForAe*/false);
                } else {
                    resultPair = captureSingleShotMaximumResolution(activeArraySize,
                            captureReader, /*waitForAe*/false, captureReaderListener);
                }
                CameraCharacteristics characteristics = mStaticInfo.getCharacteristics();

                // Test simple writeImage, no header checks
                DngCreator dngCreator = new DngCreator(characteristics, resultPair.second);
                outputStream = new ByteArrayOutputStream();
                dngCreator.writeImage(outputStream, resultPair.first);

                if (VERBOSE) {
                    // Write DNG to file
                    String dngFilePath = mDebugFileNameBase + ""/camera_basic_max_resolution_"" +
                            deviceId + ""_"" + DEBUG_DNG_FILE;
                    // Write out captured DNG file for the first camera device if setprop is enabled
                    fileStream = new FileOutputStream(dngFilePath);
                    fileStream.write(outputStream.toByteArray());
                    fileStream.flush();
                    fileStream.close();
                    Log.v(TAG, ""Test DNG file for camera "" + deviceId + "" saved to "" + dngFilePath);
                }
                assertTrue(""Generated DNG file does not pass validation"",
                        validateDngNative(outputStream.toByteArray()));
            } finally {
                closeDevice(deviceId);
                closeImageReader(captureReader);
                closeImageReader(reprocessCaptureReader);

                if (outputStream != null) {
                    outputStream.close();
                }

                if (fileStream != null) {
                    fileStream.close();
                }
            }
        }
    }

    /**
     * Test basic raw capture and DNG saving with a thumbnail, rotation, usercomment, and GPS tags
     * set.
     *
     * <p>
     * For each camera, capture a single RAW16 image at the first capture size reported for
     * the raw format on that device, and save that image as a DNG file. GPS information validation
     * is done via ExifInterface.
     * </p>
     *
     * <p>
     * Note: Enabling adb shell setprop log.tag.DngCreatorTest VERBOSE will also cause the
     * raw image captured for the first reported camera device to be saved to an output file.
     * </p>
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.DngCreatorTest"	"testSingleImageThumbnail"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/DngCreatorTest.java"	""	"public void testSingleImageThumbnail() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            String deviceId = mCameraIdsUnderTest[i];
            List<ImageReader> captureReaders = new ArrayList<ImageReader>();
            List<CameraTestUtils.SimpleImageReaderListener> captureListeners =
                    new ArrayList<CameraTestUtils.SimpleImageReaderListener>();
            FileOutputStream fileStream = null;
            ByteArrayOutputStream outputStream = null;
            try {
                if (!mAllStaticInfo.get(deviceId).isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                    Log.i(TAG, ""RAW capability is not supported in camera "" + mCameraIdsUnderTest[i] +
                            "". Skip the test."");
                    continue;
                }

                openDevice(deviceId);
                Size activeArraySize = mStaticInfo.getRawDimensChecked();

                Size[] targetPreviewSizes =
                        mStaticInfo.getAvailableSizesForFormatChecked(ImageFormat.YUV_420_888,
                                StaticMetadata.StreamDirection.Output);
                // Get smallest preview size
                Size previewSize = mOrderedPreviewSizes.get(mOrderedPreviewSizes.size() - 1);

                // Create capture image reader
                CameraTestUtils.SimpleImageReaderListener captureListener
                        = new CameraTestUtils.SimpleImageReaderListener();
                captureReaders.add(createImageReader(activeArraySize, ImageFormat.RAW_SENSOR, 2,
                        captureListener));
                captureListeners.add(captureListener);

                CameraTestUtils.SimpleImageReaderListener previewListener
                        = new CameraTestUtils.SimpleImageReaderListener();

                captureReaders.add(createImageReader(previewSize, ImageFormat.YUV_420_888, 2,
                        previewListener));
                captureListeners.add(previewListener);

                Date beforeCaptureDate = new Date();
                Pair<List<Image>, CaptureResult> resultPair = captureSingleRawShot(activeArraySize,
                        captureReaders, /*waitForAe*/false, captureListeners);
                Date afterCaptureDate = new Date();
                CameraCharacteristics characteristics = mStaticInfo.getCharacteristics();

                if (VERBOSE) {
                    Log.v(TAG, ""Sensor timestamp (ms): "" +
                            resultPair.second.get(CaptureResult.SENSOR_TIMESTAMP) / 1000000);
                    Log.v(TAG, ""SystemClock.elapsedRealtimeNanos (ms): "" +
                            SystemClock.elapsedRealtimeNanos() / 1000000);
                    Log.v(TAG, ""SystemClock.uptimeMillis(): "" + SystemClock.uptimeMillis());
                }
                // Test simple writeImage, no header checks
                DngCreator dngCreator = new DngCreator(characteristics, resultPair.second);
                Location l = new Location(""test"");
                l.reset();
                l.setLatitude(GPS_LATITUDE);
                l.setLongitude(GPS_LONGITUDE);
                l.setTime(GPS_CALENDAR.getTimeInMillis());
                dngCreator.setLocation(l);

                dngCreator.setDescription(""helloworld"");
                dngCreator.setOrientation(ExifInterface.ORIENTATION_FLIP_VERTICAL);
                dngCreator.setThumbnail(resultPair.first.get(1));
                outputStream = new ByteArrayOutputStream();
                dngCreator.writeImage(outputStream, resultPair.first.get(0));

                String filePath = mDebugFileNameBase + ""/camera_thumb_"" + deviceId + ""_"" +
                        DEBUG_DNG_FILE;
                // Write out captured DNG file for the first camera device
                fileStream = new FileOutputStream(filePath);
                fileStream.write(outputStream.toByteArray());
                fileStream.flush();
                fileStream.close();
                if (VERBOSE) {
                    Log.v(TAG, ""Test DNG file for camera "" + deviceId + "" saved to "" + filePath);
                }

                assertTrue(""Generated DNG file does not pass validation"",
                        validateDngNative(outputStream.toByteArray()));

                ExifInterface exifInterface = new ExifInterface(filePath);
                // Verify GPS data.
                float[] latLong = new float[2];
                assertTrue(exifInterface.getLatLong(latLong));
                assertEquals(GPS_LATITUDE, latLong[0], GPS_DIFFERENCE_TOLERANCE);
                assertEquals(GPS_LONGITUDE, latLong[1], GPS_DIFFERENCE_TOLERANCE);
                assertEquals(GPS_DATESTAMP,
                        exifInterface.getAttribute(ExifInterface.TAG_GPS_DATESTAMP));
                assertEquals(GPS_TIMESTAMP,
                        exifInterface.getAttribute(ExifInterface.TAG_GPS_TIMESTAMP));

                // Verify the orientation.
                assertEquals(ExifInterface.ORIENTATION_FLIP_VERTICAL,
                        exifInterface.getAttributeInt(ExifInterface.TAG_ORIENTATION,
                                ExifInterface.ORIENTATION_UNDEFINED));

                // Verify the date/time
                final SimpleDateFormat dngDateTimeStampFormat =
                        new SimpleDateFormat(""yyyy:MM:dd HH:mm:ss"");
                dngDateTimeStampFormat.setLenient(false);

                String dateTimeString =
                        exifInterface.getAttribute(ExifInterface.TAG_DATETIME);
                assertTrue(dateTimeString != null);

                Date dateTime = dngDateTimeStampFormat.parse(dateTimeString);
                long captureTimeMs = dateTime.getTime();

                Log.i(TAG, ""DNG DateTime tag: "" + dateTimeString);
                Log.i(TAG, ""Before capture time: "" + beforeCaptureDate.getTime());
                Log.i(TAG, ""Capture time: "" + captureTimeMs);
                Log.i(TAG, ""After capture time: "" + afterCaptureDate.getTime());

                // Offset beforeCaptureTime by 1 second to account for rounding down of
                // DNG tag
                long beforeCaptureTimeMs = beforeCaptureDate.getTime() - 1000;
                long afterCaptureTimeMs = afterCaptureDate.getTime();
                assertTrue(captureTimeMs >= beforeCaptureTimeMs);
                assertTrue(captureTimeMs <= afterCaptureTimeMs);

                if (!VERBOSE) {
                    // Delete the captured DNG file.
                    File dngFile = new File(filePath);
                    assertTrue(dngFile.delete());
                }
            } finally {
                closeDevice(deviceId);
                for (ImageReader r : captureReaders) {
                    closeImageReader(r);
                }

                if (outputStream != null) {
                    outputStream.close();
                }

                if (fileStream != null) {
                    fileStream.close();
                }
            }
        }
    }

    /**
     * Test basic maximum resolution RAW capture, and ensure that the rendered RAW output is
     * similar to the maximum resolution JPEG created for a similar frame.
     *
     * Since mandatory streams for maximum resolution sensor pixel mode do not guarantee 2 maximum
     * resolution streams we can't capture RAW + JPEG images of the same frame. Therefore, 2
     * sessions are created, one for RAW capture and the other for JPEG capture.
     *
     * <p>
     * This test renders the RAW buffer into an RGB bitmap using a rendering pipeline
     * similar to one in the Adobe DNG validation tool.  JPEGs produced by the vendor hardware may
     * have different tonemapping and saturation applied than the RGB bitmaps produced
     * from this DNG rendering pipeline, and this test allows for fairly wide variations
     * between the histograms for the RAW and JPEG buffers to avoid false positives.
     * </p>
     *
     * <p>
     * To ensure more subtle errors in the colorspace transforms returned for the HAL's RAW
     * metadata, the DNGs and JPEGs produced here should also be manually compared using external
     * DNG rendering tools.  The DNG, rendered RGB bitmap, and JPEG buffer for this test can be
     * dumped to the SD card for further examination by enabling the 'verbose' mode for this test
     * using:
     * adb shell setprop log.tag.DngCreatorTest VERBOSE
     * </p>
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.DngCreatorTest"	"testRaw16JpegMaximumResolutionConsistency"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/DngCreatorTest.java"	""	"public void testRaw16JpegMaximumResolutionConsistency() throws Exception {
        for (String deviceId : mCameraIdsUnderTest) {
            ImageReader rawImageReader = null;
            ImageReader jpegImageReader = null;
            FileOutputStream fileStream = null;
            FileChannel fileChannel = null;
            try {
                // All ultra high resolution sensors must necessarily support RAW
                if (!mAllStaticInfo.get(deviceId).isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR)) {
                    Log.i(TAG, ""ULTRA_HIGH_RESOLUTION_SENSOR capability is not supported in "" +
                            "" camera "" + deviceId + "". Skip "" +
                            ""testRaw16JpegMaximumResolutionConsistency"");
                    continue;
                }

                CapturedDataMaximumResolution data =
                        captureRawJpegImagePairMaximumResolution(deviceId, rawImageReader,
                                jpegImageReader);
                if (data == null) {
                    continue;
                }
                Image raw = data.raw.first;
                Image jpeg = data.jpeg.first;

                Bitmap rawBitmap = Bitmap.createBitmap(raw.getWidth(), raw.getHeight(),
                        Bitmap.Config.ARGB_8888);

                byte[] rawPlane = new byte[raw.getPlanes()[0].getRowStride() * raw.getHeight()];

                // Render RAW image to a bitmap
                raw.getPlanes()[0].getBuffer().get(rawPlane);
                raw.getPlanes()[0].getBuffer().rewind();

                RawConverter.convertToSRGB(RenderScriptSingleton.getRS(), raw.getWidth(),
                        raw.getHeight(), raw.getPlanes()[0].getRowStride(), rawPlane,
                        data.characteristics, /*captureREsult*/data.raw.second, /*offsetX*/ 0,
                        /*offsetY*/ 0, /*out*/ rawBitmap);

                rawPlane = null;
                System.gc(); // Hint to VM

                if (VERBOSE) {
                    DngDebugParams params = new DngDebugParams();
                    params.deviceId = deviceId;
                    params.characteristics = data.characteristics;
                    params.captureResult = data.raw.second;
                    params.fileStream = fileStream;
                    params.raw = raw;
                    params.jpeg = jpeg;
                    params.fileChannel = fileChannel;
                    params.rawBitmap = rawBitmap;
                    params.intermediateStr = ""maximum_resolution_"";

                    debugDumpDng(params);
                }

                validateRawJpegImagePair(rawBitmap, jpeg, deviceId);
            } finally {
                closeImageReader(rawImageReader);
                closeImageReader(jpegImageReader);

                if (fileChannel != null) {
                    fileChannel.close();
                }
                if (fileStream != null) {
                    fileStream.close();
                }
            }
        }
    }



    /**
     * Test basic RAW capture, and ensure that the rendered RAW output is similar to the JPEG
     * created for the same frame.
     *
     * <p>
     * This test renders the RAW buffer into an RGB bitmap using a rendering pipeline
     * similar to one in the Adobe DNG validation tool.  JPEGs produced by the vendor hardware may
     * have different tonemapping and saturation applied than the RGB bitmaps produced
     * from this DNG rendering pipeline, and this test allows for fairly wide variations
     * between the histograms for the RAW and JPEG buffers to avoid false positives.
     * </p>
     *
     * <p>
     * To ensure more subtle errors in the colorspace transforms returned for the HAL's RAW
     * metadata, the DNGs and JPEGs produced here should also be manually compared using external
     * DNG rendering tools.  The DNG, rendered RGB bitmap, and JPEG buffer for this test can be
     * dumped to the SD card for further examination by enabling the 'verbose' mode for this test
     * using:
     * adb shell setprop log.tag.DngCreatorTest VERBOSE
     * </p>
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.DngCreatorTest"	"testDngRenderingByBitmapFactor"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/DngCreatorTest.java"	""	"public void testDngRenderingByBitmapFactor() throws Exception {
        for (String deviceId : mCameraIdsUnderTest) {
            List<ImageReader> captureReaders = new ArrayList<>();

            CapturedData data = captureRawJpegImagePair(deviceId, captureReaders);
            if (data == null) {
                continue;
            }
            Image raw = data.imagePair.first.get(0);
            Image jpeg = data.imagePair.first.get(1);

            // Generate DNG file
            DngCreator dngCreator = new DngCreator(data.characteristics, data.imagePair.second);

            // Write DNG to file
            String dngFilePath = mDebugFileNameBase + ""/camera_"" +
                deviceId + ""_"" + TEST_DNG_FILE;

            // Write out captured DNG file for the first camera device if setprop is enabled
            try (FileOutputStream fileStream = new FileOutputStream(dngFilePath)) {
                dngCreator.writeImage(fileStream, raw);

                // Render the DNG file using BitmapFactory.
                Bitmap rawBitmap = BitmapFactory.decodeFile(dngFilePath);
                assertNotNull(rawBitmap);

                validateRawJpegImagePair(rawBitmap, jpeg, deviceId);
            } finally {
                for (ImageReader r : captureReaders) {
                    closeImageReader(r);
                }

                System.gc(); // Hint to VM
            }
        }
    }

    /*
     * Create RAW + JPEG image pair with characteristics info.
     */
    private CapturedData captureRawJpegImagePair(String deviceId, List<ImageReader> captureReaders)
            throws Exception {
        CapturedData data = new CapturedData();
        List<CameraTestUtils.SimpleImageReaderListener> captureListeners = new ArrayList<>();
        try {
            if (!mAllStaticInfo.get(deviceId).isCapabilitySupported(
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                Log.i(TAG, ""RAW capability is not supported in camera "" + deviceId
                        + "". Skip the test."");
                return null;
            }

            openDevice(deviceId);
            Size activeArraySize = mStaticInfo.getRawDimensChecked();

            // Get largest jpeg size
            Size[] targetJpegSizes = mStaticInfo.getAvailableSizesForFormatChecked(
                    ImageFormat.JPEG, StaticMetadata.StreamDirection.Output);

            Size largestJpegSize = Collections.max(Arrays.asList(targetJpegSizes),
                    new CameraTestUtils.SizeComparator());

            // Create raw image reader and capture listener
            CameraTestUtils.SimpleImageReaderListener rawListener =
                    new CameraTestUtils.SimpleImageReaderListener();
            captureReaders.add(createImageReader(activeArraySize, ImageFormat.RAW_SENSOR, 2,
                    rawListener));
            captureListeners.add(rawListener);


            // Create jpeg image reader and capture listener
            CameraTestUtils.SimpleImageReaderListener jpegListener =
                    new CameraTestUtils.SimpleImageReaderListener();
            captureReaders.add(createImageReader(largestJpegSize, ImageFormat.JPEG, 2,
                    jpegListener));
            captureListeners.add(jpegListener);

            data.imagePair = captureSingleRawShot(activeArraySize,
                    captureReaders, /*waitForAe*/ true, captureListeners);
            data.characteristics = mStaticInfo.getCharacteristics();

            Image raw = data.imagePair.first.get(0);
            Size rawBitmapSize = new Size(raw.getWidth(), raw.getHeight());
            assertTrue(""Raw bitmap size must be equal to either pre-correction active array"" +
                    "" size or pixel array size."", rawBitmapSize.equals(activeArraySize));

            return data;
        } finally {
            closeDevice(deviceId);
        }
    }

   private void debugDumpDng(DngDebugParams params) throws Exception {
        // Generate DNG file
        DngCreator dngCreator =
                new DngCreator(params.characteristics, params.captureResult);

        // Write DNG to file
        String dngFilePath = mDebugFileNameBase + ""/camera_"" + params.intermediateStr +
                params.deviceId + ""_"" + DEBUG_DNG_FILE;
        // Write out captured DNG file for the first camera device if setprop is enabled
        params.fileStream = new FileOutputStream(dngFilePath);
        dngCreator.writeImage(params.fileStream, params.raw);
        params.fileStream.flush();
        params.fileStream.close();
        Log.v(TAG, ""Test DNG file for camera "" + params.deviceId + "" saved to "" + dngFilePath);

        // Write JPEG to file
        String jpegFilePath = mDebugFileNameBase + ""/camera_"" + params.intermediateStr  +
                params.deviceId + ""_jpeg.jpg"";
        // Write out captured DNG file for the first camera device if setprop is enabled
        params.fileChannel = new FileOutputStream(jpegFilePath).getChannel();
        ByteBuffer jPlane = params.jpeg.getPlanes()[0].getBuffer();
        params.fileChannel.write(jPlane);
        params.fileChannel.close();
        jPlane.rewind();
        Log.v(TAG, ""Test JPEG file for camera "" + params.deviceId + "" saved to "" +
                jpegFilePath);

        // Write jpeg generated from demosaiced RAW frame to file
        String rawFilePath = mDebugFileNameBase + ""/camera_"" + params.intermediateStr +
                params.deviceId + ""_raw.jpg"";
        // Write out captured DNG file for the first camera device if setprop is enabled
        params.fileStream = new FileOutputStream(rawFilePath);
        params.rawBitmap.compress(Bitmap.CompressFormat.JPEG, 90, params.fileStream);
        params.fileStream.flush();
        params.fileStream.close();
        Log.v(TAG, ""Test converted RAW file for camera "" + params.deviceId + "" saved to "" +
                rawFilePath);
   }

    /*
     * Create RAW + JPEG image pair with characteristics info. Assumes the device supports the RAW
     * capability.
     */
    private CapturedDataMaximumResolution captureRawJpegImagePairMaximumResolution(String deviceId,
            ImageReader rawCaptureReader, ImageReader jpegCaptureReader)
            throws Exception {
        CapturedDataMaximumResolution data = new CapturedDataMaximumResolution();
        try {

            openDevice(deviceId);
            Size activeArraySize = mStaticInfo.getRawDimensChecked(/*maxResolution*/true);

            // Get largest jpeg size
            Size[] targetJpegSizes = mStaticInfo.getAvailableSizesForFormatChecked(
                    ImageFormat.JPEG, StaticMetadata.StreamDirection.Output, /*fastSizes*/ true,
                    /*slowSizes*/ true, /*maxResolution*/true);

            Size largestJpegSize = Collections.max(Arrays.asList(targetJpegSizes),
                    new CameraTestUtils.SizeComparator());

            // Create raw image reader and capture listener
            CameraTestUtils.SimpleImageReaderListener rawCaptureReaderListener =
                    new CameraTestUtils.SimpleImageReaderListener();
            rawCaptureReader = createImageReader(activeArraySize, ImageFormat.RAW_SENSOR, 2,
                    rawCaptureReaderListener);

            // Create jpeg image reader and capture listener
            CameraTestUtils.SimpleImageReaderListener jpegCaptureListener =
                    new CameraTestUtils.SimpleImageReaderListener();
            jpegCaptureReader = createImageReader(largestJpegSize, ImageFormat.JPEG, 2,
                    jpegCaptureListener);

            Pair<Image, CaptureResult> jpegResultPair =
                    captureSingleShotMaximumResolution(activeArraySize,
                             jpegCaptureReader, /*waitForAe*/true, jpegCaptureListener);
            data.jpeg = jpegResultPair;
            data.characteristics = mStaticInfo.getCharacteristics();
            // Create capture image reader
            CameraTestUtils.SimpleImageReaderListener outputRawCaptureReaderListener
                    = new CameraTestUtils.SimpleImageReaderListener();
            CameraTestUtils.SimpleImageReaderListener reprocessReaderListener
                    = new CameraTestUtils.SimpleImageReaderListener();

            ImageReader outputRawCaptureReader = createImageReader(activeArraySize,
                    ImageFormat.RAW_SENSOR, 2, outputRawCaptureReaderListener);
            Pair<Image, CaptureResult> rawResultPair = null;
            if (mAllStaticInfo.get(deviceId).isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_REMOSAIC_REPROCESSING)) {
                rawResultPair =
                        captureReprocessedRawShot(activeArraySize, outputRawCaptureReader,
                                    rawCaptureReader, outputRawCaptureReaderListener,
                                    reprocessReaderListener, /*waitForAe*/ true);
            } else {
                rawResultPair = captureSingleShotMaximumResolution(activeArraySize,
                        rawCaptureReader, /*waitForAe*/true, rawCaptureReaderListener);
            }
            data.raw = rawResultPair;
            Size rawBitmapSize =
                    new Size(rawResultPair.first.getWidth(), rawResultPair.first.getHeight());
            assertTrue(""Raw bitmap size must be equal to either pre-correction active array"" +
                    "" size or pixel array size."", rawBitmapSize.equals(activeArraySize));

            return data;
        } finally {
            closeDevice(deviceId);
        }
    }

    /*
     * Verify the image pair by comparing the center patch.
     */
    private void validateRawJpegImagePair(Bitmap rawBitmap, Image jpeg, String deviceId)
            throws Exception {
        // Decompress JPEG image to a bitmap
        byte[] compressedJpegData = CameraTestUtils.getDataFromImage(jpeg);

        // Get JPEG dimensions without decoding
        BitmapFactory.Options opt0 = new BitmapFactory.Options();
        opt0.inJustDecodeBounds = true;
        BitmapFactory.decodeByteArray(compressedJpegData, /*offset*/0,
                compressedJpegData.length, /*inout*/opt0);
        Rect jpegDimens = new Rect(0, 0, opt0.outWidth, opt0.outHeight);

        // Find square center patch from JPEG and RAW bitmaps
        RectF jpegRect = new RectF(jpegDimens);
        RectF rawRect = new RectF(0, 0, rawBitmap.getWidth(), rawBitmap.getHeight());
        int sideDimen = Math.min(Math.min(Math.min(Math.min(DEFAULT_PATCH_DIMEN,
                jpegDimens.width()), jpegDimens.height()), rawBitmap.getWidth()),
                rawBitmap.getHeight());

        RectF jpegIntermediate = new RectF(0, 0, sideDimen, sideDimen);
        jpegIntermediate.offset(jpegRect.centerX() - jpegIntermediate.centerX(),
                jpegRect.centerY() - jpegIntermediate.centerY());

        RectF rawIntermediate = new RectF(0, 0, sideDimen, sideDimen);
        rawIntermediate.offset(rawRect.centerX() - rawIntermediate.centerX(),
                rawRect.centerY() - rawIntermediate.centerY());
        Rect jpegFinal = new Rect();
        jpegIntermediate.roundOut(jpegFinal);
        Rect rawFinal = new Rect();
        rawIntermediate.roundOut(rawFinal);

        // Get RAW center patch, and free up rest of RAW image
        Bitmap rawPatch = Bitmap.createBitmap(rawBitmap, rawFinal.left, rawFinal.top,
                rawFinal.width(), rawFinal.height());
        rawBitmap.recycle();
        rawBitmap = null;
        System.gc(); // Hint to VM

        BitmapFactory.Options opt = new BitmapFactory.Options();
        opt.inPreferredConfig = Bitmap.Config.ARGB_8888;
        Bitmap jpegPatch = BitmapRegionDecoder.newInstance(compressedJpegData,
                /*offset*/0, compressedJpegData.length, /*isShareable*/true).
                decodeRegion(jpegFinal, opt);

        // Compare center patch from JPEG and rendered RAW bitmap
        double difference = BitmapUtils.calcDifferenceMetric(jpegPatch, rawPatch);
        if (difference > IMAGE_DIFFERENCE_TOLERANCE) {
            FileOutputStream fileStream = null;
            try {
                // Write JPEG patch to file
                String jpegFilePath = mDebugFileNameBase + ""/camera_"" + deviceId +
                        ""_jpeg_patch.jpg"";
                fileStream = new FileOutputStream(jpegFilePath);
                jpegPatch.compress(Bitmap.CompressFormat.JPEG, 90, fileStream);
                fileStream.flush();
                fileStream.close();
                Log.e(TAG, ""Failed JPEG patch file for camera "" + deviceId + "" saved to "" +
                        jpegFilePath);

                // Write RAW patch to file
                String rawFilePath = mDebugFileNameBase + ""/camera_"" + deviceId +
                        ""_raw_patch.jpg"";
                fileStream = new FileOutputStream(rawFilePath);
                rawPatch.compress(Bitmap.CompressFormat.JPEG, 90, fileStream);
                fileStream.flush();
                fileStream.close();
                Log.e(TAG, ""Failed RAW patch file for camera "" + deviceId + "" saved to "" +
                        rawFilePath);

                fail(""Camera "" + deviceId + "": RAW and JPEG image at  for the same "" +
                        ""frame are not similar, center patches have difference metric of "" +
                        difference);
            } finally {
                if (fileStream != null) {
                    fileStream.close();
                }
            }
        }
    }

    private Pair<Image, CaptureResult> captureSingleRawShot(Size s, boolean waitForAe,
            ImageReader captureReader,
            CameraTestUtils.SimpleImageReaderListener captureListener) throws Exception {
        List<ImageReader> readers = new ArrayList<ImageReader>();
        readers.add(captureReader);
        List<CameraTestUtils.SimpleImageReaderListener> listeners =
                new ArrayList<CameraTestUtils.SimpleImageReaderListener>();
        listeners.add(captureListener);
        Pair<List<Image>, CaptureResult> res = captureSingleRawShot(s, readers, waitForAe,
                listeners);
        return new Pair<Image, CaptureResult>(res.first.get(0), res.second);
    }

    private Pair<List<Image>, CaptureResult> captureSingleRawShot(Size s,
            List<ImageReader> captureReaders, boolean waitForAe,
            List<CameraTestUtils.SimpleImageReaderListener> captureListeners) throws Exception {
        return captureRawShots(s, captureReaders, waitForAe, captureListeners, 1,
                /*maxResolution*/false).get(0);
    }

    private Pair<Image, CaptureResult> captureSingleShotMaximumResolution(Size s,
            ImageReader captureReader, boolean waitForAe,
            CameraTestUtils.SimpleImageReaderListener captureListener)
            throws Exception {
        List<ImageReader> readers = new ArrayList<ImageReader>();
        readers.add(captureReader);
        List<CameraTestUtils.SimpleImageReaderListener> listeners =
                new ArrayList<CameraTestUtils.SimpleImageReaderListener>();
        listeners.add(captureListener);
        Pair<List<Image>, CaptureResult> res = captureRawShots(s, readers, waitForAe,
                listeners, /*numShots*/ 1, /*maxResolution*/ true).get(0);
        return new Pair<Image, CaptureResult>(res.first.get(0), res.second);
    }

    private Pair<Image, CaptureResult> captureReprocessedRawShot(Size sz,
            ImageReader inputReader,
            ImageReader reprocessOutputReader,
            CameraTestUtils.SimpleImageReaderListener inputReaderListener,
            CameraTestUtils.SimpleImageReaderListener reprocessReaderListener,
            boolean waitForAe) throws Exception {

        InputConfiguration inputConfig =
            new InputConfiguration(sz.getWidth(), sz.getHeight(), ImageFormat.RAW_SENSOR);
        CameraTestUtils.SimpleCaptureCallback inputCaptureListener =
                new CameraTestUtils.SimpleCaptureCallback();
        CameraTestUtils.SimpleCaptureCallback reprocessOutputCaptureListener =
                new CameraTestUtils.SimpleCaptureCallback();

        inputReader.setOnImageAvailableListener(inputReaderListener, mHandler);
        reprocessOutputReader.setOnImageAvailableListener(reprocessReaderListener, mHandler);

        ArrayList<Surface> outputSurfaces = new ArrayList<Surface>();
        outputSurfaces.add(inputReader.getSurface());
        outputSurfaces.add(reprocessOutputReader.getSurface());
        BlockingSessionCallback sessionListener = new BlockingSessionCallback();
        ImageReader previewReader = null;
        if (waitForAe) {
            // Also setup a small YUV output for AE metering if needed
            Size yuvSize = (mOrderedPreviewSizes.size() == 0) ? null :
                    mOrderedPreviewSizes.get(mOrderedPreviewSizes.size() - 1);
            assertNotNull(""Must support at least one small YUV size."", yuvSize);
            previewReader = createImageReader(yuvSize, ImageFormat.YUV_420_888,
                        /*maxNumImages*/2, new CameraTestUtils.ImageDropperListener());
            outputSurfaces.add(previewReader.getSurface());
        }

        createReprocessableSession(inputConfig, outputSurfaces);

        if (waitForAe) {
            CaptureRequest.Builder precaptureRequest =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
            assertNotNull(""Fail to get captureRequest"", precaptureRequest);
            precaptureRequest.addTarget(previewReader.getSurface());
            precaptureRequest.set(CaptureRequest.CONTROL_MODE,
                    CaptureRequest.CONTROL_MODE_AUTO);
            precaptureRequest.set(CaptureRequest.CONTROL_AE_MODE,
                    CaptureRequest.CONTROL_AE_MODE_ON);

            final ConditionVariable waitForAeCondition = new ConditionVariable(/*isOpen*/false);
            CameraCaptureSession.CaptureCallback captureCallback =
                    new CameraCaptureSession.CaptureCallback() {
                @Override
                public void onCaptureProgressed(CameraCaptureSession session,
                        CaptureRequest request, CaptureResult partialResult) {
                    Integer aeState = partialResult.get(CaptureResult.CONTROL_AE_STATE);
                    if (aeState != null &&
                            (aeState == CaptureRequest.CONTROL_AE_STATE_CONVERGED ||
                             aeState == CaptureRequest.CONTROL_AE_STATE_FLASH_REQUIRED)) {
                        waitForAeCondition.open();
                    }
                }

                @Override
                public void onCaptureCompleted(CameraCaptureSession session,
                        CaptureRequest request, TotalCaptureResult result) {
                    int aeState = result.get(CaptureResult.CONTROL_AE_STATE);
                    if (aeState == CaptureRequest.CONTROL_AE_STATE_CONVERGED ||
                            aeState == CaptureRequest.CONTROL_AE_STATE_FLASH_REQUIRED) {
                        waitForAeCondition.open();
                    }
                }
            };

            startCapture(precaptureRequest.build(), /*repeating*/true, captureCallback, mHandler);

            precaptureRequest.set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,
                    CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_START);
            startCapture(precaptureRequest.build(), /*repeating*/false, captureCallback, mHandler);
            assertTrue(""Timeout out waiting for AE to converge"",
                    waitForAeCondition.block(AE_TIMEOUT_MS));
        }
        ImageWriter inputWriter =
                ImageWriter.newInstance(mCameraSession.getInputSurface(), 1);
        // Prepare a request for reprocess input
        CaptureRequest.Builder builder = mCamera.createCaptureRequest(
                CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG);
        builder.addTarget(inputReader.getSurface());
        // This is a max resolution capture
        builder.set(CaptureRequest.SENSOR_PIXEL_MODE,
                CameraMetadata.SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION);
        CaptureRequest inputRequest = builder.build();
        mCameraSession.capture(inputRequest, inputCaptureListener, mHandler);
        List<CaptureRequest> reprocessCaptureRequests = new ArrayList<>();

        TotalCaptureResult inputResult =
                inputCaptureListener.getTotalCaptureResult(
                        MAX_RESOLUTION_CAPTURE_WAIT_TIMEOUT_MS);
        builder = mCamera.createReprocessCaptureRequest(inputResult);
        inputWriter.queueInputImage(inputReaderListener.getImage(
                        MAX_RESOLUTION_CAPTURE_WAIT_TIMEOUT_MS));
        builder.addTarget(reprocessOutputReader.getSurface());
        reprocessCaptureRequests.add(builder.build());
        mCameraSession.captureBurst(reprocessCaptureRequests, reprocessOutputCaptureListener,
                mHandler);
        TotalCaptureResult result = reprocessOutputCaptureListener.getTotalCaptureResult(
                CAPTURE_WAIT_TIMEOUT_MS);
        return new Pair<Image, CaptureResult>(reprocessReaderListener.getImage(
                MAX_RESOLUTION_CAPTURE_WAIT_TIMEOUT_MS), result);
    }

    /**
     * Capture raw images.
     *
     * <p>Capture raw images for a given size.</p>
     *
     * @param sz The size of the raw image to capture.  Must be one of the available sizes for this
     *          device.
     *
     * @param captureReaders The image readers which are associated with the targets for this
     *        capture.
     *
     * @param waitForAe Whether we should wait for AE to converge before capturing outputs for
     *                  the captureReaders targets
     *
     * @param captureListeners ImageReader listeners which wait on the captured images to be
     *                         available.
     *
     * @param numShots The number of shots to be captured
     *
     * @param maxResolution Whether the target in captureReaders are max resolution captures. If
     *                      this is set to true, captureReaders.size() must be == 1 ( in order to
     *                      satisfy mandatory streams for maximum resolution sensor pixel mode).
     *
     * @return a list of pairs containing a {@link Image} and {@link CaptureResult} used for
     *          each capture.
     */
    private List<Pair<List<Image>, CaptureResult>> captureRawShots(Size sz,
            List<ImageReader> captureReaders, boolean waitForAe,
            List<CameraTestUtils.SimpleImageReaderListener> captureListeners,
            int numShots, boolean maxResolution) throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""captureSingleRawShot - Capturing raw image."");
        }

        int timeoutScale = maxResolution ? MAX_RESOLUTION_CAPTURE_WAIT_TIMEOUT_SCALE : 1;
        Size[] targetCaptureSizes =
                mStaticInfo.getAvailableSizesForFormatChecked(ImageFormat.RAW_SENSOR,
                        StaticMetadata.StreamDirection.Output, /*fastSizes*/ true,
                        /*slowSizes*/ true, maxResolution);

        if (maxResolution) {
            assertTrue(""Maximum number of maximum resolution targets for a session should be 1 as"" +
                "" per the mandatory streams guarantee"", captureReaders.size() == 1);
        }

        // Validate size
        boolean validSize = false;
        for (int i = 0; i < targetCaptureSizes.length; ++i) {
            if (targetCaptureSizes[i].equals(sz)) {
                validSize = true;
                break;
            }
        }
        assertTrue(""Capture size is supported."", validSize);

        // Capture images.
        final List<Surface> outputSurfaces = new ArrayList<Surface>();
        for (ImageReader captureReader : captureReaders) {
            Surface captureSurface = captureReader.getSurface();
            outputSurfaces.add(captureSurface);
        }

        // Set up still capture template targeting JPEG/RAW outputs
        CaptureRequest.Builder request =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        assertNotNull(""Fail to get captureRequest"", request);
        for (Surface surface : outputSurfaces) {
            request.addTarget(surface);
        }

        ImageReader previewReader = null;
        if (waitForAe) {
            // Also setup a small YUV output for AE metering if needed
            Size yuvSize = (mOrderedPreviewSizes.size() == 0) ? null :
                    mOrderedPreviewSizes.get(mOrderedPreviewSizes.size() - 1);
            assertNotNull(""Must support at least one small YUV size."", yuvSize);
            previewReader = createImageReader(yuvSize, ImageFormat.YUV_420_888,
                        /*maxNumImages*/2, new CameraTestUtils.ImageDropperListener());
            outputSurfaces.add(previewReader.getSurface());
        }

        createSession(outputSurfaces);

        if (waitForAe) {
            CaptureRequest.Builder precaptureRequest =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
            assertNotNull(""Fail to get captureRequest"", precaptureRequest);
            precaptureRequest.addTarget(previewReader.getSurface());
            precaptureRequest.set(CaptureRequest.CONTROL_MODE,
                    CaptureRequest.CONTROL_MODE_AUTO);
            precaptureRequest.set(CaptureRequest.CONTROL_AE_MODE,
                    CaptureRequest.CONTROL_AE_MODE_ON);

            final ConditionVariable waitForAeCondition = new ConditionVariable(/*isOpen*/false);
            CameraCaptureSession.CaptureCallback captureCallback =
                    new CameraCaptureSession.CaptureCallback() {
                @Override
                public void onCaptureProgressed(CameraCaptureSession session,
                        CaptureRequest request, CaptureResult partialResult) {
                    Integer aeState = partialResult.get(CaptureResult.CONTROL_AE_STATE);
                    if (aeState != null &&
                            (aeState == CaptureRequest.CONTROL_AE_STATE_CONVERGED ||
                             aeState == CaptureRequest.CONTROL_AE_STATE_FLASH_REQUIRED)) {
                        waitForAeCondition.open();
                    }
                }

                @Override
                public void onCaptureCompleted(CameraCaptureSession session,
                        CaptureRequest request, TotalCaptureResult result) {
                    int aeState = result.get(CaptureResult.CONTROL_AE_STATE);
                    if (aeState == CaptureRequest.CONTROL_AE_STATE_CONVERGED ||
                            aeState == CaptureRequest.CONTROL_AE_STATE_FLASH_REQUIRED) {
                        waitForAeCondition.open();
                    }
                }
            };
            startCapture(precaptureRequest.build(), /*repeating*/true, captureCallback, mHandler);

            precaptureRequest.set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,
                    CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_START);
            startCapture(precaptureRequest.build(), /*repeating*/false, captureCallback, mHandler);
            assertTrue(""Timeout out waiting for AE to converge"",
                    waitForAeCondition.block(AE_TIMEOUT_MS));
        }

        request.set(CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE,
                CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE_ON);
        if (maxResolution) {
            request.set(CaptureRequest.SENSOR_PIXEL_MODE,
                    CaptureRequest.SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION);
        }
        CameraTestUtils.SimpleCaptureCallback resultListener =
                new CameraTestUtils.SimpleCaptureCallback();

        CaptureRequest request1 = request.build();
        for (int i = 0; i < numShots; i++) {
            startCapture(request1, /*repeating*/false, resultListener, mHandler);
        }
        List<Pair<List<Image>, CaptureResult>> ret = new ArrayList<>();
        for (int i = 0; i < numShots; i++) {
            // Verify capture result and images
            CaptureResult result = resultListener.getCaptureResult(CAPTURE_WAIT_TIMEOUT_MS);

            List<Image> resultImages = new ArrayList<Image>();
            for (CameraTestUtils.SimpleImageReaderListener captureListener : captureListeners) {
                Image captureImage =
                        captureListener.getImage(CAPTURE_WAIT_TIMEOUT_MS * timeoutScale);

            /*CameraTestUtils.validateImage(captureImage, s.getWidth(), s.getHeight(),
                    ImageFormat.RAW_SENSOR, null);*/
                resultImages.add(captureImage);
            }
            ret.add(new Pair<List<Image>, CaptureResult>(resultImages, result));
        }
        // Stop capture, delete the streams.
        stopCapture(/*fast*/false);

        return ret;
    }

    /**
     * Use the DNG SDK to validate a DNG file stored in the buffer.
     *
     * Returns false if the DNG has validation errors. Validation warnings/errors
     * will be printed to logcat.
     */
    private static native boolean validateDngNative(byte[] dngBuffer);
}"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.StaticMetadataTest"	"testHwSupportedLevel"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/StaticMetadataTest.java"	""	"public void testHwSupportedLevel() throws Exception {
        Key<StreamConfigurationMap> key =
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP;
        final float SIZE_ERROR_MARGIN = 0.03f;
        for (String id : mAllCameraIds) {
            initStaticMetadata(id);
            StreamConfigurationMap configs = mStaticInfo.getValueFromKeyNonNull(key);
            Rect activeRect = mStaticInfo.getActiveArraySizeChecked();
            Size sensorSize = new Size(activeRect.width(), activeRect.height());
            List<Integer> availableCaps = mStaticInfo.getAvailableCapabilitiesChecked();

            mCollector.expectTrue(""All devices must contains BACKWARD_COMPATIBLE capability or "" +
                    ""DEPTH_OUTPUT capabillity"" ,
                    availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE) ||
                    availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_DEPTH_OUTPUT) );

            if (mStaticInfo.isHardwareLevelAtLeast(
                    CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_3)) {
                mCollector.expectTrue(""Level 3 device must contain YUV_REPROCESSING capability"",
                        availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING));
                mCollector.expectTrue(""Level 3 device must contain RAW capability"",
                        availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_RAW));
            }

            if (mStaticInfo.isHardwareLevelAtLeastFull()) {
                // Capability advertisement must be right.
                mCollector.expectTrue(""Full device must contain MANUAL_SENSOR capability"",
                        availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR));
                mCollector.expectTrue(""Full device must contain MANUAL_POST_PROCESSING capability"",
                        availableCaps.contains(
                                REQUEST_AVAILABLE_CAPABILITIES_MANUAL_POST_PROCESSING));
                mCollector.expectTrue(""Full device must contain BURST_CAPTURE capability"",
                        availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_BURST_CAPTURE));

                // Need support per frame control
                mCollector.expectTrue(""Full device must support per frame control"",
                        mStaticInfo.isPerFrameControlSupported());
            }

            if (mStaticInfo.isHardwareLevelLegacy()) {
                mCollector.expectTrue(""Legacy devices must contain BACKWARD_COMPATIBLE capability"",
                        availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE));
            }

            if (availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
                mCollector.expectTrue(""MANUAL_SENSOR capability always requires "" +
                        ""READ_SENSOR_SETTINGS capability as well"",
                        availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS));
            }

            if (mStaticInfo.isColorOutputSupported()) {
                // Max jpeg resolution must be very close to  sensor resolution
                Size[] jpegSizes = mStaticInfo.getJpegOutputSizesChecked();
                Size maxJpegSize = CameraTestUtils.getMaxSize(jpegSizes);
                float croppedWidth = (float)sensorSize.getWidth();
                float croppedHeight = (float)sensorSize.getHeight();
                float sensorAspectRatio = (float)sensorSize.getWidth() / (float)sensorSize.getHeight();
                float maxJpegAspectRatio = (float)maxJpegSize.getWidth() / (float)maxJpegSize.getHeight();
                if (sensorAspectRatio < maxJpegAspectRatio) {
                    croppedHeight = (float)sensorSize.getWidth() / maxJpegAspectRatio;
                } else if (sensorAspectRatio > maxJpegAspectRatio) {
                    croppedWidth = (float)sensorSize.getHeight() * maxJpegAspectRatio;
                }
                Size croppedSensorSize = new Size((int)croppedWidth, (int)croppedHeight);
                mCollector.expectSizesAreSimilar(
                    ""Active array size or cropped active array size and max JPEG size should be similar"",
                    croppedSensorSize, maxJpegSize, SIZE_ERROR_MARGIN);
            }

            // TODO: test all the keys mandatory for all capability devices.
        }
    }

    /**
     * Test max number of output stream reported by device
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.StaticMetadataTest"	"testCapabilities"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/StaticMetadataTest.java"	""	"public void testCapabilities() throws Exception {
        for (String id : mAllCameraIds) {
            initStaticMetadata(id);
            List<Integer> availableCaps = mStaticInfo.getAvailableCapabilitiesChecked();

            for (Integer capability = REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE;
                    capability <= StaticMetadata.LAST_CAPABILITY_ENUM; capability++) {
                boolean isCapabilityAvailable = availableCaps.contains(capability);
                validateCapability(capability, isCapabilityAvailable);
            }
            // Note: Static metadata for capabilities is tested in ExtendedCameraCharacteristicsTest
        }
    }

    /**
     * Check if request keys' presence match expectation.
     *
     * @param capabilityName The name string of capability being tested. Used for output messages.
     * @param requestKeys The capture request keys to be checked
     * @param expectedPresence Expected presence of {@code requestKeys}. {@code true} for expecting
     *        all keys are available. Otherwise {@code false}
     * @return {@code true} if request keys' presence match expectation. Otherwise {@code false}
     */
    private boolean validateRequestKeysPresence(String capabilityName,
            Collection<CaptureRequest.Key<?>> requestKeys, boolean expectedPresence) {
        boolean actualPresence = mStaticInfo.areRequestKeysAvailable(requestKeys);
        if (expectedPresence != actualPresence) {
            if (expectedPresence) {
                for (CaptureRequest.Key<?> key : requestKeys) {
                    if (!mStaticInfo.areKeysAvailable(key)) {
                        mCollector.addMessage(String.format(
                                ""Camera %s list capability %s but doesn't contain request key %s"",
                                mCameraId, capabilityName, key.getName()));
                    }
                }
            } else {
                Log.w(TAG, String.format(
                        ""Camera %s doesn't list capability %s but contain all required keys"",
                        mCameraId, capabilityName));
            }
            return false;
        }
        return true;
    }

    /**
     * Check if result keys' presence match expectation.
     *
     * @param capabilityName The name string of capability being tested. Used for output messages.
     * @param resultKeys The capture result keys to be checked
     * @param expectedPresence Expected presence of {@code resultKeys}. {@code true} for expecting
     *        all keys are available. Otherwise {@code false}
     * @return {@code true} if result keys' presence match expectation. Otherwise {@code false}
     */
    private boolean validateResultKeysPresence(String capabilityName,
            Collection<CaptureResult.Key<?>> resultKeys, boolean expectedPresence) {
        boolean actualPresence = mStaticInfo.areResultKeysAvailable(resultKeys);
        if (expectedPresence != actualPresence) {
            if (expectedPresence) {
                for (CaptureResult.Key<?> key : resultKeys) {
                    if (!mStaticInfo.areKeysAvailable(key)) {
                        mCollector.addMessage(String.format(
                                ""Camera %s list capability %s but doesn't contain result key %s"",
                                mCameraId, capabilityName, key.getName()));
                    }
                }
            } else {
                Log.w(TAG, String.format(
                        ""Camera %s doesn't list capability %s but contain all required keys"",
                        mCameraId, capabilityName));
            }
            return false;
        }
        return true;
    }

    /**
     * Check if characteristics keys' presence match expectation.
     *
     * @param capabilityName The name string of capability being tested. Used for output messages.
     * @param characteristicsKeys The characteristics keys to be checked
     * @param expectedPresence Expected presence of {@code characteristicsKeys}. {@code true} for
     *        expecting all keys are available. Otherwise {@code false}
     * @return {@code true} if characteristics keys' presence match expectation.
     *         Otherwise {@code false}
     */
    private boolean validateCharacteristicsKeysPresence(String capabilityName,
            Collection<CameraCharacteristics.Key<?>> characteristicsKeys,
            boolean expectedPresence) {
        boolean actualPresence = mStaticInfo.areCharacteristicsKeysAvailable(characteristicsKeys);
        if (expectedPresence != actualPresence) {
            if (expectedPresence) {
                for (CameraCharacteristics.Key<?> key : characteristicsKeys) {
                    if (!mStaticInfo.areKeysAvailable(key)) {
                        mCollector.addMessage(String.format(
                                ""Camera %s list capability %s but doesn't contain"" +
                                ""characteristics key %s"",
                                mCameraId, capabilityName, key.getName()));
                    }
                }
            } else {
                Log.w(TAG, String.format(
                        ""Camera %s doesn't list capability %s but contain all required keys"",
                        mCameraId, capabilityName));
            }
            return false;
        }
        return true;
    }

    private void validateCapability(Integer capability, boolean isCapabilityAvailable) {
        List<CaptureRequest.Key<?>> requestKeys = new ArrayList<>();
        Set<CaptureResult.Key<?>> resultKeys = new HashSet<>();
        // Capability requirements other than key presences
        List<Pair<String, Boolean>> additionalRequirements = new ArrayList<>();

        /* For available capabilities, only check request keys in this test
           Characteristics keys are tested in ExtendedCameraCharacteristicsTest
           Result keys are tested in CaptureResultTest */
        String capabilityName;
        switch (capability) {
            case REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE:
                capabilityName = ""REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE"";
                requestKeys.add(CaptureRequest.CONTROL_AE_ANTIBANDING_MODE);
                requestKeys.add(CaptureRequest.CONTROL_AE_EXPOSURE_COMPENSATION);
                requestKeys.add(CaptureRequest.CONTROL_AE_MODE);
                requestKeys.add(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE);
                requestKeys.add(CaptureRequest.CONTROL_AF_MODE);
                requestKeys.add(CaptureRequest.CONTROL_AF_TRIGGER);
                requestKeys.add(CaptureRequest.CONTROL_AWB_MODE);
                requestKeys.add(CaptureRequest.CONTROL_CAPTURE_INTENT);
                requestKeys.add(CaptureRequest.CONTROL_EFFECT_MODE);
                requestKeys.add(CaptureRequest.CONTROL_MODE);
                requestKeys.add(CaptureRequest.CONTROL_SCENE_MODE);
                requestKeys.add(CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE);
                requestKeys.add(CaptureRequest.CONTROL_ZOOM_RATIO);
                requestKeys.add(CaptureRequest.FLASH_MODE);
                requestKeys.add(CaptureRequest.JPEG_GPS_LOCATION);
                requestKeys.add(CaptureRequest.JPEG_ORIENTATION);
                requestKeys.add(CaptureRequest.JPEG_QUALITY);
                requestKeys.add(CaptureRequest.JPEG_THUMBNAIL_QUALITY);
                requestKeys.add(CaptureRequest.JPEG_THUMBNAIL_SIZE);
                requestKeys.add(CaptureRequest.SCALER_CROP_REGION);
                requestKeys.add(CaptureRequest.STATISTICS_FACE_DETECT_MODE);
                if (mStaticInfo.getAeMaxRegionsChecked() > 0) {
                    requestKeys.add(CaptureRequest.CONTROL_AE_REGIONS);
                } else {
                    mCollector.expectTrue(
                            ""CONTROL_AE_REGIONS is available but aeMaxRegion is 0"",
                            !mStaticInfo.areKeysAvailable(CaptureRequest.CONTROL_AE_REGIONS));
                }
                if (mStaticInfo.getAwbMaxRegionsChecked() > 0) {
                    requestKeys.add(CaptureRequest.CONTROL_AWB_REGIONS);
                } else {
                    mCollector.expectTrue(
                            ""CONTROL_AWB_REGIONS is available but awbMaxRegion is 0"",
                            !mStaticInfo.areKeysAvailable(CaptureRequest.CONTROL_AWB_REGIONS));
                }
                if (mStaticInfo.getAfMaxRegionsChecked() > 0) {
                    requestKeys.add(CaptureRequest.CONTROL_AF_REGIONS);
                } else {
                    mCollector.expectTrue(
                            ""CONTROL_AF_REGIONS is available but afMaxRegion is 0"",
                            !mStaticInfo.areKeysAvailable(CaptureRequest.CONTROL_AF_REGIONS));
                }
                break;
            case REQUEST_AVAILABLE_CAPABILITIES_MANUAL_POST_PROCESSING:
                capabilityName = ""REQUEST_AVAILABLE_CAPABILITIES_MANUAL_POST_PROCESSING"";
                requestKeys.add(CaptureRequest.TONEMAP_MODE);
                requestKeys.add(CaptureRequest.COLOR_CORRECTION_GAINS);
                requestKeys.add(CaptureRequest.COLOR_CORRECTION_TRANSFORM);
                requestKeys.add(CaptureRequest.SHADING_MODE);
                requestKeys.add(CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE);
                requestKeys.add(CaptureRequest.TONEMAP_CURVE);
                requestKeys.add(CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE);
                requestKeys.add(CaptureRequest.CONTROL_AWB_LOCK);

                // Legacy mode always doesn't support these requirements
                Boolean contrastCurveModeSupported = false;
                Boolean gammaAndPresetModeSupported = false;
                Boolean offColorAberrationModeSupported = false;
                if (mStaticInfo.isHardwareLevelAtLeastLimited() && mStaticInfo.isColorOutputSupported()) {
                    int[] tonemapModes = mStaticInfo.getAvailableToneMapModesChecked();
                    List<Integer> modeList = (tonemapModes.length == 0) ?
                            new ArrayList<Integer>() :
                            Arrays.asList(CameraTestUtils.toObject(tonemapModes));
                    contrastCurveModeSupported =
                            modeList.contains(CameraMetadata.TONEMAP_MODE_CONTRAST_CURVE);
                    gammaAndPresetModeSupported =
                            modeList.contains(CameraMetadata.TONEMAP_MODE_GAMMA_VALUE) &&
                            modeList.contains(CameraMetadata.TONEMAP_MODE_PRESET_CURVE);

                    int[] colorAberrationModes =
                            mStaticInfo.getAvailableColorAberrationModesChecked();
                    modeList = (colorAberrationModes.length == 0) ?
                            new ArrayList<Integer>() :
                            Arrays.asList(CameraTestUtils.toObject(colorAberrationModes));
                    offColorAberrationModeSupported =
                            modeList.contains(CameraMetadata.COLOR_CORRECTION_ABERRATION_MODE_OFF);
                }
                Boolean tonemapModeQualified =
                        contrastCurveModeSupported || gammaAndPresetModeSupported;
                additionalRequirements.add(new Pair<String, Boolean>(
                        ""Tonemap mode must include {CONTRAST_CURVE} and/or "" +
                        ""{GAMMA_VALUE, PRESET_CURVE}"",
                        tonemapModeQualified));
                additionalRequirements.add(new Pair<String, Boolean>(
                        ""Color aberration mode must include OFF"", offColorAberrationModeSupported));
                additionalRequirements.add(new Pair<String, Boolean>(
                        ""Must support AWB lock"", mStaticInfo.isAwbLockSupported()));
                break;
            case REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR:
                capabilityName = ""REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR"";
                requestKeys.add(CaptureRequest.CONTROL_AE_LOCK);
                requestKeys.add(CaptureRequest.SENSOR_FRAME_DURATION);
                requestKeys.add(CaptureRequest.SENSOR_EXPOSURE_TIME);
                requestKeys.add(CaptureRequest.SENSOR_SENSITIVITY);
                if (mStaticInfo.hasFocuser()) {
                    requestKeys.add(CaptureRequest.LENS_APERTURE);
                    requestKeys.add(CaptureRequest.LENS_FOCUS_DISTANCE);
                    requestKeys.add(CaptureRequest.LENS_FILTER_DENSITY);
                    requestKeys.add(CaptureRequest.LENS_OPTICAL_STABILIZATION_MODE);
                }
                requestKeys.add(CaptureRequest.BLACK_LEVEL_LOCK);
                additionalRequirements.add(new Pair<String, Boolean>(
                        ""Must support AE lock"", mStaticInfo.isAeLockSupported()));
                break;
            case REQUEST_AVAILABLE_CAPABILITIES_RAW:
                // RAW_CAPABILITY needs to check for not just capture request keys
                validateRawCapability(isCapabilityAvailable);
                return;
            case REQUEST_AVAILABLE_CAPABILITIES_BURST_CAPTURE:
                // Tested in ExtendedCameraCharacteristicsTest
                return;
            case REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS:
                capabilityName = ""REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS"";
                resultKeys.add(CaptureResult.SENSOR_FRAME_DURATION);
                resultKeys.add(CaptureResult.SENSOR_EXPOSURE_TIME);
                resultKeys.add(CaptureResult.SENSOR_SENSITIVITY);
                if (mStaticInfo.hasFocuser()) {
                    resultKeys.add(CaptureResult.LENS_APERTURE);
                    resultKeys.add(CaptureResult.LENS_FOCUS_DISTANCE);
                    resultKeys.add(CaptureResult.LENS_FILTER_DENSITY);
                }
                break;

            case REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING:
            case REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING:
            case REQUEST_AVAILABLE_CAPABILITIES_REMOSAIC_REPROCESSING:
                // Tested in ExtendedCameraCharacteristicsTest
                return;
            case REQUEST_AVAILABLE_CAPABILITIES_DEPTH_OUTPUT:
                // Tested in ExtendedCameracharacteristicsTest
                return;
            case REQUEST_AVAILABLE_CAPABILITIES_CONSTRAINED_HIGH_SPEED_VIDEO:
            case REQUEST_AVAILABLE_CAPABILITIES_MOTION_TRACKING:
            case REQUEST_AVAILABLE_CAPABILITIES_LOGICAL_MULTI_CAMERA:
            case REQUEST_AVAILABLE_CAPABILITIES_MONOCHROME:
                // Tested in ExtendedCameraCharacteristicsTest
                return;
            case REQUEST_AVAILABLE_CAPABILITIES_SECURE_IMAGE_DATA:
                if (!isCapabilityAvailable) {
                    mCollector.expectTrue(
                        ""SCALER_DEFAULT_SECURE_IMAGE_SIZE must not present if the device"" +
                                ""does not support SECURE_IMAGE_DATA capability"",
                        !mStaticInfo.areKeysAvailable(
                                CameraCharacteristics.SCALER_DEFAULT_SECURE_IMAGE_SIZE));
                }
                return;
            case REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR:
                resultKeys.add(CaptureResult.SENSOR_RAW_BINNING_FACTOR_USED);
                resultKeys.add(CaptureResult.SENSOR_PIXEL_MODE);
                requestKeys.add(CaptureRequest.SENSOR_PIXEL_MODE);
                additionalRequirements.add(new Pair<String, Boolean>(
                        ""Must support maximum resolution keys"",
                        mStaticInfo.areMaximumResolutionKeysSupported()));
                return;
            default:
                capabilityName = ""Unknown"";
                assertTrue(String.format(""Unknown capability set: %d"", capability),
                           !isCapabilityAvailable);
                return;
        }

        // Check additional requirements and exit early if possible
        if (!isCapabilityAvailable) {
            for (Pair<String, Boolean> p : additionalRequirements) {
                String requirement = p.first;
                Boolean meetRequirement = p.second;
                // No further check is needed if we've found why capability cannot be advertised
                if (!meetRequirement) {
                    Log.v(TAG, String.format(
                            ""Camera %s doesn't list capability %s because of requirement: %s"",
                            mCameraId, capabilityName, requirement));
                    return;
                }
            }
        }

        boolean matchExpectation = true;
        if (!requestKeys.isEmpty()) {
            matchExpectation &= validateRequestKeysPresence(
                    capabilityName, requestKeys, isCapabilityAvailable);
        }
        if(!resultKeys.isEmpty()) {
            matchExpectation &= validateResultKeysPresence(
                    capabilityName, resultKeys, isCapabilityAvailable);
        }

        // Check additional requirements
        for (Pair<String, Boolean> p : additionalRequirements) {
            String requirement = p.first;
            Boolean meetRequirement = p.second;
            if (isCapabilityAvailable && !meetRequirement) {
                mCollector.addMessage(String.format(
                        ""Camera %s list capability %s but does not meet the requirement: %s"",
                        mCameraId, capabilityName, requirement));
            }
        }

        // In case of isCapabilityAvailable == true, error has been filed in
        // validateRequest/ResultKeysPresence
        if (!matchExpectation && !isCapabilityAvailable) {
            mCollector.addMessage(String.format(
                    ""Camera %s doesn't list capability %s but meets all requirements"",
                    mCameraId, capabilityName));
        }
    }

    private void validateRawCapability(boolean isCapabilityAvailable) {
        String capabilityName = ""REQUEST_AVAILABLE_CAPABILITIES_RAW"";

        Set<CaptureRequest.Key<?>> requestKeys = new HashSet<>();
        requestKeys.add(CaptureRequest.HOT_PIXEL_MODE);
        requestKeys.add(CaptureRequest.STATISTICS_HOT_PIXEL_MAP_MODE);

        Set<CameraCharacteristics.Key<?>> characteristicsKeys = new HashSet<>();
        characteristicsKeys.add(HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES);
        characteristicsKeys.add(SENSOR_BLACK_LEVEL_PATTERN);
        characteristicsKeys.add(SENSOR_INFO_ACTIVE_ARRAY_SIZE);
        characteristicsKeys.add(SENSOR_INFO_COLOR_FILTER_ARRANGEMENT);
        characteristicsKeys.add(SENSOR_INFO_WHITE_LEVEL);
        characteristicsKeys.add(STATISTICS_INFO_AVAILABLE_HOT_PIXEL_MAP_MODES);
        if (!mStaticInfo.isMonochromeCamera()) {
            characteristicsKeys.add(SENSOR_CALIBRATION_TRANSFORM1);
            characteristicsKeys.add(SENSOR_COLOR_TRANSFORM1);
            characteristicsKeys.add(SENSOR_FORWARD_MATRIX1);
            characteristicsKeys.add(SENSOR_REFERENCE_ILLUMINANT1);
        }

        Set<CaptureResult.Key<?>> resultKeys = new HashSet<>();
        resultKeys.add(CaptureResult.SENSOR_NOISE_PROFILE);
        if (!mStaticInfo.isMonochromeCamera()) {
            resultKeys.add(CaptureResult.SENSOR_GREEN_SPLIT);
            resultKeys.add(CaptureResult.SENSOR_NEUTRAL_COLOR_POINT);
        }

        boolean rawOutputSupported = mStaticInfo.getRawOutputSizesChecked().length > 0;
        boolean requestKeysPresent = mStaticInfo.areRequestKeysAvailable(requestKeys);
        boolean characteristicsKeysPresent =
                mStaticInfo.areCharacteristicsKeysAvailable(characteristicsKeys);
        boolean resultKeysPresent = mStaticInfo.areResultKeysAvailable(resultKeys);
        boolean expectCapabilityPresent = rawOutputSupported && requestKeysPresent &&
                characteristicsKeysPresent && resultKeysPresent;

        if (isCapabilityAvailable != expectCapabilityPresent) {
            if (isCapabilityAvailable) {
                mCollector.expectTrue(
                        ""REQUEST_AVAILABLE_CAPABILITIES_RAW should support RAW_SENSOR output"",
                        rawOutputSupported);
                validateRequestKeysPresence(capabilityName, requestKeys, isCapabilityAvailable);
                validateResultKeysPresence(capabilityName, resultKeys, isCapabilityAvailable);
                validateCharacteristicsKeysPresence(capabilityName, characteristicsKeys,
                        isCapabilityAvailable);
            } else {
                mCollector.addMessage(String.format(
                        ""Camera %s doesn't list capability %s but contain all required keys"" +
                        "" and RAW format output"",
                        mCameraId, capabilityName));
            }
        }
    }

    /**
     * Test lens facing.
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ImageReaderTest"	"testUsageRespected"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ImageReaderTest.java"	""	"public void testUsageRespected() throws Exception {
        final long REQUESTED_USAGE_BITS =
                HardwareBuffer.USAGE_GPU_COLOR_OUTPUT | HardwareBuffer.USAGE_GPU_SAMPLED_IMAGE;
        ImageReader reader = ImageReader.newInstance(1, 1, PixelFormat.RGBA_8888, 1,
                REQUESTED_USAGE_BITS);
        Surface surface = reader.getSurface();
        Canvas canvas = surface.lockHardwareCanvas();
        canvas.drawColor(Color.RED);
        surface.unlockCanvasAndPost(canvas);
        Image image = null;
        for (int i = 0; i < 100; i++) {
            image = reader.acquireNextImage();
            if (image != null) break;
            Thread.sleep(10);
        }
        assertNotNull(image);
        HardwareBuffer buffer = image.getHardwareBuffer();
        assertNotNull(buffer);
        // Mask off the upper vendor bits
        int myBits = (int) (buffer.getUsage() & 0xFFFFFFF);
        assertWithMessage(""Usage bits %s did not contain requested usage bits %s"", myBits,
                REQUESTED_USAGE_BITS).that(myBits & REQUESTED_USAGE_BITS)
                        .isEqualTo(REQUESTED_USAGE_BITS);
    }

    /**
     * Convert a rectangular patch in a YUV image to an ARGB color array.
     *
     * @param w width of the patch.
     * @param h height of the patch.
     * @param wOffset offset of the left side of the patch.
     * @param hOffset offset of the top of the patch.
     * @param yuvImage a YUV image to select a patch from.
     * @return the image patch converted to RGB as an ARGB color array.
     */
    private static int[] convertPixelYuvToRgba(int w, int h, int wOffset, int hOffset,
                                               Image yuvImage) {
        final int CHANNELS = 3; // yuv
        final float COLOR_RANGE = 255f;

        assertTrue(""Invalid argument to convertPixelYuvToRgba"",
                w > 0 && h > 0 && wOffset >= 0 && hOffset >= 0);
        assertNotNull(yuvImage);

        int imageFormat = yuvImage.getFormat();
        assertTrue(""YUV image must have YUV-type format"",
                imageFormat == ImageFormat.YUV_420_888 || imageFormat == ImageFormat.YV12 ||
                        imageFormat == ImageFormat.NV21);

        int height = yuvImage.getHeight();
        int width = yuvImage.getWidth();

        Rect imageBounds = new Rect(/*left*/0, /*top*/0, /*right*/width, /*bottom*/height);
        Rect crop = new Rect(/*left*/wOffset, /*top*/hOffset, /*right*/wOffset + w,
                /*bottom*/hOffset + h);
        assertTrue(""Output rectangle"" + crop + "" must lie within image bounds "" + imageBounds,
                imageBounds.contains(crop));
        Image.Plane[] planes = yuvImage.getPlanes();

        Image.Plane yPlane = planes[0];
        Image.Plane cbPlane = planes[1];
        Image.Plane crPlane = planes[2];

        ByteBuffer yBuf = yPlane.getBuffer();
        int yPixStride = yPlane.getPixelStride();
        int yRowStride = yPlane.getRowStride();
        ByteBuffer cbBuf = cbPlane.getBuffer();
        int cbPixStride = cbPlane.getPixelStride();
        int cbRowStride = cbPlane.getRowStride();
        ByteBuffer crBuf = crPlane.getBuffer();
        int crPixStride = crPlane.getPixelStride();
        int crRowStride = crPlane.getRowStride();

        int[] output = new int[w * h];

        // TODO: Optimize this with renderscript intrinsics
        byte[] yRow = new byte[yPixStride * (w - 1) + 1];
        byte[] cbRow = new byte[cbPixStride * (w / 2 - 1) + 1];
        byte[] crRow = new byte[crPixStride * (w / 2 - 1) + 1];
        yBuf.mark();
        cbBuf.mark();
        crBuf.mark();
        int initialYPos = yBuf.position();
        int initialCbPos = cbBuf.position();
        int initialCrPos = crBuf.position();
        int outputPos = 0;
        for (int i = hOffset; i < hOffset + h; i++) {
            yBuf.position(initialYPos + i * yRowStride + wOffset * yPixStride);
            yBuf.get(yRow);
            if ((i & 1) == (hOffset & 1)) {
                cbBuf.position(initialCbPos + (i / 2) * cbRowStride + wOffset * cbPixStride / 2);
                cbBuf.get(cbRow);
                crBuf.position(initialCrPos + (i / 2) * crRowStride + wOffset * crPixStride / 2);
                crBuf.get(crRow);
            }
            for (int j = 0, yPix = 0, crPix = 0, cbPix = 0; j < w; j++, yPix += yPixStride) {
                float y = yRow[yPix] & 0xFF;
                float cb = cbRow[cbPix] & 0xFF;
                float cr = crRow[crPix] & 0xFF;

                // convert YUV -> RGB (from JFIF's ""Conversion to and from RGB"" section)
                int r = (int) Math.max(0.0f, Math.min(COLOR_RANGE, y + 1.402f * (cr - 128)));
                int g = (int) Math.max(0.0f,
                        Math.min(COLOR_RANGE, y - 0.34414f * (cb - 128) - 0.71414f * (cr - 128)));
                int b = (int) Math.max(0.0f, Math.min(COLOR_RANGE, y + 1.772f * (cb - 128)));

                // Convert to ARGB pixel color (use opaque alpha)
                output[outputPos++] = Color.rgb(r, g, b);

                if ((j & 1) == 1) {
                    crPix += crPixStride;
                    cbPix += cbPixStride;
                }
            }
        }
        yBuf.rewind();
        cbBuf.rewind();
        crBuf.rewind();

        return output;
    }

    /**
     * Test capture a given format stream with yuv stream simultaneously.
     *
     * <p>Use fixed yuv size, varies targeted format capture size. Single capture is tested.</p>
     *
     * @param format The capture format to be tested along with yuv format.
     */
    private void bufferFormatWithYuvTestByCamera(int format) throws Exception {
        bufferFormatWithYuvTestByCamera(format, false);
    }

    /**
     * Test capture a given format stream with yuv stream simultaneously.
     *
     * <p>Use fixed yuv size, varies targeted format capture size. Single capture is tested.</p>
     *
     * @param format The capture format to be tested along with yuv format.
     * @param setUsageFlag The ImageReader factory method to be used (with or without specifying
     *                     usage flag)
     */
    private void bufferFormatWithYuvTestByCamera(int format, boolean setUsageFlag)
            throws Exception {
        if (format != ImageFormat.JPEG && format != ImageFormat.RAW_SENSOR
                && format != ImageFormat.YUV_420_888) {
            throw new IllegalArgumentException(""Unsupported format: "" + format);
        }

        final int NUM_SINGLE_CAPTURE_TESTED = MAX_NUM_IMAGES - 1;
        Size maxYuvSz = mOrderedPreviewSizes.get(0);
        Size[] targetCaptureSizes = mStaticInfo.getAvailableSizesForFormatChecked(format,
                StaticMetadata.StreamDirection.Output);

        for (Size captureSz : targetCaptureSizes) {
            if (VERBOSE) {
                Log.v(TAG, ""Testing yuv size "" + maxYuvSz.toString() + "" and capture size ""
                        + captureSz.toString() + "" for camera "" + mCamera.getId());
            }

            ImageReader captureReader = null;
            ImageReader yuvReader = null;
            try {
                // Create YUV image reader
                SimpleImageReaderListener yuvListener  = new SimpleImageReaderListener();
                if (setUsageFlag) {
                    yuvReader = createImageReader(maxYuvSz, ImageFormat.YUV_420_888, MAX_NUM_IMAGES,
                            HardwareBuffer.USAGE_CPU_READ_OFTEN, yuvListener);
                } else {
                    yuvReader = createImageReader(maxYuvSz, ImageFormat.YUV_420_888, MAX_NUM_IMAGES,
                            yuvListener);
                }

                Surface yuvSurface = yuvReader.getSurface();

                // Create capture image reader
                SimpleImageReaderListener captureListener = new SimpleImageReaderListener();
                if (setUsageFlag) {
                    captureReader = createImageReader(captureSz, format, MAX_NUM_IMAGES,
                            HardwareBuffer.USAGE_CPU_READ_OFTEN, captureListener);
                } else {
                    captureReader = createImageReader(captureSz, format, MAX_NUM_IMAGES,
                            captureListener);
                }
                Surface captureSurface = captureReader.getSurface();

                // Capture images.
                List<Surface> outputSurfaces = new ArrayList<Surface>();
                outputSurfaces.add(yuvSurface);
                outputSurfaces.add(captureSurface);
                CaptureRequest.Builder request = prepareCaptureRequestForSurfaces(outputSurfaces,
                        CameraDevice.TEMPLATE_PREVIEW);
                SimpleCaptureCallback resultListener = new SimpleCaptureCallback();

                for (int i = 0; i < NUM_SINGLE_CAPTURE_TESTED; i++) {
                    startCapture(request.build(), /*repeating*/false, resultListener, mHandler);
                }

                // Verify capture result and images
                for (int i = 0; i < NUM_SINGLE_CAPTURE_TESTED; i++) {
                    resultListener.getCaptureResult(CAPTURE_WAIT_TIMEOUT_MS);
                    if (VERBOSE) {
                        Log.v(TAG, "" Got the capture result back for "" + i + ""th capture"");
                    }

                    Image yuvImage = yuvListener.getImage(CAPTURE_WAIT_TIMEOUT_MS);
                    if (VERBOSE) {
                        Log.v(TAG, "" Got the yuv image back for "" + i + ""th capture"");
                    }

                    Image captureImage = captureListener.getImage(CAPTURE_WAIT_TIMEOUT_MS);
                    if (VERBOSE) {
                        Log.v(TAG, "" Got the capture image back for "" + i + ""th capture"");
                    }

                    //Validate captured images.
                    CameraTestUtils.validateImage(yuvImage, maxYuvSz.getWidth(),
                            maxYuvSz.getHeight(), ImageFormat.YUV_420_888, /*filePath*/null);
                    CameraTestUtils.validateImage(captureImage, captureSz.getWidth(),
                            captureSz.getHeight(), format, /*filePath*/null);
                    yuvImage.close();
                    captureImage.close();
                }

                // Stop capture, delete the streams.
                stopCapture(/*fast*/false);
            } finally {
                closeImageReader(captureReader);
                captureReader = null;
                closeImageReader(yuvReader);
                yuvReader = null;
            }
        }
    }

    private void invalidAccessTestAfterClose() throws Exception {
        final int FORMAT = mStaticInfo.isColorOutputSupported() ?
            ImageFormat.YUV_420_888 : ImageFormat.DEPTH16;

        Size[] availableSizes = mStaticInfo.getAvailableSizesForFormatChecked(FORMAT,
                StaticMetadata.StreamDirection.Output);
        Image img = null;
        // Create ImageReader.
        mListener = new SimpleImageListener();
        createDefaultImageReader(availableSizes[0], FORMAT, MAX_NUM_IMAGES, mListener);

        // Start capture.
        CaptureRequest request = prepareCaptureRequest();
        SimpleCaptureCallback listener = new SimpleCaptureCallback();
        startCapture(request, /* repeating */false, listener, mHandler);

        mListener.waitForAnyImageAvailable(CAPTURE_WAIT_TIMEOUT_MS);
        img = mReader.acquireNextImage();
        Plane firstPlane = img.getPlanes()[0];
        ByteBuffer buffer = firstPlane.getBuffer();
        img.close();

        imageInvalidAccessTestAfterClose(img, firstPlane, buffer);
    }

    /**
     * Test that images captured after discarding free buffers are valid.
     */
    private void discardFreeBuffersTestByCamera() throws Exception {
        final int FORMAT = mStaticInfo.isColorOutputSupported() ?
            ImageFormat.YUV_420_888 : ImageFormat.DEPTH16;

        final Size SIZE = mStaticInfo.getAvailableSizesForFormatChecked(FORMAT,
                StaticMetadata.StreamDirection.Output)[0];
        // Create ImageReader.
        mListener = new SimpleImageListener();
        createDefaultImageReader(SIZE, FORMAT, MAX_NUM_IMAGES, mListener);

        // Start capture.
        final boolean REPEATING = true;
        final boolean SINGLE = false;
        CaptureRequest request = prepareCaptureRequest();
        SimpleCaptureCallback listener = new SimpleCaptureCallback();
        startCapture(request, REPEATING, listener, mHandler);

        // Validate images and capture results.
        validateImage(SIZE, FORMAT, NUM_FRAME_VERIFIED, REPEATING);
        validateCaptureResult(FORMAT, SIZE, listener, NUM_FRAME_VERIFIED);

        // Discard free buffers.
        mReader.discardFreeBuffers();

        // Validate images and capture resulst again.
        validateImage(SIZE, FORMAT, NUM_FRAME_VERIFIED, REPEATING);
        validateCaptureResult(FORMAT, SIZE, listener, NUM_FRAME_VERIFIED);

        // Stop repeating request in preparation for discardFreeBuffers
        mCameraSession.stopRepeating();
        mCameraSessionListener.getStateWaiter().waitForState(
                BlockingSessionCallback.SESSION_READY, SESSION_READY_TIMEOUT_MS);

        // Drain the reader queue and discard free buffers from the reader.
        Image img = mReader.acquireLatestImage();
        if (img != null) {
            img.close();
        }
        mReader.discardFreeBuffers();

        // Do a single capture for camera device to reallocate buffers
        mListener.reset();
        startCapture(request, SINGLE, listener, mHandler);
        validateImage(SIZE, FORMAT, /*captureCount*/1, SINGLE);
    }

    private void bufferFormatTestByCamera(int format, boolean repeating) throws Exception {
        bufferFormatTestByCamera(format, /*setUsageFlag*/ false,
                HardwareBuffer.USAGE_CPU_READ_OFTEN, repeating,
                /*checkSession*/ false, /*validateImageData*/ true);
    }

    private void bufferFormatTestByCamera(int format, boolean repeating, boolean checkSession)
            throws Exception {
        bufferFormatTestByCamera(format, /*setUsageFlag*/ false,
                HardwareBuffer.USAGE_CPU_READ_OFTEN,
                repeating, checkSession, /*validateImageData*/true);
    }

    private void bufferFormatTestByCamera(int format, boolean setUsageFlag, long usageFlag,
            boolean repeating, boolean checkSession, boolean validateImageData) throws Exception {
        bufferFormatTestByCamera(format, setUsageFlag, usageFlag, repeating, checkSession,
                validateImageData, /*physicalId*/null);
    }

    private void bufferFormatTestByCamera(int format, boolean setUsageFlag, long usageFlag,
            // TODO: Consider having some sort of test configuration class passed to reduce the
            //       proliferation of parameters ?
            boolean repeating, boolean checkSession, boolean validateImageData, String physicalId)
            throws Exception {
        StaticMetadata staticInfo;
        if (physicalId == null) {
            staticInfo = mStaticInfo;
        } else {
            staticInfo = mAllStaticInfo.get(physicalId);
        }

        Size[] availableSizes = staticInfo.getAvailableSizesForFormatChecked(format,
                StaticMetadata.StreamDirection.Output);

        boolean secureTest = setUsageFlag &&
                ((usageFlag & HardwareBuffer.USAGE_PROTECTED_CONTENT) != 0);
        Size secureDataSize = null;
        if (secureTest) {
            secureDataSize = staticInfo.getCharacteristics().get(
                    CameraCharacteristics.SCALER_DEFAULT_SECURE_IMAGE_SIZE);
        }

        // for each resolution, test imageReader:
        for (Size sz : availableSizes) {
            try {
                // For secure mode test only test default secure data size if HAL advertises one.
                if (secureDataSize != null && !secureDataSize.equals(sz)) {
                    continue;
                }

                if (VERBOSE) {
                    Log.v(TAG, ""Testing size "" + sz.toString() + "" format "" + format
                            + "" for camera "" + mCamera.getId());
                }

                // Create ImageReader.
                mListener  = new SimpleImageListener();
                if (setUsageFlag) {
                    createDefaultImageReader(sz, format, MAX_NUM_IMAGES, usageFlag, mListener);
                } else {
                    createDefaultImageReader(sz, format, MAX_NUM_IMAGES, mListener);
                }

                // Don't queue up images if we won't validate them
                if (!validateImageData) {
                    ImageDropperListener imageDropperListener = new ImageDropperListener();
                    mReader.setOnImageAvailableListener(imageDropperListener, mHandler);
                }

                if (checkSession) {
                    checkImageReaderSessionConfiguration(
                            ""Camera capture session validation for format: "" + format + ""failed"",
                            physicalId);
                }

                ArrayList<OutputConfiguration> outputConfigs = new ArrayList<>();
                OutputConfiguration config = new OutputConfiguration(mReader.getSurface());
                if (physicalId != null) {
                    config.setPhysicalCameraId(physicalId);
                }
                outputConfigs.add(config);
                CaptureRequest request = prepareCaptureRequestForConfigs(
                        outputConfigs, CameraDevice.TEMPLATE_PREVIEW).build();

                SimpleCaptureCallback listener = new SimpleCaptureCallback();
                startCapture(request, repeating, listener, mHandler);

                int numFrameVerified = repeating ? NUM_FRAME_VERIFIED : 1;

                if (validateImageData) {
                    // Validate images.
                    validateImage(sz, format, numFrameVerified, repeating);
                }

                // Validate capture result.
                validateCaptureResult(format, sz, listener, numFrameVerified);

                // stop capture.
                stopCapture(/*fast*/false);
            } finally {
                closeDefaultImageReader();
            }

        }
    }

    private void bufferFormatLongProcessingTimeTestByCamera(int format)
            throws Exception {

        final int TEST_SENSITIVITY_VALUE = mStaticInfo.getSensitivityClampToRange(204);
        final long TEST_EXPOSURE_TIME_NS = mStaticInfo.getExposureClampToRange(28000000);
        final long EXPOSURE_TIME_ERROR_MARGIN_NS = 100000;

        Size[] availableSizes = mStaticInfo.getAvailableSizesForFormatChecked(format,
                StaticMetadata.StreamDirection.Output);

        // for each resolution, test imageReader:
        for (Size sz : availableSizes) {
            Log.v(TAG, ""testing size "" + sz.toString());
            try {
                if (VERBOSE) {
                    Log.v(TAG, ""Testing long processing time: size "" + sz.toString() + "" format "" +
                            format + "" for camera "" + mCamera.getId());
                }

                // Create ImageReader.
                mListener  = new SimpleImageListener();
                createDefaultImageReader(sz, format, MAX_NUM_IMAGES, mListener);

                // Setting manual controls
                List<Surface> outputSurfaces = new ArrayList<Surface>();
                outputSurfaces.add(mReader.getSurface());
                CaptureRequest.Builder requestBuilder = prepareCaptureRequestForSurfaces(
                        outputSurfaces, CameraDevice.TEMPLATE_STILL_CAPTURE);

                requestBuilder.set(
                        CaptureRequest.CONTROL_MODE, CaptureRequest.CONTROL_MODE_OFF);
                requestBuilder.set(CaptureRequest.CONTROL_AE_LOCK, true);
                requestBuilder.set(CaptureRequest.CONTROL_AWB_LOCK, true);
                requestBuilder.set(CaptureRequest.CONTROL_AE_MODE,
                        CaptureRequest.CONTROL_AE_MODE_OFF);
                requestBuilder.set(CaptureRequest.CONTROL_AWB_MODE,
                        CaptureRequest.CONTROL_AWB_MODE_OFF);
                requestBuilder.set(CaptureRequest.SENSOR_SENSITIVITY, TEST_SENSITIVITY_VALUE);
                requestBuilder.set(CaptureRequest.SENSOR_EXPOSURE_TIME, TEST_EXPOSURE_TIME_NS);

                SimpleCaptureCallback listener = new SimpleCaptureCallback();
                startCapture(requestBuilder.build(), /*repeating*/true, listener, mHandler);

                for (int i = 0; i < NUM_LONG_PROCESS_TIME_FRAME_VERIFIED; i++) {
                    mListener.waitForAnyImageAvailable(CAPTURE_WAIT_TIMEOUT_MS);

                    // Verify image.
                    Image img = mReader.acquireNextImage();
                    assertNotNull(""Unable to acquire next image"", img);
                    CameraTestUtils.validateImage(img, sz.getWidth(), sz.getHeight(), format,
                            mDebugFileNameBase);

                    // Verify the exposure time and iso match the requested values.
                    CaptureResult result = listener.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);

                    long exposureTimeDiff = TEST_EXPOSURE_TIME_NS -
                            getValueNotNull(result, CaptureResult.SENSOR_EXPOSURE_TIME);
                    int sensitivityDiff = TEST_SENSITIVITY_VALUE -
                            getValueNotNull(result, CaptureResult.SENSOR_SENSITIVITY);

                    mCollector.expectTrue(
                            String.format(""Long processing frame %d format %d size %s "" +
                                    ""exposure time was %d expecting %d."", i, format, sz.toString(),
                                    getValueNotNull(result, CaptureResult.SENSOR_EXPOSURE_TIME),
                                    TEST_EXPOSURE_TIME_NS),
                            exposureTimeDiff < EXPOSURE_TIME_ERROR_MARGIN_NS &&
                            exposureTimeDiff >= 0);

                    mCollector.expectTrue(
                            String.format(""Long processing frame %d format %d size %s "" +
                                    ""sensitivity was %d expecting %d."", i, format, sz.toString(),
                                    getValueNotNull(result, CaptureResult.SENSOR_SENSITIVITY),
                                    TEST_SENSITIVITY_VALUE),
                            sensitivityDiff >= 0);


                    // Sleep to Simulate long porcessing before closing the image.
                    Thread.sleep(LONG_PROCESS_TIME_MS);
                    img.close();
                }
                // Stop capture.
                // Drain the reader queue in case the full queue blocks
                // HAL from delivering new results
                ImageDropperListener imageDropperListener = new ImageDropperListener();
                mReader.setOnImageAvailableListener(imageDropperListener, mHandler);
                Image img = mReader.acquireLatestImage();
                if (img != null) {
                    img.close();
                }
                stopCapture(/*fast*/false);
            } finally {
                closeDefaultImageReader();
            }
        }
    }

    /**
     * Validate capture results.
     *
     * @param format The format of this capture.
     * @param size The capture size.
     * @param listener The capture listener to get capture result callbacks.
     */
    private void validateCaptureResult(int format, Size size, SimpleCaptureCallback listener,
            int numFrameVerified) {
        for (int i = 0; i < numFrameVerified; i++) {
            CaptureResult result = listener.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);

            // TODO: Update this to use availableResultKeys once shim supports this.
            if (mStaticInfo.isCapabilitySupported(
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS)) {
                Long exposureTime = getValueNotNull(result, CaptureResult.SENSOR_EXPOSURE_TIME);
                Integer sensitivity = getValueNotNull(result, CaptureResult.SENSOR_SENSITIVITY);
                mCollector.expectInRange(
                        String.format(
                                ""Capture for format %d, size %s exposure time is invalid."",
                                format, size.toString()),
                        exposureTime,
                        mStaticInfo.getExposureMinimumOrDefault(),
                        mStaticInfo.getExposureMaximumOrDefault()
                );
                mCollector.expectInRange(
                        String.format(""Capture for format %d, size %s sensitivity is invalid."",
                                format, size.toString()),
                        sensitivity,
                        mStaticInfo.getSensitivityMinimumOrDefault(),
                        mStaticInfo.getSensitivityMaximumOrDefault()
                );
            }
            // TODO: add more key validations.
        }
    }

    private final class SimpleImageListener implements ImageReader.OnImageAvailableListener {
        private final ConditionVariable imageAvailable = new ConditionVariable();
        @Override
        public void onImageAvailable(ImageReader reader) {
            if (mReader != reader) {
                return;
            }

            if (VERBOSE) Log.v(TAG, ""new image available"");
            imageAvailable.open();
        }

        public void waitForAnyImageAvailable(long timeout) {
            if (imageAvailable.block(timeout)) {
                imageAvailable.close();
            } else {
                fail(""wait for image available timed out after "" + timeout + ""ms"");
            }
        }

        public void closePendingImages() {
            Image image = mReader.acquireLatestImage();
            if (image != null) {
                image.close();
            }
        }

        public void reset() {
            imageAvailable.close();
        }
    }

    private void validateImage(Size sz, int format, int captureCount,  boolean repeating)
            throws Exception {
        // TODO: Add more format here, and wrap each one as a function.
        Image img;
        final int MAX_RETRY_COUNT = 20;
        int numImageVerified = 0;
        int reTryCount = 0;
        while (numImageVerified < captureCount) {
            assertNotNull(""Image listener is null"", mListener);
            if (VERBOSE) Log.v(TAG, ""Waiting for an Image"");
            mListener.waitForAnyImageAvailable(CAPTURE_WAIT_TIMEOUT_MS);
            if (repeating) {
                /**
                 * Acquire the latest image in case the validation is slower than
                 * the image producing rate.
                 */
                img = mReader.acquireLatestImage();
                /**
                 * Sometimes if multiple onImageAvailable callbacks being queued,
                 * acquireLatestImage will clear all buffer before corresponding callback is
                 * executed. Wait for a new frame in that case.
                 */
                if (img == null && reTryCount < MAX_RETRY_COUNT) {
                    reTryCount++;
                    continue;
                }
            } else {
                img = mReader.acquireNextImage();
            }
            assertNotNull(""Unable to acquire the latest image"", img);
            if (VERBOSE) Log.v(TAG, ""Got the latest image"");
            CameraTestUtils.validateImage(img, sz.getWidth(), sz.getHeight(), format,
                    mDebugFileNameBase);
            HardwareBuffer hwb = img.getHardwareBuffer();
            assertNotNull(""Unable to retrieve the Image's HardwareBuffer"", hwb);
            if (format == ImageFormat.DEPTH_JPEG) {
                byte [] dynamicDepthBuffer = CameraTestUtils.getDataFromImage(img);
                assertTrue(""Dynamic depth validation failed!"",
                        validateDynamicDepthNative(dynamicDepthBuffer));
            }
            if (VERBOSE) Log.v(TAG, ""finish validation of image "" + numImageVerified);
            img.close();
            numImageVerified++;
            reTryCount = 0;
        }

        // Return all pending images to the ImageReader as the validateImage may
        // take a while to return and there could be many images pending.
        mListener.closePendingImages();
    }

    /** Load dynamic depth validation jni on initialization */
    static {
        System.loadLibrary(""ctscamera2_jni"");
    }
    /**
     * Use the dynamic depth SDK to validate a dynamic depth file stored in the buffer.
     *
     * Returns false if the dynamic depth has validation errors. Validation warnings/errors
     * will be printed to logcat.
     */
    private static native boolean validateDynamicDepthNative(byte[] dynamicDepthBuffer);
}"	""	""	"minimum 12 resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ImageReaderTest"	"testAllOutputYUVResolutions"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ImageReaderTest.java"	""	"public void testAllOutputYUVResolutions() throws Exception {
        Integer[] sessionStates = {BlockingSessionCallback.SESSION_READY,
                BlockingSessionCallback.SESSION_CONFIGURE_FAILED};
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.v(TAG, ""Testing all YUV image resolutions for camera "" + id);

                if (!mAllStaticInfo.get(id).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" does not support color outputs, skipping"");
                    continue;
                }

                openDevice(id);
                // Skip warmup on FULL mode devices.
                int warmupCaptureNumber = (mStaticInfo.isHardwareLevelLegacy()) ?
                        MAX_NUM_IMAGES - 1 : 0;

                // NV21 isn't supported by ImageReader.
                final int[] YUVFormats = new int[] {ImageFormat.YUV_420_888, ImageFormat.YV12};

                CameraCharacteristics.Key<StreamConfigurationMap> key =
                        CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP;
                StreamConfigurationMap config = mStaticInfo.getValueFromKeyNonNull(key);
                int[] supportedFormats = config.getOutputFormats();
                List<Integer> supportedYUVFormats = new ArrayList<>();
                for (int format : YUVFormats) {
                    if (CameraTestUtils.contains(supportedFormats, format)) {
                        supportedYUVFormats.add(format);
                    }
                }

                Size[] jpegSizes = mStaticInfo.getAvailableSizesForFormatChecked(ImageFormat.JPEG,
                        StaticMetadata.StreamDirection.Output);
                assertFalse(""JPEG output not supported for camera "" + id +
                        "", at least one JPEG output is required."", jpegSizes.length == 0);

                Size maxJpegSize = CameraTestUtils.getMaxSize(jpegSizes);
                Size maxPreviewSize = mOrderedPreviewSizes.get(0);
                Size QCIF = new Size(176, 144);
                Size FULL_HD = new Size(1920, 1080);
                for (int format : supportedYUVFormats) {
                    Size[] targetCaptureSizes =
                            mStaticInfo.getAvailableSizesForFormatChecked(format,
                            StaticMetadata.StreamDirection.Output);

                    for (Size captureSz : targetCaptureSizes) {
                        if (VERBOSE) {
                            Log.v(TAG, ""Testing yuv size "" + captureSz + "" and jpeg size ""
                                    + maxJpegSize + "" for camera "" + mCamera.getId());
                        }

                        ImageReader jpegReader = null;
                        ImageReader yuvReader = null;
                        try {
                            // Create YUV image reader
                            SimpleImageReaderListener yuvListener = new SimpleImageReaderListener();
                            yuvReader = createImageReader(captureSz, format, MAX_NUM_IMAGES,
                                    yuvListener);
                            Surface yuvSurface = yuvReader.getSurface();

                            // Create JPEG image reader
                            SimpleImageReaderListener jpegListener =
                                    new SimpleImageReaderListener();
                            jpegReader = createImageReader(maxJpegSize,
                                    ImageFormat.JPEG, MAX_NUM_IMAGES, jpegListener);
                            Surface jpegSurface = jpegReader.getSurface();

                            // Setup session
                            List<Surface> outputSurfaces = new ArrayList<Surface>();
                            outputSurfaces.add(yuvSurface);
                            outputSurfaces.add(jpegSurface);
                            createSession(outputSurfaces);

                            int state = mCameraSessionListener.getStateWaiter().waitForAnyOfStates(
                                        Arrays.asList(sessionStates),
                                        CameraTestUtils.SESSION_CONFIGURE_TIMEOUT_MS);

                            if (state == BlockingSessionCallback.SESSION_CONFIGURE_FAILED) {
                                if (captureSz.getWidth() > maxPreviewSize.getWidth() ||
                                        captureSz.getHeight() > maxPreviewSize.getHeight()) {
                                    Log.v(TAG, ""Skip testing {yuv:"" + captureSz
                                            + "" ,jpeg:"" + maxJpegSize + ""} for camera ""
                                            + mCamera.getId() +
                                            "" because full size jpeg + yuv larger than ""
                                            + ""max preview size ("" + maxPreviewSize
                                            + "") is not supported"");
                                    continue;
                                } else if (captureSz.equals(QCIF) &&
                                        ((maxJpegSize.getWidth() > FULL_HD.getWidth()) ||
                                         (maxJpegSize.getHeight() > FULL_HD.getHeight()))) {
                                    Log.v(TAG, ""Skip testing {yuv:"" + captureSz
                                            + "" ,jpeg:"" + maxJpegSize + ""} for camera ""
                                            + mCamera.getId() +
                                            "" because QCIF + >Full_HD size is not supported"");
                                    continue;
                                } else {
                                    fail(""Camera "" + mCamera.getId() +
                                            "":session configuration failed for {jpeg: "" +
                                            maxJpegSize + "", yuv: "" + captureSz + ""}"");
                                }
                            }

                            // Warm up camera preview (mainly to give legacy devices time to do 3A).
                            CaptureRequest.Builder warmupRequest =
                                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
                            warmupRequest.addTarget(yuvSurface);
                            assertNotNull(""Fail to get CaptureRequest.Builder"", warmupRequest);
                            SimpleCaptureCallback resultListener = new SimpleCaptureCallback();

                            for (int i = 0; i < warmupCaptureNumber; i++) {
                                startCapture(warmupRequest.build(), /*repeating*/false,
                                        resultListener, mHandler);
                            }
                            for (int i = 0; i < warmupCaptureNumber; i++) {
                                resultListener.getCaptureResult(CAPTURE_WAIT_TIMEOUT_MS);
                                Image image = yuvListener.getImage(CAPTURE_WAIT_TIMEOUT_MS);
                                image.close();
                            }

                            // Capture image.
                            CaptureRequest.Builder mainRequest =
                                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
                            for (Surface s : outputSurfaces) {
                                mainRequest.addTarget(s);
                            }

                            startCapture(mainRequest.build(), /*repeating*/false, resultListener,
                                    mHandler);

                            // Verify capture result and images
                            resultListener.getCaptureResult(CAPTURE_WAIT_TIMEOUT_MS);

                            Image yuvImage = yuvListener.getImage(CAPTURE_WAIT_TIMEOUT_MS);
                            Image jpegImage = jpegListener.getImage(CAPTURE_WAIT_TIMEOUT_MS);

                            //Validate captured images.
                            CameraTestUtils.validateImage(yuvImage, captureSz.getWidth(),
                                    captureSz.getHeight(), format, /*filePath*/null);
                            CameraTestUtils.validateImage(jpegImage, maxJpegSize.getWidth(),
                                    maxJpegSize.getHeight(), ImageFormat.JPEG, /*filePath*/null);

                            // Compare the image centers.
                            RectF jpegDimens = new RectF(0, 0, jpegImage.getWidth(),
                                    jpegImage.getHeight());
                            RectF yuvDimens = new RectF(0, 0, yuvImage.getWidth(),
                                    yuvImage.getHeight());

                            // Find scale difference between YUV and JPEG output
                            Matrix m = new Matrix();
                            m.setRectToRect(yuvDimens, jpegDimens, Matrix.ScaleToFit.START);
                            RectF scaledYuv = new RectF();
                            m.mapRect(scaledYuv, yuvDimens);
                            float scale = scaledYuv.width() / yuvDimens.width();

                            final int PATCH_DIMEN = 40; // pixels in YUV

                            // Find matching square patch of pixels in YUV and JPEG output
                            RectF tempPatch = new RectF(0, 0, PATCH_DIMEN, PATCH_DIMEN);
                            tempPatch.offset(yuvDimens.centerX() - tempPatch.centerX(),
                                    yuvDimens.centerY() - tempPatch.centerY());
                            Rect yuvPatch = new Rect();
                            tempPatch.roundOut(yuvPatch);

                            tempPatch.set(0, 0, PATCH_DIMEN * scale, PATCH_DIMEN * scale);
                            tempPatch.offset(jpegDimens.centerX() - tempPatch.centerX(),
                                    jpegDimens.centerY() - tempPatch.centerY());
                            Rect jpegPatch = new Rect();
                            tempPatch.roundOut(jpegPatch);

                            // Decode center patches
                            int[] yuvColors = convertPixelYuvToRgba(yuvPatch.width(),
                                    yuvPatch.height(), yuvPatch.left, yuvPatch.top, yuvImage);
                            Bitmap yuvBmap = Bitmap.createBitmap(yuvColors, yuvPatch.width(),
                                    yuvPatch.height(), Bitmap.Config.ARGB_8888);

                            byte[] compressedJpegData = CameraTestUtils.getDataFromImage(jpegImage);
                            BitmapRegionDecoder decoder = BitmapRegionDecoder.newInstance(
                                    compressedJpegData, /*offset*/0, compressedJpegData.length,
                                    /*isShareable*/true);
                            BitmapFactory.Options opt = new BitmapFactory.Options();
                            opt.inPreferredConfig = Bitmap.Config.ARGB_8888;
                            Bitmap fullSizeJpegBmap = decoder.decodeRegion(jpegPatch, opt);
                            Bitmap jpegBmap = Bitmap.createScaledBitmap(fullSizeJpegBmap,
                                    yuvPatch.width(), yuvPatch.height(), /*filter*/true);

                            // Compare two patches using average of per-pixel differences
                            double difference = BitmapUtils.calcDifferenceMetric(yuvBmap, jpegBmap);
                            double tolerance = IMAGE_DIFFERENCE_TOLERANCE;
                            if (mStaticInfo.isHardwareLevelLegacy()) {
                                tolerance = IMAGE_DIFFERENCE_TOLERANCE_LEGACY;
                            }
                            Log.i(TAG, ""Difference for resolution "" + captureSz + "" is: "" +
                                    difference);
                            if (difference > tolerance) {
                                // Dump files if running in verbose mode
                                if (DEBUG) {
                                    String jpegFileName = mDebugFileNameBase + ""/"" + captureSz +
                                            ""_jpeg.jpg"";
                                    dumpFile(jpegFileName, jpegBmap);
                                    String fullSizeJpegFileName = mDebugFileNameBase + ""/"" +
                                            captureSz + ""_full_jpeg.jpg"";
                                    dumpFile(fullSizeJpegFileName, compressedJpegData);
                                    String yuvFileName = mDebugFileNameBase + ""/"" + captureSz +
                                            ""_yuv.jpg"";
                                    dumpFile(yuvFileName, yuvBmap);
                                    String fullSizeYuvFileName = mDebugFileNameBase + ""/"" +
                                            captureSz + ""_full_yuv.jpg"";
                                    int[] fullYUVColors = convertPixelYuvToRgba(yuvImage.getWidth(),
                                            yuvImage.getHeight(), 0, 0, yuvImage);
                                    Bitmap fullYUVBmap = Bitmap.createBitmap(fullYUVColors,
                                            yuvImage.getWidth(), yuvImage.getHeight(),
                                            Bitmap.Config.ARGB_8888);
                                    dumpFile(fullSizeYuvFileName, fullYUVBmap);
                                }
                                fail(""Camera "" + mCamera.getId() + "": YUV and JPEG image at "" +
                                        ""capture size "" + captureSz + "" for the same frame are "" +
                                        ""not similar, center patches have difference metric of "" +
                                        difference + "", tolerance is "" + tolerance);
                            }

                            // Stop capture, delete the streams.
                            stopCapture(/*fast*/false);
                            yuvImage.close();
                            jpegImage.close();
                            yuvListener.drain();
                            jpegListener.drain();
                        } finally {
                            closeImageReader(jpegReader);
                            jpegReader = null;
                            closeImageReader(yuvReader);
                            yuvReader = null;
                        }
                    }
                }

            } finally {
                closeDevice(id);
            }
        }
    }

    /**
     * Test that images captured after discarding free buffers are valid.
     */"	""	""	"12 resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ImageReaderTest"	"testImageReaderYuvAndRawWithUsageFlag"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ImageReaderTest.java"	""	"public void testImageReaderYuvAndRawWithUsageFlag() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.v(TAG, ""YUV and RAW testing for camera "" + id);
                if (!mAllStaticInfo.get(id).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                openDevice(id);
                bufferFormatWithYuvTestByCamera(ImageFormat.RAW_SENSOR, true);
            } finally {
                closeDevice(id);
            }
        }
    }

    /**
     * Check that the center patches for YUV and JPEG outputs for the same frame match for each YUV
     * resolution and format supported.
     */"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.testcases.Camera2AndroidTestRule"	"getCameraIdsUnderTest"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/testcases/Camera2AndroidTestRule.java"	""	"public void test/*
 *.
 */

package android.hardware.camera2.cts.testcases;

import static android.hardware.camera2.cts.CameraTestUtils.*;
import static com.android.ex.camera2.blocking.BlockingStateCallback.*;

import android.content.Context;
import android.graphics.Rect;

import android.hardware.camera2.cts.CameraTestUtils;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraCaptureSession.CaptureCallback;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.params.OutputConfiguration;
import android.hardware.camera2.params.SessionConfiguration;
import android.util.Size;
import android.hardware.camera2.cts.helpers.CameraErrorCollector;
import android.hardware.camera2.cts.helpers.StaticMetadata;
import android.hardware.camera2.cts.helpers.StaticMetadata.CheckLevel;
import android.media.Image;
import android.media.Image.Plane;
import android.media.ImageReader;
import android.os.Bundle;
import android.os.Handler;
import android.os.HandlerThread;
import android.util.Log;
import android.view.Surface;
import android.view.WindowManager;

import androidx.test.InstrumentationRegistry;

import com.android.ex.camera2.blocking.BlockingSessionCallback;
import com.android.ex.camera2.blocking.BlockingStateCallback;

import org.junit.rules.ExternalResource;

import java.io.File;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;

public class Camera2AndroidTestRule extends ExternalResource {
    private static final String TAG = ""Camera2AndroidBasicTestCase"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);

    // Default capture size: VGA size is required by CDD.
    protected static final Size DEFAULT_CAPTURE_SIZE = new Size(640, 480);
    protected static final int CAPTURE_WAIT_TIMEOUT_MS = 5000;

    private CameraManager mCameraManager;
    private CameraDevice mCamera;
    private CameraCaptureSession mCameraSession;
    private BlockingSessionCallback mCameraSessionListener;
    private BlockingStateCallback mCameraListener;
    private String[] mCameraIdsUnderTest;
    // include both standalone camera IDs and ""hidden"" physical camera IDs
    private String[] mAllCameraIds;
    private HashMap<String, StaticMetadata> mAllStaticInfo;
    private ImageReader mReader;
    private Surface mReaderSurface;
    private Handler mHandler;
    private HandlerThread mHandlerThread;
    private StaticMetadata mStaticInfo;
    private CameraErrorCollector mCollector;
    private List<Size> mOrderedPreviewSizes; // In descending order.
    private List<Size> mOrderedVideoSizes; // In descending order.
    private List<Size> mOrderedStillSizes; // In descending order.
    private String mDebugFileNameBase;

    private WindowManager mWindowManager;
    private Context mContext;

    private static final String CAMERA_ID_INSTR_ARG_KEY = ""camera-id"";
    private static final String CAMERA_PERF_MEASURE = ""perf-measure"";
    private static final String CAMERA_PERF_CLASS_TEST = ""perf-class-test"";
    private static final Bundle mBundle = InstrumentationRegistry.getArguments();
    private static final String mOverrideCameraId = mBundle.getString(CAMERA_ID_INSTR_ARG_KEY);
    private static final String mPerfMeasure = mBundle.getString(CAMERA_PERF_MEASURE);
    private static final String mPerfClassTest = mBundle.getString(CAMERA_PERF_CLASS_TEST);

    public Camera2AndroidTestRule(Context context) {
        mContext = context;
    }

    public String getDebugFileNameBase() {
        return mDebugFileNameBase;
    }

    public Context getContext() {
        return mContext;
    }

    public String[] getCameraIdsUnderTest() {
        return mCameraIdsUnderTest;
    }

    public StaticMetadata getStaticInfo() {
        return mStaticInfo;
    }

    public CameraManager getCameraManager() {
        return mCameraManager;
    }

    public void setStaticInfo(StaticMetadata staticInfo) {
        mStaticInfo = staticInfo;
    }

    public CameraCaptureSession getCameraSession() {
        return mCameraSession;
    }

    public CameraDevice getCamera() {
        return mCamera;
    }

    public void setCamera(CameraDevice camera) {
        mCamera = camera;
    }

    public void setCameraSession(CameraCaptureSession session) {
        mCameraSession = session;
    }

    public BlockingStateCallback getCameraListener() {
        return mCameraListener;
    }

    public BlockingSessionCallback getCameraSessionListener() {
        return mCameraSessionListener;
    }

    public Handler getHandler() {
        return mHandler;
    }

    public void setCameraSessionListener(BlockingSessionCallback listener) {
        mCameraSessionListener = listener;
    }

    public ImageReader getReader() {
        return mReader;
    }

    public HashMap<String, StaticMetadata> getAllStaticInfo() {
        return mAllStaticInfo;
    }

    public List<Size> getOrderedPreviewSizes() {
        return mOrderedPreviewSizes;
    }

    public List<Size> getOrderedStillSizes() {
        return mOrderedStillSizes;
    }

    public Surface getReaderSurface() {
        return mReaderSurface;
    }

    public void setOrderedPreviewSizes(List<Size> sizes) {
        mOrderedPreviewSizes = sizes;
    }

    public WindowManager getWindowManager() {
        return mWindowManager;
    }

    public CameraErrorCollector getCollector() {
        return mCollector;
    }

    public boolean isPerfMeasure() {
        return mPerfMeasure != null && mPerfMeasure.equals(""on"");
    }

    public boolean isPerfClassTest() {
        return mPerfClassTest != null && mPerfClassTest.equals(""on"");
    }

    private String[] deriveCameraIdsUnderTest() throws Exception {
        String[] idsUnderTest = mCameraManager.getCameraIdList();
        assertNotNull(""Camera ids shouldn't be null"", idsUnderTest);
        if (mOverrideCameraId != null) {
            if (Arrays.asList(idsUnderTest).contains(mOverrideCameraId)) {
                idsUnderTest = new String[]{mOverrideCameraId};
            } else {
                idsUnderTest = new String[]{};
            }
        }

        return idsUnderTest;
    }

    /**
     * Set up the camera2 test case required environments, including CameraManager,
     * HandlerThread, Camera IDs, and CameraStateCallback etc.
     */
    @Override
    public void before() throws Exception {
        Log.v(TAG, ""Set up..."");
        mCameraManager = (CameraManager) mContext.getSystemService(Context.CAMERA_SERVICE);
        assertNotNull(""Can't connect to camera manager!"", mCameraManager);
        mWindowManager = (WindowManager) mContext.getSystemService(Context.WINDOW_SERVICE);
        /**
         * Workaround for mockito and JB-MR2 incompatibility
         *
         * Avoid java.lang.IllegalArgumentException: dexcache == null
         * https://code.google.com/p/dexmaker/issues/detail?id=2
         */
        System.setProperty(""dexmaker.dexcache"", getContext().getCacheDir().toString());

        mCameraIdsUnderTest = deriveCameraIdsUnderTest();
        mHandlerThread = new HandlerThread(TAG);
        mHandlerThread.start();
        mHandler = new Handler(mHandlerThread.getLooper());
        mCameraListener = new BlockingStateCallback();
        mCollector = new CameraErrorCollector();

        File filesDir = mContext.getPackageManager().isInstantApp()
                ? mContext.getFilesDir()
                : mContext.getExternalFilesDir(null);

        mDebugFileNameBase = filesDir.getPath();

        mAllStaticInfo = new HashMap<String, StaticMetadata>();
        List<String> hiddenPhysicalIds = new ArrayList<>();
        for (String cameraId : mCameraIdsUnderTest) {
            CameraCharacteristics props = mCameraManager.getCameraCharacteristics(cameraId);
            StaticMetadata staticMetadata = new StaticMetadata(props,
                    CheckLevel.ASSERT, /*collector*/null);
            mAllStaticInfo.put(cameraId, staticMetadata);

            for (String physicalId : props.getPhysicalCameraIds()) {
                if (!Arrays.asList(mCameraIdsUnderTest).contains(physicalId) &&
                        !hiddenPhysicalIds.contains(physicalId)) {
                    hiddenPhysicalIds.add(physicalId);
                    props = mCameraManager.getCameraCharacteristics(physicalId);
                    staticMetadata = new StaticMetadata(
                            mCameraManager.getCameraCharacteristics(physicalId),
                            CheckLevel.ASSERT, /*collector*/null);
                    mAllStaticInfo.put(physicalId, staticMetadata);
                }
            }
        }
        mAllCameraIds = new String[mCameraIdsUnderTest.length + hiddenPhysicalIds.size()];
        System.arraycopy(mCameraIdsUnderTest, 0, mAllCameraIds, 0, mCameraIdsUnderTest.length);
        for (int i = 0; i < hiddenPhysicalIds.size(); i++) {
            mAllCameraIds[mCameraIdsUnderTest.length + i] = hiddenPhysicalIds.get(i);
        }
    }

    @Override
    public void after() {
        Log.v(TAG, ""Tear down..."");
        if (mCameraManager != null) {
            try {
                String[] cameraIdsPostTest = deriveCameraIdsUnderTest();
                Log.i(TAG, ""Camera ids in setup:"" + Arrays.toString(mCameraIdsUnderTest));
                Log.i(TAG, ""Camera ids in tearDown:"" + Arrays.toString(cameraIdsPostTest));
                assertTrue(
                        ""Number of cameras changed from "" + mCameraIdsUnderTest.length + "" to "" +
                                cameraIdsPostTest.length,
                        mCameraIdsUnderTest.length == cameraIdsPostTest.length);
                mHandlerThread.quitSafely();
                mHandler = null;
                closeDefaultImageReader();
                mCollector.verify();
            } catch (Throwable e) {
                // When new Exception(e) is used, exception info will be printed twice.
                throw new RuntimeException(e.getMessage());
            }
        }
    }

    /**
     * Start capture with given {@link #CaptureRequest}.
     *
     * @param request The {@link #CaptureRequest} to be captured.
     * @param repeating If the capture is single capture or repeating.
     * @param listener The {@link #CaptureCallback} camera device used to notify callbacks.
     * @param handler The handler camera device used to post callbacks.
     */
    public void startCapture(CaptureRequest request, boolean repeating,
            CaptureCallback listener, Handler handler) throws Exception {
        if (VERBOSE) Log.v(TAG, ""Starting capture from device"");

        if (repeating) {
            mCameraSession.setRepeatingRequest(request, listener, handler);
        } else {
            mCameraSession.capture(request, listener, handler);
        }
    }

    /**
     * Stop the current active capture.
     *
     * @param fast When it is true, {@link CameraDevice#flush} is called, the stop capture
     * could be faster.
     */
    public void stopCapture(boolean fast) throws Exception {
        if (VERBOSE) Log.v(TAG, ""Stopping capture"");

        if (fast) {
            /**
             * Flush is useful for canceling long exposure single capture, it also could help
             * to make the streaming capture stop sooner.
             */
            mCameraSession.abortCaptures();
            mCameraSessionListener.getStateWaiter().
                    waitForState(BlockingSessionCallback.SESSION_READY, CAMERA_IDLE_TIMEOUT_MS);
        } else {
            mCameraSession.close();
            mCameraSessionListener.getStateWaiter().
                    waitForState(BlockingSessionCallback.SESSION_CLOSED, CAMERA_IDLE_TIMEOUT_MS);
        }
    }

    /**
     * Open a {@link #CameraDevice camera device} and get the StaticMetadata for a given camera id.
     * The default mCameraListener is used to wait for states.
     *
     * @param cameraId The id of the camera device to be opened.
     */
    public void openDevice(String cameraId) throws Exception {
        openDevice(cameraId, mCameraListener);
    }

    /**
     * Open a {@link #CameraDevice} and get the StaticMetadata for a given camera id and listener.
     *
     * @param cameraId The id of the camera device to be opened.
     * @param listener The {@link #BlockingStateCallback} used to wait for states.
     */
    public void openDevice(String cameraId, BlockingStateCallback listener) throws Exception {
        mCamera = CameraTestUtils.openCamera(
                mCameraManager, cameraId, listener, mHandler);
        mCollector.setCameraId(cameraId);
        mStaticInfo = mAllStaticInfo.get(cameraId);
        if (mStaticInfo.isColorOutputSupported()) {
            mOrderedPreviewSizes = getSupportedPreviewSizes(
                    cameraId, mCameraManager,
                    getPreviewSizeBound(mWindowManager, PREVIEW_SIZE_BOUND));
            mOrderedVideoSizes = getSupportedVideoSizes(cameraId, mCameraManager, PREVIEW_SIZE_BOUND);
            mOrderedStillSizes = getSupportedStillSizes(cameraId, mCameraManager, null);
        }

        if (VERBOSE) {
            Log.v(TAG, ""Camera "" + cameraId + "" is opened"");
        }
    }

    /**
     * Create a {@link #CameraCaptureSession} using the currently open camera.
     *
     * @param outputSurfaces The set of output surfaces to configure for this session
     */
    public void createSession(List<Surface> outputSurfaces) throws Exception {
        mCameraSessionListener = new BlockingSessionCallback();
        mCameraSession = CameraTestUtils.configureCameraSession(mCamera, outputSurfaces,
                mCameraSessionListener, mHandler);
    }

    /**
     * Create a {@link #CameraCaptureSession} using the currently open camera with
     * OutputConfigurations.
     *
     * @param outputSurfaces The set of output surfaces to configure for this session
     */
    public void createSessionByConfigs(List<OutputConfiguration> outputConfigs) throws Exception {
        mCameraSessionListener = new BlockingSessionCallback();
        mCameraSession = CameraTestUtils.configureCameraSessionWithConfig(mCamera, outputConfigs,
                mCameraSessionListener, mHandler);
    }

    /**
     * Close a {@link #CameraDevice camera device} and clear the associated StaticInfo field for a
     * given camera id. The default mCameraListener is used to wait for states.
     * <p>
     * This function must be used along with the {@link #openDevice} for the
     * same camera id.
     * </p>
     *
     * @param cameraId The id of the {@link #CameraDevice camera device} to be closed.
     */
    public void closeDevice(String cameraId) {
        closeDevice(cameraId, mCameraListener);
    }

    /**
     * Close a {@link #CameraDevice camera device} and clear the associated StaticInfo field for a
     * given camera id and listener.
     * <p>
     * This function must be used along with the {@link #openDevice} for the
     * same camera id.
     * </p>
     *
     * @param cameraId The id of the camera device to be closed.
     * @param listener The BlockingStateCallback used to wait for states.
     */
    public void closeDevice(String cameraId, BlockingStateCallback listener) {
        if (mCamera != null) {
            if (!cameraId.equals(mCamera.getId())) {
                throw new IllegalStateException(""Try to close a device that is not opened yet"");
            }
            mCamera.close();
            listener.waitForState(STATE_CLOSED, CAMERA_CLOSE_TIMEOUT_MS);
            mCamera = null;
            mCameraSession = null;
            mCameraSessionListener = null;
            mStaticInfo = null;
            mOrderedPreviewSizes = null;
            mOrderedVideoSizes = null;
            mOrderedStillSizes = null;

            if (VERBOSE) {
                Log.v(TAG, ""Camera "" + cameraId + "" is closed"");
            }
        }
    }

    /**
     * Create an {@link ImageReader} object and get the surface.
     * <p>
     * This function creates {@link ImageReader} object and surface, then assign
     * to the default {@link mReader} and {@link mReaderSurface}. It closes the
     * current default active {@link ImageReader} if it exists.
     * </p>
     *
     * @param size The size of this ImageReader to be created.
     * @param format The format of this ImageReader to be created
     * @param maxNumImages The max number of images that can be acquired
     *            simultaneously.
     * @param listener The listener used by this ImageReader to notify
     *            callbacks.
     */
    public void createDefaultImageReader(Size size, int format, int maxNumImages,
            ImageReader.OnImageAvailableListener listener) throws Exception {
        closeDefaultImageReader();

        mReader = createImageReader(size, format, maxNumImages, listener);
        mReaderSurface = mReader.getSurface();
        if (VERBOSE) Log.v(TAG, ""Created ImageReader size "" + size.toString());
    }

    /**
     * Create an {@link ImageReader} object and get the surface.
     * <p>
     * This function creates {@link ImageReader} object and surface, then assign
     * to the default {@link mReader} and {@link mReaderSurface}. It closes the
     * current default active {@link ImageReader} if it exists.
     * </p>
     *
     * @param size The size of this ImageReader to be created.
     * @param format The format of this ImageReader to be created
     * @param maxNumImages The max number of images that can be acquired
     *            simultaneously.
     * @param usage The usage flag of the ImageReader
     * @param listener The listener used by this ImageReader to notify
     *            callbacks.
     */
    public void createDefaultImageReader(Size size, int format, int maxNumImages, long usage,
            ImageReader.OnImageAvailableListener listener) throws Exception {
        closeDefaultImageReader();

        mReader = createImageReader(size, format, maxNumImages, usage, listener);
        mReaderSurface = mReader.getSurface();
        if (VERBOSE) Log.v(TAG, ""Created ImageReader size "" + size.toString());
    }

    /**
     * Create an {@link ImageReader} object.
     *
     * <p>This function creates image reader object for given format, maxImages, and size.</p>
     *
     * @param size The size of this ImageReader to be created.
     * @param format The format of this ImageReader to be created
     * @param maxNumImages The max number of images that can be acquired simultaneously.
     * @param listener The listener used by this ImageReader to notify callbacks.
     */

    public ImageReader createImageReader(Size size, int format, int maxNumImages,
            ImageReader.OnImageAvailableListener listener) throws Exception {

        ImageReader reader = null;
        reader = ImageReader.newInstance(size.getWidth(), size.getHeight(),
                format, maxNumImages);

        reader.setOnImageAvailableListener(listener, mHandler);
        if (VERBOSE) Log.v(TAG, ""Created ImageReader size "" + size.toString());
        return reader;
    }

    /**
     * Create an {@link ImageReader} object.
     *
     * <p>This function creates image reader object for given format, maxImages, usage and size.</p>
     *
     * @param size The size of this ImageReader to be created.
     * @param format The format of this ImageReader to be created
     * @param maxNumImages The max number of images that can be acquired simultaneously.
     * @param usage The usage flag of the ImageReader
     * @param listener The listener used by this ImageReader to notify callbacks.
     */

    public ImageReader createImageReader(Size size, int format, int maxNumImages, long usage,
            ImageReader.OnImageAvailableListener listener) throws Exception {
        ImageReader reader = null;
        reader = ImageReader.newInstance(size.getWidth(), size.getHeight(),
                format, maxNumImages, usage);

        reader.setOnImageAvailableListener(listener, mHandler);
        if (VERBOSE) Log.v(TAG, ""Created ImageReader size "" + size.toString());
        return reader;
    }

    /**
     * Close the pending images then close current default {@link ImageReader} object.
     */
    public void closeDefaultImageReader() {
        closeImageReader(mReader);
        mReader = null;
        mReaderSurface = null;
    }

    /**
     * Close an image reader instance.
     *
     * @param reader
     */
    public void closeImageReader(ImageReader reader) {
        if (reader != null) {
            try {
                // Close all possible pending images first.
                Image image = reader.acquireLatestImage();
                if (image != null) {
                    image.close();
                }
            } finally {
                reader.close();
                reader = null;
            }
        }
    }

    public void checkImageReaderSessionConfiguration(String msg) throws Exception {
        List<OutputConfiguration> outputConfigs = new ArrayList<OutputConfiguration>();
        outputConfigs.add(new OutputConfiguration(mReaderSurface));

        checkSessionConfigurationSupported(mCamera, mHandler, outputConfigs, /*inputConfig*/ null,
                SessionConfiguration.SESSION_REGULAR, /*expectedResult*/ true, msg);
    }

    public CaptureRequest prepareCaptureRequest() throws Exception {
        return prepareCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
    }

    public CaptureRequest prepareCaptureRequest(int template) throws Exception {
        List<Surface> outputSurfaces = new ArrayList<Surface>();
        Surface surface = mReader.getSurface();
        assertNotNull(""Fail to get surface from ImageReader"", surface);
        outputSurfaces.add(surface);
        return prepareCaptureRequestForSurfaces(outputSurfaces, template)
                .build();
    }

    public CaptureRequest.Builder prepareCaptureRequestForSurfaces(List<Surface> surfaces,
            int template)
            throws Exception {
        createSession(surfaces);

        CaptureRequest.Builder captureBuilder =
                mCamera.createCaptureRequest(template);
        assertNotNull(""Fail to get captureRequest"", captureBuilder);
        for (Surface surface : surfaces) {
            captureBuilder.addTarget(surface);
        }

        return captureBuilder;
    }

    public CaptureRequest.Builder prepareCaptureRequestForConfigs(
            List<OutputConfiguration> outputConfigs, int template) throws Exception {
        createSessionByConfigs(outputConfigs);

        CaptureRequest.Builder captureBuilder =
                mCamera.createCaptureRequest(template);
        assertNotNull(""Fail to get captureRequest"", captureBuilder);
        for (OutputConfiguration config : outputConfigs) {
            for (Surface s : config.getSurfaces()) {
                captureBuilder.addTarget(s);
            }
        }

        return captureBuilder;
    }

    /**
     * Test the invalid Image access: accessing a closed image must result in
     * {@link IllegalStateException}.
     *
     * @param closedImage The closed image.
     * @param closedBuffer The ByteBuffer from a closed Image. buffer invalid
     *            access will be skipped if it is null.
     */
    public void imageInvalidAccessTestAfterClose(Image closedImage,
            Plane closedPlane, ByteBuffer closedBuffer) {
        if (closedImage == null) {
            throw new IllegalArgumentException("" closedImage must be non-null"");
        }
        if (closedBuffer != null && !closedBuffer.isDirect()) {
            throw new IllegalArgumentException(""The input ByteBuffer should be direct ByteBuffer"");
        }

        if (closedPlane != null) {
            // Plane#getBuffer test
            try {
                closedPlane.getBuffer(); // An ISE should be thrown here.
                fail(""Image should throw IllegalStateException when calling getBuffer""
                        + "" after the image is closed"");
            } catch (IllegalStateException e) {
                // Expected.
            }

            // Plane#getPixelStride test
            try {
                closedPlane.getPixelStride(); // An ISE should be thrown here.
                fail(""Image should throw IllegalStateException when calling getPixelStride""
                        + "" after the image is closed"");
            } catch (IllegalStateException e) {
                // Expected.
            }

            // Plane#getRowStride test
            try {
                closedPlane.getRowStride(); // An ISE should be thrown here.
                fail(""Image should throw IllegalStateException when calling getRowStride""
                        + "" after the image is closed"");
            } catch (IllegalStateException e) {
                // Expected.
            }
        }

        // ByteBuffer access test
        if (closedBuffer != null) {
            try {
                closedBuffer.get(); // An ISE should be thrown here.
                fail(""Image should throw IllegalStateException when accessing a byte buffer""
                        + "" after the image is closed"");
            } catch (IllegalStateException e) {
                // Expected.
            }
        }

        // Image#getFormat test
        try {
            closedImage.getFormat();
            fail(""Image should throw IllegalStateException when calling getFormat""
                    + "" after the image is closed"");
        } catch (IllegalStateException e) {
            // Expected.
        }

        // Image#getWidth test
        try {
            closedImage.getWidth();
            fail(""Image should throw IllegalStateException when calling getWidth""
                    + "" after the image is closed"");
        } catch (IllegalStateException e) {
            // Expected.
        }

        // Image#getHeight test
        try {
            closedImage.getHeight();
            fail(""Image should throw IllegalStateException when calling getHeight""
                    + "" after the image is closed"");
        } catch (IllegalStateException e) {
            // Expected.
        }

        // Image#getTimestamp test
        try {
            closedImage.getTimestamp();
            fail(""Image should throw IllegalStateException when calling getTimestamp""
                    + "" after the image is closed"");
        } catch (IllegalStateException e) {
            // Expected.
        }

        // Image#getTimestamp test
        try {
            closedImage.getTimestamp();
            fail(""Image should throw IllegalStateException when calling getTimestamp""
                    + "" after the image is closed"");
        } catch (IllegalStateException e) {
            // Expected.
        }

        // Image#getCropRect test
        try {
            closedImage.getCropRect();
            fail(""Image should throw IllegalStateException when calling getCropRect""
                    + "" after the image is closed"");
        } catch (IllegalStateException e) {
            // Expected.
        }

        // Image#setCropRect test
        try {
            Rect rect = new Rect();
            closedImage.setCropRect(rect);
            fail(""Image should throw IllegalStateException when calling setCropRect""
                    + "" after the image is closed"");
        } catch (IllegalStateException e) {
            // Expected.
        }

        // Image#getPlanes test
        try {
            closedImage.getPlanes();
            fail(""Image should throw IllegalStateException when calling getPlanes""
                    + "" after the image is closed"");
        } catch (IllegalStateException e) {
            // Expected.
        }
    }
}"	""	""	"cdd"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.fov.PhotoCaptureActivity"	"OnClickListener"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/fov/PhotoCaptureActivity.java"	""	"public void test/*
 *.
 */

package com.android.cts.verifier.camera.fov;

import android.app.Activity;
import android.app.AlertDialog;
import android.app.Dialog;
import android.content.Context;
import android.content.DialogInterface;
import android.content.Intent;
import android.graphics.Color;
import android.hardware.Camera;
import android.hardware.Camera.PictureCallback;
import android.hardware.Camera.ShutterCallback;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraManager;
import android.os.Bundle;
import android.os.PowerManager;
import android.os.PowerManager.WakeLock;
import android.util.Log;
import android.view.Surface;
import android.view.SurfaceHolder;
import android.view.SurfaceView;
import android.view.View;
import android.view.View.OnClickListener;
import android.widget.AdapterView;
import android.widget.AdapterView.OnItemSelectedListener;
import android.widget.ArrayAdapter;
import android.widget.Button;
import android.widget.Spinner;
import android.widget.TextView;
import android.widget.Toast;

import com.android.cts.verifier.R;
import com.android.cts.verifier.TestResult;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

/**
 * An activity for showing the camera preview and taking a picture.
 */
public class PhotoCaptureActivity extends Activity
        implements PictureCallback, SurfaceHolder.Callback {
    private static final String TAG = PhotoCaptureActivity.class.getSimpleName();
    private static final int FOV_REQUEST_CODE = 1006;
    private static final String PICTURE_FILENAME = ""photo.jpg"";
    private static float mReportedFovDegrees = 0;
    private float mReportedFovPrePictureTaken = -1;

    private SurfaceView mPreview;
    private SurfaceHolder mSurfaceHolder;
    private Spinner mResolutionSpinner;
    private List<SelectableResolution> mSupportedResolutions;
    private ArrayAdapter<SelectableResolution> mAdapter;

    private SelectableResolution mSelectedResolution;
    private Camera mCamera;
    private Size mSurfaceSize;
    private boolean mCameraInitialized = false;
    private boolean mPreviewActive = false;
    private boolean mTakingPicture = false;
    private int mResolutionSpinnerIndex = -1;
    private WakeLock mWakeLock;
    private long shutterStartTime;
    private int mPreviewOrientation;
    private int mJpegOrientation;

    private ArrayList<Integer> mPreviewSizeCamerasToProcess = new ArrayList<Integer>();

    private Dialog mActiveDialog;

    /**
     * Selected preview size per camera. If null, preview size should be
     * automatically detected.
     */
    private Size[] mPreviewSizes = null;

    public static File getPictureFile(Context context) {
        return new File(context.getExternalCacheDir(), PICTURE_FILENAME);
    }

    public static float getReportedFovDegrees() {
        return mReportedFovDegrees;
    }

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.camera_fov_calibration_photo_capture);

        int cameraToBeTested = 0;
        for (int cameraId = 0; cameraId < Camera.getNumberOfCameras(); ++cameraId) {
            if (!isExternalCamera(cameraId)) {
                cameraToBeTested++;
            }
        }

        mPreview = (SurfaceView) findViewById(R.id.camera_fov_camera_preview);
        mSurfaceHolder = mPreview.getHolder();
        mSurfaceHolder.addCallback(this);

        // This is required for older versions of Android hardware.
        mSurfaceHolder.setType(SurfaceHolder.SURFACE_TYPE_PUSH_BUFFERS);

        TextView textView = (TextView) findViewById(R.id.camera_fov_tap_to_take_photo);
        textView.setTextColor(Color.WHITE);

        Button setupButton = (Button) findViewById(R.id.camera_fov_settings_button);
        setupButton.setOnClickListener(new OnClickListener() {

            @Override
            public void onClick(View v) {
                startActivity(new Intent(
                        PhotoCaptureActivity.this, CalibrationPreferenceActivity.class));
            }
        });

        Button changePreviewSizeButton = (Button) findViewById(
                R.id.camera_fov_change_preview_size_button);
        changePreviewSizeButton.setOnClickListener(new OnClickListener() {
            @Override
            public void onClick(View v) {
                // Stop camera until preview sizes have been obtained.
                if (mCamera != null) {
                    mCamera.stopPreview();
                    mCamera.release();
                    mCamera = null;
                }

                mPreviewSizeCamerasToProcess.clear();
                mPreviewSizes =  new Size[Camera.getNumberOfCameras()];
                for (int cameraId = 0; cameraId < Camera.getNumberOfCameras(); ++cameraId) {
                    if (!isExternalCamera(cameraId)) {
                        mPreviewSizeCamerasToProcess.add(cameraId);
                    }
                }
                showNextDialogToChoosePreviewSize();
            }
        });

        View previewView = findViewById(R.id.camera_fov_preview_overlay);
        previewView.setOnClickListener(new OnClickListener() {
            @Override
            public void onClick(View v) {
                if (mPreviewActive && !mTakingPicture) {
                    mTakingPicture = true;
                    shutterStartTime = System.currentTimeMillis();

                    mCamera.takePicture(new ShutterCallback() {
                        @Override
                        public void onShutter() {
                            long dT = System.currentTimeMillis() - shutterStartTime;
                            Log.d(""CTS"", ""Shutter Lag: "" + dT);
                        }
                    }, null, PhotoCaptureActivity.this);
                }
            }
        });

        mResolutionSpinner = (Spinner) findViewById(R.id.camera_fov_resolution_selector);
        mResolutionSpinner.setOnItemSelectedListener(new OnItemSelectedListener() {
            @Override
            public void onItemSelected(
                    AdapterView<?> parent, View view, int position, long id) {
                if (mSupportedResolutions != null) {
                    SelectableResolution resolution = mSupportedResolutions.get(position);
                    switchToCamera(resolution, false);

                    // It should be guaranteed that the FOV is correctly updated after setParameters().
                    mReportedFovPrePictureTaken = mCamera.getParameters().getHorizontalViewAngle();

                    mResolutionSpinnerIndex = position;
                    startPreview();
                }
            }

            @Override
            public void onNothingSelected(AdapterView<?> arg0) {}
        });

        if (cameraToBeTested == 0) {
            Log.i(TAG, ""No cameras needs to be tested. Setting test pass."");
            Toast.makeText(this, ""No cameras needs to be tested. Test pass."",
                    Toast.LENGTH_LONG).show();

            TestResult.setPassedResult(this, getClass().getName(),
                    ""All cameras are external, test skipped!"");
            finish();
        }
    }

    @Override
    protected void onResume() {
        super.onResume();
        // Keep the device from going to sleep.
        PowerManager pm = (PowerManager) getSystemService(Context.POWER_SERVICE);
        mWakeLock = pm.newWakeLock(PowerManager.FULL_WAKE_LOCK, TAG);
        mWakeLock.acquire();

        if (mSupportedResolutions == null) {
            mSupportedResolutions = new ArrayList<SelectableResolution>();
            int numCameras = Camera.getNumberOfCameras();
            for (int cameraId = 0; cameraId < numCameras; ++cameraId) {
                if (isExternalCamera(cameraId)) {
                    continue;
                }

                Camera camera = Camera.open(cameraId);

                // Get the supported picture sizes and fill the spinner.
                List<Camera.Size> supportedSizes =
                        camera.getParameters().getSupportedPictureSizes();
                for (Camera.Size size : supportedSizes) {
                    mSupportedResolutions.add(
                            new SelectableResolution(cameraId, size.width, size.height));
                }
                camera.release();
            }
        }

        // Find the first untested entry.
        for (mResolutionSpinnerIndex = 0;
                mResolutionSpinnerIndex < mSupportedResolutions.size();
                mResolutionSpinnerIndex++) {
            if (!mSupportedResolutions.get(mResolutionSpinnerIndex).tested) {
                break;
            }
        }

        mAdapter = new ArrayAdapter<SelectableResolution>(
                this, android.R.layout.simple_spinner_dropdown_item,
                mSupportedResolutions);
        mResolutionSpinner.setAdapter(mAdapter);

        mResolutionSpinner.setSelection(mResolutionSpinnerIndex);
        setResult(RESULT_CANCELED);
    }

    @Override
    public void onPause() {
        if (mCamera != null) {
            if (mPreviewActive) {
                mCamera.stopPreview();
            }

            mCamera.release();
            mCamera = null;
        }
        mPreviewActive = false;
        mWakeLock.release();
        super.onPause();
    }

    @Override
    public void onPictureTaken(byte[] data, Camera camera) {
        File pictureFile = getPictureFile(this);
        Camera.Parameters params = mCamera.getParameters();
        mReportedFovDegrees = params.getHorizontalViewAngle();

        // Show error if FOV does not match the value reported before takePicture().
        if (mReportedFovPrePictureTaken != mReportedFovDegrees) {
            mSupportedResolutions.get(mResolutionSpinnerIndex).tested = true;
            mSupportedResolutions.get(mResolutionSpinnerIndex).passed = false;

            AlertDialog.Builder dialogBuilder = new AlertDialog.Builder(this);
            dialogBuilder.setTitle(R.string.camera_fov_reported_fov_problem);
            dialogBuilder.setNeutralButton(
                    android.R.string.ok, new DialogInterface.OnClickListener() {
                @Override
                public void onClick(DialogInterface dialog, int which) {
                    if (mActiveDialog != null) {
                        mActiveDialog.dismiss();
                        mActiveDialog = null;
                        initializeCamera();
                    }
                }
            });

            String message  = getResources().getString(R.string.camera_fov_reported_fov_problem_message);
            dialogBuilder.setMessage(String.format(message, mReportedFovPrePictureTaken, mReportedFovDegrees));
            mActiveDialog = dialogBuilder.show();
            mTakingPicture = false;
            return;
        }

        try {
            FileOutputStream fos = new FileOutputStream(pictureFile);
            fos.write(data);
            fos.close();
            Log.d(TAG, ""File saved to "" + pictureFile.getAbsolutePath());

            // Start activity which will use the taken picture to determine the
            // FOV.
            startActivityForResult(new Intent(this, DetermineFovActivity.class),
                    FOV_REQUEST_CODE + mResolutionSpinnerIndex, null);
        } catch (IOException e) {
            Log.e(TAG, ""Could not save picture file."", e);
            Toast.makeText(this, ""Could not save picture file: "" + e.getMessage(),
                    Toast.LENGTH_LONG).show();
        }
        mTakingPicture = false;
    }

    @Override
    protected void onActivityResult(int requestCode, int resultCode, Intent data) {
        if (resultCode != RESULT_OK) {
            return;
        }
        int testIndex = requestCode - FOV_REQUEST_CODE;
        SelectableResolution res = mSupportedResolutions.get(testIndex);
        res.tested = true;
        float reportedFOV = CtsTestHelper.getReportedFOV(data);
        float measuredFOV = CtsTestHelper.getMeasuredFOV(data);
        res.measuredFOV = measuredFOV;
        if (CtsTestHelper.isResultPassed(reportedFOV, measuredFOV)) {
            res.passed = true;
        }

        boolean allTested = true;
        for (int i = 0; i < mSupportedResolutions.size(); i++) {
            if (!mSupportedResolutions.get(i).tested) {
                allTested = false;
                break;
            }
        }
        if (!allTested) {
            mAdapter.notifyDataSetChanged();
            return;
        }

        boolean allPassed = true;
        for (int i = 0; i < mSupportedResolutions.size(); i++) {
            if (!mSupportedResolutions.get(i).passed) {
                allPassed = false;
                break;
            }
        }
        if (allPassed) {
            TestResult.setPassedResult(this, getClass().getName(),
                    CtsTestHelper.getTestDetails(mSupportedResolutions));
        } else {
            TestResult.setFailedResult(this, getClass().getName(),
                    CtsTestHelper.getTestDetails(mSupportedResolutions));
        }
        finish();
    }

    @Override
    public void surfaceChanged(
            SurfaceHolder holder, int format, int width, int height) {
        mSurfaceSize = new Size(width, height);
        initializeCamera();
    }

    @Override
    public void surfaceCreated(SurfaceHolder holder) {
        // Nothing to do.
    }

    @Override
    public void surfaceDestroyed(SurfaceHolder holder) {
        // Nothing to do.
    }

    private void showNextDialogToChoosePreviewSize() {
        final int cameraId = mPreviewSizeCamerasToProcess.remove(0);

        Camera camera = Camera.open(cameraId);
        final List<Camera.Size> sizes = camera.getParameters()
                .getSupportedPreviewSizes();
        String[] choices = new String[sizes.size()];
        for (int i = 0; i < sizes.size(); ++i) {
            Camera.Size size = sizes.get(i);
            choices[i] = size.width + "" x "" + size.height;
        }

        final AlertDialog.Builder builder = new AlertDialog.Builder(this);
        String dialogTitle = String.format(
                getResources().getString(R.string.camera_fov_choose_preview_size_for_camera),
                cameraId);
        builder.setTitle(
                dialogTitle).
                setOnCancelListener(new DialogInterface.OnCancelListener() {
                    @Override
                    public void onCancel(DialogInterface arg0) {
                        // User cancelled preview size selection.
                        mPreviewSizes = null;
                        switchToCamera(mSelectedResolution, true);
                    }
                }).
                setSingleChoiceItems(choices, 0, new DialogInterface.OnClickListener() {
                    @Override
                    public void onClick(DialogInterface dialog, int which) {
                        Camera.Size size = sizes.get(which);
                        mPreviewSizes[cameraId] = new Size(
                                size.width, size.height);
                        dialog.dismiss();

                        if (mPreviewSizeCamerasToProcess.isEmpty()) {
                            // We're done, re-initialize camera.
                            switchToCamera(mSelectedResolution, true);
                        } else {
                            // Process other cameras.
                            showNextDialogToChoosePreviewSize();
                        }
                    }
                }).create().show();
        camera.release();
    }

    private void initializeCamera() {
        initializeCamera(true);
    }

    private void initializeCamera(boolean startPreviewAfterInit) {
        if (mCamera == null || mSurfaceHolder.getSurface() == null) {
            return;
        }

        try {
            mCamera.setPreviewDisplay(mSurfaceHolder);
        } catch (Throwable t) {
            Log.e(TAG, ""Could not set preview display"", t);
            Toast.makeText(this, t.getMessage(), Toast.LENGTH_LONG).show();
            return;
        }

        calculateOrientations(this, mSelectedResolution.cameraId, mCamera);
        Camera.Parameters params = setCameraParams(mCamera);

        // Either use chosen preview size for current camera or automatically
        // choose preview size based on view dimensions.
        Size selectedPreviewSize = null;
        if (mPreviewSizes != null) {
            selectedPreviewSize = mPreviewSizes[mSelectedResolution.cameraId];
        } else if (mSurfaceSize != null) {
            selectedPreviewSize = getBestPreviewSize(
                    mSurfaceSize.width, mSurfaceSize.height, params);
        }

        if (selectedPreviewSize != null) {
            params.setPreviewSize(selectedPreviewSize.width, selectedPreviewSize.height);
            mCamera.setParameters(params);
            mCameraInitialized = true;
        }

        if (startPreviewAfterInit) {
            if (selectedPreviewSize == null) {
                Log.w(TAG, ""Preview started without setting preview size"");
            }
            startPreview();
        }
    }

    private void startPreview() {
        if (mCameraInitialized && mCamera != null) {
            mCamera.setDisplayOrientation(mPreviewOrientation);
            mCamera.startPreview();
            mPreviewActive = true;
        }
    }

    private void switchToCamera(SelectableResolution resolution, boolean startPreview) {
        if (mCamera != null) {
            mCamera.stopPreview();
            mCamera.release();
        }

        mSelectedResolution = resolution;
        mCamera = Camera.open(mSelectedResolution.cameraId);

        initializeCamera(startPreview);
    }

    /**
     * Get the best supported focus mode.
     *
     * @param camera - Android camera object.
     * @return the best supported focus mode.
     */
    private static String getFocusMode(Camera camera) {
        List<String> modes = camera.getParameters().getSupportedFocusModes();
        if (modes != null) {
            if (modes.contains(Camera.Parameters.FOCUS_MODE_INFINITY)) {
                Log.v(TAG, ""Using Focus mode infinity"");
                return Camera.Parameters.FOCUS_MODE_INFINITY;
            }
            if (modes.contains(Camera.Parameters.FOCUS_MODE_FIXED)) {
                Log.v(TAG, ""Using Focus mode fixed"");
                return Camera.Parameters.FOCUS_MODE_FIXED;
            }
        }
        Log.v(TAG, ""Using Focus mode auto."");
        return Camera.Parameters.FOCUS_MODE_AUTO;
    }

    /**
     * Set the common camera parameters on the given camera and returns the
     * parameter object for further modification, if needed.
     */
    private Camera.Parameters setCameraParams(Camera camera) {
        // The picture size is taken and set from the spinner selection
        // callback.
        Camera.Parameters params = camera.getParameters();
        params.setJpegThumbnailSize(0, 0);
        params.setJpegQuality(100);
        params.setRotation(mJpegOrientation);
        params.setFocusMode(getFocusMode(camera));
        params.setZoom(0);
        params.setPictureSize(mSelectedResolution.width, mSelectedResolution.height);
        return params;
    }

    private Size getBestPreviewSize(
            int width, int height, Camera.Parameters parameters) {
        Size result = null;

        for (Camera.Size size : parameters.getSupportedPreviewSizes()) {
            if (size.width <= width && size.height <= height) {
                if (result == null) {
                    result = new Size(size.width, size.height);
                } else {
                    int resultArea = result.width * result.height;
                    int newArea = size.width * size.height;

                    if (newArea > resultArea) {
                        result = new Size(size.width, size.height);
                    }
                }
            }
        }
        return result;
    }

    private void calculateOrientations(Activity activity,
            int cameraId, android.hardware.Camera camera) {
        android.hardware.Camera.CameraInfo info =
                new android.hardware.Camera.CameraInfo();
        android.hardware.Camera.getCameraInfo(cameraId, info);
        int rotation = activity.getWindowManager().getDefaultDisplay()
                .getRotation();
        int degrees = 0;
        switch (rotation) {
            case Surface.ROTATION_0: degrees = 0; break;
            case Surface.ROTATION_90: degrees = 90; break;
            case Surface.ROTATION_180: degrees = 180; break;
            case Surface.ROTATION_270: degrees = 270; break;
        }

        if (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {
            mJpegOrientation = (info.orientation + degrees) % 360;
            mPreviewOrientation = (360 - mJpegOrientation) % 360;  // compensate the mirror
        } else {  // back-facing
            mJpegOrientation = (info.orientation - degrees + 360) % 360;
            mPreviewOrientation = mJpegOrientation;
        }
    }

    private boolean isExternalCamera(int cameraId) {
        CameraManager manager = (CameraManager) this.getSystemService(Context.CAMERA_SERVICE);
        try {
            String cameraIdStr = manager.getCameraIdList()[cameraId];
            CameraCharacteristics characteristics =
                    manager.getCameraCharacteristics(cameraIdStr);

            if (characteristics.get(CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL) ==
                            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL) {
                // External camera doesn't support FOV informations
                return true;
            }
        } catch (CameraAccessException e) {
            Toast.makeText(this, ""Could not access camera "" + cameraId +
                    "": "" + e.getMessage(), Toast.LENGTH_LONG).show();
        }
        return false;
    }
}"	""	""	"resolution"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.AllocationTest"	"testBlackWhite"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/AllocationTest.java"	""	"public void testBlackWhite() throws CameraAccessException {

        /** low iso + low exposure (first shot) */
        final float THRESHOLD_LOW = 0.025f;
        /** high iso + high exposure (second shot) */
        final float THRESHOLD_HIGH = 0.975f;

        mCameraIterable.forEachCamera(/*fullHwLevel*/false, new CameraBlock() {
            @Override
            public void run(CameraDevice camera) throws CameraAccessException {
                final StaticMetadata staticInfo =
                        new StaticMetadata(mCameraManager.getCameraCharacteristics(camera.getId()));

                // This test requires PFC and manual sensor control
                if (!staticInfo.isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR) ||
                        !staticInfo.isPerFrameControlSupported()) {
                    return;
                }

                final Size maxSize = getMaxSize(
                        getSupportedSizeForFormat(YUV_420_888, camera.getId(), mCameraManager));

                try (ScriptGraph scriptGraph = createGraphForYuvCroppedMeans(maxSize)) {

                    CaptureRequest.Builder req =
                            configureAndCreateRequestForSurface(scriptGraph.getInputSurface());

                    // Take a shot with very low ISO and exposure time. Expect it to be black.
                    int minimumSensitivity = staticInfo.getSensitivityMinimumOrDefault();
                    long minimumExposure = staticInfo.getExposureMinimumOrDefault();
                    setManualCaptureRequest(req, minimumSensitivity, minimumExposure);

                    CaptureRequest lowIsoExposureShot = req.build();
                    captureSingleShotAndExecute(lowIsoExposureShot, scriptGraph);

                    float[] blackMeans = convertPixelYuvToRgb(scriptGraph.getOutputData());

                    // Take a shot with very high ISO and exposure time. Expect it to be white.
                    int maximumSensitivity = staticInfo.getSensitivityMaximumOrDefault();
                    long maximumExposure = staticInfo.getExposureMaximumOrDefault();
                    setManualCaptureRequest(req, maximumSensitivity, maximumExposure);

                    CaptureRequest highIsoExposureShot = req.build();
                    captureSingleShotAndExecute(highIsoExposureShot, scriptGraph);

                    float[] whiteMeans = convertPixelYuvToRgb(scriptGraph.getOutputData());

                    // Low iso + low exposure (first shot), just check and log the error.
                    for (int i = 0; i < blackMeans.length; ++i) {
                        if (blackMeans[i] >= THRESHOLD_LOW) {
                            Log.e(TAG,
                                    String.format(""Black means too high: (%s should be greater""
                                            + "" than %s; item index %d in %s)"", blackMeans[i],
                                            THRESHOLD_LOW, i,
                                            Arrays.toString(blackMeans)));
                        }
                    }

                    // High iso + high exposure (second shot), just check and log the error
                    for (int i = 0; i < whiteMeans.length; ++i) {
                        if (whiteMeans[i] <= THRESHOLD_HIGH) {
                            Log.e(TAG,
                                    String.format(""White means too low: (%s should be less than""
                                            + "" %s; item index %d in %s)"", whiteMeans[i],
                                            THRESHOLD_HIGH, i,
                                            Arrays.toString(whiteMeans)));
                        }
                    }
                }
            }
        });
    }

    /**
     * Test that the android.sensitivity.parameter is applied.
     */"	""	""	"minimum"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.AllocationTest"	"testParamSensitivity"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/AllocationTest.java"	""	"public void testParamSensitivity() throws CameraAccessException {
        final float THRESHOLD_MAX_MIN_DIFF = 0.3f;
        final float THRESHOLD_MAX_MIN_RATIO = 2.0f;
        final int NUM_STEPS = 5;
        final long EXPOSURE_TIME_NS = 2000000; // 2 ms
        final int RGB_CHANNELS = 3;

        mCameraIterable.forEachCamera(/*fullHwLevel*/false, new CameraBlock() {


            @Override
            public void run(CameraDevice camera) throws CameraAccessException {
                final StaticMetadata staticInfo =
                        new StaticMetadata(mCameraManager.getCameraCharacteristics(camera.getId()));
                // This test requires PFC and manual sensor control
                if (!staticInfo.isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR) ||
                        !staticInfo.isPerFrameControlSupported()) {
                    return;
                }

                final List<float[]> rgbMeans = new ArrayList<float[]>();
                final Size maxSize = getMaxSize(
                        getSupportedSizeForFormat(YUV_420_888, camera.getId(), mCameraManager));

                final int sensitivityMin = staticInfo.getSensitivityMinimumOrDefault();
                final int sensitivityMax = staticInfo.getSensitivityMaximumOrDefault();

                // List each sensitivity from min-max in NUM_STEPS increments
                int[] sensitivities = new int[NUM_STEPS];
                for (int i = 0; i < NUM_STEPS; ++i) {
                    int delta = (sensitivityMax - sensitivityMin) / (NUM_STEPS - 1);
                    sensitivities[i] = sensitivityMin + delta * i;
                }

                try (ScriptGraph scriptGraph = createGraphForYuvCroppedMeans(maxSize)) {

                    CaptureRequest.Builder req =
                            configureAndCreateRequestForSurface(scriptGraph.getInputSurface());

                    // Take burst shots with increasing sensitivity one after other.
                    for (int i = 0; i < NUM_STEPS; ++i) {
                        setManualCaptureRequest(req, sensitivities[i], EXPOSURE_TIME_NS);
                        captureSingleShotAndExecute(req.build(), scriptGraph);
                        float[] means = convertPixelYuvToRgb(scriptGraph.getOutputData());
                        rgbMeans.add(means);

                        if (VERBOSE) {
                            Log.v(TAG, ""testParamSensitivity - captured image "" + i +
                                    "" with RGB means: "" + Arrays.toString(means));
                        }
                    }

                    // Test that every consecutive image gets brighter.
                    for (int i = 0; i < rgbMeans.size() - 1; ++i) {
                        float[] curMeans = rgbMeans.get(i);
                        float[] nextMeans = rgbMeans.get(i+1);

                        float[] left = curMeans;
                        float[] right = nextMeans;
                        String leftString = Arrays.toString(left);
                        String rightString = Arrays.toString(right);

                        String msgHeader =
                                String.format(""Shot with sensitivity %d should not have higher "" +
                                ""average means than shot with sensitivity %d"",
                                sensitivities[i], sensitivities[i+1]);
                        for (int m = 0; m < left.length; ++m) {
                            String msg = String.format(
                                    ""%s: (%s should be less than or equal to %s; item index %d;""
                                    + "" left = %s; right = %s)"",
                                    msgHeader, left[m], right[m], m, leftString, rightString);
                            if (left[m] > right[m]) {
                                Log.e(TAG, msg);
                            }
                        }
                    }

                    // Test the min-max diff and ratios are within expected thresholds
                    float[] lastMeans = rgbMeans.get(NUM_STEPS - 1);
                    float[] firstMeans = rgbMeans.get(/*location*/0);
                    for (int i = 0; i < RGB_CHANNELS; ++i) {
                        if (lastMeans[i] - firstMeans[i] <= THRESHOLD_MAX_MIN_DIFF) {
                            Log.w(TAG, String.format(""Sensitivity max-min diff too small""
                                    + ""(max=%f, min=%f)"", lastMeans[i], firstMeans[i]));
                        }
                        if (lastMeans[i] / firstMeans[i] <= THRESHOLD_MAX_MIN_RATIO) {
                            Log.w(TAG, String.format(""Sensitivity max-min ratio too small""
                                    + ""(max=%f, min=%f)"", lastMeans[i], firstMeans[i]));
                        }
                    }
                }
            }
        });

    }

    /**
     * Common script graph for manual-capture based tests that determine the average pixel
     * values of a cropped sub-region.
     *
     * <p>Processing chain:
     *
     * <pre>
     * input:  YUV_420_888 surface
     * output: mean YUV value of a central section of the image,
     *         YUV 4:4:4 encoded as U8_3
     * steps:
     *      1) crop [0.45,0.45] - [0.55, 0.55]
     *      2) average columns
     *      3) average rows
     * </pre>
     * </p>
     */
    private static ScriptGraph createGraphForYuvCroppedMeans(final Size size) {
        ScriptGraph scriptGraph = ScriptGraph.create()
                .configureInputWithSurface(size, YUV_420_888)
                .configureScript(ScriptYuvCrop.class)
                    .set(ScriptYuvCrop.CROP_WINDOW,
                            new Patch(size, /*x*/0.45f, /*y*/0.45f, /*w*/0.1f, /*h*/0.1f).toRectF())
                    .buildScript()
                .chainScript(ScriptYuvMeans2dTo1d.class)
                .chainScript(ScriptYuvMeans1d.class)
                // TODO: Make a script for YUV 444 -> RGB 888 conversion
                .buildGraph();
        return scriptGraph;
    }

    /*
     * TODO: Refactor below code into separate classes and to not depend on AllocationTest
     * inner variables.
     *
     * TODO: add javadocs to below methods
     *
     * TODO: Figure out if there's some elegant way to compose these forEaches together, so that
     * the callers don't have to do a ton of nesting
     */

    interface CameraBlock {
        void run(CameraDevice camera) throws CameraAccessException;
    }

    class CameraIterable {
        public void forEachCamera(CameraBlock runnable)
                throws CameraAccessException {
            forEachCamera(/*fullHwLevel*/false, runnable);
        }

        public void forEachCamera(boolean fullHwLevel, CameraBlock runnable)
                throws CameraAccessException {
            assertNotNull(""No camera manager"", mCameraManager);
            assertNotNull(""No camera IDs"", mCameraIdsUnderTest);

            for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
                // Don't execute the runnable against non-FULL cameras if FULL is required
                CameraCharacteristics properties =
                        mCameraManager.getCameraCharacteristics(mCameraIdsUnderTest[i]);
                StaticMetadata staticInfo = new StaticMetadata(properties);
                if (fullHwLevel && !staticInfo.isHardwareLevelAtLeastFull()) {
                    Log.i(TAG, String.format(
                            ""Skipping this test for camera %s, needs FULL hw level"",
                            mCameraIdsUnderTest[i]));
                    continue;
                }
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, String.format(
                        ""Skipping this test for camera %s, does not support regular outputs"",
                        mCameraIdsUnderTest[i]));
                    continue;
                }
                // Open camera and execute test
                Log.i(TAG, ""Testing Camera "" + mCameraIdsUnderTest[i]);
                try {
                    openDevice(mCameraIdsUnderTest[i]);

                    runnable.run(mCamera);
                } finally {
                    closeDevice(mCameraIdsUnderTest[i]);
                }
            }
        }

        private void openDevice(String cameraId) {
            if (mCamera != null) {
                throw new IllegalStateException(""Already have open camera device"");
            }
            try {
                mCamera = openCamera(
                    mCameraManager, cameraId, mCameraListener, mHandler);
            } catch (CameraAccessException e) {
                fail(""Fail to open camera synchronously, "" + Log.getStackTraceString(e));
            } catch (BlockingOpenException e) {
                fail(""Fail to open camera asynchronously, "" + Log.getStackTraceString(e));
            }
            mCameraListener.waitForState(STATE_OPENED, CAMERA_OPEN_TIMEOUT_MS);
        }

        private void closeDevice(String cameraId) {
            if (mCamera != null) {
                mCamera.close();
                mCameraListener.waitForState(STATE_CLOSED, CAMERA_CLOSE_TIMEOUT_MS);
                mCamera = null;
            }
        }
    }

    interface SizeBlock {
        void run(Size size) throws CameraAccessException;
    }

    class SizeIterable {
        public void forEachSize(int format, SizeBlock runnable) throws CameraAccessException {
            assertNotNull(""No camera opened"", mCamera);
            assertNotNull(""No camera manager"", mCameraManager);

            CameraCharacteristics properties =
                    mCameraManager.getCameraCharacteristics(mCamera.getId());

            assertNotNull(""Can't get camera properties!"", properties);

            StreamConfigurationMap config =
                    properties.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            int[] availableOutputFormats = config.getOutputFormats();
            assertArrayNotEmpty(availableOutputFormats,
                    ""availableOutputFormats should not be empty"");
            Arrays.sort(availableOutputFormats);
            assertTrue(""Can't find the format "" + format + "" in supported formats "" +
                    Arrays.toString(availableOutputFormats),
                    Arrays.binarySearch(availableOutputFormats, format) >= 0);

            Size[] availableSizes = getSupportedSizeForFormat(format, mCamera.getId(),
                    mCameraManager);
            assertArrayNotEmpty(availableSizes, ""availableSizes should not be empty"");

            for (Size size : availableSizes) {

                if (VERBOSE) {
                    Log.v(TAG, ""Testing size "" + size.toString() +
                            "" for camera "" + mCamera.getId());
                }
                runnable.run(size);
            }
        }
    }

    interface ResultBlock {
        void run(CaptureResult result) throws CameraAccessException;
    }

    class ResultIterable {
        public void forEachResultOnce(CaptureRequest request, ResultBlock block)
                throws CameraAccessException {
            forEachResult(request, /*count*/1, /*repeating*/false, block);
        }

        public void forEachResultRepeating(CaptureRequest request, int count, ResultBlock block)
                throws CameraAccessException {
            forEachResult(request, count, /*repeating*/true, block);
        }

        public void forEachResult(CaptureRequest request, int count, boolean repeating,
                ResultBlock block) throws CameraAccessException {

            // TODO: start capture, i.e. configureOutputs

            SimpleCaptureCallback listener = new SimpleCaptureCallback();

            if (!repeating) {
                for (int i = 0; i < count; ++i) {
                    mSession.capture(request, listener, mHandler);
                }
            } else {
                mSession.setRepeatingRequest(request, listener, mHandler);
            }

            // Assume that the device is already IDLE.
            mSessionListener.getStateWaiter().waitForState(BlockingSessionCallback.SESSION_ACTIVE,
                    CAMERA_ACTIVE_TIMEOUT_MS);

            for (int i = 0; i < count; ++i) {
                if (VERBOSE) {
                    Log.v(TAG, String.format(""Testing with result %d of %d for camera %s"",
                            i, count, mCamera.getId()));
                }

                CaptureResult result = listener.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);
                block.run(result);
            }

            if (repeating) {
                mSession.stopRepeating();
                mSessionListener.getStateWaiter().waitForState(
                    BlockingSessionCallback.SESSION_READY, CAMERA_IDLE_TIMEOUT_MS);
            }

            // TODO: Make a Configure decorator or some such for configureOutputs
        }
    }
}"	""	""	"minimum"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-1"	"7.5/H-1-1"	"07050000.720101"	"""[7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID.  | [7.5/H-1-1] MUST have a primary rear facing camera with a resolution of at least 12 megapixels supporting video capture at 4k@30fps. The primary rear-facing camera is the rear-facing camera with the lowest camera ID. """	""	""	"cdd rear MEDIA_PERFORMANCE_CLASS 4k@30fps minimum 12 resolution"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.video.CameraVideoActivity"	"exists"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/video/CameraVideoActivity.java"	""	"public void test/*
 *.
 */
package com.android.cts.verifier.camera.video;

import android.app.AlertDialog;
import android.content.Context;
import android.content.DialogInterface;
import android.graphics.Matrix;
import android.graphics.SurfaceTexture;
import android.hardware.Camera;
import android.hardware.Camera.CameraInfo;
import android.hardware.Camera.Size;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraManager;
import android.hardware.cts.helpers.CameraUtils;
import android.media.CamcorderProfile;
import android.media.MediaPlayer;
import android.media.MediaRecorder;
import android.os.Bundle;
import android.os.Environment;
import android.os.Handler;
import android.text.method.ScrollingMovementMethod;
import android.util.Log;
import android.view.Surface;
import android.view.TextureView;
import android.view.View;
import android.widget.AdapterView;
import android.widget.ArrayAdapter;
import android.widget.Button;
import android.widget.ImageButton;
import android.widget.Spinner;
import android.widget.TextView;
import android.widget.Toast;
import android.widget.VideoView;

import com.android.cts.verifier.PassFailButtons;
import com.android.cts.verifier.R;

import java.io.File;
import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.Date;
import java.util.List;
import java.util.Optional;
import java.util.TreeSet;


/**
 * Tests for manual verification of camera video capture
 */
public class CameraVideoActivity extends PassFailButtons.Activity
        implements TextureView.SurfaceTextureListener {

    private static final String TAG = ""CtsCameraVideo"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);
    private static final int MEDIA_TYPE_IMAGE = 1;
    private static final int MEDIA_TYPE_VIDEO = 2;
    private static final int VIDEO_LENGTH = 3000; // in ms

    private TextureView mPreviewView;
    private SurfaceTexture mPreviewTexture;
    private int mPreviewTexWidth;
    private int mPreviewTexHeight;
    private int mPreviewRotation;
    private int mVideoRotation;

    private VideoView mPlaybackView;

    private Spinner mCameraSpinner;
    private Spinner mResolutionSpinner;

    private int mCurrentCameraId = -1;
    private Camera mCamera;
    private boolean mIsExternalCamera;
    private int mVideoFrameRate = -1;

    private MediaRecorder mMediaRecorder;

    private List<Size> mPreviewSizes;
    private Size mNextPreviewSize;
    private Size mPreviewSize;
    private List<Integer> mVideoSizeIds;
    private List<String> mVideoSizeNames;
    private int mCurrentVideoSizeId;
    private String mCurrentVideoSizeName;

    private boolean isRecording = false;
    private boolean isPlayingBack = false;
    private Button captureButton;
    private ImageButton mPassButton;
    private ImageButton mFailButton;

    private TextView mStatusLabel;

    private TreeSet<CameraCombination> mTestedCombinations = new TreeSet<>(COMPARATOR);
    private TreeSet<CameraCombination> mUntestedCombinations = new TreeSet<>(COMPARATOR);
    private TreeSet<String> mUntestedCameras = new TreeSet<>();

    private File outputVideoFile;

    private class CameraCombination {
        private final int mCameraIndex;
        private final int mVideoSizeIdIndex;
        private final String mVideoSizeName;

        private CameraCombination(
            int cameraIndex, int videoSizeIdIndex, String videoSizeName) {
            this.mCameraIndex = cameraIndex;
            this.mVideoSizeIdIndex = videoSizeIdIndex;
            this.mVideoSizeName = videoSizeName;
        }

        @Override
        public String toString() {
            return String.format(""Camera %d, %s"", mCameraIndex, mVideoSizeName);
        }
    }

    private static final Comparator<CameraCombination> COMPARATOR =
        Comparator.<CameraCombination, Integer>comparing(c -> c.mCameraIndex)
            .thenComparing(c -> c.mVideoSizeIdIndex);

    /**
     * @see #MEDIA_TYPE_IMAGE
     * @see #MEDIA_TYPE_VIDEO
     */
    private File getOutputMediaFile(int type) {
        File mediaStorageDir = new File(getExternalFilesDir(null), TAG);
        if (mediaStorageDir == null) {
            Log.e(TAG, ""failed to retrieve external files directory"");
            return null;
        }

        if (!mediaStorageDir.exists()) {
            if (!mediaStorageDir.mkdirs()) {
                Log.d(TAG, ""failed to create directory"");
                return null;
            }
        }

        String timeStamp = new SimpleDateFormat(""yyyyMMdd_HHmmss"").format(new Date());
        File mediaFile;
        if (type == MEDIA_TYPE_IMAGE) {
            mediaFile = new File(mediaStorageDir.getPath() + File.separator +
                    ""IMG_"" + timeStamp + "".jpg"");
        } else if (type == MEDIA_TYPE_VIDEO) {
            mediaFile = new File(mediaStorageDir.getPath() + File.separator +
                    ""VID_"" + timeStamp + "".mp4"");
            if (VERBOSE) {
                Log.v(TAG, ""getOutputMediaFile: output file "" + mediaFile.getPath());
            }
        } else {
            return null;
        }

        return mediaFile;
    }

    private static final int BIT_RATE_720P = 8000000;
    private static final int BIT_RATE_MIN = 64000;
    private static final int BIT_RATE_MAX = BIT_RATE_720P;

    private int getVideoBitRate(Camera.Size sz) {
        int rate = BIT_RATE_720P;
        float scaleFactor = sz.height * sz.width / (float)(1280 * 720);
        rate = (int)(rate * scaleFactor);

        // Clamp to the MIN, MAX range.
        return Math.max(BIT_RATE_MIN, Math.min(BIT_RATE_MAX, rate));
    }

    private int getVideoFrameRate() {
        return mVideoFrameRate;
    }

    private void setVideoFrameRate(int videoFrameRate) {
        mVideoFrameRate = videoFrameRate;
    }

    private boolean prepareVideoRecorder() {

        mMediaRecorder = new MediaRecorder();

        // Step 1: unlock and set camera to MediaRecorder
        mCamera.unlock();
        mMediaRecorder.setCamera(mCamera);

        // Step 2: set sources
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);
        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);

        // Step 3: set a CamcorderProfile
        if (mIsExternalCamera) {
            Camera.Size recordSize = null;
            switch (mCurrentVideoSizeId) {
                case CamcorderProfile.QUALITY_QCIF:
                    recordSize = mCamera.new Size(176, 144);
                break;
                case CamcorderProfile.QUALITY_QVGA:
                    recordSize = mCamera.new Size(320, 240);
                break;
                case CamcorderProfile.QUALITY_CIF:
                    recordSize = mCamera.new Size(352, 288);
                break;
                case CamcorderProfile.QUALITY_480P:
                    recordSize = mCamera.new Size(720, 480);
                break;
                case CamcorderProfile.QUALITY_720P:
                    recordSize = mCamera.new Size(1280, 720);
                break;
                default:
                    String msg = ""Unknown CamcorderProfile: "" + mCurrentVideoSizeId;
                    Log.e(TAG, msg);
                    releaseMediaRecorder();
                    throw new AssertionError(msg);
            }

            mMediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.DEFAULT);
            mMediaRecorder.setVideoEncoder(MediaRecorder.VideoEncoder.DEFAULT);
            mMediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.DEFAULT);
            mMediaRecorder.setVideoEncodingBitRate(getVideoBitRate(recordSize));
            mMediaRecorder.setVideoSize(recordSize.width, recordSize.height);
            mMediaRecorder.setVideoFrameRate(getVideoFrameRate());
        } else {
            mMediaRecorder.setProfile(CamcorderProfile.get(mCurrentCameraId, mCurrentVideoSizeId));
        }

        // Step 4: set output file
        outputVideoFile = getOutputMediaFile(MEDIA_TYPE_VIDEO);
        mMediaRecorder.setOutputFile(outputVideoFile.toString());

        // Step 5: set preview output
        // This is not necessary since preview has been taken care of

        // Step 6: set orientation hint
        mMediaRecorder.setOrientationHint(mVideoRotation);

        // Step 7: prepare configured MediaRecorder
        try {
            mMediaRecorder.prepare();
        } catch (IOException e) {
            Log.e(TAG, ""IOException preparing MediaRecorder: "", e);
            releaseMediaRecorder();
            throw new AssertionError(e);
        }

        mMediaRecorder.setOnErrorListener(
                new MediaRecorder.OnErrorListener() {
                    @Override
                    public void onError(MediaRecorder mr, int what, int extra) {
                        if (what == MediaRecorder.MEDIA_RECORDER_ERROR_UNKNOWN) {
                            Log.e(TAG, ""unknown error in media recorder, error: "" + extra);
                        } else {
                            Log.e(TAG, ""media recorder server died, error: "" + extra);
                        }

                        failTest(""Media recorder error."");
                    }
                });

        if (VERBOSE) {
            Log.v(TAG, ""prepareVideoRecorder: prepared configured MediaRecorder"");
        }

        return true;
    }

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        setContentView(R.layout.camera_video);
        setPassFailButtonClickListeners();
        setInfoResources(R.string.camera_video, R.string.video_info, /*viewId*/-1);

        mPreviewView = (TextureView) findViewById(R.id.video_capture);
        mPlaybackView = (VideoView) findViewById(R.id.video_playback);
        mPlaybackView.setOnCompletionListener(mPlaybackViewListener);

        captureButton = (Button) findViewById(R.id.record_button);
        mPassButton = (ImageButton) findViewById(R.id.pass_button);
        mFailButton = (ImageButton) findViewById(R.id.fail_button);
        mPassButton.setEnabled(false);
        mFailButton.setEnabled(true);

        mPreviewView.setSurfaceTextureListener(this);

        int numCameras = Camera.getNumberOfCameras();
        String[] cameraNames = new String[numCameras];
        for (int i = 0; i < numCameras; i++) {
            cameraNames[i] = ""Camera "" + i;
            mUntestedCameras.add(""All combinations for Camera "" + i + ""\n"");
        }
        if (VERBOSE) {
            Log.v(TAG, ""onCreate: number of cameras="" + numCameras);
        }
        mCameraSpinner = (Spinner) findViewById(R.id.cameras_selection);
        mCameraSpinner.setAdapter(
            new ArrayAdapter<String>(
                this, R.layout.camera_list_item, cameraNames));
        mCameraSpinner.setOnItemSelectedListener(mCameraSpinnerListener);

        mResolutionSpinner = (Spinner) findViewById(R.id.resolution_selection);
        mResolutionSpinner.setOnItemSelectedListener(mResolutionSelectedListener);

        mStatusLabel = (TextView) findViewById(R.id.status_label);

        Button mNextButton = (Button) findViewById(R.id.next_button);
        mNextButton.setOnClickListener(v -> {
            setUntestedCombination();
            if (VERBOSE) {
                Log.v(TAG, ""onClick: mCurrentVideoSizeId = "" +
                    mCurrentVideoSizeId + "" "" + mCurrentVideoSizeName);
                Log.v(TAG, ""onClick: setting preview size ""
                    + mNextPreviewSize.width + ""x"" + mNextPreviewSize.height);
            }

            startPreview();
            if (VERBOSE) {
                Log.v(TAG, ""onClick: started new preview"");
            }
            captureButton.performClick();
        });
    }

    /**
     * Set an untested combination of the current camera and video size.
     * Triggered by next button click.
     */
    private void setUntestedCombination() {
        Optional<CameraCombination> combination = mUntestedCombinations.stream().filter(
            c -> c.mCameraIndex == mCurrentCameraId).findFirst();
        if (!combination.isPresent()) {
            Toast.makeText(this, ""All Camera "" + mCurrentCameraId + "" tests are done."",
                Toast.LENGTH_SHORT).show();
            return;
        }

        // There is untested combination for the current camera, set the next untested combination.
        int mNextVideoSizeIdIndex = combination.get().mVideoSizeIdIndex;

        mCurrentVideoSizeId = mVideoSizeIds.get(mNextVideoSizeIdIndex);
        mCurrentVideoSizeName = mVideoSizeNames.get(mNextVideoSizeIdIndex);
        mNextPreviewSize = matchPreviewRecordSize();
        mResolutionSpinner.setSelection(mNextVideoSizeIdIndex);
    }

    @Override
    public void onResume() {
        super.onResume();

        setUpCamera(mCameraSpinner.getSelectedItemPosition());
        if (VERBOSE) {
            Log.v(TAG, ""onResume: camera has been setup"");
        }

        setUpCaptureButton();
        if (VERBOSE) {
            Log.v(TAG, ""onResume: captureButton has been setup"");
        }

    }

    @Override
    public void onPause() {
        super.onPause();

        releaseMediaRecorder();
        shutdownCamera();
        mPreviewTexture = null;
    }

    private MediaPlayer.OnCompletionListener mPlaybackViewListener =
            new MediaPlayer.OnCompletionListener() {

                @Override
                public void onCompletion(MediaPlayer mp) {
                    isPlayingBack = false;
                    mPlaybackView.stopPlayback();
                    captureButton.setEnabled(true);

                    mStatusLabel.setMovementMethod(new ScrollingMovementMethod());
                    StringBuilder progress = new StringBuilder();
                    progress.append(getResources().getString(R.string.status_ready));
                    progress.append(""\n---- Progress ----\n"");
                    progress.append(getTestDetails());
                    mStatusLabel.setText(progress.toString());
                }

    };

    private void releaseMediaRecorder() {
        if (mMediaRecorder != null) {
            mMediaRecorder.reset();
            mMediaRecorder.release();
            mMediaRecorder = null;
            mCamera.lock(); // check here, lock camera for later use
        }
    }

    @Override
    public String getTestDetails() {
        StringBuilder reportBuilder = new StringBuilder();
        reportBuilder.append(""Tested combinations:\n"");
        for (CameraCombination combination: mTestedCombinations) {
            reportBuilder.append(combination);
            reportBuilder.append(""\n"");
        }
        reportBuilder.append(""Untested combinations:\n"");
        for (String untestedCam : mUntestedCameras) {
            reportBuilder.append(untestedCam);
        }
        for (CameraCombination combination: mUntestedCombinations) {
            reportBuilder.append(combination);
            reportBuilder.append(""\n"");
        }
        return reportBuilder.toString();
    }

    @Override
    public void onSurfaceTextureAvailable(SurfaceTexture surface,
            int width, int height) {
        mPreviewTexture = surface;
        mPreviewTexWidth = width;
        mPreviewTexHeight = height;
        if (mCamera != null) {
            startPreview();
        }
    }

    @Override
    public void onSurfaceTextureSizeChanged(SurfaceTexture surface, int width, int height) {
        // Ignored, Camera does all the work for us
    }

    @Override
    public boolean onSurfaceTextureDestroyed(SurfaceTexture surface) {
        return true;
    }


    @Override
    public void onSurfaceTextureUpdated(SurfaceTexture surface) {
        // Invoked every time there's a new Camera preview frame
    }

    private AdapterView.OnItemSelectedListener mCameraSpinnerListener =
            new AdapterView.OnItemSelectedListener() {
                @Override
                public void onItemSelected(AdapterView<?> parent,
                        View view, int pos, long id) {
                    if (mCurrentCameraId != pos) {
                        setUpCamera(pos);
                    }
                }

                @Override
                public void onNothingSelected(AdapterView<?> parent) {
                    // Intentionally left blank
                }

            };

    private AdapterView.OnItemSelectedListener mResolutionSelectedListener =
            new AdapterView.OnItemSelectedListener() {
                @Override
                public void onItemSelected(AdapterView<?> parent,
                        View view, int position, long id) {
                    if (mVideoSizeIds.get(position) != mCurrentVideoSizeId) {
                        mCurrentVideoSizeId = mVideoSizeIds.get(position);
                        mCurrentVideoSizeName = mVideoSizeNames.get(position);
                        if (VERBOSE) {
                            Log.v(TAG, ""onItemSelected: mCurrentVideoSizeId = "" +
                                    mCurrentVideoSizeId + "" "" + mCurrentVideoSizeName);
                        }
                        mNextPreviewSize = matchPreviewRecordSize();
                        if (VERBOSE) {
                            Log.v(TAG, ""onItemSelected: setting preview size ""
                                    + mNextPreviewSize.width + ""x"" + mNextPreviewSize.height);
                        }

                        startPreview();
                        if (VERBOSE) {
                            Log.v(TAG, ""onItemSelected: started new preview"");
                        }
                    }
                }

                @Override
                public void onNothingSelected(AdapterView<?> parent) {
                    // Intentionally left blank
                }

            };


    private void setUpCaptureButton() {
        captureButton.setOnClickListener (
                new View.OnClickListener() {
                    @Override
                    public void onClick(View V) {
                        if ((!isRecording) && (!isPlayingBack)) {
                            if (prepareVideoRecorder()) {
                                mMediaRecorder.start();
                                if (VERBOSE) {
                                    Log.v(TAG, ""onClick: started mMediaRecorder"");
                                }
                                isRecording = true;
                                captureButton.setEnabled(false);
                                mStatusLabel.setText(getResources()
                                        .getString(R.string.status_recording));
                            } else {
                                releaseMediaRecorder();
                                Log.e(TAG, ""media recorder cannot be set up"");
                                failTest(""Unable to set up media recorder."");
                            }
                            Handler h = new Handler();
                            Runnable mDelayedPreview = new Runnable() {
                                @Override
                                public void run() {
                                    mMediaRecorder.stop();
                                    releaseMediaRecorder();

                                    mPlaybackView.setVideoPath(outputVideoFile.getPath());
                                    mPlaybackView.start();
                                    isRecording = false;
                                    isPlayingBack = true;
                                    mStatusLabel.setText(getResources()
                                            .getString(R.string.status_playback));

                                    int resIdx = mResolutionSpinner.getSelectedItemPosition();
                                    CameraCombination combination = new CameraCombination(
                                            mCurrentCameraId, resIdx,
                                            mVideoSizeNames.get(resIdx));

                                    mUntestedCombinations.remove(combination);
                                    mTestedCombinations.add(combination);

                                    if (mUntestedCombinations.isEmpty() &&
                                            mUntestedCameras.isEmpty()) {
                                        mPassButton.setEnabled(true);
                                        if (VERBOSE) {
                                            Log.v(TAG, ""run: test success"");
                                        }
                                    }
                                }
                            };
                            h.postDelayed(mDelayedPreview, VIDEO_LENGTH);
                        }

                    }
                }
        );
    }

    private class VideoSizeNamePair {
        private int sizeId;
        private String sizeName;

        public VideoSizeNamePair(int id, String name) {
            sizeId = id;
            sizeName = name;
        }

        public int getSizeId() {
            return sizeId;
        }

        public String getSizeName() {
            return sizeName;
        }
    }

    private ArrayList<VideoSizeNamePair> getVideoSizeNamePairs(int cameraId) {
        int[] qualityArray = {
                CamcorderProfile.QUALITY_LOW,
                CamcorderProfile.QUALITY_HIGH,
                CamcorderProfile.QUALITY_QCIF,  // 176x144
                CamcorderProfile.QUALITY_QVGA,  // 320x240
                CamcorderProfile.QUALITY_CIF,   // 352x288
                CamcorderProfile.QUALITY_480P,  // 720x480
                CamcorderProfile.QUALITY_720P,  // 1280x720
                CamcorderProfile.QUALITY_1080P, // 1920x1080 or 1920x1088
                CamcorderProfile.QUALITY_2160P
        };

        final Camera.Size skip = mCamera.new Size(-1, -1);
        Camera.Size[] videoSizeArray = {
                skip,
                skip,
                mCamera.new Size(176, 144),
                mCamera.new Size(320, 240),
                mCamera.new Size(352, 288),
                mCamera.new Size(720, 480),
                mCamera.new Size(1280, 720),
                skip,
                skip
        };

        String[] nameArray = {
                ""LOW"",
                ""HIGH"",
                ""QCIF"",
                ""QVGA"",
                ""CIF"",
                ""480P"",
                ""720P"",
                ""1080P"",
                ""2160P""
        };

        ArrayList<VideoSizeNamePair> availableSizes =
                new ArrayList<VideoSizeNamePair> ();

        Camera.Parameters p = mCamera.getParameters();
        List<Camera.Size> supportedVideoSizes = p.getSupportedVideoSizes();
        for (int i = 0; i < qualityArray.length; i++) {
            if (mIsExternalCamera) {
                Camera.Size videoSz = videoSizeArray[i];
                if (videoSz.equals(skip)) {
                    continue;
                }
                if (supportedVideoSizes.contains(videoSz)) {
                    VideoSizeNamePair pair = new VideoSizeNamePair(qualityArray[i], nameArray[i]);
                    availableSizes.add(pair);
                }
            } else {
                if (CamcorderProfile.hasProfile(cameraId, qualityArray[i])) {
                    VideoSizeNamePair pair = new VideoSizeNamePair(qualityArray[i], nameArray[i]);
                    availableSizes.add(pair);
                }
            }
        }
        return availableSizes;
    }

    static class ResolutionQuality {
        private int videoSizeId;
        private int width;
        private int height;

        public ResolutionQuality() {
            // intentionally left blank
        }
        public ResolutionQuality(int newSizeId, int newWidth, int newHeight) {
            videoSizeId = newSizeId;
            width = newWidth;
            height = newHeight;
        }
    }

    private Size findRecordSize(int cameraId) {
        int[] possibleQuality = {
                CamcorderProfile.QUALITY_LOW,
                CamcorderProfile.QUALITY_HIGH,
                CamcorderProfile.QUALITY_QCIF,
                CamcorderProfile.QUALITY_QVGA,
                CamcorderProfile.QUALITY_CIF,
                CamcorderProfile.QUALITY_480P,
                CamcorderProfile.QUALITY_720P,
                CamcorderProfile.QUALITY_1080P,
                CamcorderProfile.QUALITY_2160P
        };

        final Camera.Size skip = mCamera.new Size(-1, -1);
        Camera.Size[] videoSizeArray = {
                skip,
                skip,
                mCamera.new Size(176, 144),
                mCamera.new Size(320, 240),
                mCamera.new Size(352, 288),
                mCamera.new Size(720, 480),
                mCamera.new Size(1280, 720),
                skip,
                skip
        };

        ArrayList<ResolutionQuality> qualityList = new ArrayList<ResolutionQuality>();
        Camera.Parameters p = mCamera.getParameters();
        List<Camera.Size> supportedVideoSizes = p.getSupportedVideoSizes();
        for (int i = 0; i < possibleQuality.length; i++) {
            if (mIsExternalCamera) {
                Camera.Size videoSz = videoSizeArray[i];
                if (videoSz.equals(skip)) {
                    continue;
                }
                if (supportedVideoSizes.contains(videoSz)) {
                    qualityList.add(new ResolutionQuality(possibleQuality[i],
                            videoSz.width, videoSz.height));
                }
            } else {
                if (CamcorderProfile.hasProfile(cameraId, possibleQuality[i])) {
                    CamcorderProfile profile = CamcorderProfile.get(cameraId, possibleQuality[i]);
                    qualityList.add(new ResolutionQuality(possibleQuality[i],
                            profile.videoFrameWidth, profile.videoFrameHeight));
                }
            }
        }

        Size recordSize = null;
        for (int i = 0; i < qualityList.size(); i++) {
            if (mCurrentVideoSizeId == qualityList.get(i).videoSizeId) {
                recordSize = mCamera.new Size(qualityList.get(i).width,
                        qualityList.get(i).height);
                break;
            }
        }

        if (recordSize == null) {
            Log.e(TAG, ""findRecordSize: did not find a match"");
            failTest(""Cannot find video size"");
        }
        return recordSize;
    }

    // Match preview size with current recording size mCurrentVideoSizeId
    private Size matchPreviewRecordSize() {
        Size recordSize = findRecordSize(mCurrentCameraId);

        Size matchedSize = null;
        // First try to find exact match in size
        for (int i = 0; i < mPreviewSizes.size(); i++) {
            if (mPreviewSizes.get(i).equals(recordSize)) {
                matchedSize = mCamera.new Size(recordSize.width, recordSize.height);
                break;
            }
        }
        // Second try to find same ratio in size
        if (matchedSize == null) {
            for (int i = mPreviewSizes.size() - 1; i >= 0; i--) {
                if (mPreviewSizes.get(i).width * recordSize.height ==
                        mPreviewSizes.get(i).height * recordSize.width) {
                    matchedSize = mCamera.new Size(mPreviewSizes.get(i).width,
                            mPreviewSizes.get(i).height);
                    break;
                }
            }
        }
        //Third try to find one with similar if not the same apect ratio
        if (matchedSize == null) {
            for (int i = mPreviewSizes.size() - 1; i >= 0; i--) {
                if (Math.abs((float)mPreviewSizes.get(i).width * recordSize.height /
                        mPreviewSizes.get(i).height / recordSize.width - 1) < 0.12) {
                    matchedSize = mCamera.new Size(mPreviewSizes.get(i).width,
                            mPreviewSizes.get(i).height);
                    break;
                }
            }
        }
        // Last resort, just use the first preview size
        if (matchedSize == null) {
            matchedSize = mCamera.new Size(mPreviewSizes.get(0).width,
                    mPreviewSizes.get(0).height);
        }

        if (VERBOSE) {
            Log.v(TAG, ""matchPreviewRecordSize "" + matchedSize.width + ""x"" + matchedSize.height);
        }

        return matchedSize;
    }

    private void setUpCamera(int id) {
        shutdownCamera();

        mCurrentCameraId = id;
        mIsExternalCamera = isExternalCamera(id);
        try {
            mCamera = Camera.open(id);
        }
        catch (Exception e) {
            Log.e(TAG, ""camera is not available"", e);
            failTest(""camera not available"" + e.getMessage());
            return;
        }

        Camera.Parameters p = mCamera.getParameters();
        if (VERBOSE) {
            Log.v(TAG, ""setUpCamera: setUpCamera got camera parameters"");
        }

        // Get preview resolutions
        List<Size> unsortedSizes = p.getSupportedPreviewSizes();

        class SizeCompare implements Comparator<Size> {
            @Override
            public int compare(Size lhs, Size rhs) {
                if (lhs.width < rhs.width) return -1;
                if (lhs.width > rhs.width) return 1;
                if (lhs.height < rhs.height) return -1;
                if (lhs.height > rhs.height) return 1;
                return 0;
            }
        };

        if (mIsExternalCamera) {
            setVideoFrameRate(p.getPreviewFrameRate());
        }

        SizeCompare s = new SizeCompare();
        TreeSet<Size> sortedResolutions = new TreeSet<Size>(s);
        sortedResolutions.addAll(unsortedSizes);

        mPreviewSizes = new ArrayList<Size>(sortedResolutions);

        ArrayList<VideoSizeNamePair> availableVideoSizes = getVideoSizeNamePairs(id);
        String[] availableVideoSizeNames = new String[availableVideoSizes.size()];
        mVideoSizeIds = new ArrayList<Integer>();
        mVideoSizeNames = new ArrayList<String>();
        for (int i = 0; i < availableVideoSizes.size(); i++) {
            availableVideoSizeNames[i] = availableVideoSizes.get(i).getSizeName();
            mVideoSizeIds.add(availableVideoSizes.get(i).getSizeId());
            mVideoSizeNames.add(availableVideoSizeNames[i]);
        }

        mResolutionSpinner.setAdapter(
            new ArrayAdapter<String>(
                this, R.layout.camera_list_item, availableVideoSizeNames));

        // Update untested
        mUntestedCameras.remove(""All combinations for Camera "" + id + ""\n"");

        for (int videoSizeIdIndex = 0;
                videoSizeIdIndex < mVideoSizeIds.size(); videoSizeIdIndex++) {
            CameraCombination combination = new CameraCombination(
                id, videoSizeIdIndex, mVideoSizeNames.get(videoSizeIdIndex));

            if (!mTestedCombinations.contains(combination)) {
                mUntestedCombinations.add(combination);
            }
        }

        // Set initial values
        mCurrentVideoSizeId = mVideoSizeIds.get(0);
        mCurrentVideoSizeName = mVideoSizeNames.get(0);
        mNextPreviewSize = matchPreviewRecordSize();
        mResolutionSpinner.setSelection(0);

        // Set up correct display orientation
        CameraInfo info = new CameraInfo();
        Camera.getCameraInfo(id, info);
        int rotation = getWindowManager().getDefaultDisplay().getRotation();
        int degrees = 0;
        switch (rotation) {
            case Surface.ROTATION_0: degrees = 0; break;
            case Surface.ROTATION_90: degrees = 90; break;
            case Surface.ROTATION_180: degrees = 180; break;
            case Surface.ROTATION_270: degrees = 270; break;
        }

        if (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {
            mVideoRotation = (info.orientation + degrees) % 360;
            mPreviewRotation = (360 - mVideoRotation) % 360;  // compensate the mirror
        } else {  // back-facing
            mVideoRotation = (info.orientation - degrees + 360) % 360;
            mPreviewRotation = mVideoRotation;
        }
        if (mPreviewRotation != 0 && mPreviewRotation != 180) {
            Log.w(TAG,
                ""Display orientation correction is not 0 or 180, as expected!"");
        }

        mCamera.setDisplayOrientation(mPreviewRotation);

        // Start up preview if display is ready
        if (mPreviewTexture != null) {
            startPreview();
        }
    }

    private void shutdownCamera() {
        if (mCamera != null) {
            mCamera.setPreviewCallback(null);
            mCamera.stopPreview();
            mCamera.release();
            mCamera = null;
        }
    }

    /**
     * starts capturing and drawing frames on screen
     */
    private void startPreview() {

        mCamera.stopPreview();

        Matrix transform = new Matrix();
        float widthRatio = mNextPreviewSize.width / (float)mPreviewTexWidth;
        float heightRatio = mNextPreviewSize.height / (float)mPreviewTexHeight;
        if (VERBOSE) {
            Log.v(TAG, ""startPreview: widthRatio="" + widthRatio + "" "" + ""heightRatio="" +
                    heightRatio);
        }

        if (heightRatio < widthRatio) {
            transform.setScale(1, heightRatio / widthRatio);
            transform.postTranslate(0,
                    mPreviewTexHeight * (1 - heightRatio / widthRatio) / 2);
            if (VERBOSE) {
                Log.v(TAG, ""startPreview: shrink vertical by "" + heightRatio / widthRatio);
            }
        } else {
            transform.setScale(widthRatio / heightRatio, 1);
            transform.postTranslate(mPreviewTexWidth * (1 - widthRatio / heightRatio) / 2, 0);
            if (VERBOSE) {
                Log.v(TAG, ""startPreview: shrink horizontal by "" + widthRatio / heightRatio);
            }
        }

        mPreviewView.setTransform(transform);

        mPreviewSize = mNextPreviewSize;

        Camera.Parameters p = mCamera.getParameters();
        p.setPreviewSize(mPreviewSize.width, mPreviewSize.height);
        mCamera.setParameters(p);

        try {
            mCamera.setPreviewTexture(mPreviewTexture);
            if (mPreviewTexture == null) {
                Log.e(TAG, ""preview texture is null."");
            }
            if (VERBOSE) {
                Log.v(TAG, ""startPreview: set preview texture in startPreview"");
            }
            mCamera.startPreview();
            if (VERBOSE) {
                Log.v(TAG, ""startPreview: started preview in startPreview"");
            }
        } catch (IOException ioe) {
            Log.e(TAG, ""Unable to start up preview"", ioe);
            // Show a dialog box to tell user test failed
            failTest(""Unable to start preview."");
        }
    }

    private void failTest(String failMessage) {
        DialogInterface.OnClickListener dialogClickListener =
                new DialogInterface.OnClickListener() {
                    @Override
                    public void onClick(DialogInterface dialog, int which) {
                        switch (which) {
                            case DialogInterface.BUTTON_POSITIVE:
                                setTestResultAndFinish(/* passed */false);
                                break;
                            case DialogInterface.BUTTON_NEGATIVE:
                                break;
                        }
                    }
                };

        AlertDialog.Builder builder = new AlertDialog.Builder(CameraVideoActivity.this);
        builder.setMessage(getString(R.string.dialog_fail_test) + "". "" + failMessage)
                .setPositiveButton(R.string.fail_quit, dialogClickListener)
                .setNegativeButton(R.string.cancel, dialogClickListener)
                .show();
    }

    private boolean isExternalCamera(int cameraId) {
        try {
            return CameraUtils.isExternal(this, cameraId);
        } catch (Exception e) {
            Toast.makeText(this, ""Could not access camera "" + cameraId +
                    "": "" + e.getMessage(), Toast.LENGTH_LONG).show();
        }
        return false;
    }

}"	""	""	"12 resolution resolution"	""	""	""	""	""	""	""	""	""	""
