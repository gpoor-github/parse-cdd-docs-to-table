"Section"	"section_id"	"req_id"	"full_key"	"key_as_number"	"requirement"	"Test Availability"	"search_roots"	"search_terms"	"manual_search_terms"	"not_search_terms"	"not_files"	"max_matches"	"class_defs"	"methods"	"modules"	"protected"	"class_def"	"method"	"module"	"file_name"	"matched_files"	"methods_string"	"urls"	"method_text"	"matched_terms"	"qualified_method"	"Annotation?"	"New Req for S?"	"New CTS for S?"	"Comment(internal) e.g. why a test is not possible"	"CTS Bug Id"	"CDD Bug Id"	"Area"	"Shortened"	"Test Level"
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.StaticMetadataTest"	"testHwSupportedLevel"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/StaticMetadataTest.java"	""	"public void testHwSupportedLevel() throws Exception {
        Key<StreamConfigurationMap> key =
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP;
        final float SIZE_ERROR_MARGIN = 0.03f;
        for (String id : mAllCameraIds) {
            initStaticMetadata(id);
            StreamConfigurationMap configs = mStaticInfo.getValueFromKeyNonNull(key);
            Rect activeRect = mStaticInfo.getActiveArraySizeChecked();
            Size sensorSize = new Size(activeRect.width(), activeRect.height());
            List<Integer> availableCaps = mStaticInfo.getAvailableCapabilitiesChecked();

            mCollector.expectTrue(""All devices must contains BACKWARD_COMPATIBLE capability or "" +
                    ""DEPTH_OUTPUT capabillity"" ,
                    availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE) ||
                    availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_DEPTH_OUTPUT) );

            if (mStaticInfo.isHardwareLevelAtLeast(
                    CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_3)) {
                mCollector.expectTrue(""Level 3 device must contain YUV_REPROCESSING capability"",
                        availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING));
                mCollector.expectTrue(""Level 3 device must contain RAW capability"",
                        availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_RAW));
            }

            if (mStaticInfo.isHardwareLevelAtLeastFull()) {
                // Capability advertisement must be right.
                mCollector.expectTrue(""Full device must contain MANUAL_SENSOR capability"",
                        availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR));
                mCollector.expectTrue(""Full device must contain MANUAL_POST_PROCESSING capability"",
                        availableCaps.contains(
                                REQUEST_AVAILABLE_CAPABILITIES_MANUAL_POST_PROCESSING));
                mCollector.expectTrue(""Full device must contain BURST_CAPTURE capability"",
                        availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_BURST_CAPTURE));

                // Need support per frame control
                mCollector.expectTrue(""Full device must support per frame control"",
                        mStaticInfo.isPerFrameControlSupported());
            }

            if (mStaticInfo.isHardwareLevelLegacy()) {
                mCollector.expectTrue(""Legacy devices must contain BACKWARD_COMPATIBLE capability"",
                        availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE));
            }

            if (availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
                mCollector.expectTrue(""MANUAL_SENSOR capability always requires "" +
                        ""READ_SENSOR_SETTINGS capability as well"",
                        availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS));
            }

            if (mStaticInfo.isColorOutputSupported()) {
                // Max jpeg resolution must be very close to  sensor resolution
                Size[] jpegSizes = mStaticInfo.getJpegOutputSizesChecked();
                Size maxJpegSize = CameraTestUtils.getMaxSize(jpegSizes);
                float croppedWidth = (float)sensorSize.getWidth();
                float croppedHeight = (float)sensorSize.getHeight();
                float sensorAspectRatio = (float)sensorSize.getWidth() / (float)sensorSize.getHeight();
                float maxJpegAspectRatio = (float)maxJpegSize.getWidth() / (float)maxJpegSize.getHeight();
                if (sensorAspectRatio < maxJpegAspectRatio) {
                    croppedHeight = (float)sensorSize.getWidth() / maxJpegAspectRatio;
                } else if (sensorAspectRatio > maxJpegAspectRatio) {
                    croppedWidth = (float)sensorSize.getHeight() * maxJpegAspectRatio;
                }
                Size croppedSensorSize = new Size((int)croppedWidth, (int)croppedHeight);
                mCollector.expectSizesAreSimilar(
                    ""Active array size or cropped active array size and max JPEG size should be similar"",
                    croppedSensorSize, maxJpegSize, SIZE_ERROR_MARGIN);
            }

            // TODO: test all the keys mandatory for all capability devices.
        }
    }

    /**
     * Test max number of output stream reported by device
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.StaticMetadataTest"	"testCapabilities"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/StaticMetadataTest.java"	""	"public void testCapabilities() throws Exception {
        for (String id : mAllCameraIds) {
            initStaticMetadata(id);
            List<Integer> availableCaps = mStaticInfo.getAvailableCapabilitiesChecked();

            for (Integer capability = REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE;
                    capability <= StaticMetadata.LAST_CAPABILITY_ENUM; capability++) {
                boolean isCapabilityAvailable = availableCaps.contains(capability);
                validateCapability(capability, isCapabilityAvailable);
            }
            // Note: Static metadata for capabilities is tested in ExtendedCameraCharacteristicsTest
        }
    }

    /**
     * Check if request keys' presence match expectation.
     *
     * @param capabilityName The name string of capability being tested. Used for output messages.
     * @param requestKeys The capture request keys to be checked
     * @param expectedPresence Expected presence of {@code requestKeys}. {@code true} for expecting
     *        all keys are available. Otherwise {@code false}
     * @return {@code true} if request keys' presence match expectation. Otherwise {@code false}
     */
    private boolean validateRequestKeysPresence(String capabilityName,
            Collection<CaptureRequest.Key<?>> requestKeys, boolean expectedPresence) {
        boolean actualPresence = mStaticInfo.areRequestKeysAvailable(requestKeys);
        if (expectedPresence != actualPresence) {
            if (expectedPresence) {
                for (CaptureRequest.Key<?> key : requestKeys) {
                    if (!mStaticInfo.areKeysAvailable(key)) {
                        mCollector.addMessage(String.format(
                                ""Camera %s list capability %s but doesn't contain request key %s"",
                                mCameraId, capabilityName, key.getName()));
                    }
                }
            } else {
                Log.w(TAG, String.format(
                        ""Camera %s doesn't list capability %s but contain all required keys"",
                        mCameraId, capabilityName));
            }
            return false;
        }
        return true;
    }

    /**
     * Check if result keys' presence match expectation.
     *
     * @param capabilityName The name string of capability being tested. Used for output messages.
     * @param resultKeys The capture result keys to be checked
     * @param expectedPresence Expected presence of {@code resultKeys}. {@code true} for expecting
     *        all keys are available. Otherwise {@code false}
     * @return {@code true} if result keys' presence match expectation. Otherwise {@code false}
     */
    private boolean validateResultKeysPresence(String capabilityName,
            Collection<CaptureResult.Key<?>> resultKeys, boolean expectedPresence) {
        boolean actualPresence = mStaticInfo.areResultKeysAvailable(resultKeys);
        if (expectedPresence != actualPresence) {
            if (expectedPresence) {
                for (CaptureResult.Key<?> key : resultKeys) {
                    if (!mStaticInfo.areKeysAvailable(key)) {
                        mCollector.addMessage(String.format(
                                ""Camera %s list capability %s but doesn't contain result key %s"",
                                mCameraId, capabilityName, key.getName()));
                    }
                }
            } else {
                Log.w(TAG, String.format(
                        ""Camera %s doesn't list capability %s but contain all required keys"",
                        mCameraId, capabilityName));
            }
            return false;
        }
        return true;
    }

    /**
     * Check if characteristics keys' presence match expectation.
     *
     * @param capabilityName The name string of capability being tested. Used for output messages.
     * @param characteristicsKeys The characteristics keys to be checked
     * @param expectedPresence Expected presence of {@code characteristicsKeys}. {@code true} for
     *        expecting all keys are available. Otherwise {@code false}
     * @return {@code true} if characteristics keys' presence match expectation.
     *         Otherwise {@code false}
     */
    private boolean validateCharacteristicsKeysPresence(String capabilityName,
            Collection<CameraCharacteristics.Key<?>> characteristicsKeys,
            boolean expectedPresence) {
        boolean actualPresence = mStaticInfo.areCharacteristicsKeysAvailable(characteristicsKeys);
        if (expectedPresence != actualPresence) {
            if (expectedPresence) {
                for (CameraCharacteristics.Key<?> key : characteristicsKeys) {
                    if (!mStaticInfo.areKeysAvailable(key)) {
                        mCollector.addMessage(String.format(
                                ""Camera %s list capability %s but doesn't contain"" +
                                ""characteristics key %s"",
                                mCameraId, capabilityName, key.getName()));
                    }
                }
            } else {
                Log.w(TAG, String.format(
                        ""Camera %s doesn't list capability %s but contain all required keys"",
                        mCameraId, capabilityName));
            }
            return false;
        }
        return true;
    }

    private void validateCapability(Integer capability, boolean isCapabilityAvailable) {
        List<CaptureRequest.Key<?>> requestKeys = new ArrayList<>();
        Set<CaptureResult.Key<?>> resultKeys = new HashSet<>();
        // Capability requirements other than key presences
        List<Pair<String, Boolean>> additionalRequirements = new ArrayList<>();

        /* For available capabilities, only check request keys in this test
           Characteristics keys are tested in ExtendedCameraCharacteristicsTest
           Result keys are tested in CaptureResultTest */
        String capabilityName;
        switch (capability) {
            case REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE:
                capabilityName = ""REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE"";
                requestKeys.add(CaptureRequest.CONTROL_AE_ANTIBANDING_MODE);
                requestKeys.add(CaptureRequest.CONTROL_AE_EXPOSURE_COMPENSATION);
                requestKeys.add(CaptureRequest.CONTROL_AE_MODE);
                requestKeys.add(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE);
                requestKeys.add(CaptureRequest.CONTROL_AF_MODE);
                requestKeys.add(CaptureRequest.CONTROL_AF_TRIGGER);
                requestKeys.add(CaptureRequest.CONTROL_AWB_MODE);
                requestKeys.add(CaptureRequest.CONTROL_CAPTURE_INTENT);
                requestKeys.add(CaptureRequest.CONTROL_EFFECT_MODE);
                requestKeys.add(CaptureRequest.CONTROL_MODE);
                requestKeys.add(CaptureRequest.CONTROL_SCENE_MODE);
                requestKeys.add(CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE);
                requestKeys.add(CaptureRequest.CONTROL_ZOOM_RATIO);
                requestKeys.add(CaptureRequest.FLASH_MODE);
                requestKeys.add(CaptureRequest.JPEG_GPS_LOCATION);
                requestKeys.add(CaptureRequest.JPEG_ORIENTATION);
                requestKeys.add(CaptureRequest.JPEG_QUALITY);
                requestKeys.add(CaptureRequest.JPEG_THUMBNAIL_QUALITY);
                requestKeys.add(CaptureRequest.JPEG_THUMBNAIL_SIZE);
                requestKeys.add(CaptureRequest.SCALER_CROP_REGION);
                requestKeys.add(CaptureRequest.STATISTICS_FACE_DETECT_MODE);
                if (mStaticInfo.getAeMaxRegionsChecked() > 0) {
                    requestKeys.add(CaptureRequest.CONTROL_AE_REGIONS);
                } else {
                    mCollector.expectTrue(
                            ""CONTROL_AE_REGIONS is available but aeMaxRegion is 0"",
                            !mStaticInfo.areKeysAvailable(CaptureRequest.CONTROL_AE_REGIONS));
                }
                if (mStaticInfo.getAwbMaxRegionsChecked() > 0) {
                    requestKeys.add(CaptureRequest.CONTROL_AWB_REGIONS);
                } else {
                    mCollector.expectTrue(
                            ""CONTROL_AWB_REGIONS is available but awbMaxRegion is 0"",
                            !mStaticInfo.areKeysAvailable(CaptureRequest.CONTROL_AWB_REGIONS));
                }
                if (mStaticInfo.getAfMaxRegionsChecked() > 0) {
                    requestKeys.add(CaptureRequest.CONTROL_AF_REGIONS);
                } else {
                    mCollector.expectTrue(
                            ""CONTROL_AF_REGIONS is available but afMaxRegion is 0"",
                            !mStaticInfo.areKeysAvailable(CaptureRequest.CONTROL_AF_REGIONS));
                }
                break;
            case REQUEST_AVAILABLE_CAPABILITIES_MANUAL_POST_PROCESSING:
                capabilityName = ""REQUEST_AVAILABLE_CAPABILITIES_MANUAL_POST_PROCESSING"";
                requestKeys.add(CaptureRequest.TONEMAP_MODE);
                requestKeys.add(CaptureRequest.COLOR_CORRECTION_GAINS);
                requestKeys.add(CaptureRequest.COLOR_CORRECTION_TRANSFORM);
                requestKeys.add(CaptureRequest.SHADING_MODE);
                requestKeys.add(CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE);
                requestKeys.add(CaptureRequest.TONEMAP_CURVE);
                requestKeys.add(CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE);
                requestKeys.add(CaptureRequest.CONTROL_AWB_LOCK);

                // Legacy mode always doesn't support these requirements
                Boolean contrastCurveModeSupported = false;
                Boolean gammaAndPresetModeSupported = false;
                Boolean offColorAberrationModeSupported = false;
                if (mStaticInfo.isHardwareLevelAtLeastLimited() && mStaticInfo.isColorOutputSupported()) {
                    int[] tonemapModes = mStaticInfo.getAvailableToneMapModesChecked();
                    List<Integer> modeList = (tonemapModes.length == 0) ?
                            new ArrayList<Integer>() :
                            Arrays.asList(CameraTestUtils.toObject(tonemapModes));
                    contrastCurveModeSupported =
                            modeList.contains(CameraMetadata.TONEMAP_MODE_CONTRAST_CURVE);
                    gammaAndPresetModeSupported =
                            modeList.contains(CameraMetadata.TONEMAP_MODE_GAMMA_VALUE) &&
                            modeList.contains(CameraMetadata.TONEMAP_MODE_PRESET_CURVE);

                    int[] colorAberrationModes =
                            mStaticInfo.getAvailableColorAberrationModesChecked();
                    modeList = (colorAberrationModes.length == 0) ?
                            new ArrayList<Integer>() :
                            Arrays.asList(CameraTestUtils.toObject(colorAberrationModes));
                    offColorAberrationModeSupported =
                            modeList.contains(CameraMetadata.COLOR_CORRECTION_ABERRATION_MODE_OFF);
                }
                Boolean tonemapModeQualified =
                        contrastCurveModeSupported || gammaAndPresetModeSupported;
                additionalRequirements.add(new Pair<String, Boolean>(
                        ""Tonemap mode must include {CONTRAST_CURVE} and/or "" +
                        ""{GAMMA_VALUE, PRESET_CURVE}"",
                        tonemapModeQualified));
                additionalRequirements.add(new Pair<String, Boolean>(
                        ""Color aberration mode must include OFF"", offColorAberrationModeSupported));
                additionalRequirements.add(new Pair<String, Boolean>(
                        ""Must support AWB lock"", mStaticInfo.isAwbLockSupported()));
                break;
            case REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR:
                capabilityName = ""REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR"";
                requestKeys.add(CaptureRequest.CONTROL_AE_LOCK);
                requestKeys.add(CaptureRequest.SENSOR_FRAME_DURATION);
                requestKeys.add(CaptureRequest.SENSOR_EXPOSURE_TIME);
                requestKeys.add(CaptureRequest.SENSOR_SENSITIVITY);
                if (mStaticInfo.hasFocuser()) {
                    requestKeys.add(CaptureRequest.LENS_APERTURE);
                    requestKeys.add(CaptureRequest.LENS_FOCUS_DISTANCE);
                    requestKeys.add(CaptureRequest.LENS_FILTER_DENSITY);
                    requestKeys.add(CaptureRequest.LENS_OPTICAL_STABILIZATION_MODE);
                }
                requestKeys.add(CaptureRequest.BLACK_LEVEL_LOCK);
                additionalRequirements.add(new Pair<String, Boolean>(
                        ""Must support AE lock"", mStaticInfo.isAeLockSupported()));
                break;
            case REQUEST_AVAILABLE_CAPABILITIES_RAW:
                // RAW_CAPABILITY needs to check for not just capture request keys
                validateRawCapability(isCapabilityAvailable);
                return;
            case REQUEST_AVAILABLE_CAPABILITIES_BURST_CAPTURE:
                // Tested in ExtendedCameraCharacteristicsTest
                return;
            case REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS:
                capabilityName = ""REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS"";
                resultKeys.add(CaptureResult.SENSOR_FRAME_DURATION);
                resultKeys.add(CaptureResult.SENSOR_EXPOSURE_TIME);
                resultKeys.add(CaptureResult.SENSOR_SENSITIVITY);
                if (mStaticInfo.hasFocuser()) {
                    resultKeys.add(CaptureResult.LENS_APERTURE);
                    resultKeys.add(CaptureResult.LENS_FOCUS_DISTANCE);
                    resultKeys.add(CaptureResult.LENS_FILTER_DENSITY);
                }
                break;

            case REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING:
            case REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING:
            case REQUEST_AVAILABLE_CAPABILITIES_REMOSAIC_REPROCESSING:
                // Tested in ExtendedCameraCharacteristicsTest
                return;
            case REQUEST_AVAILABLE_CAPABILITIES_DEPTH_OUTPUT:
                // Tested in ExtendedCameracharacteristicsTest
                return;
            case REQUEST_AVAILABLE_CAPABILITIES_CONSTRAINED_HIGH_SPEED_VIDEO:
            case REQUEST_AVAILABLE_CAPABILITIES_MOTION_TRACKING:
            case REQUEST_AVAILABLE_CAPABILITIES_LOGICAL_MULTI_CAMERA:
            case REQUEST_AVAILABLE_CAPABILITIES_MONOCHROME:
                // Tested in ExtendedCameraCharacteristicsTest
                return;
            case REQUEST_AVAILABLE_CAPABILITIES_SECURE_IMAGE_DATA:
                if (!isCapabilityAvailable) {
                    mCollector.expectTrue(
                        ""SCALER_DEFAULT_SECURE_IMAGE_SIZE must not present if the device"" +
                                ""does not support SECURE_IMAGE_DATA capability"",
                        !mStaticInfo.areKeysAvailable(
                                CameraCharacteristics.SCALER_DEFAULT_SECURE_IMAGE_SIZE));
                }
                return;
            case REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR:
                resultKeys.add(CaptureResult.SENSOR_RAW_BINNING_FACTOR_USED);
                resultKeys.add(CaptureResult.SENSOR_PIXEL_MODE);
                requestKeys.add(CaptureRequest.SENSOR_PIXEL_MODE);
                additionalRequirements.add(new Pair<String, Boolean>(
                        ""Must support maximum resolution keys"",
                        mStaticInfo.areMaximumResolutionKeysSupported()));
                return;
            default:
                capabilityName = ""Unknown"";
                assertTrue(String.format(""Unknown capability set: %d"", capability),
                           !isCapabilityAvailable);
                return;
        }

        // Check additional requirements and exit early if possible
        if (!isCapabilityAvailable) {
            for (Pair<String, Boolean> p : additionalRequirements) {
                String requirement = p.first;
                Boolean meetRequirement = p.second;
                // No further check is needed if we've found why capability cannot be advertised
                if (!meetRequirement) {
                    Log.v(TAG, String.format(
                            ""Camera %s doesn't list capability %s because of requirement: %s"",
                            mCameraId, capabilityName, requirement));
                    return;
                }
            }
        }

        boolean matchExpectation = true;
        if (!requestKeys.isEmpty()) {
            matchExpectation &= validateRequestKeysPresence(
                    capabilityName, requestKeys, isCapabilityAvailable);
        }
        if(!resultKeys.isEmpty()) {
            matchExpectation &= validateResultKeysPresence(
                    capabilityName, resultKeys, isCapabilityAvailable);
        }

        // Check additional requirements
        for (Pair<String, Boolean> p : additionalRequirements) {
            String requirement = p.first;
            Boolean meetRequirement = p.second;
            if (isCapabilityAvailable && !meetRequirement) {
                mCollector.addMessage(String.format(
                        ""Camera %s list capability %s but does not meet the requirement: %s"",
                        mCameraId, capabilityName, requirement));
            }
        }

        // In case of isCapabilityAvailable == true, error has been filed in
        // validateRequest/ResultKeysPresence
        if (!matchExpectation && !isCapabilityAvailable) {
            mCollector.addMessage(String.format(
                    ""Camera %s doesn't list capability %s but meets all requirements"",
                    mCameraId, capabilityName));
        }
    }

    private void validateRawCapability(boolean isCapabilityAvailable) {
        String capabilityName = ""REQUEST_AVAILABLE_CAPABILITIES_RAW"";

        Set<CaptureRequest.Key<?>> requestKeys = new HashSet<>();
        requestKeys.add(CaptureRequest.HOT_PIXEL_MODE);
        requestKeys.add(CaptureRequest.STATISTICS_HOT_PIXEL_MAP_MODE);

        Set<CameraCharacteristics.Key<?>> characteristicsKeys = new HashSet<>();
        characteristicsKeys.add(HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES);
        characteristicsKeys.add(SENSOR_BLACK_LEVEL_PATTERN);
        characteristicsKeys.add(SENSOR_INFO_ACTIVE_ARRAY_SIZE);
        characteristicsKeys.add(SENSOR_INFO_COLOR_FILTER_ARRANGEMENT);
        characteristicsKeys.add(SENSOR_INFO_WHITE_LEVEL);
        characteristicsKeys.add(STATISTICS_INFO_AVAILABLE_HOT_PIXEL_MAP_MODES);
        if (!mStaticInfo.isMonochromeCamera()) {
            characteristicsKeys.add(SENSOR_CALIBRATION_TRANSFORM1);
            characteristicsKeys.add(SENSOR_COLOR_TRANSFORM1);
            characteristicsKeys.add(SENSOR_FORWARD_MATRIX1);
            characteristicsKeys.add(SENSOR_REFERENCE_ILLUMINANT1);
        }

        Set<CaptureResult.Key<?>> resultKeys = new HashSet<>();
        resultKeys.add(CaptureResult.SENSOR_NOISE_PROFILE);
        if (!mStaticInfo.isMonochromeCamera()) {
            resultKeys.add(CaptureResult.SENSOR_GREEN_SPLIT);
            resultKeys.add(CaptureResult.SENSOR_NEUTRAL_COLOR_POINT);
        }

        boolean rawOutputSupported = mStaticInfo.getRawOutputSizesChecked().length > 0;
        boolean requestKeysPresent = mStaticInfo.areRequestKeysAvailable(requestKeys);
        boolean characteristicsKeysPresent =
                mStaticInfo.areCharacteristicsKeysAvailable(characteristicsKeys);
        boolean resultKeysPresent = mStaticInfo.areResultKeysAvailable(resultKeys);
        boolean expectCapabilityPresent = rawOutputSupported && requestKeysPresent &&
                characteristicsKeysPresent && resultKeysPresent;

        if (isCapabilityAvailable != expectCapabilityPresent) {
            if (isCapabilityAvailable) {
                mCollector.expectTrue(
                        ""REQUEST_AVAILABLE_CAPABILITIES_RAW should support RAW_SENSOR output"",
                        rawOutputSupported);
                validateRequestKeysPresence(capabilityName, requestKeys, isCapabilityAvailable);
                validateResultKeysPresence(capabilityName, resultKeys, isCapabilityAvailable);
                validateCharacteristicsKeysPresence(capabilityName, characteristicsKeys,
                        isCapabilityAvailable);
            } else {
                mCollector.addMessage(String.format(
                        ""Camera %s doesn't list capability %s but contain all required keys"" +
                        "" and RAW format output"",
                        mCameraId, capabilityName));
            }
        }
    }

    /**
     * Test lens facing.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.helpers.StaticMetadat"	"getCharacteristics"	""	"/home/gpoor/cts-12-source/cts/tests/camera/utils/src/android/hardware/camera2/cts/helpers/StaticMetadata.java"	""	"public void test/*
 *.
 */

package android.hardware.camera2.cts.helpers;

import android.graphics.Rect;
import android.graphics.ImageFormat;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraCharacteristics.Key;
import android.hardware.camera2.CameraMetadata;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.cts.CameraTestUtils;
import android.hardware.camera2.params.StreamConfigurationMap;
import android.hardware.camera2.params.Capability;
import android.util.Range;
import android.util.Size;
import android.util.Log;
import android.util.Rational;

import junit.framework.Assert;

import java.lang.reflect.Array;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import static android.hardware.camera2.cts.helpers.AssertHelpers.*;
import static android.hardware.camera2.CameraCharacteristics.*;

/**
 * Helpers to get common static info out of the camera.
 *
 * <p>Avoid boiler plate by putting repetitive get/set patterns in this class.</p>
 *
 * <p>Attempt to be durable against the camera device having bad or missing metadata
 * by providing reasonable defaults and logging warnings when that happens.</p>
 */
public class StaticMetadata {

    private static final String TAG = ""StaticMetadata"";
    private static final int IGNORE_SIZE_CHECK = -1;

    private static final long SENSOR_INFO_EXPOSURE_TIME_RANGE_MIN_AT_MOST = 100000L; // 100us
    private static final long SENSOR_INFO_EXPOSURE_TIME_RANGE_MAX_AT_LEAST = 100000000; // 100ms
    private static final int SENSOR_INFO_SENSITIVITY_RANGE_MIN_AT_MOST = 100;
    private static final int SENSOR_INFO_SENSITIVITY_RANGE_MAX_AT_LEAST = 800;
    private static final int STATISTICS_INFO_MAX_FACE_COUNT_MIN_AT_LEAST = 4;
    private static final int TONEMAP_MAX_CURVE_POINTS_AT_LEAST = 64;
    private static final int CONTROL_AE_COMPENSATION_RANGE_DEFAULT_MIN = -2;
    private static final int CONTROL_AE_COMPENSATION_RANGE_DEFAULT_MAX = 2;
    private static final Rational CONTROL_AE_COMPENSATION_STEP_DEFAULT = new Rational(1, 2);
    private static final byte REQUEST_PIPELINE_MAX_DEPTH_MAX = 8;
    private static final int MAX_REPROCESS_MAX_CAPTURE_STALL = 4;

    // TODO: Consider making this work across any metadata object, not just camera characteristics
    private final CameraCharacteristics mCharacteristics;
    private final CheckLevel mLevel;
    private final CameraErrorCollector mCollector;

    // Last defined capability enum, for iterating over all of them
    public static final int LAST_CAPABILITY_ENUM =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_REMOSAIC_REPROCESSING;

    // Access via getAeModeName() to account for vendor extensions
    public static final String[] AE_MODE_NAMES = new String[] {
        ""AE_MODE_OFF"",
        ""AE_MODE_ON"",
        ""AE_MODE_ON_AUTO_FLASH"",
        ""AE_MODE_ON_ALWAYS_FLASH"",
        ""AE_MODE_ON_AUTO_FLASH_REDEYE""
    };

    // Access via getAfModeName() to account for vendor extensions
    public static final String[] AF_MODE_NAMES = new String[] {
        ""AF_MODE_OFF"",
        ""AF_MODE_AUTO"",
        ""AF_MODE_MACRO"",
        ""AF_MODE_CONTINUOUS_VIDEO"",
        ""AF_MODE_CONTINUOUS_PICTURE"",
        ""AF_MODE_EDOF""
    };

    // Index with android.control.aeState
    public static final String[] AE_STATE_NAMES = new String[] {
        ""AE_STATE_INACTIVE"",
        ""AE_STATE_SEARCHING"",
        ""AE_STATE_CONVERGED"",
        ""AE_STATE_LOCKED"",
        ""AE_STATE_FLASH_REQUIRED"",
        ""AE_STATE_PRECAPTURE""
    };

    // Index with android.control.afState
    public static final String[] AF_STATE_NAMES = new String[] {
        ""AF_STATE_INACTIVE"",
        ""AF_STATE_PASSIVE_SCAN"",
        ""AF_STATE_PASSIVE_FOCUSED"",
        ""AF_STATE_ACTIVE_SCAN"",
        ""AF_STATE_FOCUSED_LOCKED"",
        ""AF_STATE_NOT_FOCUSED_LOCKED"",
        ""AF_STATE_PASSIVE_UNFOCUSED""
    };

    // Index with android.control.aePrecaptureTrigger
    public static final String[] AE_TRIGGER_NAMES = new String[] {
        ""AE_TRIGGER_IDLE"",
        ""AE_TRIGGER_START"",
        ""AE_TRIGGER_CANCEL""
    };

    // Index with android.control.afTrigger
    public static final String[] AF_TRIGGER_NAMES = new String[] {
        ""AF_TRIGGER_IDLE"",
        ""AF_TRIGGER_START"",
        ""AF_TRIGGER_CANCEL""
    };

    public enum CheckLevel {
        /** Only log warnings for metadata check failures. Execution continues. */
        WARN,
        /**
         * Use ErrorCollector to collect the metadata check failures, Execution
         * continues.
         */
        COLLECT,
        /** Assert the metadata check failures. Execution aborts. */
        ASSERT
    }

    /**
     * Construct a new StaticMetadata object.
     *
     *<p> Default constructor, only log warnings for the static metadata check failures</p>
     *
     * @param characteristics static info for a camera
     * @throws IllegalArgumentException if characteristics was null
     */
    public StaticMetadata(CameraCharacteristics characteristics) {
        this(characteristics, CheckLevel.WARN, /*collector*/null);
    }

    /**
     * Construct a new StaticMetadata object with {@link CameraErrorCollector}.
     * <p>
     * When level is not {@link CheckLevel.COLLECT}, the {@link CameraErrorCollector} will be
     * ignored, otherwise, it will be used to log the check failures.
     * </p>
     *
     * @param characteristics static info for a camera
     * @param collector The {@link CameraErrorCollector} used by this StaticMetadata
     * @throws IllegalArgumentException if characteristics or collector was null.
     */
    public StaticMetadata(CameraCharacteristics characteristics, CameraErrorCollector collector) {
        this(characteristics, CheckLevel.COLLECT, collector);
    }

    /**
     * Construct a new StaticMetadata object with {@link CheckLevel} and
     * {@link CameraErrorCollector}.
     * <p>
     * When level is not {@link CheckLevel.COLLECT}, the {@link CameraErrorCollector} will be
     * ignored, otherwise, it will be used to log the check failures.
     * </p>
     *
     * @param characteristics static info for a camera
     * @param level The {@link CheckLevel} of this StaticMetadata
     * @param collector The {@link CameraErrorCollector} used by this StaticMetadata
     * @throws IllegalArgumentException if characteristics was null or level was
     *         {@link CheckLevel.COLLECT} but collector was null.
     */
    public StaticMetadata(CameraCharacteristics characteristics, CheckLevel level,
            CameraErrorCollector collector) {
        if (characteristics == null) {
            throw new IllegalArgumentException(""characteristics was null"");
        }
        if (level == CheckLevel.COLLECT && collector == null) {
            throw new IllegalArgumentException(""collector must valid when COLLECT level is set"");
        }

        mCharacteristics = characteristics;
        mLevel = level;
        mCollector = collector;
    }

    /**
     * Get the CameraCharacteristics associated with this StaticMetadata.
     *
     * @return A non-null CameraCharacteristics object
     */
    public CameraCharacteristics getCharacteristics() {
        return mCharacteristics;
    }

    /**
     * Whether or not the hardware level reported by android.info.supportedHardwareLevel
     * is at least {@value CameraMetadata#INFO_SUPPORTED_HARDWARE_LEVEL_FULL}.
     *
     * <p>If the camera device is not reporting the hardwareLevel, this
     * will cause the test to fail.</p>
     *
     * @return {@code true} if the device is {@code FULL}, {@code false} otherwise.
     */
    public boolean isHardwareLevelAtLeastFull() {
        return isHardwareLevelAtLeast(CameraMetadata.INFO_SUPPORTED_HARDWARE_LEVEL_FULL);
    }

    /**
     * Whether or not the hardware level reported by android.info.supportedHardwareLevel is
     * at least the desired one (but could be higher)
     */
    public boolean isHardwareLevelAtLeast(int level) {
        final int[] sortedHwLevels = {
            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY,
            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL,
            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED,
            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_FULL,
            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_3
        };
        int deviceLevel = getHardwareLevelChecked();
        if (level == deviceLevel) {
            return true;
        }

        for (int sortedlevel : sortedHwLevels) {
            if (sortedlevel == level) {
                return true;
            } else if (sortedlevel == deviceLevel) {
                return false;
            }
        }
        Assert.fail(""Unknown hardwareLevel "" + level + "" and device hardware level "" + deviceLevel);
        return false;
    }

    /**
     * Whether or not the camera is an external camera. If so the hardware level
     * reported by android.info.supportedHardwareLevel is
     * {@value CameraMetadata#INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL}.
     *
     * <p>If the camera device is not reporting the hardwareLevel, this
     * will cause the test to fail.</p>
     *
     * @return {@code true} if the device is external, {@code false} otherwise.
     */
    public boolean isExternalCamera() {
        int deviceLevel = getHardwareLevelChecked();
        return deviceLevel == CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL;
    }

    /**
     * Whether or not the hardware level reported by android.info.supportedHardwareLevel
     * Return the supported hardware level of the device, or fail if no value is reported.
     *
     * @return the supported hardware level as a constant defined for
     *      {@link CameraCharacteristics#INFO_SUPPORTED_HARDWARE_LEVEL}.
     */
    public int getHardwareLevelChecked() {
        Integer hwLevel = getValueFromKeyNonNull(
                CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL);
        if (hwLevel == null) {
            Assert.fail(""No supported hardware level reported."");
        }
        return hwLevel;
    }

    /**
     * Whether or not the hardware level reported by android.info.supportedHardwareLevel
     * is {@value CameraMetadata#INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY}.
     *
     * <p>If the camera device is not reporting the hardwareLevel, this
     * will cause the test to fail.</p>
     *
     * @return {@code true} if the device is {@code LEGACY}, {@code false} otherwise.
     */
    public boolean isHardwareLevelLegacy() {
        return getHardwareLevelChecked() == CameraMetadata.INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY;
    }

    /**
     * Whether or not the per frame control is supported by the camera device.
     *
     * @return {@code true} if per frame control is supported, {@code false} otherwise.
     */
    public boolean isPerFrameControlSupported() {
        return getSyncMaxLatency() == CameraMetadata.SYNC_MAX_LATENCY_PER_FRAME_CONTROL;
    }

    /**
     * Get the maximum number of frames to wait for a request settings being applied
     *
     * @return CameraMetadata.SYNC_MAX_LATENCY_UNKNOWN for unknown latency
     *         CameraMetadata.SYNC_MAX_LATENCY_PER_FRAME_CONTROL for per frame control
     *         a positive int otherwise
     */
    public int getSyncMaxLatency() {
        Integer value = getValueFromKeyNonNull(CameraCharacteristics.SYNC_MAX_LATENCY);
        if (value == null) {
            return CameraMetadata.SYNC_MAX_LATENCY_UNKNOWN;
        }
        return value;
    }

    /**
     * Get the color filter arrangement for this camera device.
     *
     * @return Color Filter arrangement of this camera device
     */
    public int getCFAChecked() {
        Integer cfa = getValueFromKeyNonNull(
                CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT);
        if (cfa == null) {
            Assert.fail(""No color filter array (CFA) reported."");
        }
        return cfa;
    }

    public boolean isNIRColorFilter() {
        Integer cfa = mCharacteristics.get(
                CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT);
        if (cfa == null) {
            return false;
        }
        return cfa == CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_NIR;
    }

    /**
     * Whether or not the hardware level reported by android.info.supportedHardwareLevel
     * is {@value CameraMetadata#INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED}.
     *
     * <p>If the camera device is incorrectly reporting the hardwareLevel, this
     * will always return {@code true}.</p>
     *
     * @return {@code true} if the device is {@code LIMITED}, {@code false} otherwise.
     */
    public boolean isHardwareLevelLimited() {
        return getHardwareLevelChecked() == CameraMetadata.INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED;
    }

    /**
     * Whether or not the hardware level reported by {@code android.info.supportedHardwareLevel}
     * is at least {@link CameraMetadata#INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED}.
     *
     * <p>If the camera device is incorrectly reporting the hardwareLevel, this
     * will always return {@code false}.</p>
     *
     * @return
     *          {@code true} if the device is {@code LIMITED} or {@code FULL},
     *          {@code false} otherwise (i.e. LEGACY).
     */
    public boolean isHardwareLevelAtLeastLimited() {
        return isHardwareLevelAtLeast(CameraMetadata.INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED);
    }

    /**
     * Get the maximum number of partial result a request can expect
     *
     * @return 1 if partial result is not supported.
     *         a integer value larger than 1 if partial result is supported.
     */
    public int getPartialResultCount() {
        Integer value = mCharacteristics.get(CameraCharacteristics.REQUEST_PARTIAL_RESULT_COUNT);
        if (value == null) {
            // Optional key. Default value is 1 if key is missing.
            return 1;
        }
        return value;
    }

    /**
     * Get the exposure time value and clamp to the range if needed.
     *
     * @param exposure Input exposure time value to check.
     * @return Exposure value in the legal range.
     */
    public long getExposureClampToRange(long exposure) {
        long minExposure = getExposureMinimumOrDefault(Long.MAX_VALUE);
        long maxExposure = getExposureMaximumOrDefault(Long.MIN_VALUE);
        if (minExposure > SENSOR_INFO_EXPOSURE_TIME_RANGE_MIN_AT_MOST) {
            failKeyCheck(CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE,
                    String.format(
                    ""Min value %d is too large, set to maximal legal value %d"",
                    minExposure, SENSOR_INFO_EXPOSURE_TIME_RANGE_MIN_AT_MOST));
            minExposure = SENSOR_INFO_EXPOSURE_TIME_RANGE_MIN_AT_MOST;
        }
        if (isHardwareLevelAtLeastFull() &&
                maxExposure < SENSOR_INFO_EXPOSURE_TIME_RANGE_MAX_AT_LEAST) {
            failKeyCheck(CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE,
                    String.format(
                    ""Max value %d is too small, set to minimal legal value %d"",
                    maxExposure, SENSOR_INFO_EXPOSURE_TIME_RANGE_MAX_AT_LEAST));
            maxExposure = SENSOR_INFO_EXPOSURE_TIME_RANGE_MAX_AT_LEAST;
        }

        return Math.max(minExposure, Math.min(maxExposure, exposure));
    }

    /**
     * Check if the camera device support focuser.
     *
     * @return true if camera device support focuser, false otherwise.
     */
    public boolean hasFocuser() {
        if (areKeysAvailable(CameraCharacteristics.LENS_INFO_MINIMUM_FOCUS_DISTANCE)) {
            // LEGACY devices don't have lens.info.minimumFocusDistance, so guard this query
            return (getMinimumFocusDistanceChecked() > 0);
        } else {
            // Check available AF modes
            int[] availableAfModes = mCharacteristics.get(
                    CameraCharacteristics.CONTROL_AF_AVAILABLE_MODES);

            if (availableAfModes == null) {
                return false;
            }

            // Assume that if we have an AF mode which doesn't ignore AF trigger, we have a focuser
            boolean hasFocuser = false;
            loop: for (int mode : availableAfModes) {
                switch (mode) {
                    case CameraMetadata.CONTROL_AF_MODE_AUTO:
                    case CameraMetadata.CONTROL_AF_MODE_CONTINUOUS_PICTURE:
                    case CameraMetadata.CONTROL_AF_MODE_CONTINUOUS_VIDEO:
                    case CameraMetadata.CONTROL_AF_MODE_MACRO:
                        hasFocuser = true;
                        break loop;
                }
            }

            return hasFocuser;
        }
    }

    /**
     * Check if the camera device has flash unit.
     * @return true if flash unit is available, false otherwise.
     */
    public boolean hasFlash() {
        return getFlashInfoChecked();
    }

    /**
     * Get minimum focus distance.
     *
     * @return minimum focus distance, 0 if minimum focus distance is invalid.
     */
    public float getMinimumFocusDistanceChecked() {
        Key<Float> key = CameraCharacteristics.LENS_INFO_MINIMUM_FOCUS_DISTANCE;
        Float minFocusDistance;

        /**
         * android.lens.info.minimumFocusDistance - required for FULL and MANUAL_SENSOR-capable
         *   devices; optional for all other devices.
         */
        if (isHardwareLevelAtLeastFull() || isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
            minFocusDistance = getValueFromKeyNonNull(key);
        } else {
            minFocusDistance = mCharacteristics.get(key);
        }

        if (minFocusDistance == null) {
            return 0.0f;
        }

        checkTrueForKey(key, "" minFocusDistance value shouldn't be negative"",
                minFocusDistance >= 0);
        if (minFocusDistance < 0) {
            minFocusDistance = 0.0f;
        }

        return minFocusDistance;
    }

    /**
     * Get focusDistanceCalibration.
     *
     * @return focusDistanceCalibration, UNCALIBRATED if value is invalid.
     */
    public int getFocusDistanceCalibrationChecked() {
        Key<Integer> key = CameraCharacteristics.LENS_INFO_FOCUS_DISTANCE_CALIBRATION;
        Integer calibration = getValueFromKeyNonNull(key);

        if (calibration == null) {
            return CameraMetadata.LENS_INFO_FOCUS_DISTANCE_CALIBRATION_UNCALIBRATED;
        }

        checkTrueForKey(key, "" value is out of range"" ,
                calibration >= CameraMetadata.LENS_INFO_FOCUS_DISTANCE_CALIBRATION_UNCALIBRATED &&
                calibration <= CameraMetadata.LENS_INFO_FOCUS_DISTANCE_CALIBRATION_CALIBRATED);

        return calibration;
    }

    public static String getAeModeName(int aeMode) {
        return (aeMode >= AE_MODE_NAMES.length) ? String.format(""VENDOR_AE_MODE_%d"", aeMode) :
                AE_MODE_NAMES[aeMode];
    }

    public static String getAfModeName(int afMode) {
        return (afMode >= AF_MODE_NAMES.length) ? String.format(""VENDOR_AF_MODE_%d"", afMode) :
                AF_MODE_NAMES[afMode];
    }

    /**
     * Get max AE regions and do validity check.
     *
     * @return AE max regions supported by the camera device
     */
    public int getAeMaxRegionsChecked() {
        Integer regionCount = mCharacteristics.get(CameraCharacteristics.CONTROL_MAX_REGIONS_AE);
        if (regionCount == null) {
            return 0;
        }
        return regionCount;
    }

    /**
     * Get max AWB regions and do validity check.
     *
     * @return AWB max regions supported by the camera device
     */
    public int getAwbMaxRegionsChecked() {
        Integer regionCount = mCharacteristics.get(CameraCharacteristics.CONTROL_MAX_REGIONS_AWB);
        if (regionCount == null) {
            return 0;
        }
        return regionCount;
    }

    /**
     * Get max AF regions and do validity check.
     *
     * @return AF max regions supported by the camera device
     */
    public int getAfMaxRegionsChecked() {
        Integer regionCount = mCharacteristics.get(CameraCharacteristics.CONTROL_MAX_REGIONS_AF);
        if (regionCount == null) {
            return 0;
        }
        return regionCount;
    }
    /**
     * Get the available anti-banding modes.
     *
     * @return The array contains available anti-banding modes.
     */
    public int[] getAeAvailableAntiBandingModesChecked() {
        Key<int[]> key = CameraCharacteristics.CONTROL_AE_AVAILABLE_ANTIBANDING_MODES;
        int[] modes = getValueFromKeyNonNull(key);

        boolean foundAuto = false;
        boolean found50Hz = false;
        boolean found60Hz = false;
        for (int mode : modes) {
            checkTrueForKey(key, ""mode value "" + mode + "" is out if range"",
                    mode >= CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_OFF ||
                    mode <= CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_AUTO);
            if (mode == CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_AUTO) {
                foundAuto = true;
            } else if (mode == CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_50HZ) {
                found50Hz = true;
            } else if (mode == CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_60HZ) {
                found60Hz = true;
            }
        }
        // Must contain AUTO mode or one of 50/60Hz mode.
        checkTrueForKey(key, ""Either AUTO mode or both 50HZ/60HZ mode should present"",
                foundAuto || (found50Hz && found60Hz));

        return modes;
    }

    /**
     * Check if the antibanding OFF mode is supported.
     *
     * @return true if antibanding OFF mode is supported, false otherwise.
     */
    public boolean isAntiBandingOffModeSupported() {
        List<Integer> antiBandingModes =
                Arrays.asList(CameraTestUtils.toObject(getAeAvailableAntiBandingModesChecked()));

        return antiBandingModes.contains(CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_OFF);
    }

    public Boolean getFlashInfoChecked() {
        Key<Boolean> key = CameraCharacteristics.FLASH_INFO_AVAILABLE;
        Boolean hasFlash = getValueFromKeyNonNull(key);

        // In case the failOnKey only gives warning.
        if (hasFlash == null) {
            return false;
        }

        return hasFlash;
    }

    public int[] getAvailableTestPatternModesChecked() {
        Key<int[]> key =
                CameraCharacteristics.SENSOR_AVAILABLE_TEST_PATTERN_MODES;
        int[] modes = getValueFromKeyNonNull(key);

        if (modes == null) {
            return new int[0];
        }

        int expectValue = CameraCharacteristics.SENSOR_TEST_PATTERN_MODE_OFF;
        Integer[] boxedModes = CameraTestUtils.toObject(modes);
        checkTrueForKey(key, "" value must contain OFF mode"",
                Arrays.asList(boxedModes).contains(expectValue));

        return modes;
    }

    /**
     * Get available thumbnail sizes and do the validity check.
     *
     * @return The array of available thumbnail sizes
     */
    public Size[] getAvailableThumbnailSizesChecked() {
        Key<Size[]> key = CameraCharacteristics.JPEG_AVAILABLE_THUMBNAIL_SIZES;
        Size[] sizes = getValueFromKeyNonNull(key);
        final List<Size> sizeList = Arrays.asList(sizes);

        // Size must contain (0, 0).
        checkTrueForKey(key, ""size should contain (0, 0)"", sizeList.contains(new Size(0, 0)));

        // Each size must be distinct.
        checkElementDistinct(key, sizeList);

        // Must be sorted in ascending order by area, by width if areas are same.
        List<Size> orderedSizes =
                CameraTestUtils.getAscendingOrderSizes(sizeList, /*ascending*/true);
        checkTrueForKey(key, ""Sizes should be in ascending order: Original "" + sizeList.toString()
                + "", Expected "" + orderedSizes.toString(), orderedSizes.equals(sizeList));

        // TODO: Aspect ratio match, need wait for android.scaler.availableStreamConfigurations
        // implementation see b/12958122.

        return sizes;
    }

    /**
     * Get available focal lengths and do the validity check.
     *
     * @return The array of available focal lengths
     */
    public float[] getAvailableFocalLengthsChecked() {
        Key<float[]> key = CameraCharacteristics.LENS_INFO_AVAILABLE_FOCAL_LENGTHS;
        float[] focalLengths = getValueFromKeyNonNull(key);

        checkTrueForKey(key, ""Array should contain at least one element"", focalLengths.length >= 1);

        for (int i = 0; i < focalLengths.length; i++) {
            checkTrueForKey(key,
                    String.format(""focalLength[%d] %f should be positive."", i, focalLengths[i]),
                    focalLengths[i] > 0);
        }
        checkElementDistinct(key, Arrays.asList(CameraTestUtils.toObject(focalLengths)));

        return focalLengths;
    }

    /**
     * Get available apertures and do the validity check.
     *
     * @return The non-null array of available apertures
     */
    public float[] getAvailableAperturesChecked() {
        Key<float[]> key = CameraCharacteristics.LENS_INFO_AVAILABLE_APERTURES;
        float[] apertures = getValueFromKeyNonNull(key);

        checkTrueForKey(key, ""Array should contain at least one element"", apertures.length >= 1);

        for (int i = 0; i < apertures.length; i++) {
            checkTrueForKey(key,
                    String.format(""apertures[%d] %f should be positive."", i, apertures[i]),
                    apertures[i] > 0);
        }
        checkElementDistinct(key, Arrays.asList(CameraTestUtils.toObject(apertures)));

        return apertures;
    }

    /**
     * Get and check the available hot pixel map modes.
     *
     * @return the available hot pixel map modes
     */
    public int[] getAvailableHotPixelModesChecked() {
        Key<int[]> key = CameraCharacteristics.HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES;
        int[] modes = getValueFromKeyNonNull(key);

        if (modes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
        if (isHardwareLevelAtLeastFull()) {
            checkTrueForKey(key, ""Full-capability camera devices must support FAST mode"",
                    modeList.contains(CameraMetadata.HOT_PIXEL_MODE_FAST));
        }

        if (isHardwareLevelAtLeastLimited()) {
            // FAST and HIGH_QUALITY mode must be both present or both not present
            List<Integer> coupledModes = Arrays.asList(new Integer[] {
                    CameraMetadata.HOT_PIXEL_MODE_FAST,
                    CameraMetadata.HOT_PIXEL_MODE_HIGH_QUALITY
            });
            checkTrueForKey(
                    key, "" FAST and HIGH_QUALITY mode must both present or both not present"",
                    containsAllOrNone(modeList, coupledModes));
        }
        checkElementDistinct(key, modeList);
        checkArrayValuesInRange(key, modes, CameraMetadata.HOT_PIXEL_MODE_OFF,
                CameraMetadata.HOT_PIXEL_MODE_HIGH_QUALITY);

        return modes;
    }

    /**
     * Get and check available face detection modes.
     *
     * @return The non-null array of available face detection modes
     */
    public int[] getAvailableFaceDetectModesChecked() {
        Key<int[]> key = CameraCharacteristics.STATISTICS_INFO_AVAILABLE_FACE_DETECT_MODES;
        int[] modes = getValueFromKeyNonNull(key);

        if (modes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
        checkTrueForKey(key, ""Array should contain OFF mode"",
                modeList.contains(CameraMetadata.STATISTICS_FACE_DETECT_MODE_OFF));
        checkElementDistinct(key, modeList);
        checkArrayValuesInRange(key, modes, CameraMetadata.STATISTICS_FACE_DETECT_MODE_OFF,
                CameraMetadata.STATISTICS_FACE_DETECT_MODE_FULL);

        return modes;
    }

    /**
     * Get and check max face detected count.
     *
     * @return max number of faces that can be detected
     */
    public int getMaxFaceCountChecked() {
        Key<Integer> key = CameraCharacteristics.STATISTICS_INFO_MAX_FACE_COUNT;
        Integer count = getValueFromKeyNonNull(key);

        if (count == null) {
            return 0;
        }

        List<Integer> faceDetectModes =
                Arrays.asList(CameraTestUtils.toObject(getAvailableFaceDetectModesChecked()));
        if (faceDetectModes.contains(CameraMetadata.STATISTICS_FACE_DETECT_MODE_OFF) &&
                faceDetectModes.size() == 1) {
            checkTrueForKey(key, "" value must be 0 if only OFF mode is supported in ""
                    + ""availableFaceDetectionModes"", count == 0);
        } else {
            int maxFaceCountAtLeast = STATISTICS_INFO_MAX_FACE_COUNT_MIN_AT_LEAST;

            // Legacy mode may support fewer than STATISTICS_INFO_MAX_FACE_COUNT_MIN_AT_LEAST faces.
            if (isHardwareLevelLegacy()) {
                maxFaceCountAtLeast = 1;
            }
            checkTrueForKey(key, "" value must be no less than "" + maxFaceCountAtLeast + "" if SIMPLE""
                    + ""or FULL is also supported in availableFaceDetectionModes"",
                    count >= maxFaceCountAtLeast);
        }

        return count;
    }

    /**
     * Get and check the available tone map modes.
     *
     * @return the available tone map modes
     */
    public int[] getAvailableToneMapModesChecked() {
        Key<int[]> key = CameraCharacteristics.TONEMAP_AVAILABLE_TONE_MAP_MODES;
        int[] modes = mCharacteristics.get(key);

        if (modes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
        checkTrueForKey(key, "" Camera devices must always support FAST mode"",
                modeList.contains(CameraMetadata.TONEMAP_MODE_FAST));
        // Qualification check for MANUAL_POSTPROCESSING capability is in
        // StaticMetadataTest#testCapabilities

        if (isHardwareLevelAtLeastLimited()) {
            // FAST and HIGH_QUALITY mode must be both present or both not present
            List<Integer> coupledModes = Arrays.asList(new Integer[] {
                    CameraMetadata.TONEMAP_MODE_FAST,
                    CameraMetadata.TONEMAP_MODE_HIGH_QUALITY
            });
            checkTrueForKey(
                    key, "" FAST and HIGH_QUALITY mode must both present or both not present"",
                    containsAllOrNone(modeList, coupledModes));
        }
        checkElementDistinct(key, modeList);
        checkArrayValuesInRange(key, modes, CameraMetadata.TONEMAP_MODE_CONTRAST_CURVE,
                CameraMetadata.TONEMAP_MODE_PRESET_CURVE);

        return modes;
    }

    /**
     * Get and check max tonemap curve point.
     *
     * @return Max tonemap curve points.
     */
    public int getMaxTonemapCurvePointChecked() {
        Key<Integer> key = CameraCharacteristics.TONEMAP_MAX_CURVE_POINTS;
        Integer count = getValueFromKeyNonNull(key);
        List<Integer> modeList =
                Arrays.asList(CameraTestUtils.toObject(getAvailableToneMapModesChecked()));
        boolean tonemapCurveOutputSupported =
                modeList.contains(CameraMetadata.TONEMAP_MODE_CONTRAST_CURVE) ||
                modeList.contains(CameraMetadata.TONEMAP_MODE_GAMMA_VALUE) ||
                modeList.contains(CameraMetadata.TONEMAP_MODE_PRESET_CURVE);

        if (count == null) {
            if (tonemapCurveOutputSupported) {
                Assert.fail(""Tonemap curve output is supported but MAX_CURVE_POINTS is null"");
            }
            return 0;
        }

        if (tonemapCurveOutputSupported) {
            checkTrueForKey(key, ""Tonemap curve output supported camera device must support ""
                    + ""maxCurvePoints >= "" + TONEMAP_MAX_CURVE_POINTS_AT_LEAST,
                    count >= TONEMAP_MAX_CURVE_POINTS_AT_LEAST);
        }

        return count;
    }

    /**
     * Get and check pixel array size.
     */
    public Size getPixelArraySizeChecked() {
        return getPixelArraySizeChecked(/*maxResolution*/ false);
    }

    /**
     * Get and check pixel array size.
     */
    public Size getPixelArraySizeChecked(boolean maxResolution) {
        Key<Size> key = maxResolution ?
                CameraCharacteristics.SENSOR_INFO_PIXEL_ARRAY_SIZE_MAXIMUM_RESOLUTION :
                CameraCharacteristics.SENSOR_INFO_PIXEL_ARRAY_SIZE;
        Size pixelArray = getValueFromKeyNonNull(key);
        if (pixelArray == null) {
            return new Size(0, 0);
        }

        return pixelArray;
    }

    /**
     * Get and check pre-correction active array size.
     */
    public Rect getPreCorrectedActiveArraySizeChecked() {
        return getPreCorrectedActiveArraySizeChecked(/*maxResolution*/ false);
    }

    /**
     * Get and check pre-correction active array size.
     */
    public Rect getPreCorrectedActiveArraySizeChecked(boolean maxResolution) {
        Key<Rect> key = maxResolution ?
                CameraCharacteristics.SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION :
                        CameraCharacteristics.SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE;
        Rect activeArray = getValueFromKeyNonNull(key);

        if (activeArray == null) {
            return new Rect(0, 0, 0, 0);
        }

        Size pixelArraySize = getPixelArraySizeChecked(maxResolution);
        checkTrueForKey(key, ""values left/top are invalid"", activeArray.left >= 0 && activeArray.top >= 0);
        checkTrueForKey(key, ""values width/height are invalid"",
                activeArray.width() <= pixelArraySize.getWidth() &&
                activeArray.height() <= pixelArraySize.getHeight());

        return activeArray;
    }

    /**
     * Get and check active array size.
     */
    public Rect getActiveArraySizeChecked() {
        return getActiveArraySizeChecked(/*maxResolution*/ false);
    }

    /**
     * Get and check active array size.
     */
    public Rect getActiveArraySizeChecked(boolean maxResolution) {
        Key<Rect> key = maxResolution ?
                CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION :
                        CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE;
        Rect activeArray = getValueFromKeyNonNull(key);

        if (activeArray == null) {
            return new Rect(0, 0, 0, 0);
        }

        Size pixelArraySize = getPixelArraySizeChecked(maxResolution);
        checkTrueForKey(key, ""values left/top are invalid"", activeArray.left >= 0 && activeArray.top >= 0);
        checkTrueForKey(key, ""values width/height are invalid"",
                activeArray.width() <= pixelArraySize.getWidth() &&
                activeArray.height() <= pixelArraySize.getHeight());

        return activeArray;
    }

    /**
     * Get the dimensions to use for RAW16 buffers.
     */
    public Size getRawDimensChecked() throws Exception {
        return getRawDimensChecked(/*maxResolution*/ false);
    }

    /**
     * Get the dimensions to use for RAW16 buffers.
     */
    public Size getRawDimensChecked(boolean maxResolution) throws Exception {
        Size[] targetCaptureSizes = getAvailableSizesForFormatChecked(ImageFormat.RAW_SENSOR,
                        StaticMetadata.StreamDirection.Output, /*fastSizes*/true, /*slowSizes*/true,
                        maxResolution);
        Assert.assertTrue(""No capture sizes available for RAW format!"",
                targetCaptureSizes.length != 0);
        Rect activeArray = getPreCorrectedActiveArraySizeChecked(maxResolution);
        Size preCorrectionActiveArraySize =
                new Size(activeArray.width(), activeArray.height());
        Size pixelArraySize = getPixelArraySizeChecked(maxResolution);
        Assert.assertTrue(""Missing pre-correction active array size"", activeArray.width() > 0 &&
                activeArray.height() > 0);
        Assert.assertTrue(""Missing pixel array size"", pixelArraySize.getWidth() > 0 &&
                pixelArraySize.getHeight() > 0);
        Size[] allowedArraySizes = new Size[] { preCorrectionActiveArraySize,
                pixelArraySize };
        return assertArrayContainsAnyOf(""Available sizes for RAW format"" +
                "" must include either the pre-corrected active array size, or the full "" +
                ""pixel array size"", targetCaptureSizes, allowedArraySizes);
    }

    /**
     * Get the sensitivity value and clamp to the range if needed.
     *
     * @param sensitivity Input sensitivity value to check.
     * @return Sensitivity value in legal range.
     */
    public int getSensitivityClampToRange(int sensitivity) {
        int minSensitivity = getSensitivityMinimumOrDefault();
        int maxSensitivity = getSensitivityMaximumOrDefault();
        if (minSensitivity > SENSOR_INFO_SENSITIVITY_RANGE_MIN_AT_MOST) {
            failKeyCheck(CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE,
                    String.format(
                    ""Min value %d is too large, set to maximal legal value %d"",
                    minSensitivity, SENSOR_INFO_SENSITIVITY_RANGE_MIN_AT_MOST));
            minSensitivity = SENSOR_INFO_SENSITIVITY_RANGE_MIN_AT_MOST;
        }
        if (maxSensitivity < SENSOR_INFO_SENSITIVITY_RANGE_MAX_AT_LEAST) {
            failKeyCheck(CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE,
                    String.format(
                    ""Max value %d is too small, set to minimal legal value %d"",
                    maxSensitivity, SENSOR_INFO_SENSITIVITY_RANGE_MAX_AT_LEAST));
            maxSensitivity = SENSOR_INFO_SENSITIVITY_RANGE_MAX_AT_LEAST;
        }

        return Math.max(minSensitivity, Math.min(maxSensitivity, sensitivity));
    }

    /**
     * Get maxAnalogSensitivity for a camera device.
     * <p>
     * This is only available for FULL capability device, return 0 if it is unavailable.
     * </p>
     *
     * @return maxAnalogSensitivity, 0 if it is not available.
     */
    public int getMaxAnalogSensitivityChecked() {

        Key<Integer> key = CameraCharacteristics.SENSOR_MAX_ANALOG_SENSITIVITY;
        Integer maxAnalogsensitivity = mCharacteristics.get(key);
        if (maxAnalogsensitivity == null) {
            if (isHardwareLevelAtLeastFull()) {
                Assert.fail(""Full device should report max analog sensitivity"");
            }
            return 0;
        }

        int minSensitivity = getSensitivityMinimumOrDefault();
        int maxSensitivity = getSensitivityMaximumOrDefault();
        checkTrueForKey(key, "" Max analog sensitivity "" + maxAnalogsensitivity
                + "" should be no larger than max sensitivity "" + maxSensitivity,
                maxAnalogsensitivity <= maxSensitivity);
        checkTrueForKey(key, "" Max analog sensitivity "" + maxAnalogsensitivity
                + "" should be larger than min sensitivity "" + maxSensitivity,
                maxAnalogsensitivity > minSensitivity);

        return maxAnalogsensitivity;
    }

    /**
     * Get hyperfocalDistance and do the validity check.
     * <p>
     * Note that, this tag is optional, will return -1 if this tag is not
     * available.
     * </p>
     *
     * @return hyperfocalDistance of this device, -1 if this tag is not available.
     */
    public float getHyperfocalDistanceChecked() {
        Key<Float> key = CameraCharacteristics.LENS_INFO_HYPERFOCAL_DISTANCE;
        Float hyperfocalDistance = getValueFromKeyNonNull(key);
        if (hyperfocalDistance == null) {
            return -1;
        }

        if (hasFocuser()) {
            float minFocusDistance = getMinimumFocusDistanceChecked();
            checkTrueForKey(key, String.format("" hyperfocal distance %f should be in the range of""
                    + "" should be in the range of (%f, %f]"", hyperfocalDistance, 0.0f,
                    minFocusDistance),
                    hyperfocalDistance > 0 && hyperfocalDistance <= minFocusDistance);
        }

        return hyperfocalDistance;
    }

    /**
     * Get the minimum value for a sensitivity range from android.sensor.info.sensitivityRange.
     *
     * <p>If the camera is incorrectly reporting values, log a warning and return
     * the default value instead, which is the largest minimum value required to be supported
     * by all camera devices.</p>
     *
     * @return The value reported by the camera device or the defaultValue otherwise.
     */
    public int getSensitivityMinimumOrDefault() {
        return getSensitivityMinimumOrDefault(SENSOR_INFO_SENSITIVITY_RANGE_MIN_AT_MOST);
    }

    /**
     * Get the minimum value for a sensitivity range from android.sensor.info.sensitivityRange.
     *
     * <p>If the camera is incorrectly reporting values, log a warning and return
     * the default value instead.</p>
     *
     * @param defaultValue Value to return if no legal value is available
     * @return The value reported by the camera device or the defaultValue otherwise.
     */
    public int getSensitivityMinimumOrDefault(int defaultValue) {
        Range<Integer> range = mCharacteristics.get(
                CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE);
        if (range == null) {
            if (isHardwareLevelAtLeastFull()) {
                failKeyCheck(CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE,
                        ""had no valid minimum value; using default of "" + defaultValue);
            }
            return defaultValue;
        }
        return range.getLower();
    }

    /**
     * Get the maximum value for a sensitivity range from android.sensor.info.sensitivityRange.
     *
     * <p>If the camera is incorrectly reporting values, log a warning and return
     * the default value instead, which is the smallest maximum value required to be supported
     * by all camera devices.</p>
     *
     * @return The value reported by the camera device or the defaultValue otherwise.
     */
    public int getSensitivityMaximumOrDefault() {
        return getSensitivityMaximumOrDefault(SENSOR_INFO_SENSITIVITY_RANGE_MAX_AT_LEAST);
    }

    /**
     * Get the maximum value for a sensitivity range from android.sensor.info.sensitivityRange.
     *
     * <p>If the camera is incorrectly reporting values, log a warning and return
     * the default value instead.</p>
     *
     * @param defaultValue Value to return if no legal value is available
     * @return The value reported by the camera device or the defaultValue otherwise.
     */
    public int getSensitivityMaximumOrDefault(int defaultValue) {
        Range<Integer> range = mCharacteristics.get(
                CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE);
        if (range == null) {
            if (isHardwareLevelAtLeastFull()) {
                failKeyCheck(CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE,
                        ""had no valid maximum value; using default of "" + defaultValue);
            }
            return defaultValue;
        }
        return range.getUpper();
    }

    /**
     * Get the minimum value for an exposure range from android.sensor.info.exposureTimeRange.
     *
     * <p>If the camera is incorrectly reporting values, log a warning and return
     * the default value instead.</p>
     *
     * @param defaultValue Value to return if no legal value is available
     * @return The value reported by the camera device or the defaultValue otherwise.
     */
    public long getExposureMinimumOrDefault(long defaultValue) {
        Range<Long> range = getValueFromKeyNonNull(
                CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE);
        if (range == null) {
            failKeyCheck(CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE,
                    ""had no valid minimum value; using default of "" + defaultValue);
            return defaultValue;
        }
        return range.getLower();
    }

    /**
     * Get the minimum value for an exposure range from android.sensor.info.exposureTimeRange.
     *
     * <p>If the camera is incorrectly reporting values, log a warning and return
     * the default value instead, which is the largest minimum value required to be supported
     * by all camera devices.</p>
     *
     * @return The value reported by the camera device or the defaultValue otherwise.
     */
    public long getExposureMinimumOrDefault() {
        return getExposureMinimumOrDefault(SENSOR_INFO_EXPOSURE_TIME_RANGE_MIN_AT_MOST);
    }

    /**
     * Get the maximum value for an exposure range from android.sensor.info.exposureTimeRange.
     *
     * <p>If the camera is incorrectly reporting values, log a warning and return
     * the default value instead.</p>
     *
     * @param defaultValue Value to return if no legal value is available
     * @return The value reported by the camera device or the defaultValue otherwise.
     */
    public long getExposureMaximumOrDefault(long defaultValue) {
        Range<Long> range = getValueFromKeyNonNull(
                CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE);
        if (range == null) {
            failKeyCheck(CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE,
                    ""had no valid maximum value; using default of "" + defaultValue);
            return defaultValue;
        }
        return range.getUpper();
    }

    /**
     * Get the maximum value for an exposure range from android.sensor.info.exposureTimeRange.
     *
     * <p>If the camera is incorrectly reporting values, log a warning and return
     * the default value instead, which is the smallest maximum value required to be supported
     * by all camera devices.</p>
     *
     * @return The value reported by the camera device or the defaultValue otherwise.
     */
    public long getExposureMaximumOrDefault() {
        return getExposureMaximumOrDefault(SENSOR_INFO_EXPOSURE_TIME_RANGE_MAX_AT_LEAST);
    }

    /**
     * get android.control.availableModes and do the validity check.
     *
     * @return available control modes.
     */
    public int[] getAvailableControlModesChecked() {
        Key<int[]> modesKey = CameraCharacteristics.CONTROL_AVAILABLE_MODES;
        int[] modes = getValueFromKeyNonNull(modesKey);
        if (modes == null) {
            modes = new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
        checkTrueForKey(modesKey, ""value is empty"", !modeList.isEmpty());

        // All camera device must support AUTO
        checkTrueForKey(modesKey, ""values "" + modeList.toString() + "" must contain AUTO mode"",
                modeList.contains(CameraMetadata.CONTROL_MODE_AUTO));

        boolean isAeOffSupported =  Arrays.asList(
                CameraTestUtils.toObject(getAeAvailableModesChecked())).contains(
                        CameraMetadata.CONTROL_AE_MODE_OFF);
        boolean isAfOffSupported =  Arrays.asList(
                CameraTestUtils.toObject(getAfAvailableModesChecked())).contains(
                        CameraMetadata.CONTROL_AF_MODE_OFF);
        boolean isAwbOffSupported =  Arrays.asList(
                CameraTestUtils.toObject(getAwbAvailableModesChecked())).contains(
                        CameraMetadata.CONTROL_AWB_MODE_OFF);
        if (isAeOffSupported && isAfOffSupported && isAwbOffSupported) {
            // 3A OFF controls are supported, OFF mode must be supported here.
            checkTrueForKey(modesKey, ""values "" + modeList.toString() + "" must contain OFF mode"",
                    modeList.contains(CameraMetadata.CONTROL_MODE_OFF));
        }

        if (isSceneModeSupported()) {
            checkTrueForKey(modesKey, ""values "" + modeList.toString() + "" must contain""
                    + "" USE_SCENE_MODE"",
                    modeList.contains(CameraMetadata.CONTROL_MODE_USE_SCENE_MODE));
        }

        return modes;
    }

    public boolean isSceneModeSupported() {
        List<Integer> availableSceneModes = Arrays.asList(
                CameraTestUtils.toObject(getAvailableSceneModesChecked()));

        if (availableSceneModes.isEmpty()) {
            return false;
        }

        // If sceneMode is not supported, camera device will contain single entry: DISABLED.
        return availableSceneModes.size() > 1 ||
                !availableSceneModes.contains(CameraMetadata.CONTROL_SCENE_MODE_DISABLED);
    }

    /**
     * Get aeAvailableModes and do the validity check.
     *
     * <p>Depending on the check level this class has, for WAR or COLLECT levels,
     * If the aeMode list is invalid, return an empty mode array. The the caller doesn't
     * have to abort the execution even the aeMode list is invalid.</p>
     * @return AE available modes
     */
    public int[] getAeAvailableModesChecked() {
        Key<int[]> modesKey = CameraCharacteristics.CONTROL_AE_AVAILABLE_MODES;
        int[] modes = getValueFromKeyNonNull(modesKey);
        if (modes == null) {
            modes = new int[0];
        }
        List<Integer> modeList = new ArrayList<Integer>();
        for (int mode : modes) {
            // Skip vendor-added modes
            if (mode <= CameraMetadata.CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE) {
                modeList.add(mode);
            }
        }
        checkTrueForKey(modesKey, ""value is empty"", !modeList.isEmpty());
        modes = new int[modeList.size()];
        for (int i = 0; i < modeList.size(); i++) {
            modes[i] = modeList.get(i);
        }

        // All camera device must support ON
        checkTrueForKey(modesKey, ""values "" + modeList.toString() + "" must contain ON mode"",
                modeList.contains(CameraMetadata.CONTROL_AE_MODE_ON));

        // All camera devices with flash units support ON_AUTO_FLASH and ON_ALWAYS_FLASH
        Key<Boolean> flashKey= CameraCharacteristics.FLASH_INFO_AVAILABLE;
        Boolean hasFlash = getValueFromKeyNonNull(flashKey);
        if (hasFlash == null) {
            hasFlash = false;
        }
        if (hasFlash) {
            boolean flashModeConsistentWithFlash =
                    modeList.contains(CameraMetadata.CONTROL_AE_MODE_ON_AUTO_FLASH) &&
                    modeList.contains(CameraMetadata.CONTROL_AE_MODE_ON_ALWAYS_FLASH);
            checkTrueForKey(modesKey,
                    ""value must contain ON_AUTO_FLASH and ON_ALWAYS_FLASH and  when flash is"" +
                    ""available"", flashModeConsistentWithFlash);
        } else {
            boolean flashModeConsistentWithoutFlash =
                    !(modeList.contains(CameraMetadata.CONTROL_AE_MODE_ON_AUTO_FLASH) ||
                    modeList.contains(CameraMetadata.CONTROL_AE_MODE_ON_ALWAYS_FLASH) ||
                    modeList.contains(CameraMetadata.CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE));
            checkTrueForKey(modesKey,
                    ""value must not contain ON_AUTO_FLASH, ON_ALWAYS_FLASH and"" +
                    ""ON_AUTO_FLASH_REDEYE when flash is unavailable"",
                    flashModeConsistentWithoutFlash);
        }

        // FULL mode camera devices always support OFF mode.
        boolean condition =
                !isHardwareLevelAtLeastFull() || modeList.contains(CameraMetadata.CONTROL_AE_MODE_OFF);
        checkTrueForKey(modesKey, ""Full capability device must have OFF mode"", condition);

        // Boundary check.
        for (int mode : modes) {
            checkTrueForKey(modesKey, ""Value "" + mode + "" is out of bound"",
                    mode >= CameraMetadata.CONTROL_AE_MODE_OFF
                    && mode <= CameraMetadata.CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE);
        }

        return modes;
    }

    /**
     * Get available AWB modes and do the validity check.
     *
     * @return array that contains available AWB modes, empty array if awbAvailableModes is
     * unavailable.
     */
    public int[] getAwbAvailableModesChecked() {
        Key<int[]> key =
                CameraCharacteristics.CONTROL_AWB_AVAILABLE_MODES;
        int[] awbModes = getValueFromKeyNonNull(key);

        if (awbModes == null) {
            return new int[0];
        }

        List<Integer> modesList = Arrays.asList(CameraTestUtils.toObject(awbModes));
        checkTrueForKey(key, "" All camera devices must support AUTO mode"",
                modesList.contains(CameraMetadata.CONTROL_AWB_MODE_AUTO));
        if (isHardwareLevelAtLeastFull()) {
            checkTrueForKey(key, "" Full capability camera devices must support OFF mode"",
                    modesList.contains(CameraMetadata.CONTROL_AWB_MODE_OFF));
        }

        return awbModes;
    }

    /**
     * Get available AF modes and do the validity check.
     *
     * @return array that contains available AF modes, empty array if afAvailableModes is
     * unavailable.
     */
    public int[] getAfAvailableModesChecked() {
        Key<int[]> key =
                CameraCharacteristics.CONTROL_AF_AVAILABLE_MODES;
        int[] afModes = getValueFromKeyNonNull(key);

        if (afModes == null) {
            return new int[0];
        }

        List<Integer> modesList = new ArrayList<Integer>();
        for (int afMode : afModes) {
            // Skip vendor-added AF modes
            if (afMode > CameraCharacteristics.CONTROL_AF_MODE_EDOF) continue;
            modesList.add(afMode);
        }
        afModes = new int[modesList.size()];
        for (int i = 0; i < modesList.size(); i++) {
            afModes[i] = modesList.get(i);
        }

        if (isHardwareLevelAtLeastLimited()) {
            // Some LEGACY mode devices do not support AF OFF
            checkTrueForKey(key, "" All camera devices must support OFF mode"",
                    modesList.contains(CameraMetadata.CONTROL_AF_MODE_OFF));
        }
        if (hasFocuser()) {
            checkTrueForKey(key, "" Camera devices that have focuser units must support AUTO mode"",
                    modesList.contains(CameraMetadata.CONTROL_AF_MODE_AUTO));
        }

        return afModes;
    }

    /**
     * Get supported raw output sizes and do the check.
     *
     * @return Empty size array if raw output is not supported
     */
    public Size[] getRawOutputSizesChecked() {
        return getAvailableSizesForFormatChecked(ImageFormat.RAW_SENSOR,
                StreamDirection.Output);
    }

    /**
     * Get supported jpeg output sizes and do the check.
     *
     * @return Empty size array if jpeg output is not supported
     */
    public Size[] getJpegOutputSizesChecked() {
        return getAvailableSizesForFormatChecked(ImageFormat.JPEG,
                StreamDirection.Output);
    }

    /**
     * Get supported heic output sizes and do the check.
     *
     * @return Empty size array if heic output is not supported
     */
    public Size[] getHeicOutputSizesChecked() {
        return getAvailableSizesForFormatChecked(ImageFormat.HEIC,
                StreamDirection.Output);
    }

    /**
     * Used to determine the stream direction for various helpers that look up
     * format or size information.
     */
    public enum StreamDirection {
        /** Stream is used with {@link android.hardware.camera2.CameraDevice#configureOutputs} */
        Output,
        /** Stream is used with {@code CameraDevice#configureInputs} -- NOT YET PUBLIC */
        Input
    }

    /**
     * Get available formats for a given direction.
     *
     * @param direction The stream direction, input or output.
     * @return The formats of the given direction, empty array if no available format is found.
     */
    public int[] getAvailableFormats(StreamDirection direction) {
        Key<StreamConfigurationMap> key =
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP;
        StreamConfigurationMap config = getValueFromKeyNonNull(key);

        if (config == null) {
            return new int[0];
        }

        switch (direction) {
            case Output:
                return config.getOutputFormats();
            case Input:
                return config.getInputFormats();
            default:
                throw new IllegalArgumentException(""direction must be output or input"");
        }
    }

    /**
     * Get valid output formats for a given input format.
     *
     * @param inputFormat The input format used to produce the output images.
     * @return The output formats for the given input format, empty array if
     *         no available format is found.
     */
    public int[] getValidOutputFormatsForInput(int inputFormat) {
        Key<StreamConfigurationMap> key =
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP;
        StreamConfigurationMap config = getValueFromKeyNonNull(key);

        if (config == null) {
            return new int[0];
        }

        return config.getValidOutputFormatsForInput(inputFormat);
    }

    /**
     * Get available sizes for given format and direction.
     *
     * @param format The format for the requested size array.
     * @param direction The stream direction, input or output.
     * @return The sizes of the given format, empty array if no available size is found.
     */
    public Size[] getAvailableSizesForFormatChecked(int format, StreamDirection direction) {
        return getAvailableSizesForFormatChecked(format, direction,
                /*fastSizes*/true, /*slowSizes*/true, /*maxResolution*/false);
    }

    /**
     * Get available sizes for given format and direction, and whether to limit to slow or fast
     * resolutions.
     *
     * @param format The format for the requested size array.
     * @param direction The stream direction, input or output.
     * @param fastSizes whether to include getOutputSizes() sizes (generally faster)
     * @param slowSizes whether to include getHighResolutionOutputSizes() sizes (generally slower)
     * @return The sizes of the given format, empty array if no available size is found.
     */
    public Size[] getAvailableSizesForFormatChecked(int format, StreamDirection direction,
            boolean fastSizes, boolean slowSizes) {
        return  getAvailableSizesForFormatChecked(format, direction, fastSizes, slowSizes,
                /*maxResolution*/ false);
    }

    /**
     * Get available sizes for given format and direction, and whether to limit to slow or fast
     * resolutions.
     *
     * @param format The format for the requested size array.
     * @param direction The stream direction, input or output.
     * @param fastSizes whether to include getOutputSizes() sizes (generally faster)
     * @param slowSizes whether to include getHighResolutionOutputSizes() sizes (generally slower)
     * @return The sizes of the given format, empty array if no available size is found.
     */
    public Size[] getAvailableSizesForFormatChecked(int format, StreamDirection direction,
            boolean fastSizes, boolean slowSizes, boolean maxResolution) {
        Key<StreamConfigurationMap> key = maxResolution ?
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP_MAXIMUM_RESOLUTION :
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP;
        StreamConfigurationMap config = getValueFromKeyNonNull(key);

        if (config == null) {
            return new Size[0];
        }

        Size[] sizes = null;

        switch (direction) {
            case Output:
                Size[] fastSizeList = null;
                Size[] slowSizeList = null;
                if (fastSizes) {
                    fastSizeList = config.getOutputSizes(format);
                }
                if (slowSizes) {
                    slowSizeList = config.getHighResolutionOutputSizes(format);
                }
                if (fastSizeList != null && slowSizeList != null) {
                    sizes = new Size[slowSizeList.length + fastSizeList.length];
                    System.arraycopy(fastSizeList, 0, sizes, 0, fastSizeList.length);
                    System.arraycopy(slowSizeList, 0, sizes, fastSizeList.length, slowSizeList.length);
                } else if (fastSizeList != null) {
                    sizes = fastSizeList;
                } else if (slowSizeList != null) {
                    sizes = slowSizeList;
                }
                break;
            case Input:
                sizes = config.getInputSizes(format);
                break;
            default:
                throw new IllegalArgumentException(""direction must be output or input"");
        }

        if (sizes == null) {
            sizes = new Size[0];
        }

        return sizes;
    }

    /**
     * Get available AE target fps ranges.
     *
     * @return Empty int array if aeAvailableTargetFpsRanges is invalid.
     */
    @SuppressWarnings(""raw"")
    public Range<Integer>[] getAeAvailableTargetFpsRangesChecked() {
        Key<Range<Integer>[]> key =
                CameraCharacteristics.CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES;
        Range<Integer>[] fpsRanges = getValueFromKeyNonNull(key);

        if (fpsRanges == null) {
            return new Range[0];
        }

        // Round down to 2 boundary if it is not integer times of 2, to avoid array out of bound
        // in case the above check fails.
        int fpsRangeLength = fpsRanges.length;
        int minFps, maxFps;
        long maxFrameDuration = getMaxFrameDurationChecked();
        for (int i = 0; i < fpsRangeLength; i += 1) {
            minFps = fpsRanges[i].getLower();
            maxFps = fpsRanges[i].getUpper();
            checkTrueForKey(key, "" min fps must be no larger than max fps!"",
                    minFps > 0 && maxFps >= minFps);
            long maxDuration = (long) (1e9 / minFps);
            checkTrueForKey(key, String.format(
                    "" the frame duration %d for min fps %d must smaller than maxFrameDuration %d"",
                    maxDuration, minFps, maxFrameDuration), maxDuration <= maxFrameDuration);
        }
        return fpsRanges;
    }

    /**
     * Get the highest supported target FPS range.
     * Prioritizes maximizing the min FPS, then the max FPS without lowering min FPS.
     */
    public Range<Integer> getAeMaxTargetFpsRange() {
        Range<Integer>[] fpsRanges = getAeAvailableTargetFpsRangesChecked();

        Range<Integer> targetRange = fpsRanges[0];
        // Assume unsorted list of target FPS ranges, so use two passes, first maximize min FPS
        for (Range<Integer> candidateRange : fpsRanges) {
            if (candidateRange.getLower() > targetRange.getLower()) {
                targetRange = candidateRange;
            }
        }
        // Then maximize max FPS while not lowering min FPS
        for (Range<Integer> candidateRange : fpsRanges) {
            if (candidateRange.getLower() >= targetRange.getLower() &&
                    candidateRange.getUpper() > targetRange.getUpper()) {
                targetRange = candidateRange;
            }
        }
        return targetRange;
    }

    /**
     * Get max frame duration.
     *
     * @return 0 if maxFrameDuration is null
     */
    public long getMaxFrameDurationChecked() {
        Key<Long> key =
                CameraCharacteristics.SENSOR_INFO_MAX_FRAME_DURATION;
        Long maxDuration = getValueFromKeyNonNull(key);

        if (maxDuration == null) {
            return 0;
        }

        return maxDuration;
    }

    /**
     * Get available minimal frame durations for a given format.
     *
     * @param format One of the format from {@link ImageFormat}.
     * @return HashMap of minimal frame durations for different sizes, empty HashMap
     *         if availableMinFrameDurations is null.
     */
    public HashMap<Size, Long> getAvailableMinFrameDurationsForFormatChecked(int format) {

        HashMap<Size, Long> minDurationMap = new HashMap<Size, Long>();

        Key<StreamConfigurationMap> key =
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP;
        StreamConfigurationMap config = getValueFromKeyNonNull(key);

        if (config == null) {
            return minDurationMap;
        }

        for (android.util.Size size : getAvailableSizesForFormatChecked(format,
                StreamDirection.Output)) {
            long minFrameDuration = config.getOutputMinFrameDuration(format, size);

            if (minFrameDuration != 0) {
                minDurationMap.put(new Size(size.getWidth(), size.getHeight()), minFrameDuration);
            }
        }

        return minDurationMap;
    }

    public int[] getAvailableEdgeModesChecked() {
        Key<int[]> key = CameraCharacteristics.EDGE_AVAILABLE_EDGE_MODES;
        int[] edgeModes = getValueFromKeyNonNull(key);

        if (edgeModes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(edgeModes));
        // Full device should always include OFF and FAST
        if (isHardwareLevelAtLeastFull()) {
            checkTrueForKey(key, ""Full device must contain OFF and FAST edge modes"",
                    modeList.contains(CameraMetadata.EDGE_MODE_OFF) &&
                    modeList.contains(CameraMetadata.EDGE_MODE_FAST));
        }

        if (isHardwareLevelAtLeastLimited()) {
            // FAST and HIGH_QUALITY mode must be both present or both not present
            List<Integer> coupledModes = Arrays.asList(new Integer[] {
                    CameraMetadata.EDGE_MODE_FAST,
                    CameraMetadata.EDGE_MODE_HIGH_QUALITY
            });
            checkTrueForKey(
                    key, "" FAST and HIGH_QUALITY mode must both present or both not present"",
                    containsAllOrNone(modeList, coupledModes));
        }

        return edgeModes;
    }

      public int[] getAvailableShadingModesChecked() {
        Key<int[]> key = CameraCharacteristics.SHADING_AVAILABLE_MODES;
        int[] shadingModes = getValueFromKeyNonNull(key);

        if (shadingModes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(shadingModes));
        // Full device should always include OFF and FAST
        if (isHardwareLevelAtLeastFull()) {
            checkTrueForKey(key, ""Full device must contain OFF and FAST shading modes"",
                    modeList.contains(CameraMetadata.SHADING_MODE_OFF) &&
                    modeList.contains(CameraMetadata.SHADING_MODE_FAST));
        }

        if (isHardwareLevelAtLeastLimited()) {
            // FAST and HIGH_QUALITY mode must be both present or both not present
            List<Integer> coupledModes = Arrays.asList(new Integer[] {
                    CameraMetadata.SHADING_MODE_FAST,
                    CameraMetadata.SHADING_MODE_HIGH_QUALITY
            });
            checkTrueForKey(
                    key, "" FAST and HIGH_QUALITY mode must both present or both not present"",
                    containsAllOrNone(modeList, coupledModes));
        }

        return shadingModes;
    }

    public int[] getAvailableNoiseReductionModesChecked() {
        Key<int[]> key =
                CameraCharacteristics.NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES;
        int[] noiseReductionModes = getValueFromKeyNonNull(key);

        if (noiseReductionModes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(noiseReductionModes));
        // Full device should always include OFF and FAST
        if (isHardwareLevelAtLeastFull()) {

            checkTrueForKey(key, ""Full device must contain OFF and FAST noise reduction modes"",
                    modeList.contains(CameraMetadata.NOISE_REDUCTION_MODE_OFF) &&
                    modeList.contains(CameraMetadata.NOISE_REDUCTION_MODE_FAST));
        }

        if (isHardwareLevelAtLeastLimited()) {
            // FAST and HIGH_QUALITY mode must be both present or both not present
            List<Integer> coupledModes = Arrays.asList(new Integer[] {
                    CameraMetadata.NOISE_REDUCTION_MODE_FAST,
                    CameraMetadata.NOISE_REDUCTION_MODE_HIGH_QUALITY
            });
            checkTrueForKey(
                    key, "" FAST and HIGH_QUALITY mode must both present or both not present"",
                    containsAllOrNone(modeList, coupledModes));
        }
        return noiseReductionModes;
    }

    /**
     * Get value of key android.control.aeCompensationStep and do the validity check.
     *
     * @return default value if the value is null.
     */
    public Rational getAeCompensationStepChecked() {
        Key<Rational> key =
                CameraCharacteristics.CONTROL_AE_COMPENSATION_STEP;
        Rational compensationStep = getValueFromKeyNonNull(key);

        if (compensationStep == null) {
            // Return default step.
            return CONTROL_AE_COMPENSATION_STEP_DEFAULT;
        }

        // Legacy devices don't have a minimum step requirement
        if (isHardwareLevelAtLeastLimited()) {
            float compensationStepF =
                    (float) compensationStep.getNumerator() / compensationStep.getDenominator();
            checkTrueForKey(key, "" value must be no more than 1/2"", compensationStepF <= 0.5f);
        }

        return compensationStep;
    }

    /**
     * Get value of key android.control.aeCompensationRange and do the validity check.
     *
     * @return default value if the value is null or malformed.
     */
    public Range<Integer> getAeCompensationRangeChecked() {
        Key<Range<Integer>> key =
                CameraCharacteristics.CONTROL_AE_COMPENSATION_RANGE;
        Range<Integer> compensationRange = getValueFromKeyNonNull(key);
        Rational compensationStep = getAeCompensationStepChecked();
        float compensationStepF = compensationStep.floatValue();
        final Range<Integer> DEFAULT_RANGE = Range.create(
                (int)(CONTROL_AE_COMPENSATION_RANGE_DEFAULT_MIN / compensationStepF),
                (int)(CONTROL_AE_COMPENSATION_RANGE_DEFAULT_MAX / compensationStepF));
        final Range<Integer> ZERO_RANGE = Range.create(0, 0);
        if (compensationRange == null) {
            return ZERO_RANGE;
        }

        // Legacy devices don't have a minimum range requirement
        if (isHardwareLevelAtLeastLimited() && !compensationRange.equals(ZERO_RANGE)) {
            checkTrueForKey(key, "" range value must be at least "" + DEFAULT_RANGE
                    + "", actual "" + compensationRange + "", compensation step "" + compensationStep,
                   compensationRange.getLower() <= DEFAULT_RANGE.getLower() &&
                   compensationRange.getUpper() >= DEFAULT_RANGE.getUpper());
        }

        return compensationRange;
    }

    /**
     * Get availableVideoStabilizationModes and do the validity check.
     *
     * @return available video stabilization modes, empty array if it is unavailable.
     */
    public int[] getAvailableVideoStabilizationModesChecked() {
        Key<int[]> key =
                CameraCharacteristics.CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES;
        int[] modes = getValueFromKeyNonNull(key);

        if (modes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
        checkTrueForKey(key, "" All device should support OFF mode"",
                modeList.contains(CameraMetadata.CONTROL_VIDEO_STABILIZATION_MODE_OFF));
        checkArrayValuesInRange(key, modes,
                CameraMetadata.CONTROL_VIDEO_STABILIZATION_MODE_OFF,
                CameraMetadata.CONTROL_VIDEO_STABILIZATION_MODE_ON);

        return modes;
    }

    public boolean isVideoStabilizationSupported() {
        Integer[] videoStabModes =
                CameraTestUtils.toObject(getAvailableVideoStabilizationModesChecked());
        return Arrays.asList(videoStabModes).contains(
                CameraMetadata.CONTROL_VIDEO_STABILIZATION_MODE_ON);
    }

    /**
     * Get availableOpticalStabilization and do the validity check.
     *
     * @return available optical stabilization modes, empty array if it is unavailable.
     */
    public int[] getAvailableOpticalStabilizationChecked() {
        Key<int[]> key =
                CameraCharacteristics.LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION;
        int[] modes = getValueFromKeyNonNull(key);

        if (modes == null) {
            return new int[0];
        }

        checkArrayValuesInRange(key, modes,
                CameraMetadata.LENS_OPTICAL_STABILIZATION_MODE_OFF,
                CameraMetadata.LENS_OPTICAL_STABILIZATION_MODE_ON);

        return modes;
    }

    /**
     * Get the scaler's max digital zoom ({@code >= 1.0f}) ratio between crop and active array
     * @return the max zoom ratio, or {@code 1.0f} if the value is unavailable
     */
    public float getAvailableMaxDigitalZoomChecked() {
        Key<Float> key =
                CameraCharacteristics.SCALER_AVAILABLE_MAX_DIGITAL_ZOOM;

        Float maxZoom = getValueFromKeyNonNull(key);
        if (maxZoom == null) {
            return 1.0f;
        }

        checkTrueForKey(key, "" max digital zoom should be no less than 1"",
                maxZoom >= 1.0f && !Float.isNaN(maxZoom) && !Float.isInfinite(maxZoom));

        return maxZoom;
    }

    public Range<Float> getZoomRatioRangeChecked() {
        Key<Range<Float>> key =
                CameraCharacteristics.CONTROL_ZOOM_RATIO_RANGE;

        Range<Float> zoomRatioRange = getValueFromKeyNonNull(key);
        if (zoomRatioRange == null) {
            return new Range<Float>(1.0f, 1.0f);
        }

        checkTrueForKey(key, String.format("" min zoom ratio %f should be no more than 1"",
                zoomRatioRange.getLower()), zoomRatioRange.getLower() <= 1.0);
        checkTrueForKey(key, String.format("" max zoom ratio %f should be no less than 1"",
                zoomRatioRange.getUpper()), zoomRatioRange.getUpper() >= 1.0);
        final float ZOOM_MIN_RANGE = 0.01f;
        checkTrueForKey(key, "" zoom ratio range should be reasonably large"",
                zoomRatioRange.getUpper().equals(zoomRatioRange.getLower()) ||
                zoomRatioRange.getUpper() - zoomRatioRange.getLower() > ZOOM_MIN_RANGE);
        return zoomRatioRange;
    }

    public int[] getAvailableSceneModesChecked() {
        Key<int[]> key =
                CameraCharacteristics.CONTROL_AVAILABLE_SCENE_MODES;
        int[] modes = getValueFromKeyNonNull(key);

        if (modes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
        // FACE_PRIORITY must be included if face detection is supported.
        if (areKeysAvailable(CameraCharacteristics.STATISTICS_INFO_MAX_FACE_COUNT) &&
                getMaxFaceCountChecked() > 0) {
            checkTrueForKey(key, "" FACE_PRIORITY must be included if face detection is supported"",
                    modeList.contains(CameraMetadata.CONTROL_SCENE_MODE_FACE_PRIORITY));
        }

        return modes;
    }

    public int[] getAvailableEffectModesChecked() {
        Key<int[]> key =
                CameraCharacteristics.CONTROL_AVAILABLE_EFFECTS;
        int[] modes = getValueFromKeyNonNull(key);

        if (modes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
        // OFF must be included.
        checkTrueForKey(key, "" OFF must be included"",
                modeList.contains(CameraMetadata.CONTROL_EFFECT_MODE_OFF));

        return modes;
    }

    public Capability[] getAvailableExtendedSceneModeCapsChecked() {
        final Size FULL_HD = new Size(1920, 1080);
        Rect activeRect = getValueFromKeyNonNull(
                CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE);
        Key<Capability[]> key =
                CameraCharacteristics.CONTROL_AVAILABLE_EXTENDED_SCENE_MODE_CAPABILITIES;
        Capability[] caps = mCharacteristics.get(key);
        if (caps == null) {
            return new Capability[0];
        }

        Size[] yuvSizes = getAvailableSizesForFormatChecked(ImageFormat.YUV_420_888,
                StaticMetadata.StreamDirection.Output);
        List<Size> yuvSizesList = Arrays.asList(yuvSizes);
        for (Capability cap : caps) {
            int extendedSceneMode = cap.getMode();
            Size maxStreamingSize = cap.getMaxStreamingSize();
            boolean maxStreamingSizeIsZero =
                    maxStreamingSize.getWidth() == 0 && maxStreamingSize.getHeight() == 0;
            switch (extendedSceneMode) {
                case CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE:
                    // STILL_CAPTURE: Must either be (0, 0), or one of supported yuv/private sizes.
                    // Because spec requires yuv and private sizes match, only check YUV sizes here.
                    checkTrueForKey(key,
                            String.format("" maxStreamingSize [%d, %d] for extended scene mode "" +
                            ""%d must be a supported YCBCR_420_888 size, or (0, 0)"",
                            maxStreamingSize.getWidth(), maxStreamingSize.getHeight(),
                            extendedSceneMode),
                            yuvSizesList.contains(maxStreamingSize) || maxStreamingSizeIsZero);
                    break;
                case CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_CONTINUOUS:
                    // CONTINUOUS: Must be one of supported yuv/private stream sizes.
                    checkTrueForKey(key,
                            String.format("" maxStreamingSize [%d, %d] for extended scene mode "" +
                            ""%d must be a supported YCBCR_420_888 size."",
                            maxStreamingSize.getWidth(), maxStreamingSize.getHeight(),
                            extendedSceneMode), yuvSizesList.contains(maxStreamingSize));
                    // Must be at least 1080p if sensor is at least 1080p.
                    if (activeRect.width() >= FULL_HD.getWidth() &&
                            activeRect.height() >= FULL_HD.getHeight()) {
                        checkTrueForKey(key,
                                String.format("" maxStreamingSize [%d, %d] for extended scene "" +
                                ""mode %d must be at least 1080p"", maxStreamingSize.getWidth(),
                                maxStreamingSize.getHeight(), extendedSceneMode),
                                maxStreamingSize.getWidth() >= FULL_HD.getWidth() &&
                                maxStreamingSize.getHeight() >= FULL_HD.getHeight());
                    }
                    break;
                default:
                    break;
            }
        }

        return caps;
    }

    /**
     * Get and check the available color aberration modes
     *
     * @return the available color aberration modes
     */
    public int[] getAvailableColorAberrationModesChecked() {
        Key<int[]> key =
                CameraCharacteristics.COLOR_CORRECTION_AVAILABLE_ABERRATION_MODES;
        int[] modes = getValueFromKeyNonNull(key);

        if (modes == null) {
            return new int[0];
        }

        List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
        checkTrueForKey(key, "" Camera devices must always support either OFF or FAST mode"",
                modeList.contains(CameraMetadata.COLOR_CORRECTION_ABERRATION_MODE_OFF) ||
                modeList.contains(CameraMetadata.COLOR_CORRECTION_ABERRATION_MODE_FAST));

        if (isHardwareLevelAtLeastLimited()) {
            // FAST and HIGH_QUALITY mode must be both present or both not present
            List<Integer> coupledModes = Arrays.asList(new Integer[] {
                    CameraMetadata.COLOR_CORRECTION_ABERRATION_MODE_FAST,
                    CameraMetadata.COLOR_CORRECTION_ABERRATION_MODE_HIGH_QUALITY
            });
            checkTrueForKey(
                    key, "" FAST and HIGH_QUALITY mode must both present or both not present"",
                    containsAllOrNone(modeList, coupledModes));
        }
        checkElementDistinct(key, modeList);
        checkArrayValuesInRange(key, modes,
                CameraMetadata.COLOR_CORRECTION_ABERRATION_MODE_OFF,
                CameraMetadata.COLOR_CORRECTION_ABERRATION_MODE_HIGH_QUALITY);

        return modes;
    }

    /**
     * Get max pipeline depth and do the validity check.
     *
     * @return max pipeline depth, default value if it is not available.
     */
    public byte getPipelineMaxDepthChecked() {
        Key<Byte> key =
                CameraCharacteristics.REQUEST_PIPELINE_MAX_DEPTH;
        Byte maxDepth = getValueFromKeyNonNull(key);

        if (maxDepth == null) {
            return REQUEST_PIPELINE_MAX_DEPTH_MAX;
        }

        checkTrueForKey(key, "" max pipeline depth should be no larger than ""
                + REQUEST_PIPELINE_MAX_DEPTH_MAX, maxDepth <= REQUEST_PIPELINE_MAX_DEPTH_MAX);

        return maxDepth;
    }

    /**
     * Get available lens shading modes.
     */
     public int[] getAvailableLensShadingModesChecked() {
         Key<int[]> key =
                 CameraCharacteristics.SHADING_AVAILABLE_MODES;
         int[] modes = getValueFromKeyNonNull(key);
         if (modes == null) {
             return new int[0];
         }

         List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));
         // FAST must be included.
         checkTrueForKey(key, "" FAST must be included"",
                 modeList.contains(CameraMetadata.SHADING_MODE_FAST));

         if (isCapabilitySupported(
                 CameraMetadata.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_POST_PROCESSING)) {
             checkTrueForKey(key, "" OFF must be included for MANUAL_POST_PROCESSING devices"",
                     modeList.contains(CameraMetadata.SHADING_MODE_OFF));
         }
         return modes;
     }

     /**
      * Get available lens shading map modes.
      */
      public int[] getAvailableLensShadingMapModesChecked() {
          Key<int[]> key =
                  CameraCharacteristics.STATISTICS_INFO_AVAILABLE_LENS_SHADING_MAP_MODES;
          int[] modes = getValueFromKeyNonNull(key);
          if (modes == null) {
              return new int[0];
          }

          List<Integer> modeList = Arrays.asList(CameraTestUtils.toObject(modes));

          if (isCapabilitySupported(
                  CameraMetadata.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
              checkTrueForKey(key, "" ON must be included for RAW capability devices"",
                      modeList.contains(CameraMetadata.STATISTICS_LENS_SHADING_MAP_MODE_ON));
          }
          return modes;
      }


    /**
     * Get available capabilities and do the validity check.
     *
     * @return reported available capabilities list, empty list if the value is unavailable.
     */
    public List<Integer> getAvailableCapabilitiesChecked() {
        Key<int[]> key =
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES;
        int[] availableCaps = getValueFromKeyNonNull(key);
        List<Integer> capList;

        if (availableCaps == null) {
            return new ArrayList<Integer>();
        }

        checkArrayValuesInRange(key, availableCaps,
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE,
                LAST_CAPABILITY_ENUM);
        capList = Arrays.asList(CameraTestUtils.toObject(availableCaps));
        return capList;
    }

    /**
     * Determine whether the current device supports a capability or not.
     *
     * @param capability (non-negative)
     *
     * @return {@code true} if the capability is supported, {@code false} otherwise.
     *
     * @throws IllegalArgumentException if {@code capability} was negative
     *
     * @see CameraCharacteristics#REQUEST_AVAILABLE_CAPABILITIES
     */
    public boolean isCapabilitySupported(int capability) {
        if (capability < 0) {
            throw new IllegalArgumentException(""capability must be non-negative"");
        }

        List<Integer> availableCapabilities = getAvailableCapabilitiesChecked();

        return availableCapabilities.contains(capability);
    }

    /**
     * Determine whether the current device supports a private reprocessing capability or not.
     *
     * @return {@code true} if the capability is supported, {@code false} otherwise.
     *
     * @throws IllegalArgumentException if {@code capability} was negative
     */
    public boolean isPrivateReprocessingSupported() {
        return isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING);
    }

    /**
     * Get sorted (descending order) size list for given input format. Remove the sizes larger than
     * the bound. If the bound is null, don't do the size bound filtering.
     *
     * @param format input format
     * @param bound maximum allowed size bound
     *
     * @return Sorted input size list (descending order)
     */
    public List<Size> getSortedSizesForInputFormat(int format, Size bound) {
        Size[] availableSizes = getAvailableSizesForFormatChecked(format, StreamDirection.Input);
        if (bound == null) {
            return CameraTestUtils.getAscendingOrderSizes(Arrays.asList(availableSizes),
                    /*ascending*/false);
        }

        List<Size> sizes = new ArrayList<Size>();
        for (Size sz: availableSizes) {
            if (sz.getWidth() <= bound.getWidth() && sz.getHeight() <= bound.getHeight()) {
                sizes.add(sz);
            }
        }

        return CameraTestUtils.getAscendingOrderSizes(sizes, /*ascending*/false);
    }


    /**
     * Determine whether or not all the {@code keys} are available characteristics keys
     * (as in {@link CameraCharacteristics#getKeys}.
     *
     * <p>If this returns {@code true}, then querying for this key from a characteristics
     * object will always return a non-{@code null} value.</p>
     *
     * @param keys collection of camera characteristics keys
     * @return whether or not all characteristics keys are available
     */
    public final boolean areCharacteristicsKeysAvailable(
            Collection<CameraCharacteristics.Key<?>> keys) {
        return mCharacteristics.getKeys().containsAll(keys);
    }

    /**
     * Determine whether or not all the {@code keys} are available result keys
     * (as in {@link CameraCharacteristics#getAvailableCaptureResultKeys}.
     *
     * <p>If this returns {@code true}, then querying for this key from a result
     * object will almost always return a non-{@code null} value.</p>
     *
     * <p>In some cases (e.g. lens shading map), the request must have additional settings
     * configured in order for the key to correspond to a value.</p>
     *
     * @param keys collection of capture result keys
     * @return whether or not all result keys are available
     */
    public final boolean areResultKeysAvailable(Collection<CaptureResult.Key<?>> keys) {
        return mCharacteristics.getAvailableCaptureResultKeys().containsAll(keys);
    }

    /**
     * Determine whether or not all the {@code keys} are available request keys
     * (as in {@link CameraCharacteristics#getAvailableCaptureRequestKeys}.
     *
     * <p>If this returns {@code true}, then setting this key in the request builder
     * may have some effect (and if it's {@code false}, then the camera device will
     * definitely ignore it).</p>
     *
     * <p>In some cases (e.g. manual control of exposure), other keys must be also be set
     * in order for a key to take effect (e.g. control.mode set to OFF).</p>
     *
     * @param keys collection of capture request keys
     * @return whether or not all result keys are available
     */
    public final boolean areRequestKeysAvailable(Collection<CaptureRequest.Key<?>> keys) {
        return mCharacteristics.getAvailableCaptureRequestKeys().containsAll(keys);
    }

    /**
     * Determine whether or not all the {@code keys} are available characteristics keys
     * (as in {@link CameraCharacteristics#getKeys}.
     *
     * <p>If this returns {@code true}, then querying for this key from a characteristics
     * object will always return a non-{@code null} value.</p>
     *
     * @param keys one or more camera characteristic keys
     * @return whether or not all characteristics keys are available
     */
    @SafeVarargs
    public final boolean areKeysAvailable(CameraCharacteristics.Key<?>... keys) {
        return areCharacteristicsKeysAvailable(Arrays.asList(keys));
    }

    /**
     * Determine whether or not all the {@code keys} are available result keys
     * (as in {@link CameraCharacteristics#getAvailableCaptureResultKeys}.
     *
     * <p>If this returns {@code true}, then querying for this key from a result
     * object will almost always return a non-{@code null} value.</p>
     *
     * <p>In some cases (e.g. lens shading map), the request must have additional settings
     * configured in order for the key to correspond to a value.</p>
     *
     * @param keys one or more capture result keys
     * @return whether or not all result keys are available
     */
    @SafeVarargs
    public final boolean areKeysAvailable(CaptureResult.Key<?>... keys) {
        return areResultKeysAvailable(Arrays.asList(keys));
    }

    /**
     * Determine whether or not all the {@code keys} are available request keys
     * (as in {@link CameraCharacteristics#getAvailableCaptureRequestKeys}.
     *
     * <p>If this returns {@code true}, then setting this key in the request builder
     * may have some effect (and if it's {@code false}, then the camera device will
     * definitely ignore it).</p>
     *
     * <p>In some cases (e.g. manual control of exposure), other keys must be also be set
     * in order for a key to take effect (e.g. control.mode set to OFF).</p>
     *
     * @param keys one or more capture request keys
     * @return whether or not all result keys are available
     */
    @SafeVarargs
    public final boolean areKeysAvailable(CaptureRequest.Key<?>... keys) {
        return areRequestKeysAvailable(Arrays.asList(keys));
    }

    /*
     * Determine if camera device support AE lock control
     *
     * @return {@code true} if AE lock control is supported
     */
    public boolean isAeLockSupported() {
        return getValueFromKeyNonNull(CameraCharacteristics.CONTROL_AE_LOCK_AVAILABLE);
    }

    /*
     * Determine if camera device supports keys that must be supported by
     * ULTRA_HIGH_RESOLUTION_SENSORs
     *
     * @return {@code true} if minimum set of keys are supported
     */
    public boolean areMaximumResolutionKeysSupported() {
        return mCharacteristics.get(
                CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION) != null &&
                mCharacteristics.get(
                        SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION) != null &&
                mCharacteristics.get(
                        SENSOR_INFO_PIXEL_ARRAY_SIZE_MAXIMUM_RESOLUTION) != null &&
                mCharacteristics.get(
                        SCALER_STREAM_CONFIGURATION_MAP_MAXIMUM_RESOLUTION) != null;
    }

    /*
     * Determine if camera device support AWB lock control
     *
     * @return {@code true} if AWB lock control is supported
     */
    public boolean isAwbLockSupported() {
        return getValueFromKeyNonNull(CameraCharacteristics.CONTROL_AWB_LOCK_AVAILABLE);
    }


    /*
     * Determine if camera device support manual lens shading map control
     *
     * @return {@code true} if manual lens shading map control is supported
     */
    public boolean isManualLensShadingMapSupported() {
        return areKeysAvailable(CaptureRequest.SHADING_MODE);
    }

    /**
     * Determine if camera device support manual color correction control
     *
     * @return {@code true} if manual color correction control is supported
     */
    public boolean isColorCorrectionSupported() {
        return areKeysAvailable(CaptureRequest.COLOR_CORRECTION_MODE);
    }

    /**
     * Determine if camera device support manual tone mapping control
     *
     * @return {@code true} if manual tone mapping control is supported
     */
    public boolean isManualToneMapSupported() {
        return areKeysAvailable(CaptureRequest.TONEMAP_MODE);
    }

    /**
     * Determine if camera device support manual color aberration control
     *
     * @return {@code true} if manual color aberration control is supported
     */
    public boolean isManualColorAberrationControlSupported() {
        return areKeysAvailable(CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE);
    }

    /**
     * Determine if camera device support edge mode control
     *
     * @return {@code true} if edge mode control is supported
     */
    public boolean isEdgeModeControlSupported() {
        return areKeysAvailable(CaptureRequest.EDGE_MODE);
    }

    /**
     * Determine if camera device support hot pixel mode control
     *
     * @return {@code true} if hot pixel mode control is supported
     */
    public boolean isHotPixelMapModeControlSupported() {
        return areKeysAvailable(CaptureRequest.HOT_PIXEL_MODE);
    }

    /**
     * Determine if camera device support noise reduction mode control
     *
     * @return {@code true} if noise reduction mode control is supported
     */
    public boolean isNoiseReductionModeControlSupported() {
        return areKeysAvailable(CaptureRequest.NOISE_REDUCTION_MODE);
    }

    /**
     * Get max number of output raw streams and do the basic validity check.
     *
     * @return reported max number of raw output stream
     */
    public int getMaxNumOutputStreamsRawChecked() {
        Integer maxNumStreams =
                getValueFromKeyNonNull(CameraCharacteristics.REQUEST_MAX_NUM_OUTPUT_RAW);
        if (maxNumStreams == null)
            return 0;
        return maxNumStreams;
    }

    /**
     * Get max number of output processed streams and do the basic validity check.
     *
     * @return reported max number of processed output stream
     */
    public int getMaxNumOutputStreamsProcessedChecked() {
        Integer maxNumStreams =
                getValueFromKeyNonNull(CameraCharacteristics.REQUEST_MAX_NUM_OUTPUT_PROC);
        if (maxNumStreams == null)
            return 0;
        return maxNumStreams;
    }

    /**
     * Get max number of output stalling processed streams and do the basic validity check.
     *
     * @return reported max number of stalling processed output stream
     */
    public int getMaxNumOutputStreamsProcessedStallChecked() {
        Integer maxNumStreams =
                getValueFromKeyNonNull(CameraCharacteristics.REQUEST_MAX_NUM_OUTPUT_PROC_STALLING);
        if (maxNumStreams == null)
            return 0;
        return maxNumStreams;
    }

    /**
     * Get lens facing and do the validity check
     * @return lens facing, return default value (BACK) if value is unavailable.
     */
    public int getLensFacingChecked() {
        Key<Integer> key =
                CameraCharacteristics.LENS_FACING;
        Integer facing = getValueFromKeyNonNull(key);

        if (facing == null) {
            return CameraCharacteristics.LENS_FACING_BACK;
        }

        checkTrueForKey(key, "" value is out of range "",
                facing >= CameraCharacteristics.LENS_FACING_FRONT &&
                facing <= CameraCharacteristics.LENS_FACING_EXTERNAL);
        return facing;
    }

    /**
     * Get maxCaptureStall frames or default value (if value doesn't exist)
     * @return maxCaptureStall frames or default value.
     */
    public int getMaxCaptureStallOrDefault() {
        Key<Integer> key =
                CameraCharacteristics.REPROCESS_MAX_CAPTURE_STALL;
        Integer value = getValueFromKeyNonNull(key);

        if (value == null) {
            return MAX_REPROCESS_MAX_CAPTURE_STALL;
        }

        checkTrueForKey(key, "" value is out of range "",
                value >= 0 &&
                value <= MAX_REPROCESS_MAX_CAPTURE_STALL);

        return value;
    }

    /**
     * Get the scaler's cropping type (center only or freeform)
     * @return cropping type, return default value (CENTER_ONLY) if value is unavailable
     */
    public int getScalerCroppingTypeChecked() {
        Key<Integer> key =
                CameraCharacteristics.SCALER_CROPPING_TYPE;
        Integer value = getValueFromKeyNonNull(key);

        if (value == null) {
            return CameraCharacteristics.SCALER_CROPPING_TYPE_CENTER_ONLY;
        }

        checkTrueForKey(key, "" value is out of range "",
                value >= CameraCharacteristics.SCALER_CROPPING_TYPE_CENTER_ONLY &&
                value <= CameraCharacteristics.SCALER_CROPPING_TYPE_FREEFORM);

        return value;
    }

    /**
     * Check if the constrained high speed video is supported by the camera device.
     * The high speed FPS ranges and sizes are sanitized in
     * ExtendedCameraCharacteristicsTest#testConstrainedHighSpeedCapability.
     *
     * @return true if the constrained high speed video is supported, false otherwise.
     */
    public boolean isConstrainedHighSpeedVideoSupported() {
        List<Integer> availableCapabilities = getAvailableCapabilitiesChecked();
        return (availableCapabilities.contains(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_CONSTRAINED_HIGH_SPEED_VIDEO));
    }

    /**
     * Check if this camera device is a logical multi-camera backed by multiple
     * physical cameras.
     *
     * @return true if this is a logical multi-camera.
     */
    public boolean isLogicalMultiCamera() {
        List<Integer> availableCapabilities = getAvailableCapabilitiesChecked();
        return (availableCapabilities.contains(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_LOGICAL_MULTI_CAMERA));
    }

    /**
     * Check if this camera device is an ULTRA_HIGH_RESOLUTION_SENSOR
     *
     * @return true if this is an ultra high resolution sensor
     */
    public boolean isUltraHighResolutionSensor() {
        List<Integer> availableCapabilities = getAvailableCapabilitiesChecked();
        return (availableCapabilities.contains(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR));
    }
    /**
     * Check if this camera device is a monochrome camera with Y8 support.
     *
     * @return true if this is a monochrome camera with Y8 support.
     */
    public boolean isMonochromeWithY8() {
        int[] supportedFormats = getAvailableFormats(
                StaticMetadata.StreamDirection.Output);
        return (isColorOutputSupported()
                && isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MONOCHROME)
                && CameraTestUtils.contains(supportedFormats, ImageFormat.Y8));
    }

    /**
     * Check if high speed video is supported (HIGH_SPEED_VIDEO scene mode is
     * supported, supported high speed fps ranges and sizes are valid).
     *
     * @return true if high speed video is supported.
     */
    public boolean isHighSpeedVideoSupported() {
        List<Integer> sceneModes =
                Arrays.asList(CameraTestUtils.toObject(getAvailableSceneModesChecked()));
        if (sceneModes.contains(CameraCharacteristics.CONTROL_SCENE_MODE_HIGH_SPEED_VIDEO)) {
            StreamConfigurationMap config =
                    getValueFromKeyNonNull(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            if (config == null) {
                return false;
            }
            Size[] availableSizes = config.getHighSpeedVideoSizes();
            if (availableSizes.length == 0) {
                return false;
            }

            for (Size size : availableSizes) {
                Range<Integer>[] availableFpsRanges = config.getHighSpeedVideoFpsRangesFor(size);
                if (availableFpsRanges.length == 0) {
                    return false;
                }
            }

            return true;
        } else {
            return false;
        }
    }

    /**
     * Check if depth output is supported, based on the depth capability
     */
    public boolean isDepthOutputSupported() {
        return isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_DEPTH_OUTPUT);
    }

    /**
     * Check if offline processing is supported, based on the respective capability
     */
    public boolean isOfflineProcessingSupported() {
        return isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_OFFLINE_PROCESSING);
    }

    /**
     * Check if standard outputs (PRIVATE, YUV, JPEG) outputs are supported, based on the
     * backwards-compatible capability
     */
    public boolean isColorOutputSupported() {
        return isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE);
    }

    /**
     * Check if this camera is a MONOCHROME camera.
     */
    public boolean isMonochromeCamera() {
        return isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MONOCHROME);
    }

    /**
     * Check if optical black regions key is supported.
     */
    public boolean isOpticalBlackRegionSupported() {
        return areKeysAvailable(CameraCharacteristics.SENSOR_OPTICAL_BLACK_REGIONS);
    }

    /**
     * Check if HEIC format is supported
     */
    public boolean isHeicSupported() {
        int[] formats = getAvailableFormats(StaticMetadata.StreamDirection.Output);
        return CameraTestUtils.contains(formats, ImageFormat.HEIC);
    }

    /**
     * Check if Depth Jpeg format is supported
     */
    public boolean isDepthJpegSupported() {
        int[] formats = getAvailableFormats(StaticMetadata.StreamDirection.Output);
        return CameraTestUtils.contains(formats, ImageFormat.DEPTH_JPEG);
    }

    /**
     * Check if the dynamic black level is supported.
     *
     * <p>
     * Note that: This also indicates if the white level is supported, as dynamic black and white
     * level must be all supported or none of them is supported.
     * </p>
     */
    public boolean isDynamicBlackLevelSupported() {
        return areKeysAvailable(CaptureResult.SENSOR_DYNAMIC_BLACK_LEVEL);
    }

    /**
     * Check if the enable ZSL key is supported.
     */
    public boolean isEnableZslSupported() {
        return areKeysAvailable(CaptureRequest.CONTROL_ENABLE_ZSL);
    }

    /**
     * Check if AF scene change key is supported.
     */
    public boolean isAfSceneChangeSupported() {
        return areKeysAvailable(CaptureResult.CONTROL_AF_SCENE_CHANGE);
    }

    /**
     * Check if OIS data mode is supported.
     */
    public boolean isOisDataModeSupported() {
        int[] availableOisDataModes = mCharacteristics.get(
                CameraCharacteristics.STATISTICS_INFO_AVAILABLE_OIS_DATA_MODES);

        if (availableOisDataModes == null) {
            return false;
        }

        for (int mode : availableOisDataModes) {
            if (mode == CameraMetadata.STATISTICS_OIS_DATA_MODE_ON) {
                return true;
            }
        }

        return false;
    }

    /**
     * Check if rotate and crop is supported
     */
    public boolean isRotateAndCropSupported() {
        int[] availableRotateAndCropModes = mCharacteristics.get(
                CameraCharacteristics.SCALER_AVAILABLE_ROTATE_AND_CROP_MODES);

        if (availableRotateAndCropModes == null) {
            return false;
        }

        for (int mode : availableRotateAndCropModes) {
            if (mode != CameraMetadata.SCALER_ROTATE_AND_CROP_NONE) {
                return true;
            }
        }

        return false;
    }

    /**
     * Check if distortion correction is supported.
     */
    public boolean isDistortionCorrectionSupported() {
        boolean distortionCorrectionSupported = false;
        int[] distortionModes = mCharacteristics.get(
                CameraCharacteristics.DISTORTION_CORRECTION_AVAILABLE_MODES);
        if (distortionModes == null) {
            return false;
        }

        for (int mode : distortionModes) {
            if (mode != CaptureRequest.DISTORTION_CORRECTION_MODE_OFF) {
                return true;
            }
        }

        return false;
    }

    /**
     * Check if active physical camera Id metadata is supported.
     */
    public boolean isActivePhysicalCameraIdSupported() {
        return areKeysAvailable(CaptureResult.LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID);
    }

    /**
     * Get the value in index for a fixed-size array from a given key.
     *
     * <p>If the camera device is incorrectly reporting values, log a warning and return
     * the default value instead.</p>
     *
     * @param key Key to fetch
     * @param defaultValue Default value to return if camera device uses invalid values
     * @param name Human-readable name for the array index (logging only)
     * @param index Array index of the subelement
     * @param size Expected fixed size of the array
     *
     * @return The value reported by the camera device, or the defaultValue otherwise.
     */
    private <T> T getArrayElementOrDefault(Key<?> key, T defaultValue, String name, int index,
            int size) {
        T elementValue = getArrayElementCheckRangeNonNull(
                key,
                index,
                size);

        if (elementValue == null) {
            failKeyCheck(key,
                    ""had no valid "" + name + "" value; using default of "" + defaultValue);
            elementValue = defaultValue;
        }

        return elementValue;
    }

    /**
     * Fetch an array sub-element from an array value given by a key.
     *
     * <p>
     * Prints a warning if the sub-element was null.
     * </p>
     *
     * <p>Use for variable-size arrays since this does not check the array size.</p>
     *
     * @param key Metadata key to look up
     * @param element A non-negative index value.
     * @return The array sub-element, or null if the checking failed.
     */
    private <T> T getArrayElementNonNull(Key<?> key, int element) {
        return getArrayElementCheckRangeNonNull(key, element, IGNORE_SIZE_CHECK);
    }

    /**
     * Fetch an array sub-element from an array value given by a key.
     *
     * <p>
     * Prints a warning if the array size does not match the size, or if the sub-element was null.
     * </p>
     *
     * @param key Metadata key to look up
     * @param element The index in [0,size)
     * @param size A positive size value or otherwise {@value #IGNORE_SIZE_CHECK}
     * @return The array sub-element, or null if the checking failed.
     */
    private <T> T getArrayElementCheckRangeNonNull(Key<?> key, int element, int size) {
        Object array = getValueFromKeyNonNull(key);

        if (array == null) {
            // Warning already printed
            return null;
        }

        if (size != IGNORE_SIZE_CHECK) {
            int actualLength = Array.getLength(array);
            if (actualLength != size) {
                failKeyCheck(key,
                        String.format(""had the wrong number of elements (%d), expected (%d)"",
                                actualLength, size));
                return null;
            }
        }

        @SuppressWarnings(""unchecked"")
        T val = (T) Array.get(array, element);

        if (val == null) {
            failKeyCheck(key, ""had a null element at index"" + element);
            return null;
        }

        return val;
    }

    /**
     * Gets the key, logging warnings for null values.
     */
    public <T> T getValueFromKeyNonNull(Key<T> key) {
        if (key == null) {
            throw new IllegalArgumentException(""key was null"");
        }

        T value = mCharacteristics.get(key);

        if (value == null) {
            failKeyCheck(key, ""was null"");
        }

        return value;
    }

    private void checkArrayValuesInRange(Key<int[]> key, int[] array, int min, int max) {
        for (int value : array) {
            checkTrueForKey(key, String.format("" value is out of range [%d, %d]"", min, max),
                    value <= max && value >= min);
        }
    }

    private void checkArrayValuesInRange(Key<byte[]> key, byte[] array, byte min, byte max) {
        for (byte value : array) {
            checkTrueForKey(key, String.format("" value is out of range [%d, %d]"", min, max),
                    value <= max && value >= min);
        }
    }

    /**
     * Check the uniqueness of the values in a list.
     *
     * @param key The key to be checked
     * @param list The list contains the value of the key
     */
    private <U, T> void checkElementDistinct(Key<U> key, List<T> list) {
        // Each size must be distinct.
        Set<T> sizeSet = new HashSet<T>(list);
        checkTrueForKey(key, ""Each size must be distinct"", sizeSet.size() == list.size());
    }

    private <T> void checkTrueForKey(Key<T> key, String message, boolean condition) {
        if (!condition) {
            failKeyCheck(key, message);
        }
    }

    /* Helper function to check if the coupled modes are either all present or all non-present */
    private <T> boolean containsAllOrNone(Collection<T> observedModes, Collection<T> coupledModes) {
        if (observedModes.containsAll(coupledModes)) {
            return true;
        }
        for (T mode : coupledModes) {
            if (observedModes.contains(mode)) {
                return false;
            }
        }
        return true;
    }

    private <T> void failKeyCheck(Key<T> key, String message) {
        // TODO: Consider only warning once per key/message combination if it's too spammy.
        // TODO: Consider offering other options such as throwing an assertion exception
        String failureCause = String.format(""The static info key '%s' %s"", key.getName(), message);
        switch (mLevel) {
            case WARN:
                Log.w(TAG, failureCause);
                break;
            case COLLECT:
                mCollector.addMessage(failureCause);
                break;
            case ASSERT:
                Assert.fail(failureCause);
            default:
                throw new UnsupportedOperationException(""Unhandled level "" + mLevel);
        }
    }
}"	""	""	"JPEG 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.OfflineSessionTest"	"testInvalidOutput"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/OfflineSessionTest.java"	""	"public void testInvalidOutput() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing camera device "" + mCameraIdsUnderTest[i]);

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isOfflineProcessingSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support offline processing, skipping"");
                    continue;
                }

                openDevice(mCameraIdsUnderTest[i]);

                CaptureRequest.Builder previewRequest =
                        mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
                CaptureRequest.Builder stillCaptureRequest =
                        mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
                Size previewSize = mOrderedPreviewSizes.get(0);
                Size stillSize = mOrderedStillSizes.get(0);
                SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
                SimpleImageReaderListener imageListener = new SimpleImageReaderListener();

                startPreview(previewRequest, previewSize, resultListener);

                CaptureResult result = resultListener.getCaptureResult(WAIT_FOR_FRAMES_TIMEOUT_MS);

                Long timestamp = result.get(CaptureResult.SENSOR_TIMESTAMP);
                assertNotNull(""Can't read a capture result timestamp"", timestamp);

                CaptureResult result2 = resultListener.getCaptureResult(WAIT_FOR_FRAMES_TIMEOUT_MS);

                Long timestamp2 = result2.get(CaptureResult.SENSOR_TIMESTAMP);
                assertNotNull(""Can't read a capture result 2 timestamp"", timestamp2);

                assertTrue(""Bad timestamps"", timestamp2 > timestamp);

                createImageReader(stillSize, ImageFormat.JPEG, MAX_READER_IMAGES, imageListener);

                BlockingOfflineSessionCallback offlineCb = new BlockingOfflineSessionCallback();

                try {
                    ArrayList<Surface> offlineSurfaces = new ArrayList<Surface>();
                    offlineSurfaces.add(mReaderSurface);
                    mSession.switchToOffline(offlineSurfaces, new HandlerExecutor(mHandler),
                            offlineCb);
                    fail(""Offline session switch accepts unregistered output surface"");
                } catch (IllegalArgumentException e) {
                    //Expected
                }

                if (mSession.supportsOfflineProcessing(mPreviewSurface)) {
                    ArrayList<Surface> offlineSurfaces = new ArrayList<Surface>();
                    offlineSurfaces.add(mPreviewSurface);
                    mSession.switchToOffline(offlineSurfaces, new HandlerExecutor(mHandler),
                            offlineCb);
                    // We only have a single repeating request, in this case the camera
                    // implementation should fail to find any capture requests that can
                    // be migrated to offline mode and notify the failure accordingly.
                    offlineCb.waitForState(BlockingOfflineSessionCallback.STATE_SWITCH_FAILED,
                            WAIT_FOR_STATE_TIMEOUT_MS);
                } else {
                    stopPreview();
                }

                closeImageReader();
            } finally {
                closeDevice();
            }
        }
    }

    /**
     * Test camera callback sequence during and after offline session switch.
     *
     * <p>Camera clients must receive respective capture results or failures for all
     * non-offline outputs after the offline switch call returns.
     * In case the switch was successful clients must be notified about the
     * remaining offline requests via the registered offline callback.</p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.OfflineSessionTest"	"testOfflineCallbacks"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/OfflineSessionTest.java"	""	"public void testOfflineCallbacks() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing camera2 API for camera device "" + mCameraIdsUnderTest[i]);

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isOfflineProcessingSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support offline processing, skipping"");
                    continue;
                }

                openDevice(mCameraIdsUnderTest[i]);
                camera2OfflineSessionTest(mCameraIdsUnderTest[i], mOrderedStillSizes.get(0),
                        ImageFormat.JPEG, OfflineTestSequence.NoExtraSteps);
            } finally {
                closeDevice();
            }
        }
    }

    /**
     * Test camera offline session behavior in case of depth jpeg output.
     *
     * <p>Verify that offline session and callbacks behave as expected
     * in case the camera supports offline depth jpeg output.</p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.OfflineSessionTest"	"testOfflineDepthJpeg"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/OfflineSessionTest.java"	""	"public void testOfflineDepthJpeg() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing camera2 API for camera device "" + mCameraIdsUnderTest[i]);

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isOfflineProcessingSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support offline processing, skipping"");
                    continue;
                }

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isDepthJpegSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support depth jpeg, skipping"");
                    continue;
                }

                List<Size> depthJpegSizes = CameraTestUtils.getSortedSizesForFormat(
                        mCameraIdsUnderTest[i], mCameraManager, ImageFormat.DEPTH_JPEG,
                        null /*bound*/);
                openDevice(mCameraIdsUnderTest[i]);
                camera2OfflineSessionTest(mCameraIdsUnderTest[i], depthJpegSizes.get(0),
                        ImageFormat.DEPTH_JPEG, OfflineTestSequence.NoExtraSteps);
            } finally {
                closeDevice();
            }
        }
    }

    /**
     * Test camera offline session behavior in case of HEIC output.
     *
     * <p>Verify that offline session and callbacks behave as expected
     * in case the camera supports offline HEIC output.</p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.OfflineSessionTest"	"testDeviceCloseAndOpen"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/OfflineSessionTest.java"	""	"public void testDeviceCloseAndOpen() throws Exception {
        ErrorLoggingService.ErrorServiceConnection errorConnection =
                new ErrorLoggingService.ErrorServiceConnection(mContext);

        errorConnection.start();
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing camera2 API for camera device "" + mCameraIdsUnderTest[i]);

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isOfflineProcessingSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support offline processing, skipping"");
                    continue;
                }

                openDevice(mCameraIdsUnderTest[i]);
                if (camera2OfflineSessionTest(mCameraIdsUnderTest[i], mOrderedStillSizes.get(0),
                            ImageFormat.JPEG, OfflineTestSequence.CloseDeviceAndOpenRemote)) {
                    // Verify that the remote camera was opened correctly
                    List<ErrorLoggingService.LogEvent> allEvents = null;
                    try {
                        allEvents = errorConnection.getLog(WAIT_FOR_STATE_TIMEOUT_MS,
                                TestConstants.EVENT_CAMERA_CONNECT);
                    } catch (TimeoutException e) {
                        fail(""Timed out waiting on remote offline process error log!"");
                    }
                    assertNotNull(""Failed to connect to camera device in remote offline process!"",
                            allEvents);
                }
            } finally {
                closeDevice();

            }
        }

        errorConnection.stop();
    }

    /**
     * Test camera offline session behavior during close.
     *
     * <p>Verify that clients are able to close an offline session and receive
     * all corresponding callbacks according to the documentation.</p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.OfflineSessionTest"	"testOfflineSessionClose"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/OfflineSessionTest.java"	""	"public void testOfflineSessionClose() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing camera2 API for camera device "" + mCameraIdsUnderTest[i]);

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isOfflineProcessingSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support offline processing, skipping"");
                    continue;
                }

                openDevice(mCameraIdsUnderTest[i]);
                camera2OfflineSessionTest(mCameraIdsUnderTest[i], mOrderedStillSizes.get(0),
                        ImageFormat.JPEG, OfflineTestSequence.CloseOfflineSession);
            } finally {
                closeDevice();
            }
        }
    }

    /**
     * Test camera offline session in case of new capture session
     *
     * <p>Verify that clients are able to initialize a new regular capture session
     * in parallel with the offline session.</p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.OfflineSessionTest"	"testOfflineSessionWithRegularSession"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/OfflineSessionTest.java"	""	"public void testOfflineSessionWithRegularSession() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing camera2 API for camera device "" + mCameraIdsUnderTest[i]);

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isOfflineProcessingSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support offline processing, skipping"");
                    continue;
                }

                openDevice(mCameraIdsUnderTest[i]);
                camera2OfflineSessionTest(mCameraIdsUnderTest[i], mOrderedStillSizes.get(0),
                        ImageFormat.JPEG, OfflineTestSequence.InitializeRegularSession);
            } finally {
                closeDevice();
            }
        }
    }

    /**
     * Test for aborted repeating sequences when switching to offline mode
     *
     * <p>Verify that clients receive the expected sequence abort callbacks when switching
     * to offline session.</p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.OfflineSessionTest"	"testRepeatingSequenceAbort"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/OfflineSessionTest.java"	""	"public void testRepeatingSequenceAbort() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing camera2 API for camera device "" + mCameraIdsUnderTest[i]);

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isOfflineProcessingSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support offline processing, skipping"");
                    continue;
                }

                openDevice(mCameraIdsUnderTest[i]);
                camera2OfflineSessionTest(mCameraIdsUnderTest[i], mOrderedStillSizes.get(0),
                        ImageFormat.JPEG, OfflineTestSequence.RepeatingSequenceAbort);
            } finally {
                closeDevice();
            }
        }
    }

    /**
     * Test that both shared and surface group outputs are not advertised as
     * capable working in offline mode.
     *
     * <p>Both shared and surface group outputs cannot be switched to offline mode.
     * Make sure that both cases are correctly advertised and switching to offline
     * mode is failing as expected.</p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.OfflineSessionTest"	"testUnsupportedOfflineSessionOutputs"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/OfflineSessionTest.java"	""	"public void testUnsupportedOfflineSessionOutputs() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing camera2 API for camera device "" + mCameraIdsUnderTest[i]);

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isOfflineProcessingSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support offline processing, skipping"");
                    continue;
                }

                openDevice(mCameraIdsUnderTest[i]);
                camera2UnsupportedOfflineOutputTest(true /*useSurfaceGroup*/);
                camera2UnsupportedOfflineOutputTest(false /*useSurfaceGroup*/);
            } finally {
                closeDevice();
            }
        }
    }

    private void checkForSequenceAbort(SimpleCaptureCallback resultListener, int sequenceId) {
        ArrayList<Integer> abortedSeq = resultListener.geAbortedSequences(
                1 /*maxNumbAborts*/);
        assertNotNull(""No aborted capture sequence ids present"", abortedSeq);
        assertTrue(""Unexpected number of aborted capture sequence ids : "" +
                abortedSeq.size() + "" expected 1"", abortedSeq.size() == 1);
        assertTrue(""Unexpected abort capture sequence id: "" +
                abortedSeq.get(0).intValue() + "" expected capture sequence id: "" +
                sequenceId, abortedSeq.get(0).intValue() == sequenceId);
    }

    private void verifyCaptureResults(SimpleCaptureCallback resultListener,
            SimpleImageReaderListener imageListener, int sequenceId, boolean offlineResults)
            throws Exception {
        long sequenceLastFrameNumber = resultListener.getCaptureSequenceLastFrameNumber(
                sequenceId, 0 /*timeoutMs*/);

        long lastFrameNumberReceived = -1;
        while (resultListener.hasMoreResults()) {
            TotalCaptureResult result = resultListener.getTotalCaptureResult(0 /*timeout*/);
            if (lastFrameNumberReceived < result.getFrameNumber()) {
                lastFrameNumberReceived = result.getFrameNumber();
            }

            if (imageListener != null) {
                long resultTimestamp = result.get(CaptureResult.SENSOR_TIMESTAMP);
                Image offlineImage = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                assertEquals(""Offline image timestamp: "" + offlineImage.getTimestamp() +
                        "" doesn't match with the result timestamp: "" + resultTimestamp,
                        offlineImage.getTimestamp(), resultTimestamp);
            }
        }

        while (resultListener.hasMoreFailures()) {
            ArrayList<CaptureFailure> failures = resultListener.getCaptureFailures(
                    /*maxNumFailures*/ 1);
            for (CaptureFailure failure : failures) {
                if (lastFrameNumberReceived < failure.getFrameNumber()) {
                    lastFrameNumberReceived = failure.getFrameNumber();
                }
            }
        }

        String assertString = offlineResults ?
                ""Last offline frame number from "" +
                ""onCaptureSequenceCompleted (%d) doesn't match the last frame number "" +
                ""received from results/failures (%d)"" :
                ""Last frame number from onCaptureSequenceCompleted "" +
                ""(%d) doesn't match the last frame number received from "" +
                ""results/failures (%d)"";
        assertEquals(String.format(assertString, sequenceLastFrameNumber, lastFrameNumberReceived),
                sequenceLastFrameNumber, lastFrameNumberReceived);
    }

    /**
     * Verify offline session behavior during common use cases
     *
     * @param cameraId      Id of the camera device under test
     * @param offlineSize   The offline surface size
     * @param offlineFormat The offline surface pixel format
     * @param testSequence  Specific scenario to be verified
     * @return true if the offline session switch is successful, false if there is any failure.
     */
    private boolean camera2OfflineSessionTest(String cameraId, Size offlineSize, int offlineFormat,
            OfflineTestSequence testSequence) throws Exception {
        boolean ret = false;
        int remoteOfflinePID = -1;
        Size previewSize = mOrderedPreviewSizes.get(0);
        for (Size sz : mOrderedPreviewSizes) {
            if (sz.getWidth() <= MANDATORY_STREAM_BOUND.getWidth() && sz.getHeight() <=
                    MANDATORY_STREAM_BOUND.getHeight()) {
                previewSize = sz;
                break;
            }
        }
        Size privateSize = previewSize;
        if (mAllStaticInfo.get(cameraId).isPrivateReprocessingSupported()) {
            privateSize = mAllStaticInfo.get(cameraId).getSortedSizesForInputFormat(
                    ImageFormat.PRIVATE, MANDATORY_STREAM_BOUND).get(0);
        }

        CaptureRequest.Builder previewRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder stillCaptureRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleCaptureCallback regularResultListener = new SimpleCaptureCallback();
        SimpleCaptureCallback offlineResultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
        ImageReader privateReader = null;
        ImageReader yuvCallbackReader = null;
        ImageReader jpegReader = null;
        int repeatingSeqId = -1;

        // Update preview size.
        updatePreviewSurface(previewSize);

        // Create ImageReader.
        createImageReader(offlineSize, offlineFormat, MAX_READER_IMAGES, imageListener);

        // Configure output streams with preview and offline streams.
        ArrayList<Surface> outputSurfaces = new ArrayList<Surface>();
        outputSurfaces.add(mPreviewSurface);
        outputSurfaces.add(mReaderSurface);
        final CameraCaptureSession.StateCallback sessionCb = mock(
                CameraCaptureSession.StateCallback.class);
        mSessionListener = new BlockingSessionCallback(sessionCb);
        mSession = configureCameraSession(mCamera, outputSurfaces, mSessionListener, mHandler);

        if (!mSession.supportsOfflineProcessing(mReaderSurface)) {
            Log.i(TAG, ""Camera does not support offline processing for still capture output"");
            return false;
        }

        // Configure the requests.
        previewRequest.addTarget(mPreviewSurface);
        stillCaptureRequest.addTarget(mReaderSurface);


        ArrayList<Integer> allowedOfflineStates = new ArrayList<Integer>();
        allowedOfflineStates.add(BlockingOfflineSessionCallback.STATE_READY);
        allowedOfflineStates.add(BlockingOfflineSessionCallback.STATE_SWITCH_FAILED);
        ArrayList<Surface> offlineSurfaces = new ArrayList<Surface>();
        offlineSurfaces.add(mReaderSurface);
        final CameraOfflineSessionCallback mockOfflineCb = mock(CameraOfflineSessionCallback.class);
        BlockingOfflineSessionCallback offlineCb = new BlockingOfflineSessionCallback(
                mockOfflineCb);
        ArrayList<CaptureRequest> offlineRequestList = new ArrayList<CaptureRequest>();
        for (int i = 0; i < MAX_READER_IMAGES; i++) {
            offlineRequestList.add(stillCaptureRequest.build());
        }

        if (testSequence != OfflineTestSequence.RepeatingSequenceAbort) {
            repeatingSeqId = mSession.setRepeatingRequest(previewRequest.build(), resultListener,
                    mHandler);
            checkInitialResults(resultListener);
        }

        int offlineSeqId = mSession.captureBurst(offlineRequestList, offlineResultListener,
                mHandler);

        if (testSequence == OfflineTestSequence.RepeatingSequenceAbort) {
            // Submit the preview repeating request after the offline burst so it can be delayed
            // long enough and fail to reach the camera processing pipeline.
            repeatingSeqId = mSession.setRepeatingRequest(previewRequest.build(), resultListener,
                    mHandler);
        }

        CameraOfflineSession offlineSession = mSession.switchToOffline(offlineSurfaces,
                new HandlerExecutor(mHandler), offlineCb);
        assertNotNull(""Invalid offline session"", offlineSession);

        // The regular capture session must be closed as well
        verify(sessionCb, times(1)).onClosed(mSession);

        int offlineState = offlineCb.waitForAnyOfStates(allowedOfflineStates,
                WAIT_FOR_STATE_TIMEOUT_MS);
        if (offlineState == BlockingOfflineSessionCallback.STATE_SWITCH_FAILED) {
            // A failure during offline mode switch is only allowed in case the switch gets
            // triggered too late without pending offline requests.
            verify(mockOfflineCb, times(1)).onSwitchFailed(offlineSession);
            verify(mockOfflineCb, times(0)).onReady(offlineSession);
            verify(mockOfflineCb, times(0)).onIdle(offlineSession);
            verify(mockOfflineCb, times(0)).onError(offlineSession,
                    CameraOfflineSessionCallback.STATUS_INTERNAL_ERROR);

            try {
                verifyCaptureResults(resultListener, null /*imageListener*/, repeatingSeqId,
                        false /*offlineResults*/);
            } catch (AssertionFailedError e) {
                if (testSequence == OfflineTestSequence.RepeatingSequenceAbort) {
                    checkForSequenceAbort(resultListener, repeatingSeqId);
                } else {
                    throw e;
                }
            }
            verifyCaptureResults(offlineResultListener, null /*imageListener*/, offlineSeqId,
                    true /*offlineResults*/);
        } else {
            verify(mockOfflineCb, times(1)).onReady(offlineSession);
            verify(mockOfflineCb, times(0)).onSwitchFailed(offlineSession);

            switch (testSequence) {
                case RepeatingSequenceAbort:
                    checkForSequenceAbort(resultListener, repeatingSeqId);
                    break;
                case CloseDeviceAndOpenRemote:
                    // According to the documentation, closing the initial camera device and
                    // re-opening the same device from a different client after successful
                    // offline session switch must not have any noticeable impact on the
                    // offline processing.
                    closeDevice();
                    remoteOfflinePID = startRemoteOfflineTestProcess(cameraId);

                    break;
                case CloseOfflineSession:
                    offlineSession.close();

                    break;
                case InitializeRegularSession:
                    // According to the documentation, initializing a regular capture session
                    // along with the offline session should not have any side effects.
                    // We also don't want to re-use the same offline output surface as part
                    // of the new  regular capture session.
                    outputSurfaces.remove(mReaderSurface);

                    // According to the specification, an active offline session must allow
                    // camera devices to support at least one preview stream, one yuv stream
                    // of size up-to 1080p, one jpeg stream with any supported size and
                    // an extra input/output private pair in case reprocessing is also available.
                    yuvCallbackReader = makeImageReader(previewSize, ImageFormat.YUV_420_888,
                            1 /*maxNumImages*/, new SimpleImageReaderListener(), mHandler);
                    outputSurfaces.add(yuvCallbackReader.getSurface());

                    jpegReader = makeImageReader(offlineSize, ImageFormat.JPEG,
                            1 /*maxNumImages*/, new SimpleImageReaderListener(), mHandler);
                    outputSurfaces.add(jpegReader.getSurface());

                    if (mAllStaticInfo.get(cameraId).isPrivateReprocessingSupported()) {
                        privateReader = makeImageReader(privateSize, ImageFormat.PRIVATE,
                                1 /*maxNumImages*/, new SimpleImageReaderListener(), mHandler);
                        outputSurfaces.add(privateReader.getSurface());

                        InputConfiguration inputConfig = new InputConfiguration(
                                privateSize.getWidth(), privateSize.getHeight(),
                                ImageFormat.PRIVATE);
                        mSession = CameraTestUtils.configureReprocessableCameraSession(mCamera,
                                inputConfig, outputSurfaces, mSessionListener, mHandler);

                    } else {
                        mSession = configureCameraSession(mCamera, outputSurfaces, mSessionListener,
                                mHandler);
                    }

                    mSession.setRepeatingRequest(previewRequest.build(), regularResultListener,
                            mHandler);

                    break;
                case NoExtraSteps:
                default:
            }

            if (testSequence != OfflineTestSequence.RepeatingSequenceAbort) {
                // The repeating non-offline request should be done after the switch returns.
                verifyCaptureResults(resultListener, null /*imageListener*/, repeatingSeqId,
                        false /*offlineResults*/);
            }

            if (testSequence != OfflineTestSequence.CloseOfflineSession) {
                offlineCb.waitForState(BlockingOfflineSessionCallback.STATE_IDLE,
                        WAIT_FOR_STATE_TIMEOUT_MS);
                verify(mockOfflineCb, times(1)).onIdle(offlineSession);
                verify(mockOfflineCb, times(0)).onError(offlineSession,
                        CameraOfflineSessionCallback.STATUS_INTERNAL_ERROR);

                // The offline requests should be done after we reach idle state.
                verifyCaptureResults(offlineResultListener, imageListener, offlineSeqId,
                        true /*offlineResults*/);

                offlineSession.close();
            }

            if (testSequence == OfflineTestSequence.InitializeRegularSession) {
                checkInitialResults(regularResultListener);
                stopPreview();

                if (privateReader != null) {
                    privateReader.close();
                }

                if (yuvCallbackReader != null) {
                    yuvCallbackReader.close();
                }

                if (jpegReader != null) {
                    jpegReader.close();
                }
            }

            offlineCb.waitForState(BlockingOfflineSessionCallback.STATE_CLOSED,
                  WAIT_FOR_STATE_TIMEOUT_MS);
            verify(mockOfflineCb, times(1)).onClosed(offlineSession);

            ret = true;
        }

        closeImageReader();

        stopRemoteOfflineTestProcess(remoteOfflinePID);

        return ret;
    }

    private void checkInitialResults(SimpleCaptureCallback resultListener) {
        CaptureResult result = resultListener.getCaptureResult(WAIT_FOR_FRAMES_TIMEOUT_MS);

        Long timestamp = result.get(CaptureResult.SENSOR_TIMESTAMP);
        assertNotNull(""Can't read a capture result timestamp"", timestamp);

        CaptureResult result2 = resultListener.getCaptureResult(WAIT_FOR_FRAMES_TIMEOUT_MS);

        Long timestamp2 = result2.get(CaptureResult.SENSOR_TIMESTAMP);
        assertNotNull(""Can't read a capture result 2 timestamp"", timestamp2);

        assertTrue(""Bad timestamps"", timestamp2 > timestamp);
    }

    private void camera2UnsupportedOfflineOutputTest(boolean useSurfaceGroup) throws Exception {
        CaptureRequest.Builder previewRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        Size previewSize = mOrderedPreviewSizes.get(0);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        updatePreviewSurface(previewSize);

        OutputConfiguration outConfig;
        if (useSurfaceGroup) {
            outConfig = new OutputConfiguration(1 /*surfaceGroupId*/, mPreviewSurface);
        } else {
            outConfig = new OutputConfiguration(mPreviewSurface);
            outConfig.enableSurfaceSharing();
        }

        ArrayList<OutputConfiguration> outputList = new ArrayList<OutputConfiguration>();
        outputList.add(outConfig);
        BlockingSessionCallback sessionListener = new BlockingSessionCallback();
        mCamera.createCaptureSessionByOutputConfigurations(outputList, sessionListener, mHandler);
        CameraCaptureSession session = sessionListener.waitAndGetSession(
                SESSION_CONFIGURE_TIMEOUT_MS);

        assertFalse(useSurfaceGroup ? ""Group surface outputs cannot support offline mode"" :
                ""Shared surface outputs cannot support offline mode"",
                session.supportsOfflineProcessing(mPreviewSurface));

        ArrayList<CaptureRequest> offlineRequestList = new ArrayList<CaptureRequest>();
        previewRequest.addTarget(mPreviewSurface);
        for (int i = 0; i < MAX_READER_IMAGES; i++) {
            offlineRequestList.add(previewRequest.build());
        }

        final CameraOfflineSessionCallback offlineCb = mock(CameraOfflineSessionCallback.class);
        ArrayList<Surface> offlineSurfaces = new ArrayList<Surface>();
        offlineSurfaces.add(mPreviewSurface);
        session.captureBurst(offlineRequestList, resultListener, mHandler);
        try {
            session.switchToOffline(offlineSurfaces, new HandlerExecutor(mHandler), offlineCb);
            fail(useSurfaceGroup ? ""Group surface outputs cannot be switched to offline mode"" :
                ""Shared surface outputs cannot be switched to offline mode"");
        } catch (IllegalArgumentException e) {
            // Expected
        }

        session.close();
    }

    private int startRemoteOfflineTestProcess(String cameraId) throws InterruptedException {
        // Ensure no running activity process with same name
        String cameraActivityName = mContext.getPackageName() + "":"" + REMOTE_PROCESS_NAME;
        ActivityManager activityManager = (ActivityManager) mContext.getSystemService(
                Context.ACTIVITY_SERVICE);
        List<ActivityManager.RunningAppProcessInfo> list = activityManager.getRunningAppProcesses();
        for (ActivityManager.RunningAppProcessInfo rai : list) {
            if (cameraActivityName.equals(rai.processName)) {
                fail(""Remote offline session test activity already running"");
                return -1;
            }
        }

        Activity activity = mActivityRule.getActivity();
        Intent activityIntent = new Intent(activity, REMOTE_PROCESS_CLASS);
        Bundle b = new Bundle();
        b.putString(CameraTestUtils.OFFLINE_CAMERA_ID, cameraId);
        activityIntent.putExtras(b);
        activity.startActivity(activityIntent);
        Thread.sleep(WAIT_FOR_REMOTE_ACTIVITY_LAUNCH_MS);

        // Fail if activity isn't running
        list = activityManager.getRunningAppProcesses();
        for (ActivityManager.RunningAppProcessInfo rai : list) {
            if (cameraActivityName.equals(rai.processName))
                return rai.pid;
        }

        fail(""Remote offline session test activity failed to start"");

        return -1;
    }

    private void stopRemoteOfflineTestProcess(int remotePID) throws InterruptedException {
        if (remotePID < 0) {
            return;
        }

        android.os.Process.killProcess(remotePID);
        Thread.sleep(WAIT_FOR_REMOTE_ACTIVITY_DESTROY_MS);

        ActivityManager activityManager = (ActivityManager) mContext.getSystemService(
                Context.ACTIVITY_SERVICE);
        String cameraActivityName = mContext.getPackageName() + "":"" + REMOTE_PROCESS_NAME;
        List<ActivityManager.RunningAppProcessInfo> list = activityManager.getRunningAppProcesses();
        for (ActivityManager.RunningAppProcessInfo rai : list) {
            if (cameraActivityName.equals(rai.processName))
                fail(""Remote offline session test activity is still running"");
        }

    }
}"	""	""	"JPEG 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ZoomCaptureTest"	"testJpegZoomCapture"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ZoomCaptureTest.java"	""	"public void testJpegZoomCapture() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.v(TAG, ""Testing jpeg zoom capture for Camera "" + id);
                openDevice(id);
                bufferFormatZoomTestByCamera(ImageFormat.JPEG);
            } finally {
                closeDevice(id);
            }
        }
    }"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.StillCaptureTest"	"testJpegExif"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/StillCaptureTest.java"	""	"public void testJpegExif() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing JPEG exif for Camera "" + mCameraIdsUnderTest[i]);
                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                openDevice(mCameraIdsUnderTest[i]);
                Size maxJpegSize = mOrderedStillSizes.get(0);
                stillExifTestByCamera(ImageFormat.JPEG, maxJpegSize);
            } finally {
                closeDevice();
                closeImageReader();
            }
        }
    }

    /**
     * Test HEIC capture exif fields for each camera.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.StillCaptureTest"	"testDynamicDepthCapture"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/StillCaptureTest.java"	""	"public void testDynamicDepthCapture() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing dynamic depth for Camera "" + mCameraIdsUnderTest[i]);
                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isDepthJpegSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support dynamic depth, skipping"");
                    continue;
                }

                openDevice(mCameraIdsUnderTest[i]);

                // Check the maximum supported size.
                List<Size> orderedDepthJpegSizes = CameraTestUtils.getSortedSizesForFormat(
                        mCameraIdsUnderTest[i], mCameraManager, ImageFormat.DEPTH_JPEG, null/*bound*/);
                Size maxDepthJpegSize = orderedDepthJpegSizes.get(0);
                stillDynamicDepthTestByCamera(ImageFormat.DEPTH_JPEG, maxDepthJpegSize);
            } finally {
                closeDevice();
                closeImageReader();
            }
        }
    }

    /**
     * Test normal still capture sequence.
     * <p>
     * Preview and jpeg output streams are configured. Max still capture
     * size is used for jpeg capture. The sequence of still capture being test
     * is: start preview, auto focus, precapture metering (if AE is not
     * converged), then capture jpeg. The AWB and AE are in auto modes. AF mode
     * is CONTINUOUS_PICTURE.
     * </p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.StillCaptureTest"	"testTakePicture"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/StillCaptureTest.java"	""	"public void testTakePicture() throws Exception{
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.i(TAG, ""Testing basic take picture for Camera "" + id);
                if (!mAllStaticInfo.get(id).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" does not support color outputs, skipping"");
                    continue;
                }
                openDevice(id);
                takePictureTestByCamera(/*aeRegions*/null, /*awbRegions*/null, /*afRegions*/null);
            } finally {
                closeDevice();
                closeImageReader();
            }
        }
    }

    /**
     * Test ZSL still capture sequence.
     * <p>
     * Preview and jpeg output streams are configured. Max still capture
     * size is used for jpeg capture. The sequence of still capture being test
     * is: start preview, auto focus, precapture metering (if AE is not
     * converged), then capture jpeg. The AWB and AE are in auto modes. AF mode
     * is CONTINUOUS_PICTURE. Same as testTakePicture, but with enableZSL set.
     * </p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.StillCaptureTest"	"testBasicRawZslCapture"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/StillCaptureTest.java"	""	"public void testBasicRawZslCapture()  throws Exception {
       for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
           try {
               Log.i(TAG, ""Testing raw ZSL capture for Camera "" + mCameraIdsUnderTest[i]);

               if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isCapabilitySupported(
                       CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                   Log.i(TAG, ""RAW capability is not supported in camera "" + mCameraIdsUnderTest[i] +
                           "". Skip the test."");
                   continue;
               }
               openDevice(mCameraIdsUnderTest[i]);
               CaptureRequest.Builder stillRequest =
                       mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
               stillRequest.set(CaptureRequest.CONTROL_ENABLE_ZSL, true);
               rawCaptureTestByCamera(stillRequest);
           } finally {
               closeDevice();
               closeImageReader();
           }
       }
    }


    /**
     * Test the full raw capture use case.
     *
     * This includes:
     * - Configuring the camera with a preview, jpeg, and raw output stream.
     * - Running preview until AE/AF can settle.
     * - Capturing with a request targeting all three output streams.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.StillCaptureTest"	"testFullRawCapture"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/StillCaptureTest.java"	""	"public void testFullRawCapture() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing raw+JPEG capture for Camera "" + mCameraIdsUnderTest[i]);
                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                    Log.i(TAG, ""RAW capability is not supported in camera "" + mCameraIdsUnderTest[i] +
                            "". Skip the test."");
                    continue;
                }

                openDevice(mCameraIdsUnderTest[i]);
                fullRawCaptureTestByCamera(/*stillRequest*/null);
            } finally {
                closeDevice();
                closeImageReader();
            }
        }
    }

    /**
     * Test the full raw capture ZSL use case.
     *
     * This includes:
     * - Configuring the camera with a preview, jpeg, and raw output stream.
     * - Running preview until AE/AF can settle.
     * - Capturing with a request targeting all three output streams.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.StillCaptureTest"	"testFullRawZSLCapture"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/StillCaptureTest.java"	""	"public void testFullRawZSLCapture() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing raw+JPEG ZSL capture for Camera "" + mCameraIdsUnderTest[i]);
                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                    Log.i(TAG, ""RAW capability is not supported in camera "" + mCameraIdsUnderTest[i] +
                            "". Skip the test."");
                    continue;
                }
                openDevice(mCameraIdsUnderTest[i]);
                CaptureRequest.Builder stillRequest =
                        mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
                stillRequest.set(CaptureRequest.CONTROL_ENABLE_ZSL, true);
                fullRawCaptureTestByCamera(stillRequest);
            } finally {
                closeDevice();
                closeImageReader();
            }
        }
    }

    /**
     * Test touch for focus.
     * <p>
     * AF is in CAF mode when preview is started, test uses several pre-selected
     * regions to simulate touches. Active scan is triggered to make sure the AF
     * converges in reasonable time.
     * </p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.StillCaptureTest"	"testTouchForFocus"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/StillCaptureTest.java"	""	"public void testTouchForFocus() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.i(TAG, ""Testing touch for focus for Camera "" + id);
                StaticMetadata staticInfo = mAllStaticInfo.get(id);
                int maxAfRegions = staticInfo.getAfMaxRegionsChecked();
                if (!(staticInfo.hasFocuser() && maxAfRegions > 0)) {
                    continue;
                }
                // TODO: Relax test to use non-SurfaceView output for depth cases
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" does not support color outputs, skipping"");
                    continue;
                }
                openDevice(id);
                touchForFocusTestByCamera();
            } finally {
                closeDevice();
                closeImageReader();
            }
        }
    }

    /**
     * Test all combination of available preview sizes and still sizes.
     * <p>
     * For each still capture, Only the jpeg buffer is validated, capture
     * result validation is covered by {@link #stillExifTestByCamera} test.
     * </p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.StillCaptureTest"	"testAePrecaptureTriggerCancelJpegCapture"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/StillCaptureTest.java"	""	"public void testAePrecaptureTriggerCancelJpegCapture() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.i(TAG, ""Testing AE precapture cancel for jpeg capture for Camera "" + id);

                StaticMetadata staticInfo = mAllStaticInfo.get(id);
                // Legacy device doesn't support AE precapture trigger
                if (staticInfo.isHardwareLevelLegacy()) {
                    Log.i(TAG, ""Skipping AE precapture trigger cancel test on legacy devices"");
                    continue;
                }
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" does not support color outputs, skipping"");
                    continue;
                }
                openDevice(id);
                takePictureTestByCamera(/*aeRegions*/null, /*awbRegions*/null, /*afRegions*/null,
                        /*addAeTriggerCancel*/true, /*allocateBitmap*/false,
                        /*previewRequest*/null, /*stillRequest*/null);
            } finally {
                closeDevice();
                closeImageReader();
            }
        }
    }

    /**
     * Test allocate some bitmaps while taking picture.
     * <p>
     * Per android CDD (5.0 and newer), android devices should support allocation of at least 3
     * bitmaps equal to the size of the images produced by the largest resolution camera sensor on
     * the devices.
     * </p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.StillCaptureTest"	"testFocalLengths"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/StillCaptureTest.java"	""	"public void testFocalLengths() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                StaticMetadata staticInfo = mAllStaticInfo.get(id);
                if (staticInfo.isHardwareLevelLegacy()) {
                    Log.i(TAG, ""Camera "" + id + "" is legacy, skipping"");
                    continue;
                }
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" does not support color outputs, skipping"");
                    continue;
                }
                if (staticInfo.isExternalCamera()) {
                    Log.i(TAG, ""Camera "" + id + "" is external, skipping"");
                    continue;
                }
                openDevice(id);
                focalLengthTestByCamera();
            } finally {
                closeDevice();
                closeImageReader();
            }
        }
    }

    private void focalLengthTestByCamera() throws Exception {
        float[] focalLengths = mStaticInfo.getAvailableFocalLengthsChecked();
        int numStillCaptures = focalLengths.length;

        Size maxStillSz = mOrderedStillSizes.get(0);
        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
        CaptureRequest.Builder previewRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder stillRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        Size thumbnailSize = new Size(0, 0);
        Location sTestLocation = new Location(LocationManager.GPS_PROVIDER);
        sTestLocation.setTime(1199145600000L);
        sTestLocation.setLatitude(37.736071);
        sTestLocation.setLongitude(-122.441983);
        sTestLocation.setAltitude(21.0);
        ExifTestData exifTestData = new ExifTestData(
                /* gpsLocation */ sTestLocation,
                /* orientation */ 0,
                /* jpgQuality */ (byte) 80,
                /* thumbnailQuality */ (byte) 75);
        setJpegKeys(stillRequest, exifTestData, thumbnailSize, mCollector);
        CaptureResult result;

        // Set the max number of images to number of focal lengths supported
        prepareStillCaptureAndStartPreview(previewRequest, stillRequest, maxPreviewSz,
                maxStillSz, resultListener, focalLengths.length, imageListener, false /*isHeic*/);

        for(float focalLength : focalLengths) {

            previewRequest.set(CaptureRequest.LENS_FOCAL_LENGTH, focalLength);
            mSession.setRepeatingRequest(previewRequest.build(), resultListener, mHandler);
            waitForSettingsApplied(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
            waitForResultValue(resultListener, CaptureResult.LENS_STATE,
                    CaptureResult.LENS_STATE_STATIONARY, NUM_RESULTS_WAIT_TIMEOUT);
            result = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            mCollector.expectEquals(""Focal length in preview result and request should be the same"",
                    previewRequest.get(CaptureRequest.LENS_FOCAL_LENGTH),
                    result.get(CaptureResult.LENS_FOCAL_LENGTH));

            stillRequest.set(CaptureRequest.LENS_FOCAL_LENGTH, focalLength);
            CaptureRequest request = stillRequest.build();
            resultListener = new SimpleCaptureCallback();
            mSession.capture(request, resultListener, mHandler);
            result = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            mCollector.expectEquals(
                    ""Focal length in still capture result and request should be the same"",
                    stillRequest.get(CaptureRequest.LENS_FOCAL_LENGTH),
                    result.get(CaptureResult.LENS_FOCAL_LENGTH));

            Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);

            validateJpegCapture(image, maxStillSz);
            verifyJpegKeys(image, result, maxStillSz, thumbnailSize, exifTestData,
                    mStaticInfo, mCollector, mDebugFileNameBase, ImageFormat.JPEG);
        }
    }


    /**
     * Start preview,take a picture and test preview is still running after snapshot
     */
    private void previewPersistenceTestByCamera() throws Exception {
        Size maxStillSz = mOrderedStillSizes.get(0);
        Size maxPreviewSz = mOrderedPreviewSizes.get(0);

        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleCaptureCallback stillResultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
        CaptureRequest.Builder previewRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder stillRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        prepareStillCaptureAndStartPreview(previewRequest, stillRequest, maxPreviewSz,
                maxStillSz, resultListener, imageListener, false /*isHeic*/);

        // make sure preview is actually running
        waitForNumResults(resultListener, NUM_FRAMES_WAITED);

        // take a picture
        CaptureRequest request = stillRequest.build();
        mSession.capture(request, stillResultListener, mHandler);
        stillResultListener.getCaptureResultForRequest(request,
                WAIT_FOR_RESULT_TIMEOUT_MS);

        // validate image
        Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
        validateJpegCapture(image, maxStillSz);

        // make sure preview is still running after still capture
        waitForNumResults(resultListener, NUM_FRAMES_WAITED);

        stopPreview();

        // Free image resources
        image.close();
        closeImageReader();
        return;
    }

    /**
     * Take a picture for a given set of 3A regions for a particular camera.
     * <p>
     * Before take a still capture, it triggers an auto focus and lock it first,
     * then wait for AWB to converge and lock it, then trigger a precapture
     * metering sequence and wait for AE converged. After capture is received, the
     * capture result and image are validated.
     * </p>
     *
     * @param aeRegions AE regions for this capture
     * @param awbRegions AWB regions for this capture
     * @param afRegions AF regions for this capture
     */
    private void takePictureTestByCamera(
            MeteringRectangle[] aeRegions, MeteringRectangle[] awbRegions,
            MeteringRectangle[] afRegions) throws Exception {
        takePictureTestByCamera(aeRegions, awbRegions, afRegions,
                /*addAeTriggerCancel*/false, /*allocateBitmap*/false,
                /*previewRequest*/null, /*stillRequest*/null);
    }

    /**
     * Take a picture for a given set of 3A regions for a particular camera.
     * <p>
     * Before take a still capture, it triggers an auto focus and lock it first,
     * then wait for AWB to converge and lock it, then trigger a precapture
     * metering sequence and wait for AE converged. After capture is received, the
     * capture result and image are validated. If {@code addAeTriggerCancel} is true,
     * a precapture trigger cancel will be inserted between two adjacent triggers, which
     * should effective cancel the first trigger.
     * </p>
     *
     * @param aeRegions AE regions for this capture
     * @param awbRegions AWB regions for this capture
     * @param afRegions AF regions for this capture
     * @param addAeTriggerCancel If a AE precapture trigger cancel is sent after the trigger.
     * @param allocateBitmap If a set of bitmaps are allocated during the test for memory test.
     * @param previewRequest The preview request builder to use, or null to use the default
     * @param stillRequest The still capture request to use, or null to use the default
     */
    private void takePictureTestByCamera(
            MeteringRectangle[] aeRegions, MeteringRectangle[] awbRegions,
            MeteringRectangle[] afRegions, boolean addAeTriggerCancel, boolean allocateBitmap,
            CaptureRequest.Builder previewRequest, CaptureRequest.Builder stillRequest)
                    throws Exception {

        boolean hasFocuser = mStaticInfo.hasFocuser();

        Size maxStillSz = mOrderedStillSizes.get(0);
        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        CaptureResult result;
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
        if (previewRequest == null) {
            previewRequest = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        }
        if (stillRequest == null) {
            stillRequest = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        }
        prepareStillCaptureAndStartPreview(previewRequest, stillRequest, maxPreviewSz,
                maxStillSz, resultListener, imageListener, false /*isHeic*/);

        // Set AE mode to ON_AUTO_FLASH if flash is available.
        if (mStaticInfo.hasFlash()) {
            previewRequest.set(CaptureRequest.CONTROL_AE_MODE,
                    CaptureRequest.CONTROL_AE_MODE_ON_AUTO_FLASH);
            stillRequest.set(CaptureRequest.CONTROL_AE_MODE,
                    CaptureRequest.CONTROL_AE_MODE_ON_AUTO_FLASH);
        }

        Camera2Focuser focuser = null;
        /**
         * Step 1: trigger an auto focus run, and wait for AF locked.
         */
        boolean canSetAfRegion = hasFocuser && (afRegions != null) &&
                isRegionsSupportedFor3A(MAX_REGIONS_AF_INDEX);
        if (hasFocuser) {
            SimpleAutoFocusListener afListener = new SimpleAutoFocusListener();
            focuser = new Camera2Focuser(mCamera, mSession, mPreviewSurface, afListener,
                    mStaticInfo.getCharacteristics(), mHandler);
            if (canSetAfRegion) {
                previewRequest.set(CaptureRequest.CONTROL_AF_REGIONS, afRegions);
                stillRequest.set(CaptureRequest.CONTROL_AF_REGIONS, afRegions);
            }
            focuser.startAutoFocus(afRegions);
            afListener.waitForAutoFocusDone(WAIT_FOR_FOCUS_DONE_TIMEOUT_MS);
        }

        /**
         * Have to get the current AF mode to be used for other 3A repeating
         * request, otherwise, the new AF mode in AE/AWB request could be
         * different with existing repeating requests being sent by focuser,
         * then it could make AF unlocked too early. Beside that, for still
         * capture, AF mode must not be different with the one in current
         * repeating request, otherwise, the still capture itself would trigger
         * an AF mode change, and the AF lock would be lost for this capture.
         */
        int currentAfMode = CaptureRequest.CONTROL_AF_MODE_OFF;
        if (hasFocuser) {
            currentAfMode = focuser.getCurrentAfMode();
        }
        previewRequest.set(CaptureRequest.CONTROL_AF_MODE, currentAfMode);
        stillRequest.set(CaptureRequest.CONTROL_AF_MODE, currentAfMode);

        /**
         * Step 2: AF is already locked, wait for AWB converged, then lock it.
         */
        resultListener = new SimpleCaptureCallback();
        boolean canSetAwbRegion =
                (awbRegions != null) && isRegionsSupportedFor3A(MAX_REGIONS_AWB_INDEX);
        if (canSetAwbRegion) {
            previewRequest.set(CaptureRequest.CONTROL_AWB_REGIONS, awbRegions);
            stillRequest.set(CaptureRequest.CONTROL_AWB_REGIONS, awbRegions);
        }
        mSession.setRepeatingRequest(previewRequest.build(), resultListener, mHandler);
        if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
            waitForResultValue(resultListener, CaptureResult.CONTROL_AWB_STATE,
                    CaptureResult.CONTROL_AWB_STATE_CONVERGED, NUM_RESULTS_WAIT_TIMEOUT);
        } else {
            // LEGACY Devices don't have the AWB_STATE reported in results, so just wait
            waitForSettingsApplied(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
        }
        boolean canSetAwbLock = mStaticInfo.isAwbLockSupported();
        if (canSetAwbLock) {
            previewRequest.set(CaptureRequest.CONTROL_AWB_LOCK, true);
        }
        mSession.setRepeatingRequest(previewRequest.build(), resultListener, mHandler);
        // Validate the next result immediately for region and mode.
        result = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
        mCollector.expectEquals(""AWB mode in result and request should be same"",
                previewRequest.get(CaptureRequest.CONTROL_AWB_MODE),
                result.get(CaptureResult.CONTROL_AWB_MODE));
        if (canSetAwbRegion) {
            MeteringRectangle[] resultAwbRegions =
                    getValueNotNull(result, CaptureResult.CONTROL_AWB_REGIONS);
            mCollector.expectEquals(""AWB regions in result and request should be same"",
                    awbRegions, resultAwbRegions);
        }

        /**
         * Step 3: trigger an AE precapture metering sequence and wait for AE converged.
         */
        resultListener = new SimpleCaptureCallback();
        boolean canSetAeRegion =
                (aeRegions != null) && isRegionsSupportedFor3A(MAX_REGIONS_AE_INDEX);
        if (canSetAeRegion) {
            previewRequest.set(CaptureRequest.CONTROL_AE_REGIONS, aeRegions);
            stillRequest.set(CaptureRequest.CONTROL_AE_REGIONS, aeRegions);
        }
        mSession.setRepeatingRequest(previewRequest.build(), resultListener, mHandler);
        previewRequest.set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,
                CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_START);
        mSession.capture(previewRequest.build(), resultListener, mHandler);
        if (addAeTriggerCancel) {
            // Cancel the current precapture trigger, then send another trigger.
            // The camera device should behave as if the first trigger is not sent.
            // Wait one request to make the trigger start doing something before cancel.
            waitForNumResults(resultListener, /*numResultsWait*/ 1);
            previewRequest.set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,
                    CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_CANCEL);
            mSession.capture(previewRequest.build(), resultListener, mHandler);
            waitForResultValue(resultListener, CaptureResult.CONTROL_AE_PRECAPTURE_TRIGGER,
                    CaptureResult.CONTROL_AE_PRECAPTURE_TRIGGER_CANCEL,
                    NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
            // Issue another trigger
            previewRequest.set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,
                    CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_START);
            mSession.capture(previewRequest.build(), resultListener, mHandler);
        }
        waitForAeStable(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

        // Validate the next result immediately for region and mode.
        result = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
        mCollector.expectEquals(""AE mode in result and request should be same"",
                previewRequest.get(CaptureRequest.CONTROL_AE_MODE),
                result.get(CaptureResult.CONTROL_AE_MODE));
        if (canSetAeRegion) {
            MeteringRectangle[] resultAeRegions =
                    getValueNotNull(result, CaptureResult.CONTROL_AE_REGIONS);

            mCollector.expectMeteringRegionsAreSimilar(
                    ""AE regions in result and request should be similar"",
                    aeRegions,
                    resultAeRegions,
                    METERING_REGION_ERROR_PERCENT_DELTA);
        }

        /**
         * Step 4: take a picture when all 3A are in good state.
         */
        resultListener = new SimpleCaptureCallback();
        CaptureRequest request = stillRequest.build();
        mSession.capture(request, resultListener, mHandler);
        // Validate the next result immediately for region and mode.
        result = resultListener.getCaptureResultForRequest(request, WAIT_FOR_RESULT_TIMEOUT_MS);
        mCollector.expectEquals(""AF mode in result and request should be same"",
                stillRequest.get(CaptureRequest.CONTROL_AF_MODE),
                result.get(CaptureResult.CONTROL_AF_MODE));
        if (canSetAfRegion) {
            MeteringRectangle[] resultAfRegions =
                    getValueNotNull(result, CaptureResult.CONTROL_AF_REGIONS);
            mCollector.expectMeteringRegionsAreSimilar(
                    ""AF regions in result and request should be similar"",
                    afRegions,
                    resultAfRegions,
                    METERING_REGION_ERROR_PERCENT_DELTA);
        }

        if (hasFocuser) {
            // Unlock auto focus.
            focuser.cancelAutoFocus();
        }

        // validate image
        Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
        validateJpegCapture(image, maxStillSz);
        // Test if the system can allocate 3 bitmap successfully, per android CDD camera memory
        // requirements added by CDD 5.0
        if (allocateBitmap) {
            Bitmap bm[] = new Bitmap[MAX_ALLOCATED_BITMAPS];
            for (int i = 0; i < MAX_ALLOCATED_BITMAPS; i++) {
                bm[i] = Bitmap.createBitmap(
                        maxStillSz.getWidth(), maxStillSz.getHeight(), Config.ARGB_8888);
                assertNotNull(""Created bitmap #"" + i + "" shouldn't be null"", bm[i]);
            }
        }

        // Free image resources
        image.close();

        stopPreview();
    }

    /**
     * Test touch region for focus by camera.
     */
    private void touchForFocusTestByCamera() throws Exception {
        SimpleCaptureCallback listener = new SimpleCaptureCallback();
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        startPreview(requestBuilder, maxPreviewSz, listener);

        SimpleAutoFocusListener afListener = new SimpleAutoFocusListener();
        Camera2Focuser focuser = new Camera2Focuser(mCamera, mSession, mPreviewSurface, afListener,
                mStaticInfo.getCharacteristics(), mHandler);
        ArrayList<MeteringRectangle[]> testAfRegions = get3ARegionTestCasesForCamera();

        for (MeteringRectangle[] afRegions : testAfRegions) {
            focuser.touchForAutoFocus(afRegions);
            afListener.waitForAutoFocusDone(WAIT_FOR_FOCUS_DONE_TIMEOUT_MS);
            focuser.cancelAutoFocus();
        }
    }

    private void previewStillCombinationTestByCamera() throws Exception {
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();

        Size QCIF = new Size(176, 144);
        Size FULL_HD = new Size(1920, 1080);
        for (Size stillSz : mOrderedStillSizes)
            for (Size previewSz : mOrderedPreviewSizes) {
                if (VERBOSE) {
                    Log.v(TAG, ""Testing JPEG capture size "" + stillSz.toString()
                            + "" with preview size "" + previewSz.toString() + "" for camera ""
                            + mCamera.getId());
                }

                // Skip testing QCIF + >FullHD combinations
                if (stillSz.equals(QCIF) &&
                        ((previewSz.getWidth() > FULL_HD.getWidth()) ||
                         (previewSz.getHeight() > FULL_HD.getHeight()))) {
                    continue;
                }

                if (previewSz.equals(QCIF) &&
                        ((stillSz.getWidth() > FULL_HD.getWidth()) ||
                         (stillSz.getHeight() > FULL_HD.getHeight()))) {
                    continue;
                }

                CaptureRequest.Builder previewRequest =
                        mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
                CaptureRequest.Builder stillRequest =
                        mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
                prepareStillCaptureAndStartPreview(previewRequest, stillRequest, previewSz,
                        stillSz, resultListener, imageListener, false /*isHeic*/);
                mSession.capture(stillRequest.build(), resultListener, mHandler);
                Image image = imageListener.getImage((mStaticInfo.isHardwareLevelLegacy()) ?
                        RELAXED_CAPTURE_IMAGE_TIMEOUT_MS : CAPTURE_IMAGE_TIMEOUT_MS);
                validateJpegCapture(image, stillSz);

                // Free image resources
                image.close();

                // stopPreview must be called here to make sure next time a preview stream
                // is created with new size.
                stopPreview();
                // Drain the results after each combination. Depending on the device the results
                // can be relatively big and could accumulate fairly quickly after many iterations.
                resultListener.drain();
            }
    }

    /**
     * Basic raw capture test for each camera.
     */
    private void rawCaptureTestByCamera(CaptureRequest.Builder stillRequest) throws Exception {
        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        Size size = mStaticInfo.getRawDimensChecked();

        // Prepare raw capture and start preview.
        CaptureRequest.Builder previewBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder rawBuilder = (stillRequest != null) ? stillRequest :
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
        prepareRawCaptureAndStartPreview(previewBuilder, rawBuilder, maxPreviewSz, size,
                resultListener, imageListener);

        if (VERBOSE) {
            Log.v(TAG, ""Testing Raw capture with size "" + size.toString()
                    + "", preview size "" + maxPreviewSz);
        }

        CaptureRequest rawRequest = rawBuilder.build();
        mSession.capture(rawRequest, resultListener, mHandler);

        Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
        validateRaw16Image(image, size);
        if (DEBUG) {
            byte[] rawBuffer = getDataFromImage(image);
            String rawFileName = mDebugFileNameBase + ""/test"" + ""_"" + size.toString() + ""_cam"" +
                    mCamera.getId() + "".raw16"";
            Log.d(TAG, ""Dump raw file into "" + rawFileName);
            dumpFile(rawFileName, rawBuffer);
        }

        // Free image resources
        image.close();

        stopPreview();
    }

    private void fullRawCaptureTestByCamera(CaptureRequest.Builder stillRequest) throws Exception {
        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        Size maxStillSz = mOrderedStillSizes.get(0);

        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener jpegListener = new SimpleImageReaderListener();
        SimpleImageReaderListener rawListener = new SimpleImageReaderListener();

        Size size = mStaticInfo.getRawDimensChecked();

        if (VERBOSE) {
            Log.v(TAG, ""Testing multi capture with size "" + size.toString()
                    + "", preview size "" + maxPreviewSz);
        }

        // Prepare raw capture and start preview.
        CaptureRequest.Builder previewBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder multiBuilder = (stillRequest != null) ? stillRequest :
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);

        ImageReader rawReader = null;
        ImageReader jpegReader = null;

        try {
            // Create ImageReaders.
            rawReader = makeImageReader(size,
                    ImageFormat.RAW_SENSOR, MAX_READER_IMAGES, rawListener, mHandler);
            jpegReader = makeImageReader(maxStillSz,
                    ImageFormat.JPEG, MAX_READER_IMAGES, jpegListener, mHandler);
            updatePreviewSurface(maxPreviewSz);

            // Configure output streams with preview and jpeg streams.
            List<Surface> outputSurfaces = new ArrayList<Surface>();
            outputSurfaces.add(rawReader.getSurface());
            outputSurfaces.add(jpegReader.getSurface());
            outputSurfaces.add(mPreviewSurface);
            mSessionListener = new BlockingSessionCallback();
            mSession = configureCameraSession(mCamera, outputSurfaces,
                    mSessionListener, mHandler);

            // Configure the requests.
            previewBuilder.addTarget(mPreviewSurface);
            multiBuilder.addTarget(mPreviewSurface);
            multiBuilder.addTarget(rawReader.getSurface());
            multiBuilder.addTarget(jpegReader.getSurface());

            // Start preview.
            mSession.setRepeatingRequest(previewBuilder.build(), null, mHandler);

            // Poor man's 3A, wait 2 seconds for AE/AF (if any) to settle.
            // TODO: Do proper 3A trigger and lock (see testTakePictureTest).
            Thread.sleep(3000);

            multiBuilder.set(CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE,
                    CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE_ON);
            CaptureRequest multiRequest = multiBuilder.build();

            mSession.capture(multiRequest, resultListener, mHandler);

            CaptureResult result = resultListener.getCaptureResultForRequest(multiRequest,
                    NUM_RESULTS_WAIT_TIMEOUT);
            Image jpegImage = jpegListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
            basicValidateBlobImage(jpegImage, maxStillSz, ImageFormat.JPEG);
            Image rawImage = rawListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
            validateRaw16Image(rawImage, size);
            verifyRawCaptureResult(multiRequest, result);


            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            try (DngCreator dngCreator = new DngCreator(mStaticInfo.getCharacteristics(), result)) {
                dngCreator.writeImage(outputStream, rawImage);
            }

            if (DEBUG) {
                byte[] rawBuffer = outputStream.toByteArray();
                String rawFileName = mDebugFileNameBase + ""/raw16_"" + TAG + size.toString() +
                        ""_cam_"" + mCamera.getId() + "".dng"";
                Log.d(TAG, ""Dump raw file into "" + rawFileName);
                dumpFile(rawFileName, rawBuffer);

                byte[] jpegBuffer = getDataFromImage(jpegImage);
                String jpegFileName = mDebugFileNameBase + ""/jpeg_"" + TAG + size.toString() +
                        ""_cam_"" + mCamera.getId() + "".jpg"";
                Log.d(TAG, ""Dump jpeg file into "" + rawFileName);
                dumpFile(jpegFileName, jpegBuffer);
            }

            stopPreview();
        } finally {
            CameraTestUtils.closeImageReader(rawReader);
            CameraTestUtils.closeImageReader(jpegReader);
            rawReader = null;
            jpegReader = null;
        }
    }

    /**
     * Validate that raw {@link CaptureResult}.
     *
     * @param rawRequest a {@link CaptureRequest} use to capture a RAW16 image.
     * @param rawResult the {@link CaptureResult} corresponding to the given request.
     */
    private void verifyRawCaptureResult(CaptureRequest rawRequest, CaptureResult rawResult) {
        assertNotNull(rawRequest);
        assertNotNull(rawResult);

        if (!mStaticInfo.isMonochromeCamera()) {
            Rational[] empty = new Rational[] { Rational.ZERO, Rational.ZERO, Rational.ZERO};
            Rational[] neutralColorPoint = mCollector.expectKeyValueNotNull(""NeutralColorPoint"",
                    rawResult, CaptureResult.SENSOR_NEUTRAL_COLOR_POINT);
            if (neutralColorPoint != null) {
                mCollector.expectEquals(""NeutralColorPoint length"", empty.length,
                        neutralColorPoint.length);
                mCollector.expectNotEquals(""NeutralColorPoint cannot be all zeroes, "", empty,
                        neutralColorPoint);
                mCollector.expectValuesGreaterOrEqual(""NeutralColorPoint"", neutralColorPoint,
                        Rational.ZERO);
            }

            mCollector.expectKeyValueGreaterOrEqual(rawResult,
                    CaptureResult.SENSOR_GREEN_SPLIT, 0.0f);
        }

        Pair<Double, Double>[] noiseProfile = mCollector.expectKeyValueNotNull(""NoiseProfile"",
                rawResult, CaptureResult.SENSOR_NOISE_PROFILE);
        if (noiseProfile != null) {
            int cfa = mStaticInfo.getCFAChecked();
            int numCfaChannels = 0;
            switch (cfa) {
                case CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_RGGB:
                case CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_GRBG:
                case CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_GBRG:
                case CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_BGGR:
                    numCfaChannels = 4;
                    break;
                case CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_MONO:
                case CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_NIR:
                    numCfaChannels = 1;
                    break;
                default:
                    Assert.fail(""Invalid color filter arrangement "" + cfa);
                    break;
            }
            mCollector.expectEquals(""NoiseProfile length"", noiseProfile.length, numCfaChannels);
            for (Pair<Double, Double> p : noiseProfile) {
                mCollector.expectTrue(""NoiseProfile coefficients "" + p +
                        "" must have: S > 0, O >= 0"", p.first > 0 && p.second >= 0);
            }
        }

        Integer hotPixelMode = mCollector.expectKeyValueNotNull(""HotPixelMode"", rawResult,
                CaptureResult.HOT_PIXEL_MODE);
        Boolean hotPixelMapMode = mCollector.expectKeyValueNotNull(""HotPixelMapMode"", rawResult,
                CaptureResult.STATISTICS_HOT_PIXEL_MAP_MODE);
        Point[] hotPixelMap = rawResult.get(CaptureResult.STATISTICS_HOT_PIXEL_MAP);

        Size pixelArraySize = mStaticInfo.getPixelArraySizeChecked();
        boolean[] availableHotPixelMapModes = mStaticInfo.getValueFromKeyNonNull(
                        CameraCharacteristics.STATISTICS_INFO_AVAILABLE_HOT_PIXEL_MAP_MODES);

        if (hotPixelMode != null) {
            Integer requestMode = mCollector.expectKeyValueNotNull(rawRequest,
                    CaptureRequest.HOT_PIXEL_MODE);
            if (requestMode != null) {
                mCollector.expectKeyValueEquals(rawResult, CaptureResult.HOT_PIXEL_MODE,
                        requestMode);
            }
        }

        if (hotPixelMapMode != null) {
            Boolean requestMapMode = mCollector.expectKeyValueNotNull(rawRequest,
                    CaptureRequest.STATISTICS_HOT_PIXEL_MAP_MODE);
            if (requestMapMode != null) {
                mCollector.expectKeyValueEquals(rawResult,
                        CaptureResult.STATISTICS_HOT_PIXEL_MAP_MODE, requestMapMode);
            }

            if (!hotPixelMapMode) {
                mCollector.expectTrue(""HotPixelMap must be empty"", hotPixelMap == null ||
                        hotPixelMap.length == 0);
            } else {
                mCollector.expectTrue(""HotPixelMap must not be empty"", hotPixelMap != null);
                mCollector.expectNotNull(""AvailableHotPixelMapModes must not be null"",
                        availableHotPixelMapModes);
                if (availableHotPixelMapModes != null) {
                    mCollector.expectContains(""HotPixelMapMode"", availableHotPixelMapModes, true);
                }

                int height = pixelArraySize.getHeight();
                int width = pixelArraySize.getWidth();
                for (Point p : hotPixelMap) {
                    mCollector.expectTrue(""Hotpixel "" + p + "" must be in pixelArray "" +
                            pixelArraySize, p.x >= 0 && p.x < width && p.y >= 0 && p.y < height);
                }
            }
        }
        // TODO: profileHueSatMap, and profileToneCurve aren't supported yet.

    }

    /**
     * Issue a still capture and validate the exif information.
     * <p>
     * TODO: Differentiate full and limited device, some of the checks rely on
     * per frame control and synchronization, most of them don't.
     * </p>
     */
    private void stillExifTestByCamera(int format, Size stillSize) throws Exception {
        assertTrue(format == ImageFormat.JPEG || format == ImageFormat.HEIC);
        boolean isHeic = (format == ImageFormat.HEIC);

        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        if (VERBOSE) {
            Log.v(TAG, ""Testing exif with size "" + stillSize.toString()
                    + "", preview size "" + maxPreviewSz);
        }

        // prepare capture and start preview.
        CaptureRequest.Builder previewBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder stillBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
        prepareStillCaptureAndStartPreview(previewBuilder, stillBuilder, maxPreviewSz, stillSize,
                resultListener, imageListener, isHeic);

        // Set the jpeg keys, then issue a capture
        Size[] thumbnailSizes = mStaticInfo.getAvailableThumbnailSizesChecked();
        Size maxThumbnailSize = thumbnailSizes[thumbnailSizes.length - 1];
        Size[] testThumbnailSizes = new Size[EXIF_TEST_DATA.length];
        Arrays.fill(testThumbnailSizes, maxThumbnailSize);
        // Make sure thumbnail size (0, 0) is covered.
        testThumbnailSizes[0] = new Size(0, 0);

        for (int i = 0; i < EXIF_TEST_DATA.length; i++) {
            setJpegKeys(stillBuilder, EXIF_TEST_DATA[i], testThumbnailSizes[i], mCollector);

            // Capture a jpeg/heic image.
            CaptureRequest request = stillBuilder.build();
            mSession.capture(request, resultListener, mHandler);
            CaptureResult stillResult =
                    resultListener.getCaptureResultForRequest(request, NUM_RESULTS_WAIT_TIMEOUT);
            Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);

            verifyJpegKeys(image, stillResult, stillSize, testThumbnailSizes[i], EXIF_TEST_DATA[i],
                    mStaticInfo, mCollector, mDebugFileNameBase, format);

            // Free image resources
            image.close();
        }
    }

    /**
     * Issue a still capture and validate the dynamic depth output.
     */
    private void stillDynamicDepthTestByCamera(int format, Size stillSize) throws Exception {
        assertTrue(format == ImageFormat.DEPTH_JPEG);

        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        if (VERBOSE) {
            Log.v(TAG, ""Testing dynamic depth with size "" + stillSize.toString()
                    + "", preview size "" + maxPreviewSz);
        }

        // prepare capture and start preview.
        CaptureRequest.Builder previewBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder stillBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
        prepareCaptureAndStartPreview(previewBuilder, stillBuilder, maxPreviewSz, stillSize,
                ImageFormat.DEPTH_JPEG, resultListener, /*sessionListener*/null,
                MAX_READER_IMAGES, imageListener);

        // Capture a few dynamic depth images and check whether they are valid jpegs.
        for (int i = 0; i < MAX_READER_IMAGES; i++) {
            CaptureRequest request = stillBuilder.build();
            mSession.capture(request, resultListener, mHandler);
            CaptureResult stillResult =
                resultListener.getCaptureResultForRequest(request, NUM_RESULTS_WAIT_TIMEOUT);
            Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
            assertNotNull(""Unable to acquire next image"", image);
            CameraTestUtils.validateImage(image, stillSize.getWidth(), stillSize.getHeight(),
                    format, null /*filePath*/);

            // Free image resources
            image.close();
        }
    }

    private void aeCompensationTestByCamera() throws Exception {
        Range<Integer> compensationRange = mStaticInfo.getAeCompensationRangeChecked();
        // Skip the test if exposure compensation is not supported.
        if (compensationRange.equals(Range.create(0, 0))) {
            return;
        }

        Rational step = mStaticInfo.getAeCompensationStepChecked();
        float stepF = (float) step.getNumerator() / step.getDenominator();
        int stepsPerEv = (int) Math.round(1.0 / stepF);
        int numSteps = (compensationRange.getUpper() - compensationRange.getLower()) / stepsPerEv;

        Size maxStillSz = mOrderedStillSizes.get(0);
        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
        CaptureRequest.Builder previewRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder stillRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        boolean canSetAeLock = mStaticInfo.isAeLockSupported();
        boolean canReadSensorSettings = mStaticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS);

        if (canSetAeLock) {
            stillRequest.set(CaptureRequest.CONTROL_AE_LOCK, true);
        }

        CaptureResult normalResult;
        CaptureResult compensatedResult;

        boolean canReadExposureValueRange = mStaticInfo.areKeysAvailable(
                CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE,
                CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE);
        boolean canVerifyExposureValue = canReadSensorSettings && canReadExposureValueRange;
        long minExposureValue = -1;
        long maxExposureValuePreview = -1;
        long maxExposureValueStill = -1;
        if (canReadExposureValueRange) {
            // Minimum exposure settings is mostly static while maximum exposure setting depends on
            // frame rate range which in term depends on capture request.
            minExposureValue = mStaticInfo.getSensitivityMinimumOrDefault() *
                    mStaticInfo.getExposureMinimumOrDefault() / 1000;
            long maxSensitivity = mStaticInfo.getSensitivityMaximumOrDefault();
            long maxExposureTimeUs = mStaticInfo.getExposureMaximumOrDefault() / 1000;
            maxExposureValuePreview = getMaxExposureValue(previewRequest, maxExposureTimeUs,
                    maxSensitivity);
            maxExposureValueStill = getMaxExposureValue(stillRequest, maxExposureTimeUs,
                    maxSensitivity);
        }

        // Set the max number of images to be same as the burst count, as the verification
        // could be much slower than producing rate, and we don't want to starve producer.
        prepareStillCaptureAndStartPreview(previewRequest, stillRequest, maxPreviewSz,
                maxStillSz, resultListener, numSteps, imageListener, false /*isHeic*/);

        for (int i = 0; i <= numSteps; i++) {
            int exposureCompensation = i * stepsPerEv + compensationRange.getLower();
            double expectedRatio = Math.pow(2.0, exposureCompensation / stepsPerEv);

            // Wait for AE to be stabilized before capture: CONVERGED or FLASH_REQUIRED.
            waitForAeStable(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
            normalResult = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);

            long normalExposureValue = -1;
            if (canVerifyExposureValue) {
                // get and check if current exposure value is valid
                normalExposureValue = getExposureValue(normalResult);
                mCollector.expectInRange(""Exposure setting out of bound"", normalExposureValue,
                        minExposureValue, maxExposureValuePreview);

                // Only run the test if expectedExposureValue is within valid range
                long expectedExposureValue = (long) (normalExposureValue * expectedRatio);
                if (expectedExposureValue < minExposureValue ||
                    expectedExposureValue > maxExposureValueStill) {
                    continue;
                }
                Log.v(TAG, ""Expect ratio: "" + expectedRatio +
                        "" normalExposureValue: "" + normalExposureValue +
                        "" expectedExposureValue: "" + expectedExposureValue +
                        "" minExposureValue: "" + minExposureValue +
                        "" maxExposureValuePreview: "" + maxExposureValuePreview +
                        "" maxExposureValueStill: "" + maxExposureValueStill);
            }

            // Now issue exposure compensation and wait for AE locked. AE could take a few
            // frames to go back to locked state
            previewRequest.set(CaptureRequest.CONTROL_AE_EXPOSURE_COMPENSATION,
                    exposureCompensation);
            if (canSetAeLock) {
                previewRequest.set(CaptureRequest.CONTROL_AE_LOCK, true);
            }
            mSession.setRepeatingRequest(previewRequest.build(), resultListener, mHandler);
            if (canSetAeLock) {
                waitForAeLocked(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
            } else {
                waitForSettingsApplied(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);
            }

            // Issue still capture
            if (VERBOSE) {
                Log.v(TAG, ""Verifying capture result for ae compensation value ""
                        + exposureCompensation);
            }

            stillRequest.set(CaptureRequest.CONTROL_AE_EXPOSURE_COMPENSATION, exposureCompensation);
            CaptureRequest request = stillRequest.build();
            mSession.capture(request, resultListener, mHandler);

            compensatedResult = resultListener.getCaptureResultForRequest(
                    request, WAIT_FOR_RESULT_TIMEOUT_MS);

            if (canVerifyExposureValue) {
                // Verify the exposure value compensates as requested
                long compensatedExposureValue = getExposureValue(compensatedResult);
                mCollector.expectInRange(""Exposure setting out of bound"", compensatedExposureValue,
                        minExposureValue, maxExposureValueStill);
                double observedRatio = (double) compensatedExposureValue / normalExposureValue;
                double error = observedRatio / expectedRatio;
                String errorString = String.format(
                        ""Exposure compensation ratio exceeds error tolerence:"" +
                        "" expected(%f) observed(%f)."" +
                        "" Normal exposure time %d us, sensitivity %d."" +
                        "" Compensated exposure time %d us, sensitivity %d"",
                        expectedRatio, observedRatio,
                        (int) (getValueNotNull(
                                normalResult, CaptureResult.SENSOR_EXPOSURE_TIME) / 1000),
                        getValueNotNull(normalResult, CaptureResult.SENSOR_SENSITIVITY),
                        (int) (getValueNotNull(
                                compensatedResult, CaptureResult.SENSOR_EXPOSURE_TIME) / 1000),
                        getValueNotNull(compensatedResult, CaptureResult.SENSOR_SENSITIVITY));
                mCollector.expectInRange(errorString, error,
                        1.0 - AE_COMPENSATION_ERROR_TOLERANCE,
                        1.0 + AE_COMPENSATION_ERROR_TOLERANCE);
            }

            mCollector.expectEquals(""Exposure compensation result should match requested value."",
                    exposureCompensation,
                    compensatedResult.get(CaptureResult.CONTROL_AE_EXPOSURE_COMPENSATION));
            if (canSetAeLock) {
                mCollector.expectTrue(""Exposure lock should be set"",
                        compensatedResult.get(CaptureResult.CONTROL_AE_LOCK));
            }

            Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
            validateJpegCapture(image, maxStillSz);
            image.close();

            // Recover AE compensation and lock
            previewRequest.set(CaptureRequest.CONTROL_AE_EXPOSURE_COMPENSATION, 0);
            if (canSetAeLock) {
                previewRequest.set(CaptureRequest.CONTROL_AE_LOCK, false);
            }
            mSession.setRepeatingRequest(previewRequest.build(), resultListener, mHandler);
        }
    }

    private long getExposureValue(CaptureResult result) throws Exception {
        int expTimeUs = (int) (getValueNotNull(result, CaptureResult.SENSOR_EXPOSURE_TIME) / 1000);
        int sensitivity = getValueNotNull(result, CaptureResult.SENSOR_SENSITIVITY);
        Integer postRawSensitivity = result.get(CaptureResult.CONTROL_POST_RAW_SENSITIVITY_BOOST);
        if (postRawSensitivity != null) {
            return (long) sensitivity * postRawSensitivity / 100 * expTimeUs;
        }
        return (long) sensitivity * expTimeUs;
    }

    private long getMaxExposureValue(CaptureRequest.Builder request, long maxExposureTimeUs,
                long maxSensitivity)  throws Exception {
        Range<Integer> fpsRange = request.get(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE);
        long maxFrameDurationUs = Math.round(1000000.0 / fpsRange.getLower());
        long currentMaxExposureTimeUs = Math.min(maxFrameDurationUs, maxExposureTimeUs);
        return currentMaxExposureTimeUs * maxSensitivity;
    }


    //----------------------------------------------------------------
    //---------Below are common functions for all tests.--------------
    //----------------------------------------------------------------
    /**
     * Validate standard raw (RAW16) capture image.
     *
     * @param image The raw16 format image captured
     * @param rawSize The expected raw size
     */
    private static void validateRaw16Image(Image image, Size rawSize) {
        CameraTestUtils.validateImage(image, rawSize.getWidth(), rawSize.getHeight(),
                ImageFormat.RAW_SENSOR, /*filePath*/null);
    }

    /**
     * Validate JPEG capture image object correctness and test.
     * <p>
     * In addition to image object correctness, this function also does the decoding
     * test, which is slower.
     * </p>
     *
     * @param image The JPEG image to be verified.
     * @param jpegSize The JPEG capture size to be verified against.
     */
    private static void validateJpegCapture(Image image, Size jpegSize) {
        CameraTestUtils.validateImage(image, jpegSize.getWidth(), jpegSize.getHeight(),
                ImageFormat.JPEG, /*filePath*/null);
    }

    private static class SimpleAutoFocusListener implements Camera2Focuser.AutoFocusListener {
        final ConditionVariable focusDone = new ConditionVariable();
        @Override
        public void onAutoFocusLocked(boolean success) {
            focusDone.open();
        }

        public void waitForAutoFocusDone(long timeoutMs) {
            if (focusDone.block(timeoutMs)) {
                focusDone.close();
            } else {
                throw new TimeoutRuntimeException(""Wait for auto focus done timed out after ""
                        + timeoutMs + ""ms"");
            }
        }
    }

    /**
     * Get 5 3A region test cases, each with one square region in it.
     * The first one is at center, the other four are at corners of
     * active array rectangle.
     *
     * @return array of test 3A regions
     */
    private ArrayList<MeteringRectangle[]> get3ARegionTestCasesForCamera() {
        final int TEST_3A_REGION_NUM = 5;
        final int DEFAULT_REGION_WEIGHT = 30;
        final int DEFAULT_REGION_SCALE_RATIO = 8;
        ArrayList<MeteringRectangle[]> testCases =
                new ArrayList<MeteringRectangle[]>(TEST_3A_REGION_NUM);
        final Rect activeArraySize = mStaticInfo.getActiveArraySizeChecked();
        int regionWidth = activeArraySize.width() / DEFAULT_REGION_SCALE_RATIO - 1;
        int regionHeight = activeArraySize.height() / DEFAULT_REGION_SCALE_RATIO - 1;
        int centerX = activeArraySize.width() / 2;
        int centerY = activeArraySize.height() / 2;
        int bottomRightX = activeArraySize.width() - 1;
        int bottomRightY = activeArraySize.height() - 1;

        // Center region
        testCases.add(
                new MeteringRectangle[] {
                    new MeteringRectangle(
                            centerX - regionWidth / 2,  // x
                            centerY - regionHeight / 2, // y
                            regionWidth,                // width
                            regionHeight,               // height
                            DEFAULT_REGION_WEIGHT)});

        // Upper left corner
        testCases.add(
                new MeteringRectangle[] {
                    new MeteringRectangle(
                            0,                // x
                            0,                // y
                            regionWidth,      // width
                            regionHeight,     // height
                            DEFAULT_REGION_WEIGHT)});

        // Upper right corner
        testCases.add(
                new MeteringRectangle[] {
                    new MeteringRectangle(
                            bottomRightX - regionWidth, // x
                            0,                          // y
                            regionWidth,                // width
                            regionHeight,               // height
                            DEFAULT_REGION_WEIGHT)});

        // Bottom left corner
        testCases.add(
                new MeteringRectangle[] {
                    new MeteringRectangle(
                            0,                           // x
                            bottomRightY - regionHeight, // y
                            regionWidth,                 // width
                            regionHeight,                // height
                            DEFAULT_REGION_WEIGHT)});

        // Bottom right corner
        testCases.add(
                new MeteringRectangle[] {
                    new MeteringRectangle(
                            bottomRightX - regionWidth,  // x
                            bottomRightY - regionHeight, // y
                            regionWidth,                 // width
                            regionHeight,                // height
                            DEFAULT_REGION_WEIGHT)});

        if (VERBOSE) {
            StringBuilder sb = new StringBuilder();
            for (MeteringRectangle[] mr : testCases) {
                sb.append(""{"");
                sb.append(Arrays.toString(mr));
                sb.append(""}, "");
            }
            if (sb.length() > 1)
                sb.setLength(sb.length() - 2); // Remove the redundant comma and space at the end
            Log.v(TAG, ""Generated test regions are: "" + sb.toString());
        }

        return testCases;
    }

    private boolean isRegionsSupportedFor3A(int index) {
        int maxRegions = 0;
        switch (index) {
            case MAX_REGIONS_AE_INDEX:
                maxRegions = mStaticInfo.getAeMaxRegionsChecked();
                break;
            case MAX_REGIONS_AWB_INDEX:
                maxRegions = mStaticInfo.getAwbMaxRegionsChecked();
                break;
            case  MAX_REGIONS_AF_INDEX:
                maxRegions = mStaticInfo.getAfMaxRegionsChecked();
                break;
            default:
                throw new IllegalArgumentException(""Unknown algorithm index"");
        }
        boolean isRegionsSupported = maxRegions > 0;
        if (index == MAX_REGIONS_AF_INDEX && isRegionsSupported) {
            mCollector.expectTrue(
                    ""Device reports non-zero max AF region count for a camera without focuser!"",
                    mStaticInfo.hasFocuser());
            isRegionsSupported = isRegionsSupported && mStaticInfo.hasFocuser();
        }

        return isRegionsSupported;
    }
}"	""	""	"JPEG 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ImageWriterTest"	"testWriterReaderBlobFormats"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ImageWriterTest.java"	""	"public void testWriterReaderBlobFormats() throws Exception {
        int[] READER_TEST_FORMATS = {ImageFormat.JPEG, ImageFormat.DEPTH_JPEG,
                                     ImageFormat.HEIC, ImageFormat.DEPTH_POINT_CLOUD};

        for (int format : READER_TEST_FORMATS) {
            ImageReader reader = ImageReader.newInstance(640, 480, format, 1 /*maxImages*/);
            ImageWriter writer = ImageWriter.newInstance(reader.getSurface(), 1 /*maxImages*/);
            writer.close();
            reader.close();
        }
    }"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ImageWriterTest"	"testAbandonedSurfaceExceptions"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ImageWriterTest.java"	""	"public void testAbandonedSurfaceExceptions() throws Exception {
        final int READER_WIDTH = 1920;
        final int READER_HEIGHT = 1080;
        final int READER_FORMAT = ImageFormat.YUV_420_888;

        // Verify that if the image writer's input surface is abandoned, dequeueing an image
        // throws IllegalStateException
        ImageReader reader = ImageReader.newInstance(READER_WIDTH, READER_HEIGHT, READER_FORMAT,
                MAX_NUM_IMAGES);
        ImageWriter writer = ImageWriter.newInstance(reader.getSurface(), MAX_NUM_IMAGES);

        // Close image reader to abandon the input surface.
        reader.close();

        Image image;
        try {
            image = writer.dequeueInputImage();
            fail(""Should get an IllegalStateException"");
        } catch (IllegalStateException e) {
            // Expected
        } finally {
            writer.close();
        }

        // Verify that if the image writer's input surface is abandoned, queueing an image
        // throws IllegalStateException
        reader = ImageReader.newInstance(READER_WIDTH, READER_HEIGHT, READER_FORMAT,
                MAX_NUM_IMAGES);
        writer = ImageWriter.newInstance(reader.getSurface(), MAX_NUM_IMAGES);
        image = writer.dequeueInputImage();

        // Close image reader to abandon the input surface.
        reader.close();

        try {
            writer.queueInputImage(image);
            fail(""Should get an IllegalStateException"");
        } catch (IllegalStateException e) {
            // Expected
        } finally {
            writer.close();
        }
    }"	""	""	"1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"TestShutterCallback"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"/*
 *.
 */

package android.hardware.cts;

import android.content.pm.PackageManager;
import android.graphics.BitmapFactory;
import android.graphics.ImageFormat;
import android.graphics.Rect;
import android.hardware.Camera;
import android.hardware.Camera.Area;
import android.hardware.Camera.CameraInfo;
import android.hardware.Camera.ErrorCallback;
import android.hardware.Camera.Face;
import android.hardware.Camera.FaceDetectionListener;
import android.hardware.Camera.Parameters;
import android.hardware.Camera.PictureCallback;
import android.hardware.Camera.ShutterCallback;
import android.hardware.Camera.Size;
import android.hardware.cts.helpers.CameraUtils;
import android.media.CamcorderProfile;
import android.media.ExifInterface;
import android.media.MediaRecorder;
import android.os.Build;
import android.os.ConditionVariable;
import android.os.Looper;
import android.os.SystemClock;
import android.test.MoreAsserts;
import android.test.UiThreadTest;
import android.test.suitebuilder.annotation.LargeTest;
import android.util.Log;
import android.view.SurfaceHolder;

import androidx.test.rule.ActivityTestRule;

import junit.framework.Assert;
import junit.framework.AssertionFailedError;

import org.junit.After;
import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.text.ParseException;
import java.text.ParsePosition;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Date;
import java.util.Iterator;
import java.util.List;
import java.util.TimeZone;

import com.android.compatibility.common.util.WindowUtil;

/**
 * This test case must run with hardware. It can't be tested in emulator
 */
@LargeTest
public class CameraTest extends Assert {
    private static final String TAG = ""CameraTest"";
    private static final String PACKAGE = ""android.hardware.cts"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);
    private String mRecordingPath = null;
    private String mJpegPath = null;
    private byte[] mJpegData;

    private static final int PREVIEW_CALLBACK_NOT_RECEIVED = 0;
    private static final int PREVIEW_CALLBACK_RECEIVED = 1;
    private static final int PREVIEW_CALLBACK_DATA_NULL = 2;
    private static final int PREVIEW_CALLBACK_INVALID_FRAME_SIZE = 3;
    private int mPreviewCallbackResult = PREVIEW_CALLBACK_NOT_RECEIVED;

    private boolean mShutterCallbackResult = false;
    private boolean mRawPictureCallbackResult = false;
    private boolean mJpegPictureCallbackResult = false;
    private static final int NO_ERROR = -1;
    private int mCameraErrorCode = NO_ERROR;
    private boolean mAutoFocusSucceeded = false;

    private static final int WAIT_FOR_COMMAND_TO_COMPLETE = 5000;  // Milliseconds.
    private static final int WAIT_FOR_FOCUS_TO_COMPLETE = 5000;
    private static final int WAIT_FOR_SNAPSHOT_TO_COMPLETE = 5000;

    private static final int FOCUS_AREA = 0;
    private static final int METERING_AREA = 1;

    private static final int AUTOEXPOSURE_LOCK = 0;
    private static final int AUTOWHITEBALANCE_LOCK = 1;

    // For external camera recording
    private static final int VIDEO_BIT_RATE_IN_BPS = 128000;

    // Some exif tags that are not defined by ExifInterface but supported.
    private static final String TAG_DATETIME_DIGITIZED = ""DateTimeDigitized"";
    private static final String TAG_SUBSEC_TIME = ""SubSecTime"";
    private static final String TAG_SUBSEC_TIME_ORIG = ""SubSecTimeOriginal"";
    private static final String TAG_SUBSEC_TIME_DIG = ""SubSecTimeDigitized"";

    private PreviewCallback mPreviewCallback = new PreviewCallback();
    private TestShutterCallback mShutterCallback = new TestShutterCallback();
    private RawPictureCallback mRawPictureCallback = new RawPictureCallback();
    private JpegPictureCallback mJpegPictureCallback = new JpegPictureCallback();
    private TestErrorCallback mErrorCallback = new TestErrorCallback();
    private AutoFocusCallback mAutoFocusCallback = new AutoFocusCallback();
    private AutoFocusMoveCallback mAutoFocusMoveCallback = new AutoFocusMoveCallback();

    private Looper mLooper = null;
    private final ConditionVariable mPreviewDone = new ConditionVariable();
    private final ConditionVariable mFocusDone = new ConditionVariable();
    private final ConditionVariable mSnapshotDone = new ConditionVariable();
    private int[] mCameraIds;

    Camera mCamera;
    boolean mIsExternalCamera;

    @Rule
    public ActivityTestRule<CameraCtsActivity> mActivityRule =
            new ActivityTestRule<>(CameraCtsActivity.class);

    @Before
    public void setUp() throws Exception {
        // Some of the tests run on the UI thread. In case some of the operations take a long time to complete,
        // wait for window to receive focus. This ensure that the focus event from input flinger has been handled,
        // and avoids getting ANR.
        WindowUtil.waitForFocus(mActivityRule.getActivity());
        mCameraIds = CameraUtils.deriveCameraIdsUnderTest();
    }

    @After
    public void tearDown() throws Exception {
        if (mCamera != null) {
            mCamera.release();
            mCamera = null;
        }
    }

    /*
     * Initializes the message looper so that the Camera object can
     * receive the callback messages.
     */
    private void initializeMessageLooper(final int cameraId) throws IOException {
        LooperInfo looperInfo = new LooperInfo();
        initializeMessageLooper(cameraId, mErrorCallback, looperInfo);
        mIsExternalCamera = looperInfo.isExternalCamera;
        mCamera = looperInfo.camera;
        mLooper = looperInfo.looper;
    }

    private final class LooperInfo {
        Camera camera = null;
        Looper looper = null;
        boolean isExternalCamera = false;
    };

    /*
     * Initializes the message looper so that the Camera object can
     * receive the callback messages.
     */
    private void initializeMessageLooper(final int cameraId, final ErrorCallback errorCallback,
            LooperInfo looperInfo) throws IOException {
        final ConditionVariable startDone = new ConditionVariable();
        final CameraCtsActivity activity = mActivityRule.getActivity();
        new Thread() {
            @Override
            public void run() {
                Log.v(TAG, ""start loopRun for cameraId "" + cameraId);
                // Set up a looper to be used by camera.
                Looper.prepare();
                // Save the looper so that we can terminate this thread
                // after we are done with it.
                looperInfo.looper = Looper.myLooper();
                try {
                    looperInfo.isExternalCamera = CameraUtils.isExternal(
                            activity.getApplicationContext(), cameraId);
                } catch (Exception e) {
                    Log.e(TAG, ""Unable to query external camera!"" + e);
                }

                try {
                    looperInfo.camera = Camera.open(cameraId);
                    looperInfo.camera.setErrorCallback(errorCallback);
                } catch (RuntimeException e) {
                    Log.e(TAG, ""Fail to open camera id "" + cameraId + "": "" + e);
                }
                Log.v(TAG, ""camera"" + cameraId + "" is opened"");
                startDone.open();
                Looper.loop(); // Blocks forever until Looper.quit() is called.
                if (VERBOSE) Log.v(TAG, ""initializeMessageLooper: quit."");
            }
        }.start();

        Log.v(TAG, ""start waiting for looper"");
        if (!startDone.block(WAIT_FOR_COMMAND_TO_COMPLETE)) {
            Log.v(TAG, ""initializeMessageLooper: start timeout"");
            fail(""initializeMessageLooper: start timeout"");
        }
        assertNotNull(""Fail to open camera "" + cameraId, looperInfo.camera);
        looperInfo.camera.setPreviewDisplay(activity.getSurfaceView().getHolder());
        File parent = activity.getPackageManager().isInstantApp()
                ? activity.getFilesDir()
                : activity.getExternalFilesDir(null);

        mJpegPath = parent.getPath() + ""/test.jpg"";
        mRecordingPath = parent.getPath() + ""/test_video.mp4"";
    }

    /*
     * Terminates the message looper thread.
     */
    private void terminateMessageLooper() throws Exception {
        terminateMessageLooper(false);
    }

    /*
     * Terminates the message looper thread, optionally allowing evict error
     */
    private void terminateMessageLooper(boolean allowEvict) throws Exception {
        LooperInfo looperInfo = new LooperInfo();
        looperInfo.camera = mCamera;
        looperInfo.looper = mLooper;
        terminateMessageLooper(allowEvict, mCameraErrorCode, looperInfo);
        mCamera = null;
    }

    /*
     * Terminates the message looper thread, optionally allowing evict error
     */
    private void terminateMessageLooper(final boolean allowEvict, final int errorCode,
            final LooperInfo looperInfo) throws Exception {
        looperInfo.looper.quit();
        // Looper.quit() is asynchronous. The looper may still has some
        // preview callbacks in the queue after quit is called. The preview
        // callback still uses the camera object (setHasPreviewCallback).
        // After camera is released, RuntimeException will be thrown from
        // the method. So we need to join the looper thread here.
        looperInfo.looper.getThread().join();
        looperInfo.camera.release();
        looperInfo.camera = null;
        if (allowEvict) {
            assertTrue(""Got unexpected camera error callback."",
                    (NO_ERROR == errorCode ||
                    Camera.CAMERA_ERROR_EVICTED == errorCode));
        } else {
            assertEquals(""Got camera error callback."", NO_ERROR, errorCode);
        }
    }

    // Align 'x' to 'to', which should be a power of 2
    private static int align(int x, int to) {
        return (x + (to-1)) & ~(to - 1);
    }
    private static int calculateBufferSize(int width, int height,
                                           int format, int bpp) {

        if (VERBOSE) {
            Log.v(TAG, ""calculateBufferSize: w="" + width + "",h="" + height
            + "",f="" + format + "",bpp="" + bpp);
        }

        if (format == ImageFormat.YV12) {
            /*
            http://developer.android.com/reference/android/graphics/ImageFormat.html#YV12
            */

            int stride = align(width, 16);

            int y_size = stride * height;
            int c_stride = align(stride/2, 16);
            int c_size = c_stride * height/2;
            int size = y_size + c_size * 2;

            if (VERBOSE) {
                Log.v(TAG, ""calculateBufferSize: YV12 size= "" + size);
            }

            return size;

        }
        else {
            return width * height * bpp / 8;
        }
    }

    //Implement the previewCallback
    private final class PreviewCallback
            implements android.hardware.Camera.PreviewCallback {
        public void onPreviewFrame(byte [] data, Camera camera) {
            if (data == null) {
                mPreviewCallbackResult = PREVIEW_CALLBACK_DATA_NULL;
                mPreviewDone.open();
                return;
            }
            Size size = camera.getParameters().getPreviewSize();
            int format = camera.getParameters().getPreviewFormat();
            int bitsPerPixel = ImageFormat.getBitsPerPixel(format);
            if (calculateBufferSize(size.width, size.height,
                    format, bitsPerPixel) != data.length) {
                Log.e(TAG, ""Invalid frame size "" + data.length + "". width="" + size.width
                        + "". height="" + size.height + "". bitsPerPixel="" + bitsPerPixel);
                mPreviewCallbackResult = PREVIEW_CALLBACK_INVALID_FRAME_SIZE;
                mPreviewDone.open();
                return;
            }
            mPreviewCallbackResult = PREVIEW_CALLBACK_RECEIVED;
            mCamera.stopPreview();
            if (VERBOSE) Log.v(TAG, ""notify the preview callback"");
            mPreviewDone.open();
            if (VERBOSE) Log.v(TAG, ""Preview callback stop"");
        }
    }

    //Implement the shutterCallback
    private final class TestShutterCallback implements ShutterCallback {
        public void onShutter() {
            mShutterCallbackResult = true;
            if (VERBOSE) Log.v(TAG, ""onShutter called"");
        }
    }

    //Implement the RawPictureCallback
    private final class RawPictureCallback implements PictureCallback {
        public void onPictureTaken(byte [] rawData, Camera camera) {
            mRawPictureCallbackResult = true;
            if (VERBOSE) Log.v(TAG, ""RawPictureCallback callback"");
        }
    }

    // Implement the JpegPictureCallback
    private final class JpegPictureCallback implements PictureCallback {
        public void onPictureTaken(byte[] rawData, Camera camera) {
            try {
                mJpegData = rawData;
                if (rawData != null) {
                    // try to store the picture on the SD card
                    File rawoutput = new File(mJpegPath);
                    FileOutputStream outStream = new FileOutputStream(rawoutput);
                    outStream.write(rawData);
                    outStream.close();
                    mJpegPictureCallbackResult = true;

                    if (VERBOSE) {
                        Log.v(TAG, ""JpegPictureCallback rawDataLength = "" + rawData.length);
                    }
                } else {
                    mJpegPictureCallbackResult = false;
                }
                mSnapshotDone.open();
                if (VERBOSE) Log.v(TAG, ""Jpeg Picture callback"");
            } catch (IOException e) {
                // no need to fail here; callback worked fine
                Log.w(TAG, ""Error writing picture to sd card."");
            }
        }
    }

    // Implement the ErrorCallback
    private final class TestErrorCallback implements ErrorCallback {
        public void onError(int error, Camera camera) {
            Log.e(TAG, ""Got camera error="" + error);
            mCameraErrorCode = error;
        }
    }

    // parent independent version of TestErrorCallback
    private static final class TestErrorCallbackI implements ErrorCallback {
        private int mCameraErrorCode = NO_ERROR;
        public void onError(int error, Camera camera) {
            Log.e(TAG, ""Got camera error="" + error);
            mCameraErrorCode = error;
        }
    }

    private final class AutoFocusCallback
            implements android.hardware.Camera.AutoFocusCallback {
        public void onAutoFocus(boolean success, Camera camera) {
            mAutoFocusSucceeded = success;
            Log.v(TAG, ""AutoFocusCallback success="" + success);
            mFocusDone.open();
        }
    }

    private final class AutoFocusMoveCallback
            implements android.hardware.Camera.AutoFocusMoveCallback {
        @Override
        public void onAutoFocusMoving(boolean start, Camera camera) {
        }
    }

    private void waitForPreviewDone() {
        if (VERBOSE) Log.v(TAG, ""Wait for preview callback"");
        if (!mPreviewDone.block(WAIT_FOR_COMMAND_TO_COMPLETE)) {
            // timeout could be expected or unexpected. The caller will decide.
            Log.v(TAG, ""waitForPreviewDone: timeout"");
        }
        mPreviewDone.close();
    }

    private boolean waitForFocusDone() {
        boolean result = mFocusDone.block(WAIT_FOR_FOCUS_TO_COMPLETE);
        if (!result) {
            // timeout could be expected or unexpected. The caller will decide.
            Log.v(TAG, ""waitForFocusDone: timeout"");
        }
        mFocusDone.close();
        return result;
    }

    private void waitForSnapshotDone() {
        if (!mSnapshotDone.block(WAIT_FOR_SNAPSHOT_TO_COMPLETE)) {
            // timeout could be expected or unexpected. The caller will decide.
            Log.v(TAG, ""waitForSnapshotDone: timeout"");
        }
        mSnapshotDone.close();
    }

    private void checkPreviewCallback() throws Exception {
        if (VERBOSE) Log.v(TAG, ""check preview callback"");
        mCamera.startPreview();
        waitForPreviewDone();
        mCamera.setPreviewCallback(null);
    }

    /**
     * Start preview and wait for the first preview callback, which indicates the
     * preview becomes active.
     */
    private void blockingStartPreview() {
        mCamera.setPreviewCallback(new SimplePreviewStreamCb(/*Id*/0));
        mCamera.startPreview();
        waitForPreviewDone();
        mCamera.setPreviewCallback(null);
    }

    /*
     * Test case 1: Take a picture and verify all the callback
     * functions are called properly.
     */
    @UiThreadTest"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"testTakePicture"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"public void testTakePicture() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            initializeMessageLooper(id);
            mCamera.startPreview();
            subtestTakePictureByCamera(false, 0, 0);
            terminateMessageLooper();
        }
    }

    private void subtestTakePictureByCamera(boolean isVideoSnapshot,
            int videoWidth, int videoHeight) throws Exception {
        int videoSnapshotMinArea =
                videoWidth * videoHeight; // Temporary until new API definitions

        Size pictureSize = mCamera.getParameters().getPictureSize();
        mCamera.autoFocus(mAutoFocusCallback);
        assertTrue(waitForFocusDone());
        mJpegData = null;
        mCamera.takePicture(mShutterCallback, mRawPictureCallback, mJpegPictureCallback);
        waitForSnapshotDone();
        assertTrue(""Shutter callback not received"", mShutterCallbackResult);
        assertTrue(""Raw picture callback not received"", mRawPictureCallbackResult);
        assertTrue(""Jpeg picture callback not recieved"", mJpegPictureCallbackResult);
        assertNotNull(mJpegData);
        BitmapFactory.Options bmpOptions = new BitmapFactory.Options();
        bmpOptions.inJustDecodeBounds = true;
        BitmapFactory.decodeByteArray(mJpegData, 0, mJpegData.length, bmpOptions);
        if (!isVideoSnapshot) {
            assertEquals(pictureSize.width, bmpOptions.outWidth);
            assertEquals(pictureSize.height, bmpOptions.outHeight);
        } else {
            int realArea = bmpOptions.outWidth * bmpOptions.outHeight;
            if (VERBOSE) Log.v(TAG, ""Video snapshot is "" +
                    bmpOptions.outWidth + "" x "" + bmpOptions.outHeight +
                    "", video size is "" + videoWidth + "" x "" + videoHeight);
            assertTrue (""Video snapshot too small! Expected at least "" +
                    videoWidth + "" x "" + videoHeight + "" ("" +
                    videoSnapshotMinArea/1000000. + "" MP)"",
                    realArea >= videoSnapshotMinArea);
        }
    }

    @UiThreadTest"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"testParameters"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"public void testParameters() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            testParametersByCamera(id);
        }
    }

    private void testParametersByCamera(int cameraId) throws Exception {
        initializeMessageLooper(cameraId);
        // we can get parameters just by getxxx method due to the private constructor
        Parameters pSet = mCamera.getParameters();
        assertParameters(pSet);
        terminateMessageLooper();
    }

    // Also test Camera.Parameters
    private void assertParameters(Parameters parameters) {
        // Parameters constants
        final int PICTURE_FORMAT = ImageFormat.JPEG;
        final int PREVIEW_FORMAT = ImageFormat.NV21;

        // Before setting Parameters
        final int origPictureFormat = parameters.getPictureFormat();
        final int origPictureWidth = parameters.getPictureSize().width;
        final int origPictureHeight = parameters.getPictureSize().height;
        final int origPreviewFormat = parameters.getPreviewFormat();
        final int origPreviewWidth = parameters.getPreviewSize().width;
        final int origPreviewHeight = parameters.getPreviewSize().height;
        final int origPreviewFrameRate = parameters.getPreviewFrameRate();

        assertTrue(origPictureWidth > 0);
        assertTrue(origPictureHeight > 0);
        assertTrue(origPreviewWidth > 0);
        assertTrue(origPreviewHeight > 0);
        assertTrue(origPreviewFrameRate > 0);

        // The default preview format must be yuv420 (NV21).
        assertEquals(ImageFormat.NV21, origPreviewFormat);

        // The default picture format must be Jpeg.
        assertEquals(ImageFormat.JPEG, origPictureFormat);

        // If camera supports flash, the default flash mode must be off.
        String flashMode = parameters.getFlashMode();
        assertTrue(flashMode == null || flashMode.equals(parameters.FLASH_MODE_OFF));
        String wb = parameters.getWhiteBalance();
        assertTrue(wb == null || wb.equals(parameters.WHITE_BALANCE_AUTO));
        String effect = parameters.getColorEffect();
        assertTrue(effect == null || effect.equals(parameters.EFFECT_NONE));

        // Some parameters must be supported.
        List<Size> previewSizes = parameters.getSupportedPreviewSizes();
        List<Size> pictureSizes = parameters.getSupportedPictureSizes();
        List<Integer> previewFormats = parameters.getSupportedPreviewFormats();
        List<Integer> pictureFormats = parameters.getSupportedPictureFormats();
        List<Integer> frameRates = parameters.getSupportedPreviewFrameRates();
        List<String> focusModes = parameters.getSupportedFocusModes();
        String focusMode = parameters.getFocusMode();
        float focalLength = parameters.getFocalLength();
        float horizontalViewAngle = parameters.getHorizontalViewAngle();
        float verticalViewAngle = parameters.getVerticalViewAngle();
        int jpegQuality = parameters.getJpegQuality();
        int jpegThumnailQuality = parameters.getJpegThumbnailQuality();
        assertTrue(previewSizes != null && previewSizes.size() != 0);
        assertTrue(pictureSizes != null && pictureSizes.size() != 0);
        assertTrue(previewFormats != null && previewFormats.size() >= 2);
        assertTrue(previewFormats.contains(ImageFormat.NV21));
        assertTrue(previewFormats.contains(ImageFormat.YV12));
        assertTrue(pictureFormats != null && pictureFormats.size() != 0);
        assertTrue(frameRates != null && frameRates.size() != 0);
        assertTrue(focusModes != null && focusModes.size() != 0);
        assertNotNull(focusMode);
        // The default focus mode must be auto if it exists.
        if (focusModes.contains(Parameters.FOCUS_MODE_AUTO)) {
            assertEquals(Parameters.FOCUS_MODE_AUTO, focusMode);
        }

        if (mIsExternalCamera) {
            // External camera by default reports -1.0, but don't fail if
            // the HAL implementation somehow chooses to report this information.
            assertTrue(focalLength == -1.0 || focalLength > 0);
            assertTrue(horizontalViewAngle == -1.0 ||
                    (horizontalViewAngle > 0 && horizontalViewAngle <= 360));
            assertTrue(verticalViewAngle == -1.0 ||
                    (verticalViewAngle > 0 && verticalViewAngle <= 360));
        } else {
            assertTrue(focalLength > 0);
            assertTrue(horizontalViewAngle > 0 && horizontalViewAngle <= 360);
            assertTrue(verticalViewAngle > 0 && verticalViewAngle <= 360);
        }

        Size previewSize = previewSizes.get(0);
        Size pictureSize = pictureSizes.get(0);
        assertTrue(jpegQuality >= 1 && jpegQuality <= 100);
        assertTrue(jpegThumnailQuality >= 1 && jpegThumnailQuality <= 100);

        // If a parameter is supported, both getXXX and getSupportedXXX have to
        // be non null.
        if (parameters.getWhiteBalance() != null) {
            assertNotNull(parameters.getSupportedWhiteBalance());
        }
        if (parameters.getSupportedWhiteBalance() != null) {
            assertNotNull(parameters.getWhiteBalance());
        }
        if (parameters.getColorEffect() != null) {
            assertNotNull(parameters.getSupportedColorEffects());
        }
        if (parameters.getSupportedColorEffects() != null) {
            assertNotNull(parameters.getColorEffect());
        }
        if (parameters.getAntibanding() != null) {
            assertNotNull(parameters.getSupportedAntibanding());
        }
        if (parameters.getSupportedAntibanding() != null) {
            assertNotNull(parameters.getAntibanding());
        }
        if (parameters.getSceneMode() != null) {
            assertNotNull(parameters.getSupportedSceneModes());
        }
        if (parameters.getSupportedSceneModes() != null) {
            assertNotNull(parameters.getSceneMode());
        }
        if (parameters.getFlashMode() != null) {
            assertNotNull(parameters.getSupportedFlashModes());
        }
        if (parameters.getSupportedFlashModes() != null) {
            assertNotNull(parameters.getFlashMode());
        }

        // Check if the sizes value contain invalid characters.
        assertNoLetters(parameters.get(""preview-size-values""), ""preview-size-values"");
        assertNoLetters(parameters.get(""picture-size-values""), ""picture-size-values"");
        assertNoLetters(parameters.get(""jpeg-thumbnail-size-values""),
                ""jpeg-thumbnail-size-values"");

        // Set the parameters.
        parameters.setPictureFormat(PICTURE_FORMAT);
        assertEquals(PICTURE_FORMAT, parameters.getPictureFormat());
        parameters.setPictureSize(pictureSize.width, pictureSize.height);
        assertEquals(pictureSize.width, parameters.getPictureSize().width);
        assertEquals(pictureSize.height, parameters.getPictureSize().height);
        parameters.setPreviewFormat(PREVIEW_FORMAT);
        assertEquals(PREVIEW_FORMAT, parameters.getPreviewFormat());
        parameters.setPreviewFrameRate(frameRates.get(0));
        assertEquals(frameRates.get(0).intValue(), parameters.getPreviewFrameRate());
        parameters.setPreviewSize(previewSize.width, previewSize.height);
        assertEquals(previewSize.width, parameters.getPreviewSize().width);
        assertEquals(previewSize.height, parameters.getPreviewSize().height);

        mCamera.setParameters(parameters);
        Parameters paramActual = mCamera.getParameters();

        assertTrue(isValidPixelFormat(paramActual.getPictureFormat()));
        assertEquals(pictureSize.width, paramActual.getPictureSize().width);
        assertEquals(pictureSize.height, paramActual.getPictureSize().height);
        assertTrue(isValidPixelFormat(paramActual.getPreviewFormat()));
        assertEquals(previewSize.width, paramActual.getPreviewSize().width);
        assertEquals(previewSize.height, paramActual.getPreviewSize().height);
        assertTrue(paramActual.getPreviewFrameRate() > 0);

        checkExposureCompensation(parameters);
        checkPreferredPreviewSizeForVideo(parameters);
    }

    private void checkPreferredPreviewSizeForVideo(Parameters parameters) {
        List<Size> videoSizes = parameters.getSupportedVideoSizes();
        Size preferredPreviewSize = parameters.getPreferredPreviewSizeForVideo();

        // If getSupportedVideoSizes() returns null,
        // getPreferredPreviewSizeForVideo() will return null;
        // otherwise, if getSupportedVideoSizes() does not return null,
        // getPreferredPreviewSizeForVideo() will not return null.
        if (videoSizes == null) {
            assertNull(preferredPreviewSize);
        } else {
            assertNotNull(preferredPreviewSize);
        }

        // If getPreferredPreviewSizeForVideo() returns null,
        // getSupportedVideoSizes() will return null;
        // otherwise, if getPreferredPreviewSizeForVideo() does not return null,
        // getSupportedVideoSizes() will not return null.
        if (preferredPreviewSize == null) {
            assertNull(videoSizes);
        } else {
            assertNotNull(videoSizes);
        }

        if (videoSizes != null) {  // implies: preferredPreviewSize != null
            // If getSupportedVideoSizes() does not return null,
            // the returned list will contain at least one size.
            assertTrue(videoSizes.size() > 0);

            // In addition, getPreferredPreviewSizeForVideo() returns a size
            // that is among the supported preview sizes.
            List<Size> previewSizes = parameters.getSupportedPreviewSizes();
            assertNotNull(previewSizes);
            assertTrue(previewSizes.size() > 0);
            assertTrue(previewSizes.contains(preferredPreviewSize));
        }
    }

    private void checkExposureCompensation(Parameters parameters) {
        assertEquals(0, parameters.getExposureCompensation());
        int max = parameters.getMaxExposureCompensation();
        int min = parameters.getMinExposureCompensation();
        float step = parameters.getExposureCompensationStep();
        if (max == 0 && min == 0) {
            assertEquals(0f, step, 0.000001f);
            return;
        }
        assertTrue(step > 0);
        assertTrue(max >= 0);
        assertTrue(min <= 0);
    }

    private boolean isValidPixelFormat(int format) {
        return (format == ImageFormat.RGB_565) || (format == ImageFormat.NV21)
                || (format == ImageFormat.JPEG) || (format == ImageFormat.YUY2);
    }

    @UiThreadTest"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"testJpegThumbnailSize"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"public void testJpegThumbnailSize() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            initializeMessageLooper(id);
            testJpegThumbnailSizeByCamera(false, 0, 0);
            terminateMessageLooper();
        }
    }

    private void testJpegThumbnailSizeByCamera(boolean recording,
            int recordingWidth, int recordingHeight) throws Exception {
        // Thumbnail size parameters should have valid values.
        Parameters p = mCamera.getParameters();
        Size size = p.getJpegThumbnailSize();
        assertTrue(size.width > 0 && size.height > 0);
        List<Size> sizes = p.getSupportedJpegThumbnailSizes();
        assertTrue(sizes.size() >= 2);
        assertTrue(sizes.contains(size));
        assertTrue(sizes.contains(mCamera.new Size(0, 0)));
        Size pictureSize = p.getPictureSize();

        // Test if the thumbnail size matches the setting.
        if (!recording) mCamera.startPreview();
        mCamera.takePicture(mShutterCallback, mRawPictureCallback, mJpegPictureCallback);
        waitForSnapshotDone();
        assertTrue(mJpegPictureCallbackResult);
        ExifInterface exif = new ExifInterface(mJpegPath);
        assertTrue(exif.hasThumbnail());
        byte[] thumb = exif.getThumbnail();
        BitmapFactory.Options bmpOptions = new BitmapFactory.Options();
        bmpOptions.inJustDecodeBounds = true;
        BitmapFactory.decodeByteArray(thumb, 0, thumb.length, bmpOptions);
        if (!recording) {
            assertEquals(size.width, bmpOptions.outWidth);
            assertEquals(size.height, bmpOptions.outHeight);
        } else {
            assertTrue(bmpOptions.outWidth >= recordingWidth ||
                    bmpOptions.outWidth == size.width);
            assertTrue(bmpOptions.outHeight >= recordingHeight ||
                    bmpOptions.outHeight == size.height);
        }

        // Test no thumbnail case.
        p.setJpegThumbnailSize(0, 0);
        mCamera.setParameters(p);
        Size actual = mCamera.getParameters().getJpegThumbnailSize();
        assertEquals(0, actual.width);
        assertEquals(0, actual.height);
        if (!recording) mCamera.startPreview();
        mCamera.takePicture(mShutterCallback, mRawPictureCallback, mJpegPictureCallback);
        waitForSnapshotDone();
        assertTrue(mJpegPictureCallbackResult);
        exif = new ExifInterface(mJpegPath);
        assertFalse(exif.hasThumbnail());
        // Primary image should still be valid for no thumbnail capture.
        BitmapFactory.decodeFile(mJpegPath, bmpOptions);
        if (!recording) {
            assertTrue(""Jpeg primary image size should match requested size"",
                    bmpOptions.outWidth == pictureSize.width &&
                    bmpOptions.outHeight == pictureSize.height);
        } else {
            assertTrue(bmpOptions.outWidth >= recordingWidth ||
                    bmpOptions.outWidth == size.width);
            assertTrue(bmpOptions.outHeight >= recordingHeight ||
                    bmpOptions.outHeight == size.height);
        }

        assertNotNull(""Jpeg primary image data should be decodable"",
                BitmapFactory.decodeFile(mJpegPath));
    }

    @UiThreadTest"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"testJpegExif"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"public void testJpegExif() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            initializeMessageLooper(id);
            testJpegExifByCamera(false);
            terminateMessageLooper();
        }
    }

    private void testJpegExifByCamera(boolean recording) throws Exception {
        if (!recording) mCamera.startPreview();
        // Get current time in milliseconds, removing the millisecond part
        long captureStartTime = System.currentTimeMillis() / 1000 * 1000;
        mCamera.takePicture(mShutterCallback, mRawPictureCallback, mJpegPictureCallback);
        waitForSnapshotDone();

        Camera.Parameters parameters = mCamera.getParameters();
        double focalLength = parameters.getFocalLength();

        // Test various exif tags.
        ExifInterface exif = new ExifInterface(mJpegPath);
        StringBuffer failedCause = new StringBuffer(""Jpeg exif test failed:\n"");
        boolean extraExiftestPassed = checkExtraExifTagsSucceeds(failedCause, exif);

        if (VERBOSE) Log.v(TAG, ""Testing exif tag TAG_DATETIME"");
        String datetime = exif.getAttribute(ExifInterface.TAG_DATETIME);
        assertNotNull(datetime);
        assertTrue(datetime.length() == 19); // EXIF spec is ""yyyy:MM:dd HH:mm:ss"".
        // Datetime should be local time.
        SimpleDateFormat exifDateFormat = new SimpleDateFormat(""yyyy:MM:dd HH:mm:ss"");
        try {
            Date exifDateTime = exifDateFormat.parse(datetime);
            long captureFinishTime = exifDateTime.getTime();
            long timeDelta = captureFinishTime - captureStartTime;
            assertTrue(String.format(""Snapshot delay (%d ms) is not in range of [0, %d]"", timeDelta,
                    WAIT_FOR_SNAPSHOT_TO_COMPLETE),
                    timeDelta >= 0 && timeDelta <= WAIT_FOR_SNAPSHOT_TO_COMPLETE);
        } catch (ParseException e) {
            fail(String.format(""Invalid string value on exif tag TAG_DATETIME: %s"", datetime));
        }
        checkGpsDataNull(exif);
        double exifFocalLength = exif.getAttributeDouble(ExifInterface.TAG_FOCAL_LENGTH, -1);
        assertEquals(focalLength, exifFocalLength, 0.001);
        // Test image width and height exif tags. They should match the jpeg.
        assertBitmapAndJpegSizeEqual(mJpegData, exif);

        // Test gps exif tags.
        if (VERBOSE) Log.v(TAG, ""Testing exif GPS tags"");
        testGpsExifValues(parameters, 37.736071, -122.441983, 21, 1199145600,
            ""GPS NETWORK HYBRID ARE ALL FINE."");
        testGpsExifValues(parameters, 0.736071, 0.441983, 1, 1199145601, ""GPS"");
        testGpsExifValues(parameters, -89.736071, -179.441983, 100000, 1199145602, ""NETWORK"");

        // Test gps tags do not exist after calling removeGpsData. Also check if
        // image width and height exif match the jpeg when jpeg rotation is set.
        if (VERBOSE) Log.v(TAG, ""Testing exif GPS tag removal"");
        if (!recording) mCamera.startPreview();
        parameters.removeGpsData();
        parameters.setRotation(90); // For testing image width and height exif.
        mCamera.setParameters(parameters);
        mCamera.takePicture(mShutterCallback, mRawPictureCallback, mJpegPictureCallback);
        waitForSnapshotDone();
        exif = new ExifInterface(mJpegPath);
        checkGpsDataNull(exif);
        assertBitmapAndJpegSizeEqual(mJpegData, exif);
        // Reset the rotation to prevent from affecting other tests.
        parameters.setRotation(0);
        mCamera.setParameters(parameters);
    }

    /**
     * Correctness check of some extra exif tags.
     * <p>
     * Check some extra exif tags without asserting the check failures
     * immediately. When a failure is detected, the failure cause is logged,
     * the rest of the tests are still executed. The caller can assert with the
     * failure cause based on the returned test status.
     * </p>
     *
     * @param logBuf Log failure cause to this StringBuffer if there is
     * any failure.
     * @param exif The exif data associated with a jpeg image being tested.
     * @return true if no test failure is found, false if there is any failure.
     */
    private boolean checkExtraExifTagsSucceeds(StringBuffer logBuf, ExifInterface exif) {
        if (logBuf == null || exif == null) {
            throw new IllegalArgumentException(""failureCause and exif shouldn't be null"");
        }

        if (VERBOSE) Log.v(TAG, ""Testing extra exif tags"");
        boolean allTestsPassed = true;
        boolean passedSoFar = true;
        String failureMsg;

        // TAG_EXPOSURE_TIME
        // ExifInterface API gives exposure time value in the form of float instead of rational
        String exposureTime = exif.getAttribute(ExifInterface.TAG_EXPOSURE_TIME);
        passedSoFar = expectNotNull(""Exif TAG_EXPOSURE_TIME is null!"", logBuf, exposureTime);
        if (passedSoFar) {
            double exposureTimeValue = Double.parseDouble(exposureTime);
            failureMsg = ""Exif exposure time "" + exposureTime + "" should be a positive value"";
            passedSoFar = expectTrue(failureMsg, logBuf, exposureTimeValue > 0);
        }
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_APERTURE
        // ExifInterface API gives aperture value in the form of float instead of rational
        String aperture = exif.getAttribute(ExifInterface.TAG_APERTURE);
        passedSoFar = expectNotNull(""Exif TAG_APERTURE is null!"", logBuf, aperture);
        if (passedSoFar) {
            double apertureValue = Double.parseDouble(aperture);
            passedSoFar = expectTrue(""Exif TAG_APERTURE value "" + aperture + "" should be positive!"",
                    logBuf, apertureValue > 0);
        }
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_FLASH
        String flash = exif.getAttribute(ExifInterface.TAG_FLASH);
        passedSoFar = expectNotNull(""Exif TAG_FLASH is null!"", logBuf, flash);
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_WHITE_BALANCE
        String whiteBalance = exif.getAttribute(ExifInterface.TAG_WHITE_BALANCE);
        passedSoFar = expectNotNull(""Exif TAG_WHITE_BALANCE is null!"", logBuf, whiteBalance);
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_MAKE
        String make = exif.getAttribute(ExifInterface.TAG_MAKE);
        passedSoFar = expectNotNull(""Exif TAG_MAKE is null!"", logBuf, make);
        if (passedSoFar) {
            passedSoFar = expectTrue(""Exif TAG_MODEL value: "" + make
                    + "" should match build manufacturer: "" + Build.MANUFACTURER, logBuf,
                    make.equals(Build.MANUFACTURER));
        }
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_MODEL
        String model = exif.getAttribute(ExifInterface.TAG_MODEL);
        passedSoFar = expectNotNull(""Exif TAG_MODEL is null!"", logBuf, model);
        if (passedSoFar) {
            passedSoFar = expectTrue(""Exif TAG_MODEL value: "" + model
                    + "" should match build manufacturer: "" + Build.MODEL, logBuf,
                    model.equals(Build.MODEL));
        }
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_ISO
        int iso = exif.getAttributeInt(ExifInterface.TAG_ISO, -1);
        passedSoFar = expectTrue(""Exif ISO value "" + iso + "" is invalid"", logBuf, iso > 0);
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_DATETIME_DIGITIZED (a.k.a Create time for digital cameras).
        String digitizedTime = exif.getAttribute(TAG_DATETIME_DIGITIZED);
        passedSoFar = expectNotNull(""Exif TAG_DATETIME_DIGITIZED is null!"", logBuf, digitizedTime);
        if (passedSoFar) {
            String datetime = exif.getAttribute(ExifInterface.TAG_DATETIME);
            passedSoFar = expectNotNull(""Exif TAG_DATETIME is null!"", logBuf, datetime);
            if (passedSoFar) {
                passedSoFar = expectTrue(""dataTime should match digitizedTime"", logBuf,
                        digitizedTime.equals(datetime));
            }
        }
        allTestsPassed = allTestsPassed && passedSoFar;

        /**
         * TAG_SUBSEC_TIME. Since the sub second tag strings are truncated to at
         * most 9 digits in ExifInterface implementation, use getAttributeInt to
         * sanitize it. When the default value -1 is returned, it means that
         * this exif tag either doesn't exist or is a non-numerical invalid
         * string. Same rule applies to the rest of sub second tags.
         */
        int subSecTime = exif.getAttributeInt(TAG_SUBSEC_TIME, -1);
        passedSoFar = expectTrue(
                ""Exif TAG_SUBSEC_TIME value is null or invalid!"", logBuf, subSecTime > 0);
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_SUBSEC_TIME_ORIG
        int subSecTimeOrig = exif.getAttributeInt(TAG_SUBSEC_TIME_ORIG, -1);
        passedSoFar = expectTrue(
                ""Exif TAG_SUBSEC_TIME_ORIG value is null or invalid!"", logBuf, subSecTimeOrig > 0);
        allTestsPassed = allTestsPassed && passedSoFar;

        // TAG_SUBSEC_TIME_DIG
        int subSecTimeDig = exif.getAttributeInt(TAG_SUBSEC_TIME_DIG, -1);
        passedSoFar = expectTrue(
                ""Exif TAG_SUBSEC_TIME_DIG value is null or invalid!"", logBuf, subSecTimeDig > 0);
        allTestsPassed = allTestsPassed && passedSoFar;

        return allTestsPassed;
    }

    /**
     * Check if object is null and log failure msg.
     *
     * @param msg Failure msg.
     * @param logBuffer StringBuffer to log the failure msg.
     * @param obj Object to test.
     * @return true if object is not null, otherwise return false.
     */
    private boolean expectNotNull(String msg, StringBuffer logBuffer, Object obj)
    {
        if (obj == null) {
            logBuffer.append(msg + ""\n"");
        }
        return (obj != null);
    }

    /**
     * Check if condition is false and log failure msg.
     *
     * @param msg Failure msg.
     * @param logBuffer StringBuffer to log the failure msg.
     * @param condition Condition to test.
     * @return The value of the condition.
     */
    private boolean expectTrue(String msg, StringBuffer logBuffer, boolean condition) {
        if (!condition) {
            logBuffer.append(msg + ""\n"");
        }
        return condition;
    }

    private void assertBitmapAndJpegSizeEqual(byte[] jpegData, ExifInterface exif) {
        int exifWidth = exif.getAttributeInt(ExifInterface.TAG_IMAGE_WIDTH, 0);
        int exifHeight = exif.getAttributeInt(ExifInterface.TAG_IMAGE_LENGTH, 0);
        assertTrue(exifWidth != 0 && exifHeight != 0);
        BitmapFactory.Options bmpOptions = new BitmapFactory.Options();
        bmpOptions.inJustDecodeBounds = true;
        BitmapFactory.decodeByteArray(jpegData, 0, jpegData.length, bmpOptions);
        assertEquals(bmpOptions.outWidth, exifWidth);
        assertEquals(bmpOptions.outHeight, exifHeight);
    }

    private void testGpsExifValues(Parameters parameters, double latitude,
            double longitude, double altitude, long timestamp, String method)
            throws IOException {
        mCamera.startPreview();
        parameters.setGpsLatitude(latitude);
        parameters.setGpsLongitude(longitude);
        parameters.setGpsAltitude(altitude);
        parameters.setGpsTimestamp(timestamp);
        parameters.setGpsProcessingMethod(method);
        mCamera.setParameters(parameters);
        mCamera.takePicture(mShutterCallback, mRawPictureCallback, mJpegPictureCallback);
        waitForSnapshotDone();
        ExifInterface exif = new ExifInterface(mJpegPath);
        assertNotNull(exif.getAttribute(ExifInterface.TAG_GPS_LATITUDE));
        assertNotNull(exif.getAttribute(ExifInterface.TAG_GPS_LONGITUDE));
        assertNotNull(exif.getAttribute(ExifInterface.TAG_GPS_LATITUDE_REF));
        assertNotNull(exif.getAttribute(ExifInterface.TAG_GPS_LONGITUDE_REF));
        assertNotNull(exif.getAttribute(ExifInterface.TAG_GPS_TIMESTAMP));
        assertNotNull(exif.getAttribute(ExifInterface.TAG_GPS_DATESTAMP));
        assertEquals(method, exif.getAttribute(ExifInterface.TAG_GPS_PROCESSING_METHOD));
        float[] latLong = new float[2];
        assertTrue(exif.getLatLong(latLong));
        assertEquals((float)latitude, latLong[0], 0.0001f);
        assertEquals((float)longitude, latLong[1], 0.0001f);
        assertEquals(altitude, exif.getAltitude(-1), 1);
        assertEquals(timestamp, getGpsDateTimeFromJpeg(exif) / 1000);
    }

    private long getGpsDateTimeFromJpeg(ExifInterface exif) {
        String date = exif.getAttribute(ExifInterface.TAG_GPS_DATESTAMP);
        String time = exif.getAttribute(ExifInterface.TAG_GPS_TIMESTAMP);
        if (date == null || time == null) return -1;

        String dateTimeString = date + ' ' + time;
        ParsePosition pos = new ParsePosition(0);
        try {
            SimpleDateFormat formatter = new SimpleDateFormat(""yyyy:MM:dd HH:mm:ss"");
            formatter.setTimeZone(TimeZone.getTimeZone(""UTC""));

            Date datetime = formatter.parse(dateTimeString, pos);
            if (datetime == null) return -1;
            return datetime.getTime();
        } catch (IllegalArgumentException ex) {
            return -1;
        }
    }

    private void checkGpsDataNull(ExifInterface exif) {
        assertNull(exif.getAttribute(ExifInterface.TAG_GPS_LATITUDE));
        assertNull(exif.getAttribute(ExifInterface.TAG_GPS_LONGITUDE));
        assertNull(exif.getAttribute(ExifInterface.TAG_GPS_LATITUDE_REF));
        assertNull(exif.getAttribute(ExifInterface.TAG_GPS_LONGITUDE_REF));
        assertNull(exif.getAttribute(ExifInterface.TAG_GPS_TIMESTAMP));
        assertNull(exif.getAttribute(ExifInterface.TAG_GPS_DATESTAMP));
        assertNull(exif.getAttribute(ExifInterface.TAG_GPS_PROCESSING_METHOD));
    }

    @UiThreadTest"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"testImmediateZoom"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"public void testImmediateZoom() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            testImmediateZoomByCamera(id);
        }
    }

    private void testImmediateZoomByCamera(int id) throws Exception {
        initializeMessageLooper(id);

        Parameters parameters = mCamera.getParameters();
        if (!parameters.isZoomSupported()) {
            terminateMessageLooper();
            return;
        }

        // Test the zoom parameters.
        assertEquals(0, parameters.getZoom());  // default zoom should be 0.
        for (Size size: parameters.getSupportedPreviewSizes()) {
            parameters = mCamera.getParameters();
            Size pictureSize = getPictureSizeForPreview(size, parameters);
            parameters.setPreviewSize(size.width, size.height);
            parameters.setPictureSize(pictureSize.width, pictureSize.height);
            mCamera.setParameters(parameters);
            parameters = mCamera.getParameters();
            int maxZoom = parameters.getMaxZoom();
            assertTrue(maxZoom >= 0);

            // Zoom ratios should be sorted from small to large.
            List<Integer> ratios = parameters.getZoomRatios();
            assertEquals(maxZoom + 1, ratios.size());
            assertEquals(100, ratios.get(0).intValue());
            for (int i = 0; i < ratios.size() - 1; i++) {
                assertTrue(ratios.get(i) < ratios.get(i + 1));
            }
            blockingStartPreview();

            // Test each zoom step.
            for (int i = 0; i <= maxZoom; i++) {
                parameters.setZoom(i);
                mCamera.setParameters(parameters);
                assertEquals(i, mCamera.getParameters().getZoom());
            }

            // It should throw exception if an invalid value is passed.
            try {
                parameters.setZoom(maxZoom + 1);
                mCamera.setParameters(parameters);
                fail(""setZoom should throw exception."");
            } catch (RuntimeException e) {
                // expected
            }
            assertEquals(maxZoom, mCamera.getParameters().getZoom());

            mCamera.takePicture(mShutterCallback, mRawPictureCallback,
                                mJpegPictureCallback);
            waitForSnapshotDone();
        }

        terminateMessageLooper();
    }

    @UiThreadTest"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"testFocusDistances"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"public void testFocusDistances() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            testFocusDistancesByCamera(id);
        }
    }

    private void testFocusDistancesByCamera(int cameraId) throws Exception {
        initializeMessageLooper(cameraId);
        blockingStartPreview();

        Parameters parameters = mCamera.getParameters();

        // Test every supported focus mode.
        for (String focusMode: parameters.getSupportedFocusModes()) {
            parameters.setFocusMode(focusMode);
            mCamera.setParameters(parameters);
            parameters = mCamera.getParameters();
            assertEquals(focusMode, parameters.getFocusMode());
            checkFocusDistances(parameters);
            if (Parameters.FOCUS_MODE_AUTO.equals(focusMode)
                    || Parameters.FOCUS_MODE_MACRO.equals(focusMode)
                    || Parameters.FOCUS_MODE_CONTINUOUS_VIDEO.equals(focusMode)
                    || Parameters.FOCUS_MODE_CONTINUOUS_PICTURE.equals(focusMode)) {
                Log.v(TAG, ""Focus mode="" + focusMode);
                mCamera.autoFocus(mAutoFocusCallback);
                assertTrue(waitForFocusDone());
                parameters = mCamera.getParameters();
                checkFocusDistances(parameters);
                float[] initialFocusDistances = new float[3];
                parameters.getFocusDistances(initialFocusDistances);

                // Focus position should not change after autoFocus call.
                // Continuous autofocus should have stopped. Sleep some time and
                // check. Make sure continuous autofocus is not working. If the
                // focus mode is auto or macro, it is no harm to do the extra
                // test.
                Thread.sleep(500);
                parameters = mCamera.getParameters();
                float[] currentFocusDistances = new float[3];
                parameters.getFocusDistances(currentFocusDistances);
                assertEquals(initialFocusDistances, currentFocusDistances);

                // Focus position should not change after stopping preview.
                mCamera.stopPreview();
                parameters = mCamera.getParameters();
                parameters.getFocusDistances(currentFocusDistances);
                assertEquals(initialFocusDistances, currentFocusDistances);

                // Focus position should not change after taking a picture.
                mCamera.startPreview();
                mCamera.takePicture(mShutterCallback, mRawPictureCallback, mJpegPictureCallback);
                waitForSnapshotDone();
                parameters = mCamera.getParameters();
                parameters.getFocusDistances(currentFocusDistances);
                assertEquals(initialFocusDistances, currentFocusDistances);
                mCamera.startPreview();
            }
        }

        // Test if the method throws exception if the argument is invalid.
        try {
            parameters.getFocusDistances(null);
            fail(""getFocusDistances should not accept null."");
        } catch (IllegalArgumentException e) {
            // expected
        }

        try {
            parameters.getFocusDistances(new float[2]);
            fail(""getFocusDistances should not accept a float array with two elements."");
        } catch (IllegalArgumentException e) {
            // expected
        }

        try {
            parameters.getFocusDistances(new float[4]);
            fail(""getFocusDistances should not accept a float array with four elements."");
        } catch (IllegalArgumentException e) {
            // expected
        }
        terminateMessageLooper();
    }

    private void checkFocusDistances(Parameters parameters) {
        float[] distances = new float[3];
        parameters.getFocusDistances(distances);

        // Focus distances should be greater than 0.
        assertTrue(distances[Parameters.FOCUS_DISTANCE_NEAR_INDEX] > 0);
        assertTrue(distances[Parameters.FOCUS_DISTANCE_OPTIMAL_INDEX] > 0);
        assertTrue(distances[Parameters.FOCUS_DISTANCE_FAR_INDEX] > 0);

        // Make sure far focus distance >= optimal focus distance >= near focus distance.
        assertTrue(distances[Parameters.FOCUS_DISTANCE_FAR_INDEX] >=
                   distances[Parameters.FOCUS_DISTANCE_OPTIMAL_INDEX]);
        assertTrue(distances[Parameters.FOCUS_DISTANCE_OPTIMAL_INDEX] >=
                   distances[Parameters.FOCUS_DISTANCE_NEAR_INDEX]);

        // Far focus distance should be infinity in infinity focus mode.
        if (Parameters.FOCUS_MODE_INFINITY.equals(parameters.getFocusMode())) {
            assertEquals(Float.POSITIVE_INFINITY,
                         distances[Parameters.FOCUS_DISTANCE_FAR_INDEX]);
        }
    }

    @UiThreadTest"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"testPreviewPictureSizesCombination"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"(timeout=60*60*1000) // timeout = 60 mins for long running tests
    public void testPreviewPictureSizesCombination() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            testPreviewPictureSizesCombinationByCamera(id);
        }
    }

    // API exception on QCIF size. QCIF size along with anything larger than
    // 1920x1080 on either width/height is not guaranteed to be supported.
    private boolean isWaivedCombination(Size previewSize, Size pictureSize) {
        Size QCIF = mCamera.new Size(176, 144);
        Size FULL_HD = mCamera.new Size(1920, 1080);
        if (previewSize.equals(QCIF) && (pictureSize.width > FULL_HD.width ||
                pictureSize.height > FULL_HD.height)) {
            return true;
        }
        if (pictureSize.equals(QCIF) && (previewSize.width > FULL_HD.width ||
                previewSize.height > FULL_HD.height)) {
            return true;
        }
        return false;
    }

    private void testPreviewPictureSizesCombinationByCamera(int cameraId) throws Exception {
        initializeMessageLooper(cameraId);
        Parameters parameters = mCamera.getParameters();
        PreviewCbForPreviewPictureSizesCombination callback =
            new PreviewCbForPreviewPictureSizesCombination();

        // Test all combination of preview sizes and picture sizes.
        for (Size previewSize: parameters.getSupportedPreviewSizes()) {
            for (Size pictureSize: parameters.getSupportedPictureSizes()) {
                Log.v(TAG, ""Test previewSize=("" + previewSize.width + "","" +
                        previewSize.height + "") pictureSize=("" +
                        pictureSize.width + "","" + pictureSize.height + "")"");
                mPreviewCallbackResult = PREVIEW_CALLBACK_NOT_RECEIVED;
                mCamera.setPreviewCallback(callback);
                callback.expectedPreviewSize = previewSize;
                parameters.setPreviewSize(previewSize.width, previewSize.height);
                parameters.setPictureSize(pictureSize.width, pictureSize.height);
                try {
                    mCamera.setParameters(parameters);
                } catch (RuntimeException e) {
                    if (isWaivedCombination(previewSize, pictureSize)) {
                        Log.i(TAG, String.format(""Preview %dx%d and still %dx%d combination is"" +
                                ""waived"", previewSize.width, previewSize.height,
                                pictureSize.width, pictureSize.height));
                        continue;
                    }
                    throw e;
                }

                assertEquals(previewSize, mCamera.getParameters().getPreviewSize());
                assertEquals(pictureSize, mCamera.getParameters().getPictureSize());

                // Check if the preview size is the same as requested.
                try {
                    mCamera.startPreview();
                } catch (RuntimeException e) {
                    if (isWaivedCombination(previewSize, pictureSize)) {
                        Log.i(TAG, String.format(""Preview %dx%d and still %dx%d combination is"" +
                                ""waived"", previewSize.width, previewSize.height,
                                pictureSize.width, pictureSize.height));
                        continue;
                    }
                    throw e;
                }
                waitForPreviewDone();
                assertEquals(PREVIEW_CALLBACK_RECEIVED, mPreviewCallbackResult);

                // Check if the picture size is the same as requested.
                mCamera.takePicture(mShutterCallback, mRawPictureCallback, mJpegPictureCallback);
                waitForSnapshotDone();
                assertTrue(mJpegPictureCallbackResult);
                assertNotNull(mJpegData);
                BitmapFactory.Options bmpOptions = new BitmapFactory.Options();
                bmpOptions.inJustDecodeBounds = true;
                BitmapFactory.decodeByteArray(mJpegData, 0, mJpegData.length, bmpOptions);
                assertEquals(pictureSize.width, bmpOptions.outWidth);
                assertEquals(pictureSize.height, bmpOptions.outHeight);
            }
        }
        terminateMessageLooper();
    }

    private final class PreviewCbForPreviewPictureSizesCombination
            implements android.hardware.Camera.PreviewCallback {
        public Size expectedPreviewSize;
        public void onPreviewFrame(byte[] data, Camera camera) {
            if (data == null) {
                mPreviewCallbackResult = PREVIEW_CALLBACK_DATA_NULL;
                mPreviewDone.open();
                return;
            }
            Size size = camera.getParameters().getPreviewSize();
            int format = camera.getParameters().getPreviewFormat();
            int bitsPerPixel = ImageFormat.getBitsPerPixel(format);
            if (!expectedPreviewSize.equals(size) ||
                    calculateBufferSize(size.width, size.height,
                        format, bitsPerPixel) != data.length) {
                Log.e(TAG, ""Expected preview width="" + expectedPreviewSize.width + "", height=""
                        + expectedPreviewSize.height + "". Actual width="" + size.width + "", height=""
                        + size.height);
                Log.e(TAG, ""Frame data length="" + data.length + "". bitsPerPixel="" + bitsPerPixel);
                mPreviewCallbackResult = PREVIEW_CALLBACK_INVALID_FRAME_SIZE;
                mPreviewDone.open();
                return;
            }
            camera.setPreviewCallback(null);
            mPreviewCallbackResult = PREVIEW_CALLBACK_RECEIVED;
            mPreviewDone.open();
        }
    }

    @UiThreadTest"	""	""	"JPEG 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"testMeteringAreas"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"public void testMeteringAreas() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            initializeMessageLooper(id);
            Parameters parameters = mCamera.getParameters();
            int maxNumMeteringAreas = parameters.getMaxNumMeteringAreas();
            assertTrue(maxNumMeteringAreas >= 0);
            if (maxNumMeteringAreas > 0) {
                testAreas(METERING_AREA, maxNumMeteringAreas);
            }
            terminateMessageLooper();
        }
    }

    private void testAreas(int type, int maxNumAreas) throws Exception {
        mCamera.startPreview();

        // Test various valid cases.
        testValidAreas(type, null);                                  // the default area
        testValidAreas(type, makeAreas(-1000, -1000, 1000, 1000, 1)); // biggest area
        testValidAreas(type, makeAreas(-500, -500, 500, 500, 1000)); // medium area & biggest weight
        testValidAreas(type, makeAreas(0, 0, 1, 1, 1));              // smallest area

        ArrayList<Area> areas = new ArrayList();
        if (maxNumAreas > 1) {
            // Test overlapped areas.
            testValidAreas(type, makeAreas(-250, -250, 250, 250, 1, 0, 0, 500, 500, 2));
            // Test completely disjoint areas.
            testValidAreas(type, makeAreas(-250, -250, 0, 0, 1, 900, 900, 1000, 1000, 1));
            // Test the maximum number of areas.
            testValidAreas(type, makeAreas(-1000, -1000, 1000, 1000, 1000, maxNumAreas));
        }

        // Test various invalid cases.
        testInvalidAreas(type, makeAreas(-1001, -1000, 1000, 1000, 1));    // left should >= -1000
        testInvalidAreas(type, makeAreas(-1000, -1001, 1000, 1000, 1));    // top should >= -1000
        testInvalidAreas(type, makeAreas(-1000, -1000, 1001, 1000, 1));    // right should <= 1000
        testInvalidAreas(type, makeAreas(-1000, -1000, 1000, 1001, 1));    // bottom should <= 1000
        testInvalidAreas(type, makeAreas(-1000, -1000, 1000, 1000, 0));    // weight should >= 1
        testInvalidAreas(type, makeAreas(-1000, -1000, 1000, 1001, 1001)); // weight should <= 1000
        testInvalidAreas(type, makeAreas(500, -1000, 500, 1000, 1));       // left should < right
        testInvalidAreas(type, makeAreas(-1000, 500, 1000, 500, 1));       // top should < bottom
        testInvalidAreas(type, makeAreas(500, -1000, 499, 1000, 1));       // left should < right
        testInvalidAreas(type, makeAreas(-1000, 500, 100, 499, 1));        // top should < bottom
        testInvalidAreas(type, makeAreas(-250, -250, 250, 250, -1));       // weight should >= 1
        // Test when the number of areas exceeds maximum.
        testInvalidAreas(type, makeAreas(-1000, -1000, 1000, 1000, 1000, maxNumAreas + 1));
    }

    private static ArrayList<Area> makeAreas(int left, int top, int right, int bottom, int weight) {
        ArrayList<Area> areas = new ArrayList<Area>();
        areas.add(new Area(new Rect(left, top, right, bottom), weight));
        return areas;
    }

    private static ArrayList<Area> makeAreas(int left, int top, int right, int bottom,
            int weight, int number) {
        ArrayList<Area> areas = new ArrayList<Area>();
        for (int i = 0; i < number; i++) {
            areas.add(new Area(new Rect(left, top, right, bottom), weight));
        }
        return areas;
    }

    private static ArrayList<Area> makeAreas(int left1, int top1, int right1,
            int bottom1, int weight1, int left2, int top2, int right2,
            int bottom2, int weight2) {
        ArrayList<Area> areas = new ArrayList<Area>();
        areas.add(new Area(new Rect(left1, top1, right1, bottom1), weight1));
        areas.add(new Area(new Rect(left2, top2, right2, bottom2), weight2));
        return areas;
    }

    private void testValidAreas(int areaType, ArrayList<Area> areas) {
        if (areaType == FOCUS_AREA) {
            testValidFocusAreas(areas);
        } else {
            testValidMeteringAreas(areas);
        }
    }

    private void testInvalidAreas(int areaType, ArrayList<Area> areas) {
        if (areaType == FOCUS_AREA) {
            testInvalidFocusAreas(areas);
        } else {
            testInvalidMeteringAreas(areas);
        }
    }

    private void testValidFocusAreas(ArrayList<Area> areas) {
        Parameters parameters = mCamera.getParameters();
        parameters.setFocusAreas(areas);
        mCamera.setParameters(parameters);
        parameters = mCamera.getParameters();
        assertEquals(areas, parameters.getFocusAreas());
        mCamera.autoFocus(mAutoFocusCallback);
        waitForFocusDone();
    }

    private void testInvalidFocusAreas(ArrayList<Area> areas) {
        Parameters parameters = mCamera.getParameters();
        List<Area> originalAreas = parameters.getFocusAreas();
        try {
            parameters.setFocusAreas(areas);
            mCamera.setParameters(parameters);
            fail(""Should throw exception when focus area is invalid."");
        } catch (RuntimeException e) {
            parameters = mCamera.getParameters();
            assertEquals(originalAreas, parameters.getFocusAreas());
        }
    }

    private void testValidMeteringAreas(ArrayList<Area> areas) {
        Parameters parameters = mCamera.getParameters();
        parameters.setMeteringAreas(areas);
        mCamera.setParameters(parameters);
        parameters = mCamera.getParameters();
        assertEquals(areas, parameters.getMeteringAreas());
    }

    private void testInvalidMeteringAreas(ArrayList<Area> areas) {
        Parameters parameters = mCamera.getParameters();
        List<Area> originalAreas = parameters.getMeteringAreas();
        try {
            parameters.setMeteringAreas(areas);
            mCamera.setParameters(parameters);
            fail(""Should throw exception when metering area is invalid."");
        } catch (RuntimeException e) {
            parameters = mCamera.getParameters();
            assertEquals(originalAreas, parameters.getMeteringAreas());
        }
    }

    // Apps should be able to call startPreview in jpeg callback.
    @UiThreadTest"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"testJpegCallbackStartPreview"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"public void testJpegCallbackStartPreview() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            testJpegCallbackStartPreviewByCamera(id);
        }
    }

    private void testJpegCallbackStartPreviewByCamera(int cameraId) throws Exception {
        initializeMessageLooper(cameraId);
        mCamera.startPreview();
        mCamera.takePicture(mShutterCallback, mRawPictureCallback, new JpegStartPreviewCallback());
        waitForSnapshotDone();
        terminateMessageLooper();
        assertTrue(mJpegPictureCallbackResult);
    }

    private final class JpegStartPreviewCallback implements PictureCallback {
        public void onPictureTaken(byte[] rawData, Camera camera) {
            try {
                camera.startPreview();
                mJpegPictureCallbackResult = true;
            } catch (Exception e) {
            }
            mSnapshotDone.open();
        }
    }

    @UiThreadTest"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"testRecordingHint"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"public void testRecordingHint() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            testRecordingHintByCamera(id);
        }
    }

    private void testRecordingHintByCamera(int cameraId) throws Exception {
        initializeMessageLooper(cameraId);
        Parameters parameters = mCamera.getParameters();

        SurfaceHolder holder = mActivityRule.getActivity().getSurfaceView().getHolder();
        CamcorderProfile profile = null; // for built-in camera
        Camera.Size videoSize = null; // for external camera
        int frameRate = -1; // for external camera

        if (mIsExternalCamera) {
            videoSize = setupExternalCameraRecord(parameters);
            frameRate = parameters.getPreviewFrameRate();
        } else {
            profile = CamcorderProfile.get(cameraId, CamcorderProfile.QUALITY_LOW);
            setPreviewSizeByProfile(parameters, profile);
        }


        // Test recording videos and taking pictures when the hint is off and on.
        for (int i = 0; i < 2; i++) {
            parameters.setRecordingHint(i == 0 ? false : true);
            mCamera.setParameters(parameters);
            mCamera.startPreview();
            if (mIsExternalCamera) {
                recordVideoSimpleBySize(videoSize, frameRate, holder);
            } else {
                recordVideoSimple(profile, holder);
            }
            mCamera.takePicture(mShutterCallback, mRawPictureCallback, mJpegPictureCallback);
            waitForSnapshotDone();
            assertTrue(mJpegPictureCallbackResult);
        }

        // Can change recording hint when the preview is active.
        mCamera.startPreview();
        parameters.setRecordingHint(false);
        mCamera.setParameters(parameters);
        parameters.setRecordingHint(true);
        mCamera.setParameters(parameters);
        terminateMessageLooper();
    }

    private void recordVideoSimpleBySize(Camera.Size size, int frameRate,
            SurfaceHolder holder) throws Exception {
        mCamera.unlock();
        MediaRecorder recorder = new MediaRecorder();
        try {
            recorder.setCamera(mCamera);
            recorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);
            recorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);
            recorder.setOutputFormat(MediaRecorder.OutputFormat.DEFAULT);
            recorder.setVideoEncodingBitRate(VIDEO_BIT_RATE_IN_BPS);
            recorder.setVideoEncoder(MediaRecorder.VideoEncoder.DEFAULT);
            recorder.setAudioEncoder(MediaRecorder.AudioEncoder.DEFAULT);
            recorder.setVideoSize(size.width, size.height);
            recorder.setVideoFrameRate(frameRate);
            recorder.setOutputFile(mRecordingPath);
            recorder.setPreviewDisplay(holder.getSurface());
            recorder.prepare();
            recorder.start();
            Thread.sleep(2000);
            recorder.stop();
        } finally {
            recorder.release();
            mCamera.lock();
        }
    }

    private void recordVideoSimple(CamcorderProfile profile,
            SurfaceHolder holder) throws Exception {
        mCamera.unlock();
        MediaRecorder recorder = new MediaRecorder();
        try {
            recorder.setCamera(mCamera);
            recorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);
            recorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);
            recorder.setProfile(profile);
            recorder.setOutputFile(mRecordingPath);
            recorder.setPreviewDisplay(holder.getSurface());
            recorder.prepare();
            recorder.start();
            Thread.sleep(2000);
            recorder.stop();
        } finally {
            recorder.release();
            mCamera.lock();
        }
    }

    @UiThreadTest"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"test3ALockInteraction"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"public void test3ALockInteraction() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            initializeMessageLooper(id);
            Parameters parameters = mCamera.getParameters();
            boolean locksSupported =
                    parameters.isAutoWhiteBalanceLockSupported() &&
                    parameters.isAutoExposureLockSupported();
            if (locksSupported) {
                subtestLockInteractions();
            }
            terminateMessageLooper();
        }
    }

    private void subtestLockCommon(int type) {
        // Verify lock is not set on open()
        assert3ALockState(""Lock not released after open()"", type, false);

        // Verify lock can be set, unset before preview
        set3ALockState(true, type);
        assert3ALockState(""Lock could not be set before 1st preview!"",
                type, true);

        set3ALockState(false, type);
        assert3ALockState(""Lock could not be unset before 1st preview!"",
                type, false);

        // Verify preview start does not set lock
        mCamera.startPreview();
        assert3ALockState(""Lock state changed by preview start!"", type, false);

        // Verify lock can be set, unset during preview
        set3ALockState(true, type);
        assert3ALockState(""Lock could not be set during preview!"", type, true);

        set3ALockState(false, type);
        assert3ALockState(""Lock could not be unset during preview!"",
                type, false);

        // Verify lock is not cleared by stop preview
        set3ALockState(true, type);
        mCamera.stopPreview();
        assert3ALockState(""Lock was cleared by stopPreview!"", type, true);

        // Verify that preview start does not clear lock
        set3ALockState(true, type);
        mCamera.startPreview();
        assert3ALockState(""Lock state changed by preview start!"", type, true);

        // Verify that taking a picture does not clear the lock
        set3ALockState(true, type);
        mCamera.takePicture(mShutterCallback, mRawPictureCallback,
                mJpegPictureCallback);
        waitForSnapshotDone();
        assert3ALockState(""Lock state was cleared by takePicture!"", type, true);

        mCamera.startPreview();
        Parameters parameters = mCamera.getParameters();
        for (String focusMode: parameters.getSupportedFocusModes()) {
            // TODO: Test this for other focus modes as well, once agreement is
            // reached on which ones it should apply to
            if (!Parameters.FOCUS_MODE_AUTO.equals(focusMode) ) {
                continue;
            }

            parameters.setFocusMode(focusMode);
            mCamera.setParameters(parameters);

            // Verify that autoFocus does not change the lock
            set3ALockState(false, type);
            mCamera.autoFocus(mAutoFocusCallback);
            assert3ALockState(""Lock was set by autoFocus in mode: "" + focusMode, type, false);
            assertTrue(waitForFocusDone());
            assert3ALockState(""Lock was set by autoFocus in mode: "" + focusMode, type, false);

            // Verify that cancelAutoFocus does not change the lock
            mCamera.cancelAutoFocus();
            assert3ALockState(""Lock was set by cancelAutoFocus!"", type, false);

            // Verify that autoFocus does not change the lock
            set3ALockState(true, type);
            mCamera.autoFocus(mAutoFocusCallback);
            assert3ALockState(""Lock was cleared by autoFocus in mode: "" + focusMode, type, true);
            assertTrue(waitForFocusDone());
            assert3ALockState(""Lock was cleared by autoFocus in mode: "" + focusMode, type, true);

            // Verify that cancelAutoFocus does not change the lock
            mCamera.cancelAutoFocus();
            assert3ALockState(""Lock was cleared by cancelAutoFocus!"", type, true);
        }
        mCamera.stopPreview();
    }

    private void subtestLockAdditionalAE() {
        // Verify that exposure compensation can be used while
        // AE lock is active
        mCamera.startPreview();
        Parameters parameters = mCamera.getParameters();
        parameters.setAutoExposureLock(true);
        mCamera.setParameters(parameters);
        parameters.setExposureCompensation(parameters.getMaxExposureCompensation());
        mCamera.setParameters(parameters);
        parameters = mCamera.getParameters();
        assertTrue(""Could not adjust exposure compensation with AE locked!"",
                parameters.getExposureCompensation() ==
                parameters.getMaxExposureCompensation() );

        parameters.setExposureCompensation(parameters.getMinExposureCompensation());
        mCamera.setParameters(parameters);
        parameters = mCamera.getParameters();
        assertTrue(""Could not adjust exposure compensation with AE locked!"",
                parameters.getExposureCompensation() ==
                parameters.getMinExposureCompensation() );
        mCamera.stopPreview();
    }

    private void subtestLockAdditionalAWB() {
        // Verify that switching AWB modes clears AWB lock
        mCamera.startPreview();
        Parameters parameters = mCamera.getParameters();
        String firstWb = null;
        for ( String wbMode: parameters.getSupportedWhiteBalance() ) {
            if (firstWb == null) {
                firstWb = wbMode;
            }
            parameters.setWhiteBalance(firstWb);
            mCamera.setParameters(parameters);
            parameters.setAutoWhiteBalanceLock(true);
            mCamera.setParameters(parameters);

            parameters.setWhiteBalance(wbMode);
            mCamera.setParameters(parameters);

            if (firstWb == wbMode) {
                assert3ALockState(""AWB lock was cleared when WB mode was unchanged!"",
                        AUTOWHITEBALANCE_LOCK, true);
            } else {
                assert3ALockState(""Changing WB mode did not clear AWB lock!"",
                        AUTOWHITEBALANCE_LOCK, false);
            }
        }
        mCamera.stopPreview();
    }

    private void subtestLockInteractions() {
        // Verify that toggling AE does not change AWB lock state
        set3ALockState(false, AUTOWHITEBALANCE_LOCK);
        set3ALockState(false, AUTOEXPOSURE_LOCK);

        set3ALockState(true, AUTOEXPOSURE_LOCK);
        assert3ALockState(""Changing AE lock affected AWB lock!"",
                AUTOWHITEBALANCE_LOCK, false);

        set3ALockState(false, AUTOEXPOSURE_LOCK);
        assert3ALockState(""Changing AE lock affected AWB lock!"",
                AUTOWHITEBALANCE_LOCK, false);

        set3ALockState(true, AUTOWHITEBALANCE_LOCK);

        set3ALockState(true, AUTOEXPOSURE_LOCK);
        assert3ALockState(""Changing AE lock affected AWB lock!"",
                AUTOWHITEBALANCE_LOCK, true);

        set3ALockState(false, AUTOEXPOSURE_LOCK);
        assert3ALockState(""Changing AE lock affected AWB lock!"",
                AUTOWHITEBALANCE_LOCK, true);

        // Verify that toggling AWB does not change AE lock state
        set3ALockState(false, AUTOWHITEBALANCE_LOCK);
        set3ALockState(false, AUTOEXPOSURE_LOCK);

        set3ALockState(true, AUTOWHITEBALANCE_LOCK);
        assert3ALockState(""Changing AWB lock affected AE lock!"",
                AUTOEXPOSURE_LOCK, false);

        set3ALockState(false, AUTOWHITEBALANCE_LOCK);
        assert3ALockState(""Changing AWB lock affected AE lock!"",
                AUTOEXPOSURE_LOCK, false);

        set3ALockState(true, AUTOEXPOSURE_LOCK);

        set3ALockState(true, AUTOWHITEBALANCE_LOCK);
        assert3ALockState(""Changing AWB lock affected AE lock!"",
                AUTOEXPOSURE_LOCK, true);

        set3ALockState(false, AUTOWHITEBALANCE_LOCK);
        assert3ALockState(""Changing AWB lock affected AE lock!"",
                AUTOEXPOSURE_LOCK, true);
    }

    private void assert3ALockState(String msg, int type, boolean state) {
        Parameters parameters = mCamera.getParameters();
        switch (type) {
            case AUTOEXPOSURE_LOCK:
                assertTrue(msg, state == parameters.getAutoExposureLock());
                break;
            case AUTOWHITEBALANCE_LOCK:
                assertTrue(msg, state == parameters.getAutoWhiteBalanceLock());
                break;
            default:
                assertTrue(""Unknown lock type "" + type, false);
                break;
        }
    }

    private void set3ALockState(boolean state, int type) {
        Parameters parameters = mCamera.getParameters();
        switch (type) {
            case AUTOEXPOSURE_LOCK:
                parameters.setAutoExposureLock(state);
                break;
            case AUTOWHITEBALANCE_LOCK:
                parameters.setAutoWhiteBalanceLock(state);
                break;
            default:
                assertTrue(""Unknown lock type ""+type, false);
                break;
        }
        mCamera.setParameters(parameters);
    }

    @UiThreadTest"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"testFaceDetection"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"public void testFaceDetection() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            testFaceDetectionByCamera(id);
        }
    }

    private void testFaceDetectionByCamera(int cameraId) throws Exception {
        final int FACE_DETECTION_TEST_DURATION = 3000;
        initializeMessageLooper(cameraId);
        mCamera.startPreview();
        Parameters parameters = mCamera.getParameters();
        int maxNumOfFaces = parameters.getMaxNumDetectedFaces();
        assertTrue(maxNumOfFaces >= 0);
        if (maxNumOfFaces == 0) {
            try {
                mCamera.startFaceDetection();
                fail(""Should throw an exception if face detection is not supported."");
            } catch (IllegalArgumentException e) {
                // expected
            }
            terminateMessageLooper();
            return;
        }

        mCamera.startFaceDetection();
        try {
            mCamera.startFaceDetection();
            fail(""Starting face detection twice should throw an exception"");
        } catch (RuntimeException e) {
            // expected
        }
        FaceListener listener = new FaceListener();
        mCamera.setFaceDetectionListener(listener);
        // Sleep some time so the camera has chances to detect faces.
        Thread.sleep(FACE_DETECTION_TEST_DURATION);
        // The face callback runs in another thread. Release the camera and stop
        // the looper. So we do not access the face array from two threads at
        // the same time.
        terminateMessageLooper();

        // Check if the optional fields are supported.
        boolean optionalFieldSupported = false;
        Face firstFace = null;
        for (Face[] faces: listener.mFacesArray) {
            for (Face face: faces) {
                if (face != null) firstFace = face;
            }
        }
        if (firstFace != null) {
            if (firstFace.id != -1 || firstFace.leftEye != null
                    || firstFace.rightEye != null || firstFace.mouth != null) {
                optionalFieldSupported = true;
            }
        }

        // Verify the faces array.
        for (Face[] faces: listener.mFacesArray) {
            testFaces(faces, maxNumOfFaces, optionalFieldSupported);
        }

        // After taking a picture, face detection should be started again.
        // Also make sure autofocus move callback is supported.
        initializeMessageLooper(cameraId);
        mCamera.setAutoFocusMoveCallback(mAutoFocusMoveCallback);
        mCamera.startPreview();
        mCamera.startFaceDetection();
        mCamera.takePicture(mShutterCallback, mRawPictureCallback, mJpegPictureCallback);
        waitForSnapshotDone();
        mCamera.startPreview();
        mCamera.startFaceDetection();
        terminateMessageLooper();
    }

    private class FaceListener implements FaceDetectionListener {
        public ArrayList<Face[]> mFacesArray = new ArrayList<Face[]>();

        @Override
        public void onFaceDetection(Face[] faces, Camera camera) {
            mFacesArray.add(faces);
        }
    }

    private void testFaces(Face[] faces, int maxNumOfFaces,
            boolean optionalFieldSupported) {
        Rect bounds = new Rect(-1000, -1000, 1000, 1000);
        assertNotNull(faces);
        assertTrue(faces.length <= maxNumOfFaces);
        for (int i = 0; i < faces.length; i++) {
            Face face = faces[i];
            Rect rect = face.rect;
            // Check the bounds.
            assertNotNull(rect);
            assertTrue(rect.width() > 0);
            assertTrue(rect.height() > 0);
            assertTrue(""Coordinates out of bounds. rect="" + rect,
                    bounds.contains(rect) || Rect.intersects(bounds, rect));

            // Check the score.
            assertTrue(face.score >= 1 && face.score <= 100);

            // Check id, left eye, right eye, and the mouth.
            // Optional fields should be all valid or none of them.
            if (!optionalFieldSupported) {
                assertEquals(-1, face.id);
                assertNull(face.leftEye);
                assertNull(face.rightEye);
                assertNull(face.mouth);
            } else {
                assertTrue(face.id != -1);
                assertNotNull(face.leftEye);
                assertNotNull(face.rightEye);
                assertNotNull(face.mouth);
                assertTrue(bounds.contains(face.leftEye.x, face.leftEye.y));
                assertTrue(bounds.contains(face.rightEye.x, face.rightEye.y));
                assertTrue(bounds.contains(face.mouth.x, face.mouth.y));
                // ID should be unique.
                if (i != faces.length - 1) {
                    assertTrue(face.id != faces[i + 1].id);
                }
            }
        }
    }

    @UiThreadTest"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"testVideoSnapshot"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"(timeout=60*60*1000) // timeout = 60 mins for long running tests
    public void testVideoSnapshot() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            testVideoSnapshotByCamera(id, /*recordingHint*/false);
            testVideoSnapshotByCamera(id, /*recordingHint*/true);
        }
    }

    private static final int[] mCamcorderProfileList = {
        CamcorderProfile.QUALITY_2160P,
        CamcorderProfile.QUALITY_1080P,
        CamcorderProfile.QUALITY_480P,
        CamcorderProfile.QUALITY_720P,
        CamcorderProfile.QUALITY_CIF,
        CamcorderProfile.QUALITY_HIGH,
        CamcorderProfile.QUALITY_LOW,
        CamcorderProfile.QUALITY_QCIF,
        CamcorderProfile.QUALITY_QVGA,
    };

    private void testVideoSnapshotByCamera(int cameraId, boolean recordingHint) throws Exception {
        initializeMessageLooper(cameraId);
        Camera.Parameters parameters = mCamera.getParameters();
        terminateMessageLooper();
        if (!parameters.isVideoSnapshotSupported()) {
            return;
        }

        if (mIsExternalCamera) {
            return;
        }

        SurfaceHolder holder = mActivityRule.getActivity().getSurfaceView().getHolder();

        for (int profileId: mCamcorderProfileList) {
            if (!CamcorderProfile.hasProfile(cameraId, profileId)) {
                continue;
            }
            initializeMessageLooper(cameraId);
            // Set the preview size.
            CamcorderProfile profile = CamcorderProfile.get(cameraId,
                    profileId);
            setPreviewSizeByProfile(parameters, profile);

            // Set the biggest picture size.
            Size biggestSize = mCamera.new Size(-1, -1);
            for (Size size: parameters.getSupportedPictureSizes()) {
                if (biggestSize.width < size.width) {
                    biggestSize = size;
                }
            }
            parameters.setPictureSize(biggestSize.width, biggestSize.height);
            parameters.setRecordingHint(recordingHint);

            mCamera.setParameters(parameters);
            mCamera.startPreview();
            mCamera.unlock();
            MediaRecorder recorder = new MediaRecorder();
            try {
                recorder.setCamera(mCamera);
                recorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);
                recorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);
                recorder.setProfile(profile);
                recorder.setOutputFile(mRecordingPath);
                recorder.setPreviewDisplay(holder.getSurface());
                recorder.prepare();
                recorder.start();
                subtestTakePictureByCamera(true,
                        profile.videoFrameWidth, profile.videoFrameHeight);
                testJpegExifByCamera(true);
                testJpegThumbnailSizeByCamera(true,
                        profile.videoFrameWidth, profile.videoFrameHeight);
                Thread.sleep(2000);
                recorder.stop();
            } finally {
                recorder.release();
                mCamera.lock();
            }
            mCamera.stopPreview();
            terminateMessageLooper();
        }
    }"	""	""	"JPEG 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.CameraTest"	"testPreviewCallbackWithPicture"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/CameraTest.java"	""	"public void testPreviewCallbackWithPicture() throws Exception {
        for (int id : mCameraIds) {
            Log.v(TAG, ""Camera id="" + id);
            testPreviewCallbackWithPictureByCamera(id);
        }
    }

    private void testPreviewCallbackWithPictureByCamera(int cameraId)
            throws Exception {
        initializeMessageLooper(cameraId);

        SimplePreviewStreamCb callback = new SimplePreviewStreamCb(1);
        mCamera.setPreviewCallback(callback);

        Log.v(TAG, ""Starting preview"");
        mCamera.startPreview();

        // Wait until callbacks are flowing
        for (int i = 0; i < 30; i++) {
            assertTrue(""testPreviewCallbackWithPicture: Not receiving preview callbacks!"",
                    mPreviewDone.block( WAIT_FOR_COMMAND_TO_COMPLETE ) );
            mPreviewDone.close();
        }

        // Now take a picture
        Log.v(TAG, ""Taking picture now"");

        Size pictureSize = mCamera.getParameters().getPictureSize();
        mCamera.takePicture(mShutterCallback, mRawPictureCallback,
                mJpegPictureCallback);

        waitForSnapshotDone();

        assertTrue(""Shutter callback not received"", mShutterCallbackResult);
        assertTrue(""Raw picture callback not received"", mRawPictureCallbackResult);
        assertTrue(""Jpeg picture callback not received"", mJpegPictureCallbackResult);
        assertNotNull(mJpegData);
        BitmapFactory.Options bmpOptions = new BitmapFactory.Options();
        bmpOptions.inJustDecodeBounds = true;
        BitmapFactory.decodeByteArray(mJpegData, 0, mJpegData.length, bmpOptions);
        assertEquals(pictureSize.width, bmpOptions.outWidth);
        assertEquals(pictureSize.height, bmpOptions.outHeight);

        // Restart preview, confirm callbacks still happen
        Log.v(TAG, ""Restarting preview"");
        mCamera.startPreview();

        for (int i = 0; i < 30; i++) {
            assertTrue(""testPreviewCallbackWithPicture: Not receiving preview callbacks!"",
                    mPreviewDone.block( WAIT_FOR_COMMAND_TO_COMPLETE ) );
            mPreviewDone.close();
        }

        mCamera.stopPreview();

        terminateMessageLooper();
    }"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraExtensionSessionTest"	"testMultiFrameCapture"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CameraExtensionSessionTest.java"	""	"public void testMultiFrameCapture() throws Exception {
        final int IMAGE_COUNT = 10;
        final int SUPPORTED_CAPTURE_OUTPUT_FORMATS[] = {
                ImageFormat.YUV_420_888,
                ImageFormat.JPEG
        };
        for (String id : mCameraIdsUnderTest) {
            StaticMetadata staticMeta =
                    new StaticMetadata(mTestRule.getCameraManager().getCameraCharacteristics(id));
            if (!staticMeta.isColorOutputSupported()) {
                continue;
            }
            updatePreviewSurfaceTexture();
            CameraExtensionCharacteristics extensionChars =
                    mTestRule.getCameraManager().getCameraExtensionCharacteristics(id);
            List<Integer> supportedExtensions = extensionChars.getSupportedExtensions();
            for (Integer extension : supportedExtensions) {
                for (int captureFormat : SUPPORTED_CAPTURE_OUTPUT_FORMATS) {
                    List<Size> extensionSizes = extensionChars.getExtensionSupportedSizes(extension,
                            captureFormat);
                    if (extensionSizes.isEmpty()) {
                        continue;
                    }
                    Size maxSize = CameraTestUtils.getMaxSize(extensionSizes.toArray(new Size[0]));
                    SimpleImageReaderListener imageListener = new SimpleImageReaderListener(false,
                            1);
                    ImageReader extensionImageReader = CameraTestUtils.makeImageReader(maxSize,
                            captureFormat, /*maxImages*/ 1, imageListener,
                            mTestRule.getHandler());
                    Surface imageReaderSurface = extensionImageReader.getSurface();
                    OutputConfiguration readerOutput = new OutputConfiguration(
                            OutputConfiguration.SURFACE_GROUP_ID_NONE, imageReaderSurface);
                    List<OutputConfiguration> outputConfigs = new ArrayList<>();
                    outputConfigs.add(readerOutput);

                    BlockingExtensionSessionCallback sessionListener =
                            new BlockingExtensionSessionCallback(mock(
                                    CameraExtensionSession.StateCallback.class));
                    ExtensionSessionConfiguration configuration =
                            new ExtensionSessionConfiguration(extension, outputConfigs,
                                    new HandlerExecutor(mTestRule.getHandler()),
                                    sessionListener);
                    String streamName = ""test_extension_capture"";
                    mReportLog = new DeviceReportLog(REPORT_LOG_NAME, streamName);
                    mReportLog.addValue(""camera_id"", id, ResultType.NEUTRAL, ResultUnit.NONE);
                    mReportLog.addValue(""extension_id"", extension, ResultType.NEUTRAL,
                            ResultUnit.NONE);
                    double[] captureTimes = new double[IMAGE_COUNT];

                    try {
                        mTestRule.openDevice(id);
                        CameraDevice camera = mTestRule.getCamera();
                        camera.createExtensionSession(configuration);
                        CameraExtensionSession extensionSession =
                                sessionListener.waitAndGetSession(
                                        SESSION_CONFIGURE_TIMEOUT_MS);
                        assertNotNull(extensionSession);

                        CaptureRequest.Builder captureBuilder =
                                mTestRule.getCamera().createCaptureRequest(
                                        CameraDevice.TEMPLATE_STILL_CAPTURE);
                        captureBuilder.addTarget(imageReaderSurface);
                        CameraExtensionSession.ExtensionCaptureCallback captureCallback =
                                mock(CameraExtensionSession.ExtensionCaptureCallback.class);

                        for (int i = 0; i < IMAGE_COUNT; i++) {
                            int jpegOrientation = (i * 90) % 360; // degrees [0..270]
                            if (captureFormat == ImageFormat.JPEG) {
                                captureBuilder.set(CaptureRequest.JPEG_ORIENTATION,
                                        jpegOrientation);
                            }
                            CaptureRequest request = captureBuilder.build();
                            long startTimeMs = SystemClock.elapsedRealtime();
                            int sequenceId = extensionSession.capture(request,
                                    new HandlerExecutor(mTestRule.getHandler()), captureCallback);

                            Image img =
                                    imageListener.getImage(MULTI_FRAME_CAPTURE_IMAGE_TIMEOUT_MS);
                            captureTimes[i] = SystemClock.elapsedRealtime() - startTimeMs;
                            if (captureFormat == ImageFormat.JPEG) {
                                verifyJpegOrientation(img, maxSize, jpegOrientation);
                            } else {
                                validateImage(img, maxSize.getWidth(), maxSize.getHeight(),
                                        captureFormat, null);
                            }
                            img.close();

                            verify(captureCallback, times(1))
                                    .onCaptureStarted(eq(extensionSession), eq(request), anyLong());
                            verify(captureCallback,
                                    timeout(MULTI_FRAME_CAPTURE_IMAGE_TIMEOUT_MS).times(1))
                                    .onCaptureProcessStarted(extensionSession, request);
                            verify(captureCallback,
                                    timeout(MULTI_FRAME_CAPTURE_IMAGE_TIMEOUT_MS).times(1))
                                    .onCaptureSequenceCompleted(extensionSession, sequenceId);
                        }

                        long avgCaptureLatency = (long) Stat.getAverage(captureTimes);
                        String resultFormat = ""avg_latency size: "" + maxSize.toString() +
                                "" image format: "" + captureFormat;
                        mReportLog.addValue(resultFormat, avgCaptureLatency,
                                ResultType.LOWER_BETTER, ResultUnit.MS);

                        verify(captureCallback, times(0))
                                .onCaptureSequenceAborted(any(CameraExtensionSession.class),
                                        anyInt());
                        verify(captureCallback, times(0))
                                .onCaptureFailed(any(CameraExtensionSession.class),
                                        any(CaptureRequest.class));
                        Range<Long> latencyRange =
                                extensionChars.getEstimatedCaptureLatencyRangeMillis(extension,
                                        maxSize, captureFormat);
                        if (latencyRange != null) {
                            String msg = String.format(""Camera [%s]: The measured average ""
                                            + ""capture latency of %d ms. for extension type %d  ""
                                            + ""with image format: %d and size: %dx%d must be ""
                                            + ""within the advertised range of [%d, %d] ms."",
                                    id, avgCaptureLatency, extension, captureFormat,
                                    maxSize.getWidth(), maxSize.getHeight(),
                                    latencyRange.getLower(), latencyRange.getUpper());
                            assertTrue(msg, latencyRange.contains(avgCaptureLatency));
                        }


                        extensionSession.close();

                        sessionListener.getStateWaiter().waitForState(
                                BlockingExtensionSessionCallback.SESSION_CLOSED,
                                SESSION_CLOSE_TIMEOUT_MS);
                    } finally {
                        mTestRule.closeDevice(id);
                        extensionImageReader.close();
                        mReportLog.submit(InstrumentationRegistry.getInstrumentation());
                    }
                }
            }
        }
    }

    // Verify concurrent extension sessions behavior"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraExtensionSessionTest"	"testRepeatingAndCaptureCombined"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CameraExtensionSessionTest.java"	""	"public void testRepeatingAndCaptureCombined() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            StaticMetadata staticMeta =
                    new StaticMetadata(mTestRule.getCameraManager().getCameraCharacteristics(id));
            if (!staticMeta.isColorOutputSupported()) {
                continue;
            }
            updatePreviewSurfaceTexture();
            CameraExtensionCharacteristics extensionChars =
                    mTestRule.getCameraManager().getCameraExtensionCharacteristics(id);
            List<Integer> supportedExtensions = extensionChars.getSupportedExtensions();
            for (Integer extension : supportedExtensions) {
                int captureFormat = ImageFormat.JPEG;
                List<Size> captureSizes = extensionChars.getExtensionSupportedSizes(extension,
                        captureFormat);
                assertFalse(""No Jpeg output supported"", captureSizes.isEmpty());
                Size captureMaxSize =
                        CameraTestUtils.getMaxSize(captureSizes.toArray(new Size[0]));

                SimpleImageReaderListener imageListener = new SimpleImageReaderListener(false
                        , 1);
                ImageReader extensionImageReader = CameraTestUtils.makeImageReader(
                        captureMaxSize, captureFormat, /*maxImages*/ 1, imageListener,
                        mTestRule.getHandler());
                Surface imageReaderSurface = extensionImageReader.getSurface();
                OutputConfiguration readerOutput = new OutputConfiguration(
                        OutputConfiguration.SURFACE_GROUP_ID_NONE, imageReaderSurface);
                List<OutputConfiguration> outputConfigs = new ArrayList<>();
                outputConfigs.add(readerOutput);

                // Pick a supported preview/repeating size with aspect ratio close to the
                // multi-frame capture size
                List<Size> repeatingSizes = extensionChars.getExtensionSupportedSizes(extension,
                        mSurfaceTexture.getClass());
                Size maxRepeatingSize =
                        CameraTestUtils.getMaxSize(repeatingSizes.toArray(new Size[0]));
                List<Size> previewSizes = getSupportedPreviewSizes(id,
                        mTestRule.getCameraManager(),
                        getPreviewSizeBound(mTestRule.getWindowManager(), PREVIEW_SIZE_BOUND));
                List<Size> supportedPreviewSizes =
                        previewSizes.stream().filter(repeatingSizes::contains).collect(
                                Collectors.toList());
                if (!supportedPreviewSizes.isEmpty()) {
                    float targetAr =
                            ((float) captureMaxSize.getWidth()) / captureMaxSize.getHeight();
                    for (Size s : supportedPreviewSizes) {
                        float currentAr = ((float) s.getWidth()) / s.getHeight();
                        if (Math.abs(targetAr - currentAr) < 0.01) {
                            maxRepeatingSize = s;
                            break;
                        }
                    }
                }

                mSurfaceTexture.setDefaultBufferSize(maxRepeatingSize.getWidth(),
                        maxRepeatingSize.getHeight());
                Surface texturedSurface = new Surface(mSurfaceTexture);
                outputConfigs.add(new OutputConfiguration(
                        OutputConfiguration.SURFACE_GROUP_ID_NONE, texturedSurface));

                BlockingExtensionSessionCallback sessionListener =
                        new BlockingExtensionSessionCallback(mock(
                                CameraExtensionSession.StateCallback.class));
                ExtensionSessionConfiguration configuration =
                        new ExtensionSessionConfiguration(extension, outputConfigs,
                                new HandlerExecutor(mTestRule.getHandler()),
                                sessionListener);
                try {
                    mTestRule.openDevice(id);
                    CameraDevice camera = mTestRule.getCamera();
                    camera.createExtensionSession(configuration);
                    CameraExtensionSession extensionSession =
                            sessionListener.waitAndGetSession(
                                    SESSION_CONFIGURE_TIMEOUT_MS);
                    assertNotNull(extensionSession);

                    CaptureRequest.Builder captureBuilder =
                            mTestRule.getCamera().createCaptureRequest(
                                    android.hardware.camera2.CameraDevice.TEMPLATE_PREVIEW);
                    captureBuilder.addTarget(texturedSurface);
                    CameraExtensionSession.ExtensionCaptureCallback repeatingCallbackMock =
                            mock(CameraExtensionSession.ExtensionCaptureCallback.class);
                    SimpleCaptureCallback repeatingCaptureCallback =
                            new SimpleCaptureCallback(repeatingCallbackMock);
                    CaptureRequest repeatingRequest = captureBuilder.build();
                    int repeatingSequenceId =
                            extensionSession.setRepeatingRequest(repeatingRequest,
                                    new HandlerExecutor(mTestRule.getHandler()),
                                    repeatingCaptureCallback);

                    Thread.sleep(REPEATING_REQUEST_TIMEOUT_MS);

                    verify(repeatingCallbackMock, atLeastOnce())
                            .onCaptureStarted(eq(extensionSession), eq(repeatingRequest),
                                    anyLong());
                    verify(repeatingCallbackMock, atLeastOnce())
                            .onCaptureProcessStarted(extensionSession, repeatingRequest);

                    captureBuilder = mTestRule.getCamera().createCaptureRequest(
                            android.hardware.camera2.CameraDevice.TEMPLATE_STILL_CAPTURE);
                    captureBuilder.addTarget(imageReaderSurface);
                    CameraExtensionSession.ExtensionCaptureCallback captureCallback =
                            mock(CameraExtensionSession.ExtensionCaptureCallback.class);

                    CaptureRequest captureRequest = captureBuilder.build();
                    int captureSequenceId = extensionSession.capture(captureRequest,
                            new HandlerExecutor(mTestRule.getHandler()), captureCallback);

                    Image img =
                            imageListener.getImage(MULTI_FRAME_CAPTURE_IMAGE_TIMEOUT_MS);
                    validateImage(img, captureMaxSize.getWidth(),
                            captureMaxSize.getHeight(), captureFormat, null);
                    img.close();

                    verify(captureCallback, times(1))
                            .onCaptureStarted(eq(extensionSession), eq(captureRequest),
                                    anyLong());
                    verify(captureCallback, timeout(MULTI_FRAME_CAPTURE_IMAGE_TIMEOUT_MS).times(1))
                            .onCaptureProcessStarted(extensionSession, captureRequest);
                    verify(captureCallback, timeout(MULTI_FRAME_CAPTURE_IMAGE_TIMEOUT_MS).times(1))
                            .onCaptureSequenceCompleted(extensionSession,
                                    captureSequenceId);
                    verify(captureCallback, times(0))
                            .onCaptureSequenceAborted(any(CameraExtensionSession.class),
                                    anyInt());
                    verify(captureCallback, times(0))
                            .onCaptureFailed(any(CameraExtensionSession.class),
                                    any(CaptureRequest.class));

                    extensionSession.stopRepeating();

                    verify(repeatingCallbackMock,
                            timeout(MULTI_FRAME_CAPTURE_IMAGE_TIMEOUT_MS).times(1))
                            .onCaptureSequenceCompleted(extensionSession, repeatingSequenceId);

                    verify(repeatingCallbackMock, times(0))
                            .onCaptureSequenceAborted(any(CameraExtensionSession.class),
                                    anyInt());

                    extensionSession.close();

                    sessionListener.getStateWaiter().waitForState(
                            BlockingExtensionSessionCallback.SESSION_CLOSED,
                            SESSION_CLOSE_TIMEOUT_MS);

                    assertTrue(""The sum of onCaptureProcessStarted and onCaptureFailed"" +
                                    "" callbacks must be greater or equal than the number of calls"" +
                                    "" to onCaptureStarted!"",
                            repeatingCaptureCallback.getTotalFramesArrived() +
                                    repeatingCaptureCallback.getTotalFramesFailed() >=
                            repeatingCaptureCallback.getTotalFramesStarted());
                    assertTrue(String.format(""The last repeating request surface timestamp "" +
                                    ""%d must be less than or equal to the last "" +
                                    ""onCaptureStarted "" +
                                    ""timestamp %d"", mSurfaceTexture.getTimestamp(),
                            repeatingCaptureCallback.getLastTimestamp()),
                            mSurfaceTexture.getTimestamp() <=
                                    repeatingCaptureCallback.getLastTimestamp());

                } finally {
                    mTestRule.closeDevice(id);
                    texturedSurface.release();
                    extensionImageReader.close();
                }
            }
        }
    }

    private void verifyJpegOrientation(Image img, Size jpegSize, int requestedOrientation)
            throws IOException {
        byte[] blobBuffer = getDataFromImage(img);
        String blobFilename = mTestRule.getDebugFileNameBase() + ""/verifyJpegKeys.jpeg"";
        dumpFile(blobFilename, blobBuffer);
        ExifInterface exif = new ExifInterface(blobFilename);
        int exifWidth = exif.getAttributeInt(ExifInterface.TAG_IMAGE_WIDTH, /*defaultValue*/0);
        int exifHeight = exif.getAttributeInt(ExifInterface.TAG_IMAGE_LENGTH, /*defaultValue*/0);
        Size exifSize = new Size(exifWidth, exifHeight);
        int exifOrientation = exif.getAttributeInt(ExifInterface.TAG_ORIENTATION,
                /*defaultValue*/ ExifInterface.ORIENTATION_UNDEFINED);
        final int ORIENTATION_MIN = ExifInterface.ORIENTATION_UNDEFINED;
        final int ORIENTATION_MAX = ExifInterface.ORIENTATION_ROTATE_270;
        assertTrue(String.format(""Exif orientation must be in range of [%d, %d]"",
                ORIENTATION_MIN, ORIENTATION_MAX),
                exifOrientation >= ORIENTATION_MIN && exifOrientation <= ORIENTATION_MAX);

        /**
         * Device captured image doesn't respect the requested orientation,
         * which means it rotates the image buffer physically. Then we
         * should swap the exif width/height accordingly to compare.
         */
        boolean deviceRotatedImage = exifOrientation == ExifInterface.ORIENTATION_UNDEFINED;

        if (deviceRotatedImage) {
            // Case 1.
            boolean needSwap = (requestedOrientation % 180 == 90);
            if (needSwap) {
                exifSize = new Size(exifHeight, exifWidth);
            }
        } else {
            // Case 2.
            assertEquals(""Exif orientation should match requested orientation"",
                    requestedOrientation, getExifOrientationInDegree(exifOrientation));
        }

        assertEquals(""Exif size should match jpeg capture size"", jpegSize, exifSize);
    }

    private static int getExifOrientationInDegree(int exifOrientation) {
        switch (exifOrientation) {
            case ExifInterface.ORIENTATION_NORMAL:
                return 0;
            case ExifInterface.ORIENTATION_ROTATE_90:
                return 90;
            case ExifInterface.ORIENTATION_ROTATE_180:
                return 180;
            case ExifInterface.ORIENTATION_ROTATE_270:
                return 270;
            default:
                fail(""It is impossible to get non 0, 90, 180, 270 degress exif"" +
                        ""info based on the request orientation range"");
                return -1;
        }
    }

    public static class SimpleCaptureCallback
            extends CameraExtensionSession.ExtensionCaptureCallback {
        private long mLastTimestamp = -1;
        private int mNumFramesArrived = 0;
        private int mNumFramesStarted = 0;
        private int mNumFramesFailed = 0;
        private boolean mNonIncreasingTimestamps = false;
        private final CameraExtensionSession.ExtensionCaptureCallback mProxy;

        public SimpleCaptureCallback(CameraExtensionSession.ExtensionCaptureCallback proxy) {
            mProxy = proxy;
        }

        @Override
        public void onCaptureStarted(CameraExtensionSession session,
                                     CaptureRequest request, long timestamp) {

            if (timestamp < mLastTimestamp) {
                mNonIncreasingTimestamps = true;
            }
            mLastTimestamp = timestamp;
            mNumFramesStarted++;
            if (mProxy != null) {
                mProxy.onCaptureStarted(session, request, timestamp);
            }
        }

        @Override
        public void onCaptureProcessStarted(CameraExtensionSession session,
                                            CaptureRequest request) {
            mNumFramesArrived++;
            if (mProxy != null) {
                mProxy.onCaptureProcessStarted(session, request);
            }
        }

        @Override
        public void onCaptureFailed(CameraExtensionSession session,
                                    CaptureRequest request) {
            mNumFramesFailed++;
            if (mProxy != null) {
                mProxy.onCaptureFailed(session, request);
            }
        }

        @Override
        public void onCaptureSequenceAborted(CameraExtensionSession session,
                                             int sequenceId) {
            if (mProxy != null) {
                mProxy.onCaptureSequenceAborted(session, sequenceId);
            }
        }

        @Override
        public void onCaptureSequenceCompleted(CameraExtensionSession session,
                                               int sequenceId) {
            if (mProxy != null) {
                mProxy.onCaptureSequenceCompleted(session, sequenceId);
            }
        }

        public int getTotalFramesArrived() {
            return mNumFramesArrived;
        }

        public int getTotalFramesStarted() {
            return mNumFramesStarted;
        }

        public int getTotalFramesFailed() {
            return mNumFramesFailed;
        }

        public long getLastTimestamp() throws IllegalStateException {
            if (mNonIncreasingTimestamps) {
                throw new IllegalStateException(""Non-monotonically increasing timestamps!"");
            }
            return mLastTimestamp;
        }
    }"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraExtensionSessionTest"	"testIllegalArguments"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CameraExtensionSessionTest.java"	""	"public void testIllegalArguments() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            StaticMetadata staticMeta =
                    new StaticMetadata(mTestRule.getCameraManager().getCameraCharacteristics(id));
            if (!staticMeta.isColorOutputSupported()) {
                continue;
            }
            updatePreviewSurfaceTexture();
            CameraExtensionCharacteristics extensionChars =
                    mTestRule.getCameraManager().getCameraExtensionCharacteristics(id);
            List<Integer> supportedExtensions = extensionChars.getSupportedExtensions();
            for (Integer extension : supportedExtensions) {
                List<OutputConfiguration> outputConfigs = new ArrayList<>();
                BlockingExtensionSessionCallback sessionListener =
                        new BlockingExtensionSessionCallback(mock(
                                CameraExtensionSession.StateCallback.class));
                ExtensionSessionConfiguration configuration =
                        new ExtensionSessionConfiguration(extension, outputConfigs,
                                new HandlerExecutor(mTestRule.getHandler()),
                                sessionListener);

                try {
                    mTestRule.openDevice(id);
                    CameraDevice camera = mTestRule.getCamera();
                    try {
                        camera.createExtensionSession(configuration);
                        fail(""should get IllegalArgumentException due to absent output surfaces"");
                    } catch (IllegalArgumentException e) {
                        // Expected, we can proceed further
                    }

                    int captureFormat = ImageFormat.YUV_420_888;
                    List<Size> captureSizes = extensionChars.getExtensionSupportedSizes(extension,
                            captureFormat);
                    if (captureSizes.isEmpty()) {
                        captureFormat = ImageFormat.JPEG;
                        captureSizes = extensionChars.getExtensionSupportedSizes(extension,
                                captureFormat);
                    }
                    Size captureMaxSize =
                            CameraTestUtils.getMaxSize(captureSizes.toArray(new Size[0]));

                    mSurfaceTexture.setDefaultBufferSize(1, 1);
                    Surface texturedSurface = new Surface(mSurfaceTexture);
                    outputConfigs.add(new OutputConfiguration(
                            OutputConfiguration.SURFACE_GROUP_ID_NONE, texturedSurface));
                    configuration = new ExtensionSessionConfiguration(extension, outputConfigs,
                            new HandlerExecutor(mTestRule.getHandler()), sessionListener);

                    try {
                        camera.createExtensionSession(configuration);
                        fail(""should get IllegalArgumentException due to illegal repeating request""
                                + "" output surface"");
                    } catch (IllegalArgumentException e) {
                        // Expected, we can proceed further
                    } finally {
                        outputConfigs.clear();
                    }

                    SimpleImageReaderListener imageListener = new SimpleImageReaderListener(false,
                            1);
                    Size invalidCaptureSize = new Size(1, 1);
                    ImageReader extensionImageReader = CameraTestUtils.makeImageReader(
                            invalidCaptureSize, captureFormat, /*maxImages*/ 1,
                            imageListener, mTestRule.getHandler());
                    Surface imageReaderSurface = extensionImageReader.getSurface();
                    OutputConfiguration readerOutput = new OutputConfiguration(
                            OutputConfiguration.SURFACE_GROUP_ID_NONE, imageReaderSurface);
                    outputConfigs.add(readerOutput);
                    configuration = new ExtensionSessionConfiguration(extension, outputConfigs,
                            new HandlerExecutor(mTestRule.getHandler()), sessionListener);

                    try{
                        camera.createExtensionSession(configuration);
                        fail(""should get IllegalArgumentException due to illegal multi-frame""
                                + "" request output surface"");
                    } catch (IllegalArgumentException e) {
                        // Expected, we can proceed further
                    } finally {
                        outputConfigs.clear();
                        extensionImageReader.close();
                    }

                    // Pick a supported preview/repeating size with aspect ratio close to the
                    // multi-frame capture size
                    List<Size> repeatingSizes = extensionChars.getExtensionSupportedSizes(extension,
                            mSurfaceTexture.getClass());
                    Size maxRepeatingSize =
                            CameraTestUtils.getMaxSize(repeatingSizes.toArray(new Size[0]));
                    List<Size> previewSizes = getSupportedPreviewSizes(id,
                            mTestRule.getCameraManager(),
                            getPreviewSizeBound(mTestRule.getWindowManager(), PREVIEW_SIZE_BOUND));
                    List<Size> supportedPreviewSizes =
                            previewSizes.stream().filter(repeatingSizes::contains).collect(
                                    Collectors.toList());
                    if (!supportedPreviewSizes.isEmpty()) {
                        float targetAr =
                                ((float) captureMaxSize.getWidth()) / captureMaxSize.getHeight();
                        for (Size s : supportedPreviewSizes) {
                            float currentAr = ((float) s.getWidth()) / s.getHeight();
                            if (Math.abs(targetAr - currentAr) < 0.01) {
                                maxRepeatingSize = s;
                                break;
                            }
                        }
                    }

                    imageListener = new SimpleImageReaderListener(false, 1);
                    extensionImageReader = CameraTestUtils.makeImageReader(captureMaxSize,
                            captureFormat, /*maxImages*/ 1, imageListener, mTestRule.getHandler());
                    imageReaderSurface = extensionImageReader.getSurface();
                    readerOutput = new OutputConfiguration(OutputConfiguration.SURFACE_GROUP_ID_NONE,
                            imageReaderSurface);
                    outputConfigs.add(readerOutput);

                    mSurfaceTexture.setDefaultBufferSize(maxRepeatingSize.getWidth(),
                            maxRepeatingSize.getHeight());
                    texturedSurface = new Surface(mSurfaceTexture);
                    outputConfigs.add(new OutputConfiguration(
                            OutputConfiguration.SURFACE_GROUP_ID_NONE, texturedSurface));

                    configuration = new ExtensionSessionConfiguration(extension, outputConfigs,
                            new HandlerExecutor(mTestRule.getHandler()), sessionListener);
                    camera.createExtensionSession(configuration);
                    CameraExtensionSession extensionSession =
                            sessionListener.waitAndGetSession(
                                    SESSION_CONFIGURE_TIMEOUT_MS);
                    assertNotNull(extensionSession);

                    CaptureRequest.Builder captureBuilder =
                            mTestRule.getCamera().createCaptureRequest(
                                    android.hardware.camera2.CameraDevice.TEMPLATE_PREVIEW);
                    captureBuilder.addTarget(imageReaderSurface);
                    CameraExtensionSession.ExtensionCaptureCallback repeatingCallbackMock =
                            mock(CameraExtensionSession.ExtensionCaptureCallback.class);
                    SimpleCaptureCallback repeatingCaptureCallback =
                            new SimpleCaptureCallback(repeatingCallbackMock);
                    CaptureRequest repeatingRequest = captureBuilder.build();
                    try {
                        extensionSession.setRepeatingRequest(repeatingRequest,
                                new HandlerExecutor(mTestRule.getHandler()),
                                repeatingCaptureCallback);
                        fail(""should get IllegalArgumentException due to illegal repeating request""
                                + "" output target"");
                    } catch (IllegalArgumentException e) {
                        // Expected, we can proceed further
                    }

                    captureBuilder = mTestRule.getCamera().createCaptureRequest(
                            android.hardware.camera2.CameraDevice.TEMPLATE_STILL_CAPTURE);
                    captureBuilder.addTarget(texturedSurface);
                    CameraExtensionSession.ExtensionCaptureCallback captureCallback =
                            mock(CameraExtensionSession.ExtensionCaptureCallback.class);

                    CaptureRequest captureRequest = captureBuilder.build();
                    try {
                        extensionSession.capture(captureRequest,
                                new HandlerExecutor(mTestRule.getHandler()), captureCallback);
                        fail(""should get IllegalArgumentException due to illegal multi-frame""
                                + "" request output target"");
                    } catch (IllegalArgumentException e) {
                        // Expected, we can proceed further
                    }

                    extensionSession.close();

                    sessionListener.getStateWaiter().waitForState(
                            BlockingExtensionSessionCallback.SESSION_CLOSED,
                            SESSION_CLOSE_TIMEOUT_MS);

                    texturedSurface.release();
                    extensionImageReader.close();

                    try {
                        extensionSession.setRepeatingRequest(captureRequest,
                                new HandlerExecutor(mTestRule.getHandler()), captureCallback);
                        fail(""should get IllegalStateException due to closed session"");
                    } catch (IllegalStateException e) {
                        // Expected, we can proceed further
                    }

                    try {
                        extensionSession.stopRepeating();
                        fail(""should get IllegalStateException due to closed session"");
                    } catch (IllegalStateException e) {
                        // Expected, we can proceed further
                    }

                    try {
                        extensionSession.capture(captureRequest,
                                new HandlerExecutor(mTestRule.getHandler()), captureCallback);
                        fail(""should get IllegalStateException due to closed session"");
                    } catch (IllegalStateException e) {
                        // Expected, we can proceed further
                    }
                } finally {
                    mTestRule.closeDevice(id);
                }
            }
        }
    }
}"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.BurstCaptureTest"	"testYuvBurst"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/BurstCaptureTest.java"	""	"public void testYuvBurst() throws Exception {
        final int YUV_BURST_SIZE = 100;
        testBurst(ImageFormat.YUV_420_888, YUV_BURST_SIZE, true/*checkFrameRate*/,
                false/*testStillBokeh*/);
    }

    /**
     * Test JPEG burst capture with full-AUTO control.
     *
     * Also verifies sensor settings operation if READ_SENSOR_SETTINGS is available.
     * Compared to testYuvBurst, this test uses STILL_CAPTURE intent, and exercises path where
     * enableZsl is enabled.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.BurstCaptureTest"	"testJpegBurst"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/BurstCaptureTest.java"	""	"public void testJpegBurst() throws Exception {
        final int JPEG_BURST_SIZE = 10;
        testBurst(ImageFormat.JPEG, JPEG_BURST_SIZE, false/*checkFrameRate*/,
                false/*testStillBokeh*/);
    }

    /**
     * Test YUV burst capture with full-AUTO control and STILL_CAPTURE bokeh mode.
     * Also verifies sensor settings operation if READ_SENSOR_SETTINGS is available.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.BurstCaptureTest"	"testYuvBurstWithStillBokeh"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/BurstCaptureTest.java"	""	"public void testYuvBurstWithStillBokeh() throws Exception {
        final int YUV_BURST_SIZE = 100;
        testBurst(ImageFormat.YUV_420_888, YUV_BURST_SIZE, true/*checkFrameRate*/,
                true/*testStillBokeh*/);
    }

    private void testBurst(int fmt, int burstSize, boolean checkFrameRate, boolean testStillBokeh)
            throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                String id = mCameraIdsUnderTest[i];

                StaticMetadata staticInfo = mAllStaticInfo.get(id);
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" does not support color outputs, skipping"");
                }
                if (!staticInfo.isAeLockSupported() || !staticInfo.isAwbLockSupported()) {
                    Log.i(TAG, ""AE/AWB lock is not supported in camera "" + id +
                            "". Skip the test"");
                    continue;
                }

                if (staticInfo.isHardwareLevelLegacy()) {
                    Log.i(TAG, ""Legacy camera doesn't report min frame duration"" +
                            "". Skip the test"");
                    continue;
                }

                Capability[] extendedSceneModeCaps =
                        staticInfo.getAvailableExtendedSceneModeCapsChecked();
                boolean supportStillBokeh = false;
                for (Capability cap : extendedSceneModeCaps) {
                    if (cap.getMode() ==
                            CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE) {
                        supportStillBokeh = true;
                        break;
                    }
                }
                if (testStillBokeh && !supportStillBokeh) {
                    Log.v(TAG, ""Device doesn't support STILL_CAPTURE bokeh. Skip the test"");
                    continue;
                }

                openDevice(id);
                burstTestByCamera(id, fmt, burstSize, checkFrameRate, testStillBokeh);
            } finally {
                closeDevice();
                closeImageReader();
            }
        }
    }

    private void burstTestByCamera(String cameraId, int fmt, int burstSize,
            boolean checkFrameRate, boolean testStillBokeh) throws Exception {
        // Parameters
        final int MAX_CONVERGENCE_FRAMES = 150; // 5 sec at 30fps
        final long MAX_PREVIEW_RESULT_TIMEOUT_MS = 2000;
        final float FRAME_DURATION_MARGIN_FRACTION = 0.1f;

        // Find a good preview size (bound to 1080p)
        final Size previewSize = mOrderedPreviewSizes.get(0);

        // Get maximum size for fmt
        final Size stillSize = getSortedSizesForFormat(
                cameraId, mCameraManager, fmt, /*bound*/null).get(0);

        // Find max pipeline depth and sync latency
        final int maxPipelineDepth = mStaticInfo.getCharacteristics().get(
            CameraCharacteristics.REQUEST_PIPELINE_MAX_DEPTH);
        final int maxSyncLatency = mStaticInfo.getCharacteristics().get(
            CameraCharacteristics.SYNC_MAX_LATENCY);

        // Find minimum frame duration for full-res resolution
        StreamConfigurationMap config = mStaticInfo.getCharacteristics().get(
            CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        final long minStillFrameDuration =
                config.getOutputMinFrameDuration(fmt, stillSize);


        Range<Integer> targetRange = getSuitableFpsRangeForDuration(cameraId, minStillFrameDuration);

        Log.i(TAG, String.format(""Selected frame rate range %d - %d for YUV burst"",
                        targetRange.getLower(), targetRange.getUpper()));

        // Check if READ_SENSOR_SETTINGS is supported
        final boolean checkSensorSettings = mStaticInfo.isCapabilitySupported(
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS);

        // Configure basic preview and burst settings

        CaptureRequest.Builder previewBuilder =
            mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        int burstTemplate = (fmt == ImageFormat.JPEG) ?
                CameraDevice.TEMPLATE_STILL_CAPTURE : CameraDevice.TEMPLATE_PREVIEW;
        CaptureRequest.Builder burstBuilder = mCamera.createCaptureRequest(burstTemplate);
        Boolean enableZsl = burstBuilder.get(CaptureRequest.CONTROL_ENABLE_ZSL);
        boolean zslStillEnabled = enableZsl != null && enableZsl &&
                burstTemplate == CameraDevice.TEMPLATE_STILL_CAPTURE;

        previewBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE,
                targetRange);
        burstBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE,
                targetRange);
        burstBuilder.set(CaptureRequest.CONTROL_AE_LOCK, true);
        burstBuilder.set(CaptureRequest.CONTROL_AWB_LOCK, true);
        if (testStillBokeh) {
            previewBuilder.set(CaptureRequest.CONTROL_EXTENDED_SCENE_MODE,
                    CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE);
            burstBuilder.set(CaptureRequest.CONTROL_EXTENDED_SCENE_MODE,
                    CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE);
        }

        // Create session and start up preview

        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleCaptureCallback burstResultListener = new SimpleCaptureCallback();
        ImageDropperListener imageDropper = new ImageDropperListener();

        prepareCaptureAndStartPreview(
            previewBuilder, burstBuilder,
            previewSize, stillSize,
            fmt, resultListener,
            /*maxNumImages*/ 3, imageDropper);

        // Create burst

        List<CaptureRequest> burst = new ArrayList<>();
        for (int i = 0; i < burstSize; i++) {
            burst.add(burstBuilder.build());
        }

        // Converge AE/AWB

        int frameCount = 0;
        while (true) {
            CaptureResult result = resultListener.getCaptureResult(MAX_PREVIEW_RESULT_TIMEOUT_MS);
            int aeState = result.get(CaptureResult.CONTROL_AE_STATE);
            int awbState = result.get(CaptureResult.CONTROL_AWB_STATE);

            if (DEBUG) {
                Log.d(TAG, ""aeState: "" + aeState + "". awbState: "" + awbState);
            }

            if ((aeState == CaptureResult.CONTROL_AE_STATE_CONVERGED ||
                    aeState == CaptureResult.CONTROL_AE_STATE_FLASH_REQUIRED) &&
                    awbState == CaptureResult.CONTROL_AWB_STATE_CONVERGED) {
                break;
            }
            frameCount++;
            assertTrue(String.format(""Cam %s: Can not converge AE and AWB within %d frames"",
                    cameraId, MAX_CONVERGENCE_FRAMES),
                frameCount < MAX_CONVERGENCE_FRAMES);
        }

        // Lock AF if there's a focuser

        if (mStaticInfo.hasFocuser()) {
            previewBuilder.set(CaptureRequest.CONTROL_AF_TRIGGER,
                CaptureRequest.CONTROL_AF_TRIGGER_START);
            mSession.capture(previewBuilder.build(), resultListener, mHandler);
            previewBuilder.set(CaptureRequest.CONTROL_AF_TRIGGER,
                CaptureRequest.CONTROL_AF_TRIGGER_IDLE);

            frameCount = 0;
            while (true) {
                CaptureResult result = resultListener.getCaptureResult(MAX_PREVIEW_RESULT_TIMEOUT_MS);
                int afState = result.get(CaptureResult.CONTROL_AF_STATE);

                if (DEBUG) {
                    Log.d(TAG, ""afState: "" + afState);
                }

                if (afState == CaptureResult.CONTROL_AF_STATE_FOCUSED_LOCKED ||
                    afState == CaptureResult.CONTROL_AF_STATE_NOT_FOCUSED_LOCKED) {
                    break;
                }
                frameCount++;
                assertTrue(String.format(""Cam %s: Cannot lock AF within %d frames"", cameraId,
                        MAX_CONVERGENCE_FRAMES),
                    frameCount < MAX_CONVERGENCE_FRAMES);
            }
        }

        // Lock AE/AWB

        previewBuilder.set(CaptureRequest.CONTROL_AE_LOCK, true);
        previewBuilder.set(CaptureRequest.CONTROL_AWB_LOCK, true);

        CaptureRequest lockedRequest = previewBuilder.build();
        mSession.setRepeatingRequest(lockedRequest, resultListener, mHandler);

        // Wait for first result with locking
        resultListener.drain();
        CaptureResult lockedResult =
                resultListener.getCaptureResultForRequest(lockedRequest, maxPipelineDepth);

        int pipelineDepth = lockedResult.get(CaptureResult.REQUEST_PIPELINE_DEPTH);

        // Then start waiting on results to get the first result that should be synced
        // up, and also fire the burst as soon as possible

        if (maxSyncLatency == CameraCharacteristics.SYNC_MAX_LATENCY_PER_FRAME_CONTROL) {
            // The locked result we have is already synchronized so start the burst
            mSession.captureBurst(burst, burstResultListener, mHandler);
        } else {
            // Need to get a synchronized result, and may need to start burst later to
            // be synchronized correctly

            boolean burstSent = false;

            // Calculate how many requests we need to still send down to camera before we
            // know the settings have settled for the burst

            int numFramesWaited = maxSyncLatency;
            if (numFramesWaited == CameraCharacteristics.SYNC_MAX_LATENCY_UNKNOWN) {
                numFramesWaited = NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY;
            }

            int requestsNeededToSync = numFramesWaited - pipelineDepth;
            for (int i = 0; i < numFramesWaited; i++) {
                if (!burstSent && requestsNeededToSync <= 0) {
                    mSession.captureBurst(burst, burstResultListener, mHandler);
                    burstSent = true;
                }
                lockedResult = resultListener.getCaptureResult(MAX_PREVIEW_RESULT_TIMEOUT_MS);
                requestsNeededToSync--;
            }

            assertTrue(""Cam "" + cameraId + "": Burst failed to fire!"", burstSent);
        }

        // Read in locked settings if supported

        long burstExposure = 0;
        long burstFrameDuration = 0;
        int burstSensitivity = 0;
        if (checkSensorSettings) {
            burstExposure = lockedResult.get(CaptureResult.SENSOR_EXPOSURE_TIME);
            burstFrameDuration = lockedResult.get(CaptureResult.SENSOR_FRAME_DURATION);
            burstSensitivity = lockedResult.get(CaptureResult.SENSOR_SENSITIVITY);

            assertTrue(String.format(""Cam %s: Frame duration %d ns too short compared to "" +
                    ""exposure time %d ns"", cameraId, burstFrameDuration, burstExposure),
                burstFrameDuration >= burstExposure);

            assertTrue(String.format(""Cam %s: Exposure time is not valid: %d"",
                    cameraId, burstExposure),
                burstExposure > 0);
            assertTrue(String.format(""Cam %s: Frame duration is not valid: %d"",
                    cameraId, burstFrameDuration),
                burstFrameDuration > 0);
            assertTrue(String.format(""Cam %s: Sensitivity is not valid: %d"",
                    cameraId, burstSensitivity),
                burstSensitivity > 0);
        }

        // Process burst results
        int burstIndex = 0;
        CaptureResult burstResult =
                burstResultListener.getCaptureResult(MAX_PREVIEW_RESULT_TIMEOUT_MS);
        long prevTimestamp = -1;
        final long frameDurationBound = (long)
                (minStillFrameDuration * (1 + FRAME_DURATION_MARGIN_FRACTION) );

        long burstStartTimestamp = burstResult.get(CaptureResult.SENSOR_TIMESTAMP);
        long burstEndTimeStamp = 0;

        List<Long> frameDurations = new ArrayList<>();

        while(true) {
            // Verify the result
            assertTrue(""Cam "" + cameraId + "": Result doesn't match expected request"",
                    burstResult.getRequest() == burst.get(burstIndex));

            // Verify locked settings
            if (checkSensorSettings) {
                long exposure = burstResult.get(CaptureResult.SENSOR_EXPOSURE_TIME);
                int sensitivity = burstResult.get(CaptureResult.SENSOR_SENSITIVITY);
                assertTrue(""Cam "" + cameraId + "": Exposure not locked!"",
                    exposure == burstExposure);
                assertTrue(""Cam "" + cameraId + "": Sensitivity not locked!"",
                    sensitivity == burstSensitivity);
            }

            // Collect inter-frame durations
            long timestamp = burstResult.get(CaptureResult.SENSOR_TIMESTAMP);
            if (prevTimestamp != -1) {
                long frameDuration = timestamp - prevTimestamp;
                frameDurations.add(frameDuration);
                if (DEBUG) {
                    Log.i(TAG, String.format(""Frame %03d    Duration %.2f ms"", burstIndex,
                            frameDuration/1e6));
                }
            }
            prevTimestamp = timestamp;

            // Get next result
            burstIndex++;
            if (burstIndex == burstSize) {
                burstEndTimeStamp = burstResult.get(CaptureResult.SENSOR_TIMESTAMP);
                break;
            }
            burstResult = burstResultListener.getCaptureResult(MAX_PREVIEW_RESULT_TIMEOUT_MS);
        }

        // Verify no preview frames interleaved in burst results
        while (true) {
            CaptureResult previewResult =
                    resultListener.getCaptureResult(MAX_PREVIEW_RESULT_TIMEOUT_MS);
            long previewTimestamp = previewResult.get(CaptureResult.SENSOR_TIMESTAMP);
            if (!zslStillEnabled && previewTimestamp >= burstStartTimestamp
                    && previewTimestamp <= burstEndTimeStamp) {
                fail(""Preview frame is interleaved with burst frames! Preview timestamp:"" +
                        previewTimestamp + "", burst ["" + burstStartTimestamp + "", "" +
                        burstEndTimeStamp + ""]"");
            } else if (previewTimestamp > burstEndTimeStamp) {
                break;
            }
        }

        // Verify inter-frame durations
        if (checkFrameRate) {
            long meanFrameSum = 0;
            for (Long duration : frameDurations) {
                meanFrameSum += duration;
            }
            float meanFrameDuration = (float) meanFrameSum / frameDurations.size();

            float stddevSum = 0;
            for (Long duration : frameDurations) {
                stddevSum += (duration - meanFrameDuration) * (duration - meanFrameDuration);
            }
            float stddevFrameDuration = (float)
                    Math.sqrt(1.f / (frameDurations.size() - 1 ) * stddevSum);

            Log.i(TAG, String.format(""Cam %s: Burst frame duration mean: %.1f, stddev: %.1f"",
                    cameraId, meanFrameDuration, stddevFrameDuration));

            assertTrue(
                String.format(""Cam %s: Burst frame duration mean %.1f ns is larger than "" +
                    ""acceptable, expecting below %d ns, allowing below %d"", cameraId,
                    meanFrameDuration, minStillFrameDuration, frameDurationBound),
                meanFrameDuration <= frameDurationBound);

            // Calculate upper 97.5% bound (assuming durations are normally distributed...)
            float limit95FrameDuration = meanFrameDuration + 2 * stddevFrameDuration;

            // Don't enforce this yet, but warn
            if (limit95FrameDuration > frameDurationBound) {
                Log.w(TAG,
                    String.format(""Cam %s: Standard deviation is too large compared to limit: "" +
                        ""mean: %.1f ms, stddev: %.1f ms: 95%% bound: %f ms"", cameraId,
                        meanFrameDuration/1e6, stddevFrameDuration/1e6,
                        limit95FrameDuration/1e6));
            }
        }
    }
}"	""	""	"JPEG 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ImageReaderTest"	"testDynamicDepth"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ImageReaderTest.java"	""	"public void testDynamicDepth() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                openDevice(id);
                bufferFormatTestByCamera(ImageFormat.DEPTH_JPEG, /*repeating*/true,
                        /*checkSession*/ true);
            } finally {
                closeDevice(id);
            }
        }
    }"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ImageReaderTest"	"testJpeg"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ImageReaderTest.java"	""	"public void testJpeg() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.v(TAG, ""Testing jpeg capture for Camera "" + id);
                openDevice(id);
                bufferFormatTestByCamera(ImageFormat.JPEG, /*repeating*/false);
            } finally {
                closeDevice(id);
            }
        }
    }"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ImageReaderTest"	"testRepeatingJpeg"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ImageReaderTest.java"	""	"public void testRepeatingJpeg() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.v(TAG, ""Testing repeating jpeg capture for Camera "" + id);
                openDevice(id);
                bufferFormatTestByCamera(ImageFormat.JPEG, /*repeating*/true);
            } finally {
                closeDevice(id);
            }
        }
    }"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ImageReaderTest"	"testInvalidAccessTest"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ImageReaderTest.java"	""	"public void testInvalidAccessTest() throws Exception {
        // Test byte buffer access after an image is released, it should throw ISE.
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.v(TAG, ""Testing invalid image access for Camera "" + id);
                openDevice(id);
                invalidAccessTestAfterClose();
            } finally {
                closeDevice(id);
                closeDefaultImageReader();
            }
        }
    }

    /**
     * Test two image stream (YUV420_888 and JPEG) capture by using ImageReader.
     *
     * <p>Both stream formats are mandatory for Camera2 API</p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ImageReaderTest"	"testYuvAndJpeg"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ImageReaderTest.java"	""	"public void testYuvAndJpeg() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.v(TAG, ""YUV and JPEG testing for camera "" + id);
                if (!mAllStaticInfo.get(id).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                openDevice(id);
                bufferFormatWithYuvTestByCamera(ImageFormat.JPEG);
            } finally {
                closeDevice(id);
            }
        }
    }

    /**
     * Test two image stream (YUV420_888 and JPEG) capture by using ImageReader with the ImageReader
     * factory method that has usage flag argument.
     *
     * <p>Both stream formats are mandatory for Camera2 API</p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ImageReaderTest"	"testYuvAndJpegWithUsageFlag"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ImageReaderTest.java"	""	"public void testYuvAndJpegWithUsageFlag() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.v(TAG, ""YUV and JPEG testing for camera "" + id);
                if (!mAllStaticInfo.get(id).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                openDevice(id);
                bufferFormatWithYuvTestByCamera(ImageFormat.JPEG, true);
            } finally {
                closeDevice(id);
            }
        }
    }

    /**
     * Test two image stream (YUV420_888 and RAW_SENSOR) capture by using ImageReader.
     *
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ImageReaderTest"	"testImageReaderYuvAndRawWithUsageFlag"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ImageReaderTest.java"	""	"public void testImageReaderYuvAndRawWithUsageFlag() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.v(TAG, ""YUV and RAW testing for camera "" + id);
                if (!mAllStaticInfo.get(id).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                openDevice(id);
                bufferFormatWithYuvTestByCamera(ImageFormat.RAW_SENSOR, true);
            } finally {
                closeDevice(id);
            }
        }
    }

    /**
     * Check that the center patches for YUV and JPEG outputs for the same frame match for each YUV
     * resolution and format supported.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ImageReaderTest"	"testAllOutputYUVResolutions"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ImageReaderTest.java"	""	"public void testAllOutputYUVResolutions() throws Exception {
        Integer[] sessionStates = {BlockingSessionCallback.SESSION_READY,
                BlockingSessionCallback.SESSION_CONFIGURE_FAILED};
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.v(TAG, ""Testing all YUV image resolutions for camera "" + id);

                if (!mAllStaticInfo.get(id).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" does not support color outputs, skipping"");
                    continue;
                }

                openDevice(id);
                // Skip warmup on FULL mode devices.
                int warmupCaptureNumber = (mStaticInfo.isHardwareLevelLegacy()) ?
                        MAX_NUM_IMAGES - 1 : 0;

                // NV21 isn't supported by ImageReader.
                final int[] YUVFormats = new int[] {ImageFormat.YUV_420_888, ImageFormat.YV12};

                CameraCharacteristics.Key<StreamConfigurationMap> key =
                        CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP;
                StreamConfigurationMap config = mStaticInfo.getValueFromKeyNonNull(key);
                int[] supportedFormats = config.getOutputFormats();
                List<Integer> supportedYUVFormats = new ArrayList<>();
                for (int format : YUVFormats) {
                    if (CameraTestUtils.contains(supportedFormats, format)) {
                        supportedYUVFormats.add(format);
                    }
                }

                Size[] jpegSizes = mStaticInfo.getAvailableSizesForFormatChecked(ImageFormat.JPEG,
                        StaticMetadata.StreamDirection.Output);
                assertFalse(""JPEG output not supported for camera "" + id +
                        "", at least one JPEG output is required."", jpegSizes.length == 0);

                Size maxJpegSize = CameraTestUtils.getMaxSize(jpegSizes);
                Size maxPreviewSize = mOrderedPreviewSizes.get(0);
                Size QCIF = new Size(176, 144);
                Size FULL_HD = new Size(1920, 1080);
                for (int format : supportedYUVFormats) {
                    Size[] targetCaptureSizes =
                            mStaticInfo.getAvailableSizesForFormatChecked(format,
                            StaticMetadata.StreamDirection.Output);

                    for (Size captureSz : targetCaptureSizes) {
                        if (VERBOSE) {
                            Log.v(TAG, ""Testing yuv size "" + captureSz + "" and jpeg size ""
                                    + maxJpegSize + "" for camera "" + mCamera.getId());
                        }

                        ImageReader jpegReader = null;
                        ImageReader yuvReader = null;
                        try {
                            // Create YUV image reader
                            SimpleImageReaderListener yuvListener = new SimpleImageReaderListener();
                            yuvReader = createImageReader(captureSz, format, MAX_NUM_IMAGES,
                                    yuvListener);
                            Surface yuvSurface = yuvReader.getSurface();

                            // Create JPEG image reader
                            SimpleImageReaderListener jpegListener =
                                    new SimpleImageReaderListener();
                            jpegReader = createImageReader(maxJpegSize,
                                    ImageFormat.JPEG, MAX_NUM_IMAGES, jpegListener);
                            Surface jpegSurface = jpegReader.getSurface();

                            // Setup session
                            List<Surface> outputSurfaces = new ArrayList<Surface>();
                            outputSurfaces.add(yuvSurface);
                            outputSurfaces.add(jpegSurface);
                            createSession(outputSurfaces);

                            int state = mCameraSessionListener.getStateWaiter().waitForAnyOfStates(
                                        Arrays.asList(sessionStates),
                                        CameraTestUtils.SESSION_CONFIGURE_TIMEOUT_MS);

                            if (state == BlockingSessionCallback.SESSION_CONFIGURE_FAILED) {
                                if (captureSz.getWidth() > maxPreviewSize.getWidth() ||
                                        captureSz.getHeight() > maxPreviewSize.getHeight()) {
                                    Log.v(TAG, ""Skip testing {yuv:"" + captureSz
                                            + "" ,jpeg:"" + maxJpegSize + ""} for camera ""
                                            + mCamera.getId() +
                                            "" because full size jpeg + yuv larger than ""
                                            + ""max preview size ("" + maxPreviewSize
                                            + "") is not supported"");
                                    continue;
                                } else if (captureSz.equals(QCIF) &&
                                        ((maxJpegSize.getWidth() > FULL_HD.getWidth()) ||
                                         (maxJpegSize.getHeight() > FULL_HD.getHeight()))) {
                                    Log.v(TAG, ""Skip testing {yuv:"" + captureSz
                                            + "" ,jpeg:"" + maxJpegSize + ""} for camera ""
                                            + mCamera.getId() +
                                            "" because QCIF + >Full_HD size is not supported"");
                                    continue;
                                } else {
                                    fail(""Camera "" + mCamera.getId() +
                                            "":session configuration failed for {jpeg: "" +
                                            maxJpegSize + "", yuv: "" + captureSz + ""}"");
                                }
                            }

                            // Warm up camera preview (mainly to give legacy devices time to do 3A).
                            CaptureRequest.Builder warmupRequest =
                                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
                            warmupRequest.addTarget(yuvSurface);
                            assertNotNull(""Fail to get CaptureRequest.Builder"", warmupRequest);
                            SimpleCaptureCallback resultListener = new SimpleCaptureCallback();

                            for (int i = 0; i < warmupCaptureNumber; i++) {
                                startCapture(warmupRequest.build(), /*repeating*/false,
                                        resultListener, mHandler);
                            }
                            for (int i = 0; i < warmupCaptureNumber; i++) {
                                resultListener.getCaptureResult(CAPTURE_WAIT_TIMEOUT_MS);
                                Image image = yuvListener.getImage(CAPTURE_WAIT_TIMEOUT_MS);
                                image.close();
                            }

                            // Capture image.
                            CaptureRequest.Builder mainRequest =
                                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
                            for (Surface s : outputSurfaces) {
                                mainRequest.addTarget(s);
                            }

                            startCapture(mainRequest.build(), /*repeating*/false, resultListener,
                                    mHandler);

                            // Verify capture result and images
                            resultListener.getCaptureResult(CAPTURE_WAIT_TIMEOUT_MS);

                            Image yuvImage = yuvListener.getImage(CAPTURE_WAIT_TIMEOUT_MS);
                            Image jpegImage = jpegListener.getImage(CAPTURE_WAIT_TIMEOUT_MS);

                            //Validate captured images.
                            CameraTestUtils.validateImage(yuvImage, captureSz.getWidth(),
                                    captureSz.getHeight(), format, /*filePath*/null);
                            CameraTestUtils.validateImage(jpegImage, maxJpegSize.getWidth(),
                                    maxJpegSize.getHeight(), ImageFormat.JPEG, /*filePath*/null);

                            // Compare the image centers.
                            RectF jpegDimens = new RectF(0, 0, jpegImage.getWidth(),
                                    jpegImage.getHeight());
                            RectF yuvDimens = new RectF(0, 0, yuvImage.getWidth(),
                                    yuvImage.getHeight());

                            // Find scale difference between YUV and JPEG output
                            Matrix m = new Matrix();
                            m.setRectToRect(yuvDimens, jpegDimens, Matrix.ScaleToFit.START);
                            RectF scaledYuv = new RectF();
                            m.mapRect(scaledYuv, yuvDimens);
                            float scale = scaledYuv.width() / yuvDimens.width();

                            final int PATCH_DIMEN = 40; // pixels in YUV

                            // Find matching square patch of pixels in YUV and JPEG output
                            RectF tempPatch = new RectF(0, 0, PATCH_DIMEN, PATCH_DIMEN);
                            tempPatch.offset(yuvDimens.centerX() - tempPatch.centerX(),
                                    yuvDimens.centerY() - tempPatch.centerY());
                            Rect yuvPatch = new Rect();
                            tempPatch.roundOut(yuvPatch);

                            tempPatch.set(0, 0, PATCH_DIMEN * scale, PATCH_DIMEN * scale);
                            tempPatch.offset(jpegDimens.centerX() - tempPatch.centerX(),
                                    jpegDimens.centerY() - tempPatch.centerY());
                            Rect jpegPatch = new Rect();
                            tempPatch.roundOut(jpegPatch);

                            // Decode center patches
                            int[] yuvColors = convertPixelYuvToRgba(yuvPatch.width(),
                                    yuvPatch.height(), yuvPatch.left, yuvPatch.top, yuvImage);
                            Bitmap yuvBmap = Bitmap.createBitmap(yuvColors, yuvPatch.width(),
                                    yuvPatch.height(), Bitmap.Config.ARGB_8888);

                            byte[] compressedJpegData = CameraTestUtils.getDataFromImage(jpegImage);
                            BitmapRegionDecoder decoder = BitmapRegionDecoder.newInstance(
                                    compressedJpegData, /*offset*/0, compressedJpegData.length,
                                    /*isShareable*/true);
                            BitmapFactory.Options opt = new BitmapFactory.Options();
                            opt.inPreferredConfig = Bitmap.Config.ARGB_8888;
                            Bitmap fullSizeJpegBmap = decoder.decodeRegion(jpegPatch, opt);
                            Bitmap jpegBmap = Bitmap.createScaledBitmap(fullSizeJpegBmap,
                                    yuvPatch.width(), yuvPatch.height(), /*filter*/true);

                            // Compare two patches using average of per-pixel differences
                            double difference = BitmapUtils.calcDifferenceMetric(yuvBmap, jpegBmap);
                            double tolerance = IMAGE_DIFFERENCE_TOLERANCE;
                            if (mStaticInfo.isHardwareLevelLegacy()) {
                                tolerance = IMAGE_DIFFERENCE_TOLERANCE_LEGACY;
                            }
                            Log.i(TAG, ""Difference for resolution "" + captureSz + "" is: "" +
                                    difference);
                            if (difference > tolerance) {
                                // Dump files if running in verbose mode
                                if (DEBUG) {
                                    String jpegFileName = mDebugFileNameBase + ""/"" + captureSz +
                                            ""_jpeg.jpg"";
                                    dumpFile(jpegFileName, jpegBmap);
                                    String fullSizeJpegFileName = mDebugFileNameBase + ""/"" +
                                            captureSz + ""_full_jpeg.jpg"";
                                    dumpFile(fullSizeJpegFileName, compressedJpegData);
                                    String yuvFileName = mDebugFileNameBase + ""/"" + captureSz +
                                            ""_yuv.jpg"";
                                    dumpFile(yuvFileName, yuvBmap);
                                    String fullSizeYuvFileName = mDebugFileNameBase + ""/"" +
                                            captureSz + ""_full_yuv.jpg"";
                                    int[] fullYUVColors = convertPixelYuvToRgba(yuvImage.getWidth(),
                                            yuvImage.getHeight(), 0, 0, yuvImage);
                                    Bitmap fullYUVBmap = Bitmap.createBitmap(fullYUVColors,
                                            yuvImage.getWidth(), yuvImage.getHeight(),
                                            Bitmap.Config.ARGB_8888);
                                    dumpFile(fullSizeYuvFileName, fullYUVBmap);
                                }
                                fail(""Camera "" + mCamera.getId() + "": YUV and JPEG image at "" +
                                        ""capture size "" + captureSz + "" for the same frame are "" +
                                        ""not similar, center patches have difference metric of "" +
                                        difference + "", tolerance is "" + tolerance);
                            }

                            // Stop capture, delete the streams.
                            stopCapture(/*fast*/false);
                            yuvImage.close();
                            jpegImage.close();
                            yuvListener.drain();
                            jpegListener.drain();
                        } finally {
                            closeImageReader(jpegReader);
                            jpegReader = null;
                            closeImageReader(yuvReader);
                            yuvReader = null;
                        }
                    }
                }

            } finally {
                closeDevice(id);
            }
        }
    }

    /**
     * Test that images captured after discarding free buffers are valid.
     */"	""	""	"JPEG 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ImageReaderTest"	"testUsageRespected"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ImageReaderTest.java"	""	"public void testUsageRespected() throws Exception {
        final long REQUESTED_USAGE_BITS =
                HardwareBuffer.USAGE_GPU_COLOR_OUTPUT | HardwareBuffer.USAGE_GPU_SAMPLED_IMAGE;
        ImageReader reader = ImageReader.newInstance(1, 1, PixelFormat.RGBA_8888, 1,
                REQUESTED_USAGE_BITS);
        Surface surface = reader.getSurface();
        Canvas canvas = surface.lockHardwareCanvas();
        canvas.drawColor(Color.RED);
        surface.unlockCanvasAndPost(canvas);
        Image image = null;
        for (int i = 0; i < 100; i++) {
            image = reader.acquireNextImage();
            if (image != null) break;
            Thread.sleep(10);
        }
        assertNotNull(image);
        HardwareBuffer buffer = image.getHardwareBuffer();
        assertNotNull(buffer);
        // Mask off the upper vendor bits
        int myBits = (int) (buffer.getUsage() & 0xFFFFFFF);
        assertWithMessage(""Usage bits %s did not contain requested usage bits %s"", myBits,
                REQUESTED_USAGE_BITS).that(myBits & REQUESTED_USAGE_BITS)
                        .isEqualTo(REQUESTED_USAGE_BITS);
    }

    /**
     * Convert a rectangular patch in a YUV image to an ARGB color array.
     *
     * @param w width of the patch.
     * @param h height of the patch.
     * @param wOffset offset of the left side of the patch.
     * @param hOffset offset of the top of the patch.
     * @param yuvImage a YUV image to select a patch from.
     * @return the image patch converted to RGB as an ARGB color array.
     */
    private static int[] convertPixelYuvToRgba(int w, int h, int wOffset, int hOffset,
                                               Image yuvImage) {
        final int CHANNELS = 3; // yuv
        final float COLOR_RANGE = 255f;

        assertTrue(""Invalid argument to convertPixelYuvToRgba"",
                w > 0 && h > 0 && wOffset >= 0 && hOffset >= 0);
        assertNotNull(yuvImage);

        int imageFormat = yuvImage.getFormat();
        assertTrue(""YUV image must have YUV-type format"",
                imageFormat == ImageFormat.YUV_420_888 || imageFormat == ImageFormat.YV12 ||
                        imageFormat == ImageFormat.NV21);

        int height = yuvImage.getHeight();
        int width = yuvImage.getWidth();

        Rect imageBounds = new Rect(/*left*/0, /*top*/0, /*right*/width, /*bottom*/height);
        Rect crop = new Rect(/*left*/wOffset, /*top*/hOffset, /*right*/wOffset + w,
                /*bottom*/hOffset + h);
        assertTrue(""Output rectangle"" + crop + "" must lie within image bounds "" + imageBounds,
                imageBounds.contains(crop));
        Image.Plane[] planes = yuvImage.getPlanes();

        Image.Plane yPlane = planes[0];
        Image.Plane cbPlane = planes[1];
        Image.Plane crPlane = planes[2];

        ByteBuffer yBuf = yPlane.getBuffer();
        int yPixStride = yPlane.getPixelStride();
        int yRowStride = yPlane.getRowStride();
        ByteBuffer cbBuf = cbPlane.getBuffer();
        int cbPixStride = cbPlane.getPixelStride();
        int cbRowStride = cbPlane.getRowStride();
        ByteBuffer crBuf = crPlane.getBuffer();
        int crPixStride = crPlane.getPixelStride();
        int crRowStride = crPlane.getRowStride();

        int[] output = new int[w * h];

        // TODO: Optimize this with renderscript intrinsics
        byte[] yRow = new byte[yPixStride * (w - 1) + 1];
        byte[] cbRow = new byte[cbPixStride * (w / 2 - 1) + 1];
        byte[] crRow = new byte[crPixStride * (w / 2 - 1) + 1];
        yBuf.mark();
        cbBuf.mark();
        crBuf.mark();
        int initialYPos = yBuf.position();
        int initialCbPos = cbBuf.position();
        int initialCrPos = crBuf.position();
        int outputPos = 0;
        for (int i = hOffset; i < hOffset + h; i++) {
            yBuf.position(initialYPos + i * yRowStride + wOffset * yPixStride);
            yBuf.get(yRow);
            if ((i & 1) == (hOffset & 1)) {
                cbBuf.position(initialCbPos + (i / 2) * cbRowStride + wOffset * cbPixStride / 2);
                cbBuf.get(cbRow);
                crBuf.position(initialCrPos + (i / 2) * crRowStride + wOffset * crPixStride / 2);
                crBuf.get(crRow);
            }
            for (int j = 0, yPix = 0, crPix = 0, cbPix = 0; j < w; j++, yPix += yPixStride) {
                float y = yRow[yPix] & 0xFF;
                float cb = cbRow[cbPix] & 0xFF;
                float cr = crRow[crPix] & 0xFF;

                // convert YUV -> RGB (from JFIF's ""Conversion to and from RGB"" section)
                int r = (int) Math.max(0.0f, Math.min(COLOR_RANGE, y + 1.402f * (cr - 128)));
                int g = (int) Math.max(0.0f,
                        Math.min(COLOR_RANGE, y - 0.34414f * (cb - 128) - 0.71414f * (cr - 128)));
                int b = (int) Math.max(0.0f, Math.min(COLOR_RANGE, y + 1.772f * (cb - 128)));

                // Convert to ARGB pixel color (use opaque alpha)
                output[outputPos++] = Color.rgb(r, g, b);

                if ((j & 1) == 1) {
                    crPix += crPixStride;
                    cbPix += cbPixStride;
                }
            }
        }
        yBuf.rewind();
        cbBuf.rewind();
        crBuf.rewind();

        return output;
    }

    /**
     * Test capture a given format stream with yuv stream simultaneously.
     *
     * <p>Use fixed yuv size, varies targeted format capture size. Single capture is tested.</p>
     *
     * @param format The capture format to be tested along with yuv format.
     */
    private void bufferFormatWithYuvTestByCamera(int format) throws Exception {
        bufferFormatWithYuvTestByCamera(format, false);
    }

    /**
     * Test capture a given format stream with yuv stream simultaneously.
     *
     * <p>Use fixed yuv size, varies targeted format capture size. Single capture is tested.</p>
     *
     * @param format The capture format to be tested along with yuv format.
     * @param setUsageFlag The ImageReader factory method to be used (with or without specifying
     *                     usage flag)
     */
    private void bufferFormatWithYuvTestByCamera(int format, boolean setUsageFlag)
            throws Exception {
        if (format != ImageFormat.JPEG && format != ImageFormat.RAW_SENSOR
                && format != ImageFormat.YUV_420_888) {
            throw new IllegalArgumentException(""Unsupported format: "" + format);
        }

        final int NUM_SINGLE_CAPTURE_TESTED = MAX_NUM_IMAGES - 1;
        Size maxYuvSz = mOrderedPreviewSizes.get(0);
        Size[] targetCaptureSizes = mStaticInfo.getAvailableSizesForFormatChecked(format,
                StaticMetadata.StreamDirection.Output);

        for (Size captureSz : targetCaptureSizes) {
            if (VERBOSE) {
                Log.v(TAG, ""Testing yuv size "" + maxYuvSz.toString() + "" and capture size ""
                        + captureSz.toString() + "" for camera "" + mCamera.getId());
            }

            ImageReader captureReader = null;
            ImageReader yuvReader = null;
            try {
                // Create YUV image reader
                SimpleImageReaderListener yuvListener  = new SimpleImageReaderListener();
                if (setUsageFlag) {
                    yuvReader = createImageReader(maxYuvSz, ImageFormat.YUV_420_888, MAX_NUM_IMAGES,
                            HardwareBuffer.USAGE_CPU_READ_OFTEN, yuvListener);
                } else {
                    yuvReader = createImageReader(maxYuvSz, ImageFormat.YUV_420_888, MAX_NUM_IMAGES,
                            yuvListener);
                }

                Surface yuvSurface = yuvReader.getSurface();

                // Create capture image reader
                SimpleImageReaderListener captureListener = new SimpleImageReaderListener();
                if (setUsageFlag) {
                    captureReader = createImageReader(captureSz, format, MAX_NUM_IMAGES,
                            HardwareBuffer.USAGE_CPU_READ_OFTEN, captureListener);
                } else {
                    captureReader = createImageReader(captureSz, format, MAX_NUM_IMAGES,
                            captureListener);
                }
                Surface captureSurface = captureReader.getSurface();

                // Capture images.
                List<Surface> outputSurfaces = new ArrayList<Surface>();
                outputSurfaces.add(yuvSurface);
                outputSurfaces.add(captureSurface);
                CaptureRequest.Builder request = prepareCaptureRequestForSurfaces(outputSurfaces,
                        CameraDevice.TEMPLATE_PREVIEW);
                SimpleCaptureCallback resultListener = new SimpleCaptureCallback();

                for (int i = 0; i < NUM_SINGLE_CAPTURE_TESTED; i++) {
                    startCapture(request.build(), /*repeating*/false, resultListener, mHandler);
                }

                // Verify capture result and images
                for (int i = 0; i < NUM_SINGLE_CAPTURE_TESTED; i++) {
                    resultListener.getCaptureResult(CAPTURE_WAIT_TIMEOUT_MS);
                    if (VERBOSE) {
                        Log.v(TAG, "" Got the capture result back for "" + i + ""th capture"");
                    }

                    Image yuvImage = yuvListener.getImage(CAPTURE_WAIT_TIMEOUT_MS);
                    if (VERBOSE) {
                        Log.v(TAG, "" Got the yuv image back for "" + i + ""th capture"");
                    }

                    Image captureImage = captureListener.getImage(CAPTURE_WAIT_TIMEOUT_MS);
                    if (VERBOSE) {
                        Log.v(TAG, "" Got the capture image back for "" + i + ""th capture"");
                    }

                    //Validate captured images.
                    CameraTestUtils.validateImage(yuvImage, maxYuvSz.getWidth(),
                            maxYuvSz.getHeight(), ImageFormat.YUV_420_888, /*filePath*/null);
                    CameraTestUtils.validateImage(captureImage, captureSz.getWidth(),
                            captureSz.getHeight(), format, /*filePath*/null);
                    yuvImage.close();
                    captureImage.close();
                }

                // Stop capture, delete the streams.
                stopCapture(/*fast*/false);
            } finally {
                closeImageReader(captureReader);
                captureReader = null;
                closeImageReader(yuvReader);
                yuvReader = null;
            }
        }
    }

    private void invalidAccessTestAfterClose() throws Exception {
        final int FORMAT = mStaticInfo.isColorOutputSupported() ?
            ImageFormat.YUV_420_888 : ImageFormat.DEPTH16;

        Size[] availableSizes = mStaticInfo.getAvailableSizesForFormatChecked(FORMAT,
                StaticMetadata.StreamDirection.Output);
        Image img = null;
        // Create ImageReader.
        mListener = new SimpleImageListener();
        createDefaultImageReader(availableSizes[0], FORMAT, MAX_NUM_IMAGES, mListener);

        // Start capture.
        CaptureRequest request = prepareCaptureRequest();
        SimpleCaptureCallback listener = new SimpleCaptureCallback();
        startCapture(request, /* repeating */false, listener, mHandler);

        mListener.waitForAnyImageAvailable(CAPTURE_WAIT_TIMEOUT_MS);
        img = mReader.acquireNextImage();
        Plane firstPlane = img.getPlanes()[0];
        ByteBuffer buffer = firstPlane.getBuffer();
        img.close();

        imageInvalidAccessTestAfterClose(img, firstPlane, buffer);
    }

    /**
     * Test that images captured after discarding free buffers are valid.
     */
    private void discardFreeBuffersTestByCamera() throws Exception {
        final int FORMAT = mStaticInfo.isColorOutputSupported() ?
            ImageFormat.YUV_420_888 : ImageFormat.DEPTH16;

        final Size SIZE = mStaticInfo.getAvailableSizesForFormatChecked(FORMAT,
                StaticMetadata.StreamDirection.Output)[0];
        // Create ImageReader.
        mListener = new SimpleImageListener();
        createDefaultImageReader(SIZE, FORMAT, MAX_NUM_IMAGES, mListener);

        // Start capture.
        final boolean REPEATING = true;
        final boolean SINGLE = false;
        CaptureRequest request = prepareCaptureRequest();
        SimpleCaptureCallback listener = new SimpleCaptureCallback();
        startCapture(request, REPEATING, listener, mHandler);

        // Validate images and capture results.
        validateImage(SIZE, FORMAT, NUM_FRAME_VERIFIED, REPEATING);
        validateCaptureResult(FORMAT, SIZE, listener, NUM_FRAME_VERIFIED);

        // Discard free buffers.
        mReader.discardFreeBuffers();

        // Validate images and capture resulst again.
        validateImage(SIZE, FORMAT, NUM_FRAME_VERIFIED, REPEATING);
        validateCaptureResult(FORMAT, SIZE, listener, NUM_FRAME_VERIFIED);

        // Stop repeating request in preparation for discardFreeBuffers
        mCameraSession.stopRepeating();
        mCameraSessionListener.getStateWaiter().waitForState(
                BlockingSessionCallback.SESSION_READY, SESSION_READY_TIMEOUT_MS);

        // Drain the reader queue and discard free buffers from the reader.
        Image img = mReader.acquireLatestImage();
        if (img != null) {
            img.close();
        }
        mReader.discardFreeBuffers();

        // Do a single capture for camera device to reallocate buffers
        mListener.reset();
        startCapture(request, SINGLE, listener, mHandler);
        validateImage(SIZE, FORMAT, /*captureCount*/1, SINGLE);
    }

    private void bufferFormatTestByCamera(int format, boolean repeating) throws Exception {
        bufferFormatTestByCamera(format, /*setUsageFlag*/ false,
                HardwareBuffer.USAGE_CPU_READ_OFTEN, repeating,
                /*checkSession*/ false, /*validateImageData*/ true);
    }

    private void bufferFormatTestByCamera(int format, boolean repeating, boolean checkSession)
            throws Exception {
        bufferFormatTestByCamera(format, /*setUsageFlag*/ false,
                HardwareBuffer.USAGE_CPU_READ_OFTEN,
                repeating, checkSession, /*validateImageData*/true);
    }

    private void bufferFormatTestByCamera(int format, boolean setUsageFlag, long usageFlag,
            boolean repeating, boolean checkSession, boolean validateImageData) throws Exception {
        bufferFormatTestByCamera(format, setUsageFlag, usageFlag, repeating, checkSession,
                validateImageData, /*physicalId*/null);
    }

    private void bufferFormatTestByCamera(int format, boolean setUsageFlag, long usageFlag,
            // TODO: Consider having some sort of test configuration class passed to reduce the
            //       proliferation of parameters ?
            boolean repeating, boolean checkSession, boolean validateImageData, String physicalId)
            throws Exception {
        StaticMetadata staticInfo;
        if (physicalId == null) {
            staticInfo = mStaticInfo;
        } else {
            staticInfo = mAllStaticInfo.get(physicalId);
        }

        Size[] availableSizes = staticInfo.getAvailableSizesForFormatChecked(format,
                StaticMetadata.StreamDirection.Output);

        boolean secureTest = setUsageFlag &&
                ((usageFlag & HardwareBuffer.USAGE_PROTECTED_CONTENT) != 0);
        Size secureDataSize = null;
        if (secureTest) {
            secureDataSize = staticInfo.getCharacteristics().get(
                    CameraCharacteristics.SCALER_DEFAULT_SECURE_IMAGE_SIZE);
        }

        // for each resolution, test imageReader:
        for (Size sz : availableSizes) {
            try {
                // For secure mode test only test default secure data size if HAL advertises one.
                if (secureDataSize != null && !secureDataSize.equals(sz)) {
                    continue;
                }

                if (VERBOSE) {
                    Log.v(TAG, ""Testing size "" + sz.toString() + "" format "" + format
                            + "" for camera "" + mCamera.getId());
                }

                // Create ImageReader.
                mListener  = new SimpleImageListener();
                if (setUsageFlag) {
                    createDefaultImageReader(sz, format, MAX_NUM_IMAGES, usageFlag, mListener);
                } else {
                    createDefaultImageReader(sz, format, MAX_NUM_IMAGES, mListener);
                }

                // Don't queue up images if we won't validate them
                if (!validateImageData) {
                    ImageDropperListener imageDropperListener = new ImageDropperListener();
                    mReader.setOnImageAvailableListener(imageDropperListener, mHandler);
                }

                if (checkSession) {
                    checkImageReaderSessionConfiguration(
                            ""Camera capture session validation for format: "" + format + ""failed"",
                            physicalId);
                }

                ArrayList<OutputConfiguration> outputConfigs = new ArrayList<>();
                OutputConfiguration config = new OutputConfiguration(mReader.getSurface());
                if (physicalId != null) {
                    config.setPhysicalCameraId(physicalId);
                }
                outputConfigs.add(config);
                CaptureRequest request = prepareCaptureRequestForConfigs(
                        outputConfigs, CameraDevice.TEMPLATE_PREVIEW).build();

                SimpleCaptureCallback listener = new SimpleCaptureCallback();
                startCapture(request, repeating, listener, mHandler);

                int numFrameVerified = repeating ? NUM_FRAME_VERIFIED : 1;

                if (validateImageData) {
                    // Validate images.
                    validateImage(sz, format, numFrameVerified, repeating);
                }

                // Validate capture result.
                validateCaptureResult(format, sz, listener, numFrameVerified);

                // stop capture.
                stopCapture(/*fast*/false);
            } finally {
                closeDefaultImageReader();
            }

        }
    }

    private void bufferFormatLongProcessingTimeTestByCamera(int format)
            throws Exception {

        final int TEST_SENSITIVITY_VALUE = mStaticInfo.getSensitivityClampToRange(204);
        final long TEST_EXPOSURE_TIME_NS = mStaticInfo.getExposureClampToRange(28000000);
        final long EXPOSURE_TIME_ERROR_MARGIN_NS = 100000;

        Size[] availableSizes = mStaticInfo.getAvailableSizesForFormatChecked(format,
                StaticMetadata.StreamDirection.Output);

        // for each resolution, test imageReader:
        for (Size sz : availableSizes) {
            Log.v(TAG, ""testing size "" + sz.toString());
            try {
                if (VERBOSE) {
                    Log.v(TAG, ""Testing long processing time: size "" + sz.toString() + "" format "" +
                            format + "" for camera "" + mCamera.getId());
                }

                // Create ImageReader.
                mListener  = new SimpleImageListener();
                createDefaultImageReader(sz, format, MAX_NUM_IMAGES, mListener);

                // Setting manual controls
                List<Surface> outputSurfaces = new ArrayList<Surface>();
                outputSurfaces.add(mReader.getSurface());
                CaptureRequest.Builder requestBuilder = prepareCaptureRequestForSurfaces(
                        outputSurfaces, CameraDevice.TEMPLATE_STILL_CAPTURE);

                requestBuilder.set(
                        CaptureRequest.CONTROL_MODE, CaptureRequest.CONTROL_MODE_OFF);
                requestBuilder.set(CaptureRequest.CONTROL_AE_LOCK, true);
                requestBuilder.set(CaptureRequest.CONTROL_AWB_LOCK, true);
                requestBuilder.set(CaptureRequest.CONTROL_AE_MODE,
                        CaptureRequest.CONTROL_AE_MODE_OFF);
                requestBuilder.set(CaptureRequest.CONTROL_AWB_MODE,
                        CaptureRequest.CONTROL_AWB_MODE_OFF);
                requestBuilder.set(CaptureRequest.SENSOR_SENSITIVITY, TEST_SENSITIVITY_VALUE);
                requestBuilder.set(CaptureRequest.SENSOR_EXPOSURE_TIME, TEST_EXPOSURE_TIME_NS);

                SimpleCaptureCallback listener = new SimpleCaptureCallback();
                startCapture(requestBuilder.build(), /*repeating*/true, listener, mHandler);

                for (int i = 0; i < NUM_LONG_PROCESS_TIME_FRAME_VERIFIED; i++) {
                    mListener.waitForAnyImageAvailable(CAPTURE_WAIT_TIMEOUT_MS);

                    // Verify image.
                    Image img = mReader.acquireNextImage();
                    assertNotNull(""Unable to acquire next image"", img);
                    CameraTestUtils.validateImage(img, sz.getWidth(), sz.getHeight(), format,
                            mDebugFileNameBase);

                    // Verify the exposure time and iso match the requested values.
                    CaptureResult result = listener.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);

                    long exposureTimeDiff = TEST_EXPOSURE_TIME_NS -
                            getValueNotNull(result, CaptureResult.SENSOR_EXPOSURE_TIME);
                    int sensitivityDiff = TEST_SENSITIVITY_VALUE -
                            getValueNotNull(result, CaptureResult.SENSOR_SENSITIVITY);

                    mCollector.expectTrue(
                            String.format(""Long processing frame %d format %d size %s "" +
                                    ""exposure time was %d expecting %d."", i, format, sz.toString(),
                                    getValueNotNull(result, CaptureResult.SENSOR_EXPOSURE_TIME),
                                    TEST_EXPOSURE_TIME_NS),
                            exposureTimeDiff < EXPOSURE_TIME_ERROR_MARGIN_NS &&
                            exposureTimeDiff >= 0);

                    mCollector.expectTrue(
                            String.format(""Long processing frame %d format %d size %s "" +
                                    ""sensitivity was %d expecting %d."", i, format, sz.toString(),
                                    getValueNotNull(result, CaptureResult.SENSOR_SENSITIVITY),
                                    TEST_SENSITIVITY_VALUE),
                            sensitivityDiff >= 0);


                    // Sleep to Simulate long porcessing before closing the image.
                    Thread.sleep(LONG_PROCESS_TIME_MS);
                    img.close();
                }
                // Stop capture.
                // Drain the reader queue in case the full queue blocks
                // HAL from delivering new results
                ImageDropperListener imageDropperListener = new ImageDropperListener();
                mReader.setOnImageAvailableListener(imageDropperListener, mHandler);
                Image img = mReader.acquireLatestImage();
                if (img != null) {
                    img.close();
                }
                stopCapture(/*fast*/false);
            } finally {
                closeDefaultImageReader();
            }
        }
    }

    /**
     * Validate capture results.
     *
     * @param format The format of this capture.
     * @param size The capture size.
     * @param listener The capture listener to get capture result callbacks.
     */
    private void validateCaptureResult(int format, Size size, SimpleCaptureCallback listener,
            int numFrameVerified) {
        for (int i = 0; i < numFrameVerified; i++) {
            CaptureResult result = listener.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);

            // TODO: Update this to use availableResultKeys once shim supports this.
            if (mStaticInfo.isCapabilitySupported(
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS)) {
                Long exposureTime = getValueNotNull(result, CaptureResult.SENSOR_EXPOSURE_TIME);
                Integer sensitivity = getValueNotNull(result, CaptureResult.SENSOR_SENSITIVITY);
                mCollector.expectInRange(
                        String.format(
                                ""Capture for format %d, size %s exposure time is invalid."",
                                format, size.toString()),
                        exposureTime,
                        mStaticInfo.getExposureMinimumOrDefault(),
                        mStaticInfo.getExposureMaximumOrDefault()
                );
                mCollector.expectInRange(
                        String.format(""Capture for format %d, size %s sensitivity is invalid."",
                                format, size.toString()),
                        sensitivity,
                        mStaticInfo.getSensitivityMinimumOrDefault(),
                        mStaticInfo.getSensitivityMaximumOrDefault()
                );
            }
            // TODO: add more key validations.
        }
    }

    private final class SimpleImageListener implements ImageReader.OnImageAvailableListener {
        private final ConditionVariable imageAvailable = new ConditionVariable();
        @Override
        public void onImageAvailable(ImageReader reader) {
            if (mReader != reader) {
                return;
            }

            if (VERBOSE) Log.v(TAG, ""new image available"");
            imageAvailable.open();
        }

        public void waitForAnyImageAvailable(long timeout) {
            if (imageAvailable.block(timeout)) {
                imageAvailable.close();
            } else {
                fail(""wait for image available timed out after "" + timeout + ""ms"");
            }
        }

        public void closePendingImages() {
            Image image = mReader.acquireLatestImage();
            if (image != null) {
                image.close();
            }
        }

        public void reset() {
            imageAvailable.close();
        }
    }

    private void validateImage(Size sz, int format, int captureCount,  boolean repeating)
            throws Exception {
        // TODO: Add more format here, and wrap each one as a function.
        Image img;
        final int MAX_RETRY_COUNT = 20;
        int numImageVerified = 0;
        int reTryCount = 0;
        while (numImageVerified < captureCount) {
            assertNotNull(""Image listener is null"", mListener);
            if (VERBOSE) Log.v(TAG, ""Waiting for an Image"");
            mListener.waitForAnyImageAvailable(CAPTURE_WAIT_TIMEOUT_MS);
            if (repeating) {
                /**
                 * Acquire the latest image in case the validation is slower than
                 * the image producing rate.
                 */
                img = mReader.acquireLatestImage();
                /**
                 * Sometimes if multiple onImageAvailable callbacks being queued,
                 * acquireLatestImage will clear all buffer before corresponding callback is
                 * executed. Wait for a new frame in that case.
                 */
                if (img == null && reTryCount < MAX_RETRY_COUNT) {
                    reTryCount++;
                    continue;
                }
            } else {
                img = mReader.acquireNextImage();
            }
            assertNotNull(""Unable to acquire the latest image"", img);
            if (VERBOSE) Log.v(TAG, ""Got the latest image"");
            CameraTestUtils.validateImage(img, sz.getWidth(), sz.getHeight(), format,
                    mDebugFileNameBase);
            HardwareBuffer hwb = img.getHardwareBuffer();
            assertNotNull(""Unable to retrieve the Image's HardwareBuffer"", hwb);
            if (format == ImageFormat.DEPTH_JPEG) {
                byte [] dynamicDepthBuffer = CameraTestUtils.getDataFromImage(img);
                assertTrue(""Dynamic depth validation failed!"",
                        validateDynamicDepthNative(dynamicDepthBuffer));
            }
            if (VERBOSE) Log.v(TAG, ""finish validation of image "" + numImageVerified);
            img.close();
            numImageVerified++;
            reTryCount = 0;
        }

        // Return all pending images to the ImageReader as the validateImage may
        // take a while to return and there could be many images pending.
        mListener.closePendingImages();
    }

    /** Load dynamic depth validation jni on initialization */
    static {
        System.loadLibrary(""ctscamera2_jni"");
    }
    /**
     * Use the dynamic depth SDK to validate a dynamic depth file stored in the buffer.
     *
     * Returns false if the dynamic depth has validation errors. Validation warnings/errors
     * will be printed to logcat.
     */
    private static native boolean validateDynamicDepthNative(byte[] dynamicDepthBuffer);
}"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraDeviceTest"	"testCreateCustomSession"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CameraDeviceTest.java"	""	"public void testCreateCustomSession() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                openDevice(mCameraIdsUnderTest[i], mCameraMockListener);
                waitForDeviceState(STATE_OPENED, CAMERA_OPEN_TIMEOUT_MS);

                testCreateCustomSessionByCamera(mCameraIdsUnderTest[i]);
            }
            finally {
                closeDevice(mCameraIdsUnderTest[i], mCameraMockListener);
            }
        }
    }

    /**
     * Verify creating a custom mode session works
     */
    private void testCreateCustomSessionByCamera(String cameraId) throws Exception {
        final int SESSION_TIMEOUT_MS = 1000;
        final int CAPTURE_TIMEOUT_MS = 3000;

        if (VERBOSE) {
            Log.v(TAG, ""Testing creating custom session for camera "" + cameraId);
        }

        Size yuvSize = mOrderedPreviewSizes.get(0);

        // Create a list of image readers. JPEG for last one and YUV for the rest.
        ImageReader imageReader = ImageReader.newInstance(yuvSize.getWidth(), yuvSize.getHeight(),
                ImageFormat.YUV_420_888, /*maxImages*/1);

        try {
            // Create a normal-mode session via createCustomCaptureSession
            mSessionMockListener = spy(new BlockingSessionCallback());
            mSessionWaiter = mSessionMockListener.getStateWaiter();
            List<OutputConfiguration> outputs = new ArrayList<>();
            outputs.add(new OutputConfiguration(imageReader.getSurface()));
            mCamera.createCustomCaptureSession(/*inputConfig*/null, outputs,
                    CameraDevice.SESSION_OPERATION_MODE_NORMAL, mSessionMockListener, mHandler);
            mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);

            // Verify we can capture a frame with the session.
            SimpleCaptureCallback captureListener = new SimpleCaptureCallback();
            SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
            imageReader.setOnImageAvailableListener(imageListener, mHandler);

            CaptureRequest.Builder builder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
            builder.addTarget(imageReader.getSurface());
            CaptureRequest request = builder.build();

            mSession.capture(request, captureListener, mHandler);
            captureListener.getCaptureResultForRequest(request, CAPTURE_TIMEOUT_MS);
            imageListener.getImage(CAPTURE_TIMEOUT_MS).close();

            // Create a few invalid custom sessions by using undefined non-vendor mode indices, and
            // check that they fail to configure
            mSessionMockListener = spy(new BlockingSessionCallback());
            mSessionWaiter = mSessionMockListener.getStateWaiter();
            mCamera.createCustomCaptureSession(/*inputConfig*/null, outputs,
                    CameraDevice.SESSION_OPERATION_MODE_VENDOR_START - 1, mSessionMockListener, mHandler);
            mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
            waitForSessionState(BlockingSessionCallback.SESSION_CONFIGURE_FAILED,
                    SESSION_CONFIGURE_TIMEOUT_MS);

            mSessionMockListener = spy(new BlockingSessionCallback());
            mCamera.createCustomCaptureSession(/*inputConfig*/null, outputs,
                    CameraDevice.SESSION_OPERATION_MODE_CONSTRAINED_HIGH_SPEED + 1, mSessionMockListener,
                    mHandler);
            mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
            mSessionWaiter = mSessionMockListener.getStateWaiter();
            waitForSessionState(BlockingSessionCallback.SESSION_CONFIGURE_FAILED,
                    SESSION_CONFIGURE_TIMEOUT_MS);

        } finally {
            imageReader.close();
            mSession.close();
        }
    }

    /**
     * Test session configuration.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraDeviceTest"	"testCreateSessionWithParameters"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CameraDeviceTest.java"	""	"public void testCreateSessionWithParameters() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                openDevice(mCameraIdsUnderTest[i], mCameraMockListener);
                waitForDeviceState(STATE_OPENED, CAMERA_OPEN_TIMEOUT_MS);

                testCreateSessionWithParametersByCamera(mCameraIdsUnderTest[i], /*reprocessable*/false);
                testCreateSessionWithParametersByCamera(mCameraIdsUnderTest[i], /*reprocessable*/true);
            }
            finally {
                closeDevice(mCameraIdsUnderTest[i], mCameraMockListener);
            }
        }
    }

    /**
     * Verify creating a session with additional parameters works
     */
    private void testCreateSessionWithParametersByCamera(String cameraId, boolean reprocessable)
            throws Exception {
        final int SESSION_TIMEOUT_MS = 1000;
        final int CAPTURE_TIMEOUT_MS = 3000;
        int inputFormat = ImageFormat.YUV_420_888;
        int outputFormat = inputFormat;
        Size outputSize = mOrderedPreviewSizes.get(0);
        Size inputSize = outputSize;
        InputConfiguration inputConfig = null;

        if (VERBOSE) {
            Log.v(TAG, ""Testing creating session with parameters for camera "" + cameraId);
        }

        CameraCharacteristics characteristics = mCameraManager.getCameraCharacteristics(cameraId);
        StreamConfigurationMap config = characteristics.get(
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);

        if (reprocessable) {
            //Pick a supported i/o format and size combination.
            //Ideally the input format should match the output.
            boolean found = false;
            int inputFormats [] = config.getInputFormats();
            if (inputFormats.length == 0) {
                return;
            }

            for (int inFormat : inputFormats) {
                int outputFormats [] = config.getValidOutputFormatsForInput(inFormat);
                for (int outFormat : outputFormats) {
                    if (inFormat == outFormat) {
                        inputFormat = inFormat;
                        outputFormat = outFormat;
                        found = true;
                        break;
                    }
                }
                if (found) {
                    break;
                }
            }

            //In case the above combination doesn't exist, pick the first first supported
            //pair.
            if (!found) {
                inputFormat = inputFormats[0];
                int outputFormats [] = config.getValidOutputFormatsForInput(inputFormat);
                assertTrue(""No output formats supported for input format: "" + inputFormat,
                        (outputFormats.length > 0));
                outputFormat = outputFormats[0];
            }

            Size inputSizes[] = config.getInputSizes(inputFormat);
            Size outputSizes[] = config.getOutputSizes(outputFormat);
            assertTrue(""No valid sizes supported for input format: "" + inputFormat,
                    (inputSizes.length > 0));
            assertTrue(""No valid sizes supported for output format: "" + outputFormat,
                    (outputSizes.length > 0));

            inputSize = inputSizes[0];
            outputSize = outputSizes[0];
            inputConfig = new InputConfiguration(inputSize.getWidth(),
                    inputSize.getHeight(), inputFormat);
        } else {
            if (config.isOutputSupportedFor(outputFormat)) {
                outputSize = config.getOutputSizes(outputFormat)[0];
            } else {
                return;
            }
        }

        ImageReader imageReader = ImageReader.newInstance(outputSize.getWidth(),
                outputSize.getHeight(), outputFormat, /*maxImages*/1);

        try {
            mSessionMockListener = spy(new BlockingSessionCallback());
            mSessionWaiter = mSessionMockListener.getStateWaiter();
            List<OutputConfiguration> outputs = new ArrayList<>();
            outputs.add(new OutputConfiguration(imageReader.getSurface()));
            SessionConfiguration sessionConfig = new SessionConfiguration(
                    SessionConfiguration.SESSION_REGULAR, outputs,
                    new HandlerExecutor(mHandler), mSessionMockListener);

            CaptureRequest.Builder builder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
            builder.addTarget(imageReader.getSurface());
            CaptureRequest request = builder.build();

            sessionConfig.setInputConfiguration(inputConfig);
            sessionConfig.setSessionParameters(request);
            mCamera.createCaptureSession(sessionConfig);

            mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);

            // Verify we can capture a frame with the session.
            SimpleCaptureCallback captureListener = new SimpleCaptureCallback();
            SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
            imageReader.setOnImageAvailableListener(imageListener, mHandler);

            mSession.capture(request, captureListener, mHandler);
            captureListener.getCaptureResultForRequest(request, CAPTURE_TIMEOUT_MS);
            imageListener.getImage(CAPTURE_TIMEOUT_MS).close();
        } finally {
            imageReader.close();
            mSession.close();
        }
    }

    /**
     * Verify creating sessions back to back and only the last one is valid for
     * submitting requests.
     */
    private void testCreateSessionsByCamera(String cameraId) throws Exception {
        final int NUM_SESSIONS = 3;
        final int SESSION_TIMEOUT_MS = 1000;
        final int CAPTURE_TIMEOUT_MS = 3000;

        if (VERBOSE) {
            Log.v(TAG, ""Testing creating sessions for camera "" + cameraId);
        }

        Size yuvSize = getSortedSizesForFormat(cameraId, mCameraManager, ImageFormat.YUV_420_888,
                /*bound*/null).get(0);
        Size jpegSize = getSortedSizesForFormat(cameraId, mCameraManager, ImageFormat.JPEG,
                /*bound*/null).get(0);

        // Create a list of image readers. JPEG for last one and YUV for the rest.
        List<ImageReader> imageReaders = new ArrayList<>();
        List<CameraCaptureSession> allSessions = new ArrayList<>();

        try {
            for (int i = 0; i < NUM_SESSIONS - 1; i++) {
                imageReaders.add(ImageReader.newInstance(yuvSize.getWidth(), yuvSize.getHeight(),
                        ImageFormat.YUV_420_888, /*maxImages*/1));
            }
            imageReaders.add(ImageReader.newInstance(jpegSize.getWidth(), jpegSize.getHeight(),
                    ImageFormat.JPEG, /*maxImages*/1));

            // Create multiple sessions back to back.
            MultipleSessionCallback sessionListener =
                    new MultipleSessionCallback(/*failOnConfigureFailed*/true);
            for (int i = 0; i < NUM_SESSIONS; i++) {
                List<Surface> outputs = new ArrayList<>();
                outputs.add(imageReaders.get(i).getSurface());
                mCamera.createCaptureSession(outputs, sessionListener, mHandler);
            }

            // Verify we get onConfigured() for all sessions.
            allSessions = sessionListener.getAllSessions(NUM_SESSIONS,
                    SESSION_TIMEOUT_MS * NUM_SESSIONS);
            assertEquals(String.format(""Got %d sessions but configured %d sessions"",
                    allSessions.size(), NUM_SESSIONS), allSessions.size(), NUM_SESSIONS);

            // Verify all sessions except the last one are closed.
            for (int i = 0; i < NUM_SESSIONS - 1; i++) {
                sessionListener.waitForSessionClose(allSessions.get(i), SESSION_TIMEOUT_MS);
            }

            // Verify we can capture a frame with the last session.
            CameraCaptureSession session = allSessions.get(allSessions.size() - 1);
            SimpleCaptureCallback captureListener = new SimpleCaptureCallback();
            ImageReader reader = imageReaders.get(imageReaders.size() - 1);
            SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
            reader.setOnImageAvailableListener(imageListener, mHandler);

            CaptureRequest.Builder builder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
            builder.addTarget(reader.getSurface());
            CaptureRequest request = builder.build();

            session.capture(request, captureListener, mHandler);
            captureListener.getCaptureResultForRequest(request, CAPTURE_TIMEOUT_MS);
            imageListener.getImage(CAPTURE_TIMEOUT_MS).close();
        } finally {
            for (ImageReader reader : imageReaders) {
                reader.close();
            }
            for (CameraCaptureSession session : allSessions) {
                session.close();
            }
        }
    }

    private void prepareTestByCamera() throws Exception {
        final int PREPARE_TIMEOUT_MS = 10000;

        mSessionMockListener = spy(new BlockingSessionCallback());

        SurfaceTexture output1 = new SurfaceTexture(1);
        Surface output1Surface = new Surface(output1);
        SurfaceTexture output2 = new SurfaceTexture(2);
        Surface output2Surface = new Surface(output2);

        ArrayList<OutputConfiguration> outConfigs = new ArrayList<OutputConfiguration> ();
        outConfigs.add(new OutputConfiguration(output1Surface));
        outConfigs.add(new OutputConfiguration(output2Surface));
        SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_REGULAR, outConfigs,
                new HandlerExecutor(mHandler), mSessionMockListener);
        CaptureRequest.Builder r = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        sessionConfig.setSessionParameters(r.build());
        mCamera.createCaptureSession(sessionConfig);

        mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);

        // Try basic prepare

        mSession.prepare(output1Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(1))
                .onSurfacePrepared(eq(mSession), eq(output1Surface));

        // Should not complain if preparing already prepared stream

        mSession.prepare(output1Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(2))
                .onSurfacePrepared(eq(mSession), eq(output1Surface));

        // Check surface not included in session

        SurfaceTexture output3 = new SurfaceTexture(3);
        Surface output3Surface = new Surface(output3);
        try {
            mSession.prepare(output3Surface);
            // Legacy camera prepare always succeed
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                fail(""Preparing surface not part of session must throw IllegalArgumentException"");
            }
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Ensure second prepare also works

        mSession.prepare(output2Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(1))
                .onSurfacePrepared(eq(mSession), eq(output2Surface));

        // Use output1

        r = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        r.addTarget(output1Surface);

        mSession.capture(r.build(), null, null);

        try {
            mSession.prepare(output1Surface);
            // Legacy camera prepare always succeed
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                fail(""Preparing already-used surface must throw IllegalArgumentException"");
            }
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Create new session with outputs 1 and 3, ensure output1Surface still can't be prepared
        // again

        mSessionMockListener = spy(new BlockingSessionCallback());

        ArrayList<Surface> outputSurfaces = new ArrayList<Surface>(
            Arrays.asList(output1Surface, output3Surface));
        mCamera.createCaptureSession(outputSurfaces, mSessionMockListener, mHandler);

        mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);

        try {
            mSession.prepare(output1Surface);
            // Legacy camera prepare always succeed
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                fail(""Preparing surface used in previous session must throw "" +
                        ""IllegalArgumentException"");
            }
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Use output3, wait for result, then make sure prepare still doesn't work

        r = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        r.addTarget(output3Surface);

        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        mSession.capture(r.build(), resultListener, mHandler);

        resultListener.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);

        try {
            mSession.prepare(output3Surface);
            // Legacy camera prepare always succeed
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                fail(""Preparing already-used surface must throw IllegalArgumentException"");
            }
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Create new session with outputs 1 and 2, ensure output2Surface can be prepared again

        mSessionMockListener = spy(new BlockingSessionCallback());

        outputSurfaces = new ArrayList<>(
            Arrays.asList(output1Surface, output2Surface));
        mCamera.createCaptureSession(outputSurfaces, mSessionMockListener, mHandler);

        mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);

        mSession.prepare(output2Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(1))
                .onSurfacePrepared(eq(mSession), eq(output2Surface));

        try {
            mSession.prepare(output1Surface);
            // Legacy camera prepare always succeed
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                fail(""Preparing surface used in previous session must throw "" +
                        ""IllegalArgumentException"");
            }
        } catch (IllegalArgumentException e) {
            // expected
        }

        output1.release();
        output2.release();
        output3.release();
    }

    private void prepareTestForSharedSurfacesByCamera() throws Exception {
        final int PREPARE_TIMEOUT_MS = 10000;

        mSessionMockListener = spy(new BlockingSessionCallback());

        SurfaceTexture output1 = new SurfaceTexture(1);
        Surface output1Surface = new Surface(output1);
        SurfaceTexture output2 = new SurfaceTexture(2);
        Surface output2Surface = new Surface(output2);

        List<Surface> outputSurfaces = new ArrayList<>(
            Arrays.asList(output1Surface, output2Surface));
        OutputConfiguration surfaceSharedConfig = new OutputConfiguration(
            OutputConfiguration.SURFACE_GROUP_ID_NONE, output1Surface);
        surfaceSharedConfig.enableSurfaceSharing();
        surfaceSharedConfig.addSurface(output2Surface);

        List<OutputConfiguration> outputConfigurations = new ArrayList<>();
        outputConfigurations.add(surfaceSharedConfig);
        mCamera.createCaptureSessionByOutputConfigurations(
                outputConfigurations, mSessionMockListener, mHandler);

        mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);

        // Try prepare on output1Surface
        mSession.prepare(output1Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(1))
                .onSurfacePrepared(eq(mSession), eq(output1Surface));
        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(1))
                .onSurfacePrepared(eq(mSession), eq(output2Surface));

        // Try prepare on output2Surface
        mSession.prepare(output2Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(2))
                .onSurfacePrepared(eq(mSession), eq(output1Surface));
        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(2))
                .onSurfacePrepared(eq(mSession), eq(output2Surface));

        // Try prepare on output1Surface again
        mSession.prepare(output1Surface);

        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(3))
                .onSurfacePrepared(eq(mSession), eq(output1Surface));
        verify(mSessionMockListener, timeout(PREPARE_TIMEOUT_MS).times(3))
                .onSurfacePrepared(eq(mSession), eq(output2Surface));
    }

    private void invalidRequestCaptureTestByCamera() throws Exception {
        if (VERBOSE) Log.v(TAG, ""invalidRequestCaptureTestByCamera"");

        List<CaptureRequest> emptyRequests = new ArrayList<CaptureRequest>();
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest unConfiguredRequest = requestBuilder.build();
        List<CaptureRequest> unConfiguredRequests = new ArrayList<CaptureRequest>();
        unConfiguredRequests.add(unConfiguredRequest);

        try {
            // Test: CameraCaptureSession capture should throw IAE for null request.
            mSession.capture(/*request*/null, /*listener*/null, mHandler);
            mCollector.addMessage(
                    ""Session capture should throw IllegalArgumentException for null request"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession capture should throw IAE for request
            // without surface configured.
            mSession.capture(unConfiguredRequest, /*listener*/null, mHandler);
            mCollector.addMessage(""Session capture should throw "" +
                    ""IllegalArgumentException for request without surface configured"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession setRepeatingRequest should throw IAE for null request.
            mSession.setRepeatingRequest(/*request*/null, /*listener*/null, mHandler);
            mCollector.addMessage(""Session setRepeatingRequest should throw "" +
                    ""IllegalArgumentException for null request"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession setRepeatingRequest should throw IAE for for request
            // without surface configured.
            mSession.setRepeatingRequest(unConfiguredRequest, /*listener*/null, mHandler);
            mCollector.addMessage(""Capture zero burst should throw IllegalArgumentException "" +
                    ""for request without surface configured"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession captureBurst should throw IAE for null request list.
            mSession.captureBurst(/*requests*/null, /*listener*/null, mHandler);
            mCollector.addMessage(""Session captureBurst should throw "" +
                    ""IllegalArgumentException for null request list"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession captureBurst should throw IAE for empty request list.
            mSession.captureBurst(emptyRequests, /*listener*/null, mHandler);
            mCollector.addMessage(""Session captureBurst should throw "" +
                    "" IllegalArgumentException for empty request list"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession captureBurst should throw IAE for request
            // without surface configured.
            mSession.captureBurst(unConfiguredRequests, /*listener*/null, mHandler);
            fail(""Session captureBurst should throw IllegalArgumentException "" +
                    ""for null request list"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession setRepeatingBurst should throw IAE for null request list.
            mSession.setRepeatingBurst(/*requests*/null, /*listener*/null, mHandler);
            mCollector.addMessage(""Session setRepeatingBurst should throw "" +
                    ""IllegalArgumentException for null request list"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession setRepeatingBurst should throw IAE for empty request list.
            mSession.setRepeatingBurst(emptyRequests, /*listener*/null, mHandler);
            mCollector.addMessage(""Session setRepeatingBurst should throw "" +
                    ""IllegalArgumentException for empty request list"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }

        try {
            // Test: CameraCaptureSession setRepeatingBurst should throw IAE for request
            // without surface configured.
            mSession.setRepeatingBurst(unConfiguredRequests, /*listener*/null, mHandler);
            mCollector.addMessage(""Session setRepeatingBurst should throw "" +
                    ""IllegalArgumentException for request without surface configured"");
        } catch (IllegalArgumentException e) {
            // Pass.
        }
    }

    private class IsCaptureResultNotEmpty
            implements ArgumentMatcher<TotalCaptureResult> {
        @Override
        public boolean matches(TotalCaptureResult result) {
            /**
             * Do the simple verification here. Only verify the timestamp for now.
             * TODO: verify more required capture result metadata fields.
             */
            Long timeStamp = result.get(CaptureResult.SENSOR_TIMESTAMP);
            if (timeStamp != null && timeStamp.longValue() > 0L) {
                return true;
            }
            return false;
        }
    }

    /**
     * Run capture test with different test configurations.
     *
     * @param burst If the test uses {@link CameraCaptureSession#captureBurst} or
     * {@link CameraCaptureSession#setRepeatingBurst} to capture the burst.
     * @param repeating If the test uses {@link CameraCaptureSession#setRepeatingBurst} or
     * {@link CameraCaptureSession#setRepeatingRequest} for repeating capture.
     * @param abort If the test uses {@link CameraCaptureSession#abortCaptures} to stop the
     * repeating capture.  It has no effect if repeating is false.
     * @param useExecutor If the test uses {@link java.util.concurrent.Executor} instead of
     * {@link android.os.Handler} for callback invocation.
     */
    private void runCaptureTest(boolean burst, boolean repeating, boolean abort,
            boolean useExecutor) throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                openDevice(mCameraIdsUnderTest[i], mCameraMockListener);
                waitForDeviceState(STATE_OPENED, CAMERA_OPEN_TIMEOUT_MS);

                prepareCapture();

                if (!burst) {
                    // Test: that a single capture of each template type succeeds.
                    for (int j = 0; j < sTemplates.length; j++) {
                        // Skip video snapshots for LEGACY mode
                        if (mStaticInfo.isHardwareLevelLegacy() &&
                                sTemplates[j] == CameraDevice.TEMPLATE_VIDEO_SNAPSHOT) {
                            continue;
                        }
                        // Skip non-PREVIEW templates for non-color output
                        if (!mStaticInfo.isColorOutputSupported() &&
                                sTemplates[j] != CameraDevice.TEMPLATE_PREVIEW) {
                            continue;
                        }

                        captureSingleShot(mCameraIdsUnderTest[i], sTemplates[j], repeating, abort,
                                useExecutor);
                    }
                }
                else {
                    // Test: burst of one shot
                    captureBurstShot(mCameraIdsUnderTest[i], sTemplates, 1, repeating, abort, useExecutor);

                    int template = mStaticInfo.isColorOutputSupported() ?
                        CameraDevice.TEMPLATE_STILL_CAPTURE :
                        CameraDevice.TEMPLATE_PREVIEW;
                    int[] templates = new int[] {
                        template,
                        template,
                        template,
                        template,
                        template
                    };

                    // Test: burst of 5 shots of the same template type
                    captureBurstShot(mCameraIdsUnderTest[i], templates, templates.length, repeating, abort,
                            useExecutor);

                    if (mStaticInfo.isColorOutputSupported()) {
                        // Test: burst of 6 shots of different template types
                        captureBurstShot(mCameraIdsUnderTest[i], sTemplates, sTemplates.length, repeating,
                                abort, useExecutor);
                    }
                }
                verify(mCameraMockListener, never())
                        .onError(
                                any(CameraDevice.class),
                                anyInt());
            } catch (Exception e) {
                mCollector.addError(e);
            } finally {
                try {
                    closeSession();
                } catch (Exception e) {
                    mCollector.addError(e);
                }finally {
                    closeDevice(mCameraIdsUnderTest[i], mCameraMockListener);
                }
            }
        }
    }

    private void captureSingleShot(
            String id,
            int template,
            boolean repeating, boolean abort, boolean useExecutor) throws Exception {

        assertEquals(""Bad initial state for preparing to capture"",
                mLatestSessionState, SESSION_READY);

        final Executor executor = useExecutor ? new HandlerExecutor(mHandler) : null;
        CaptureRequest.Builder requestBuilder = mCamera.createCaptureRequest(template);
        assertNotNull(""Failed to create capture request"", requestBuilder);
        requestBuilder.addTarget(mReaderSurface);
        CameraCaptureSession.CaptureCallback mockCaptureCallback =
                mock(CameraCaptureSession.CaptureCallback.class);

        if (VERBOSE) {
            Log.v(TAG, String.format(""Capturing shot for device %s, template %d"",
                    id, template));
        }

        if (executor != null) {
            startCapture(requestBuilder.build(), repeating, mockCaptureCallback, executor);
        } else {
            startCapture(requestBuilder.build(), repeating, mockCaptureCallback, mHandler);
        }
        waitForSessionState(SESSION_ACTIVE, SESSION_ACTIVE_TIMEOUT_MS);

        int expectedCaptureResultCount = repeating ? REPEATING_CAPTURE_EXPECTED_RESULT_COUNT : 1;
        verifyCaptureResults(mockCaptureCallback, expectedCaptureResultCount);

        if (repeating) {
            if (abort) {
                mSession.abortCaptures();
                // Have to make sure abort and new requests aren't interleave together.
                waitForSessionState(SESSION_READY, SESSION_READY_TIMEOUT_MS);

                // Capture a single capture, and verify the result.
                SimpleCaptureCallback resultCallback = new SimpleCaptureCallback();
                CaptureRequest singleRequest = requestBuilder.build();
                if (executor != null) {
                    mSession.captureSingleRequest(singleRequest, executor, resultCallback);
                } else {
                    mSession.capture(singleRequest, resultCallback, mHandler);
                }
                resultCallback.getCaptureResultForRequest(singleRequest, CAPTURE_RESULT_TIMEOUT_MS);

                // Resume the repeating, and verify that results are returned.
                if (executor != null) {
                    mSession.setSingleRepeatingRequest(singleRequest, executor, resultCallback);
                } else {
                    mSession.setRepeatingRequest(singleRequest, resultCallback, mHandler);
                }
                for (int i = 0; i < REPEATING_CAPTURE_EXPECTED_RESULT_COUNT; i++) {
                    resultCallback.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);
                }
            }
            mSession.stopRepeating();
        }
        waitForSessionState(SESSION_READY, SESSION_READY_TIMEOUT_MS);
    }

    private void captureBurstShot(
            String id,
            int[] templates,
            int len,
            boolean repeating,
            boolean abort, boolean useExecutor) throws Exception {

        assertEquals(""Bad initial state for preparing to capture"",
                mLatestSessionState, SESSION_READY);

        assertTrue(""Invalid args to capture function"", len <= templates.length);
        List<CaptureRequest> requests = new ArrayList<CaptureRequest>();
        List<CaptureRequest> postAbortRequests = new ArrayList<CaptureRequest>();
        final Executor executor = useExecutor ? new HandlerExecutor(mHandler) : null;
        for (int i = 0; i < len; i++) {
            // Skip video snapshots for LEGACY mode
            if (mStaticInfo.isHardwareLevelLegacy() &&
                    templates[i] == CameraDevice.TEMPLATE_VIDEO_SNAPSHOT) {
                continue;
            }
            // Skip non-PREVIEW templates for non-color outpu
            if (!mStaticInfo.isColorOutputSupported() &&
                    templates[i] != CameraDevice.TEMPLATE_PREVIEW) {
                continue;
            }

            CaptureRequest.Builder requestBuilder = mCamera.createCaptureRequest(templates[i]);
            assertNotNull(""Failed to create capture request"", requestBuilder);
            requestBuilder.addTarget(mReaderSurface);
            requests.add(requestBuilder.build());
            if (abort) {
                postAbortRequests.add(requestBuilder.build());
            }
        }
        CameraCaptureSession.CaptureCallback mockCaptureCallback =
                mock(CameraCaptureSession.CaptureCallback.class);

        if (VERBOSE) {
            Log.v(TAG, String.format(""Capturing burst shot for device %s"", id));
        }

        if (!repeating) {
            if (executor != null) {
                mSession.captureBurstRequests(requests, executor, mockCaptureCallback);
            } else {
                mSession.captureBurst(requests, mockCaptureCallback, mHandler);
            }
        }
        else {
            if (executor != null) {
                mSession.setRepeatingBurstRequests(requests, executor, mockCaptureCallback);
            } else {
                mSession.setRepeatingBurst(requests, mockCaptureCallback, mHandler);
            }
        }
        waitForSessionState(SESSION_ACTIVE, SESSION_READY_TIMEOUT_MS);

        int expectedResultCount = requests.size();
        if (repeating) {
            expectedResultCount *= REPEATING_CAPTURE_EXPECTED_RESULT_COUNT;
        }

        verifyCaptureResults(mockCaptureCallback, expectedResultCount);

        if (repeating) {
            if (abort) {
                mSession.abortCaptures();
                // Have to make sure abort and new requests aren't interleave together.
                waitForSessionState(SESSION_READY, SESSION_READY_TIMEOUT_MS);

                // Capture a burst of captures, and verify the results.
                SimpleCaptureCallback resultCallback = new SimpleCaptureCallback();
                if (executor != null) {
                    mSession.captureBurstRequests(postAbortRequests, executor, resultCallback);
                } else {
                    mSession.captureBurst(postAbortRequests, resultCallback, mHandler);
                }
                // Verify that the results are returned.
                for (int i = 0; i < postAbortRequests.size(); i++) {
                    resultCallback.getCaptureResultForRequest(
                            postAbortRequests.get(i), CAPTURE_RESULT_TIMEOUT_MS);
                }

                // Resume the repeating, and verify that results are returned.
                if (executor != null) {
                    mSession.setRepeatingBurstRequests(requests, executor, resultCallback);
                } else {
                    mSession.setRepeatingBurst(requests, resultCallback, mHandler);
                }
                for (int i = 0; i < REPEATING_CAPTURE_EXPECTED_RESULT_COUNT; i++) {
                    resultCallback.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);
                }
            }
            mSession.stopRepeating();
        }
        waitForSessionState(SESSION_READY, SESSION_READY_TIMEOUT_MS);
    }

    /**
     * Precondition: Device must be in known OPENED state (has been waited for).
     *
     * <p>Creates a new capture session and waits until it is in the {@code SESSION_READY} state.
     * </p>
     *
     * <p>Any existing capture session will be closed as a result of calling this.</p>
     * */
    private void prepareCapture() throws Exception {
        if (VERBOSE) Log.v(TAG, ""prepareCapture"");

        assertTrue(""Bad initial state for preparing to capture"",
                mLatestDeviceState == STATE_OPENED);

        if (mSession != null) {
            if (VERBOSE) Log.v(TAG, ""prepareCapture - closing existing session"");
            closeSession();
        }

        // Create a new session listener each time, it's not reusable across cameras
        mSessionMockListener = spy(new BlockingSessionCallback());
        mSessionWaiter = mSessionMockListener.getStateWaiter();

        if (!mStaticInfo.isColorOutputSupported()) {
            createDefaultImageReader(getMaxDepthSize(mCamera.getId(), mCameraManager),
                    ImageFormat.DEPTH16, MAX_NUM_IMAGES, new ImageDropperListener());
        } else {
            createDefaultImageReader(DEFAULT_CAPTURE_SIZE, ImageFormat.YUV_420_888, MAX_NUM_IMAGES,
                    new ImageDropperListener());
        }

        List<Surface> outputSurfaces = new ArrayList<>(Arrays.asList(mReaderSurface));
        mCamera.createCaptureSession(outputSurfaces, mSessionMockListener, mHandler);

        mSession = mSessionMockListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        waitForSessionState(SESSION_CONFIGURED, SESSION_CONFIGURE_TIMEOUT_MS);
        waitForSessionState(SESSION_READY, SESSION_READY_TIMEOUT_MS);
    }

    private void waitForDeviceState(int state, long timeoutMs) {
        mCameraMockListener.waitForState(state, timeoutMs);
        mLatestDeviceState = state;
    }

    private void waitForSessionState(int state, long timeoutMs) {
        mSessionWaiter.waitForState(state, timeoutMs);
        mLatestSessionState = state;
    }

    private void verifyCaptureResults(
            CameraCaptureSession.CaptureCallback mockListener,
            int expectResultCount) {
        final int TIMEOUT_PER_RESULT_MS = 2000;
        // Should receive expected number of capture results.
        verify(mockListener,
                timeout(TIMEOUT_PER_RESULT_MS * expectResultCount).atLeast(expectResultCount))
                        .onCaptureCompleted(
                                eq(mSession),
                                isA(CaptureRequest.class),
                                argThat(new IsCaptureResultNotEmpty()));
        // Should not receive any capture failed callbacks.
        verify(mockListener, never())
                        .onCaptureFailed(
                                eq(mSession),
                                isA(CaptureRequest.class),
                                isA(CaptureFailure.class));
        // Should receive expected number of capture shutter calls
        verify(mockListener,
                atLeast(expectResultCount))
                        .onCaptureStarted(
                               eq(mSession),
                               isA(CaptureRequest.class),
                               anyLong(),
                               anyLong());
    }

    private void checkFpsRange(CaptureRequest.Builder request, int template,
            CameraCharacteristics props) {
        CaptureRequest.Key<Range<Integer>> fpsRangeKey = CONTROL_AE_TARGET_FPS_RANGE;
        Range<Integer> fpsRange;
        if ((fpsRange = mCollector.expectKeyValueNotNull(request, fpsRangeKey)) == null) {
            return;
        }

        int minFps = fpsRange.getLower();
        int maxFps = fpsRange.getUpper();
        Range<Integer>[] availableFpsRange = props
                .get(CameraCharacteristics.CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES);
        boolean foundRange = false;
        for (int i = 0; i < availableFpsRange.length; i += 1) {
            if (minFps == availableFpsRange[i].getLower()
                    && maxFps == availableFpsRange[i].getUpper()) {
                foundRange = true;
                break;
            }
        }
        if (!foundRange) {
            mCollector.addMessage(String.format(""Unable to find the fps range (%d, %d)"",
                    minFps, maxFps));
            return;
        }


        if (template != CameraDevice.TEMPLATE_MANUAL &&
                template != CameraDevice.TEMPLATE_STILL_CAPTURE) {
            if (maxFps < MIN_FPS_REQUIRED_FOR_STREAMING) {
                mCollector.addMessage(""Max fps should be at least ""
                        + MIN_FPS_REQUIRED_FOR_STREAMING);
                return;
            }

            // Relax framerate constraints on legacy mode
            if (mStaticInfo.isHardwareLevelAtLeastLimited()) {
                // Need give fixed frame rate for video recording template.
                if (template == CameraDevice.TEMPLATE_RECORD) {
                    if (maxFps != minFps) {
                        mCollector.addMessage(""Video recording frame rate should be fixed"");
                    }
                }
            }
        }
    }

    private void checkAfMode(CaptureRequest.Builder request, int template,
            CameraCharacteristics props) {
        boolean hasFocuser = props.getKeys().contains(CameraCharacteristics.
                LENS_INFO_MINIMUM_FOCUS_DISTANCE) &&
                (props.get(CameraCharacteristics.LENS_INFO_MINIMUM_FOCUS_DISTANCE) > 0f);

        if (!hasFocuser) {
            return;
        }

        int targetAfMode = CaptureRequest.CONTROL_AF_MODE_AUTO;
        int[] availableAfMode = props.get(CameraCharacteristics.CONTROL_AF_AVAILABLE_MODES);
        if (template == CameraDevice.TEMPLATE_PREVIEW ||
                template == CameraDevice.TEMPLATE_STILL_CAPTURE ||
                template == CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG) {
            // Default to CONTINUOUS_PICTURE if it is available, otherwise AUTO.
            for (int i = 0; i < availableAfMode.length; i++) {
                if (availableAfMode[i] == CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_PICTURE) {
                    targetAfMode = CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_PICTURE;
                    break;
                }
            }
        } else if (template == CameraDevice.TEMPLATE_RECORD ||
                template == CameraDevice.TEMPLATE_VIDEO_SNAPSHOT) {
            // Default to CONTINUOUS_VIDEO if it is available, otherwise AUTO.
            for (int i = 0; i < availableAfMode.length; i++) {
                if (availableAfMode[i] == CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_VIDEO) {
                    targetAfMode = CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_VIDEO;
                    break;
                }
            }
        } else if (template == CameraDevice.TEMPLATE_MANUAL) {
            targetAfMode = CaptureRequest.CONTROL_AF_MODE_OFF;
        }

        mCollector.expectKeyValueEquals(request, CONTROL_AF_MODE, targetAfMode);
        if (mStaticInfo.areKeysAvailable(CaptureRequest.LENS_FOCUS_DISTANCE)) {
            mCollector.expectKeyValueNotNull(request, LENS_FOCUS_DISTANCE);
        }
    }

    private void checkAntiBandingMode(CaptureRequest.Builder request, int template) {
        if (template == CameraDevice.TEMPLATE_MANUAL) {
            return;
        }

        if (!mStaticInfo.isColorOutputSupported()) return;

        List<Integer> availableAntiBandingModes =
                Arrays.asList(toObject(mStaticInfo.getAeAvailableAntiBandingModesChecked()));

        if (availableAntiBandingModes.contains(CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_AUTO)) {
            mCollector.expectKeyValueEquals(request, CONTROL_AE_ANTIBANDING_MODE,
                    CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_AUTO);
        } else {
            mCollector.expectKeyValueIsIn(request, CONTROL_AE_ANTIBANDING_MODE,
                    CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_50HZ,
                    CameraMetadata.CONTROL_AE_ANTIBANDING_MODE_60HZ);
        }
    }

    /**
     * <p>Check if 3A metering settings are ""up to HAL"" in request template</p>
     *
     * <p>This function doesn't fail the test immediately, it updates the
     * test pass/fail status and appends the failure message to the error collector each key.</p>
     *
     * @param regions The metering rectangles to be checked
     */
    private void checkMeteringRect(MeteringRectangle[] regions) {
        if (regions == null) {
            return;
        }
        mCollector.expectNotEquals(""Number of metering region should not be 0"", 0, regions.length);
        for (int i = 0; i < regions.length; i++) {
            mCollector.expectEquals(""Default metering regions should have all zero weight"",
                    0, regions[i].getMeteringWeight());
        }
    }

    /**
     * <p>Check if the request settings are suitable for a given request template.</p>
     *
     * <p>This function doesn't fail the test immediately, it updates the
     * test pass/fail status and appends the failure message to the error collector each key.</p>
     *
     * @param request The request to be checked.
     * @param template The capture template targeted by this request.
     * @param props The CameraCharacteristics this request is checked against with.
     */
    private void checkRequestForTemplate(CaptureRequest.Builder request, int template,
            CameraCharacteristics props) {
        Integer hwLevel = props.get(CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL);
        boolean isExternalCamera = (hwLevel ==
                CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL);

        // 3A settings--AE/AWB/AF.
        Integer maxRegionsAeVal = props.get(CameraCharacteristics.CONTROL_MAX_REGIONS_AE);
        int maxRegionsAe = maxRegionsAeVal != null ? maxRegionsAeVal : 0;
        Integer maxRegionsAwbVal = props.get(CameraCharacteristics.CONTROL_MAX_REGIONS_AWB);
        int maxRegionsAwb = maxRegionsAwbVal != null ? maxRegionsAwbVal : 0;
        Integer maxRegionsAfVal = props.get(CameraCharacteristics.CONTROL_MAX_REGIONS_AF);
        int maxRegionsAf = maxRegionsAfVal != null ? maxRegionsAfVal : 0;

        checkFpsRange(request, template, props);

        checkAfMode(request, template, props);
        checkAntiBandingMode(request, template);

        if (template == CameraDevice.TEMPLATE_MANUAL) {
            mCollector.expectKeyValueEquals(request, CONTROL_MODE, CaptureRequest.CONTROL_MODE_OFF);
            mCollector.expectKeyValueEquals(request, CONTROL_AE_MODE,
                    CaptureRequest.CONTROL_AE_MODE_OFF);
            mCollector.expectKeyValueEquals(request, CONTROL_AWB_MODE,
                    CaptureRequest.CONTROL_AWB_MODE_OFF);
        } else {
            mCollector.expectKeyValueEquals(request, CONTROL_MODE,
                    CaptureRequest.CONTROL_MODE_AUTO);
            if (mStaticInfo.isColorOutputSupported()) {
                mCollector.expectKeyValueEquals(request, CONTROL_AE_MODE,
                        CaptureRequest.CONTROL_AE_MODE_ON);
                mCollector.expectKeyValueEquals(request, CONTROL_AE_EXPOSURE_COMPENSATION, 0);
                mCollector.expectKeyValueEquals(request, CONTROL_AE_PRECAPTURE_TRIGGER,
                        CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_IDLE);
                // if AE lock is not supported, expect the control key to be non-exist or false
                if (mStaticInfo.isAeLockSupported() || request.get(CONTROL_AE_LOCK) != null) {
                    mCollector.expectKeyValueEquals(request, CONTROL_AE_LOCK, false);
                }

                mCollector.expectKeyValueEquals(request, CONTROL_AF_TRIGGER,
                        CaptureRequest.CONTROL_AF_TRIGGER_IDLE);

                mCollector.expectKeyValueEquals(request, CONTROL_AWB_MODE,
                        CaptureRequest.CONTROL_AWB_MODE_AUTO);
                // if AWB lock is not supported, expect the control key to be non-exist or false
                if (mStaticInfo.isAwbLockSupported() || request.get(CONTROL_AWB_LOCK) != null) {
                    mCollector.expectKeyValueEquals(request, CONTROL_AWB_LOCK, false);
                }

                // Check 3A regions.
                if (VERBOSE) {
                    Log.v(TAG, String.format(""maxRegions is: {AE: %s, AWB: %s, AF: %s}"",
                                    maxRegionsAe, maxRegionsAwb, maxRegionsAf));
                }
                if (maxRegionsAe > 0) {
                    mCollector.expectKeyValueNotNull(request, CONTROL_AE_REGIONS);
                    MeteringRectangle[] aeRegions = request.get(CONTROL_AE_REGIONS);
                    checkMeteringRect(aeRegions);
                }
                if (maxRegionsAwb > 0) {
                    mCollector.expectKeyValueNotNull(request, CONTROL_AWB_REGIONS);
                    MeteringRectangle[] awbRegions = request.get(CONTROL_AWB_REGIONS);
                    checkMeteringRect(awbRegions);
                }
                if (maxRegionsAf > 0) {
                    mCollector.expectKeyValueNotNull(request, CONTROL_AF_REGIONS);
                    MeteringRectangle[] afRegions = request.get(CONTROL_AF_REGIONS);
                    checkMeteringRect(afRegions);
                }
            }
        }

        // Sensor settings.

        mCollector.expectEquals(""Lens aperture must be present in request if available apertures "" +
                        ""are present in metadata, and vice-versa."",
                mStaticInfo.areKeysAvailable(CameraCharacteristics.LENS_INFO_AVAILABLE_APERTURES),
                mStaticInfo.areKeysAvailable(CaptureRequest.LENS_APERTURE));
        if (mStaticInfo.areKeysAvailable(CameraCharacteristics.LENS_INFO_AVAILABLE_APERTURES)) {
            float[] availableApertures =
                    props.get(CameraCharacteristics.LENS_INFO_AVAILABLE_APERTURES);
            if (availableApertures.length > 1) {
                mCollector.expectKeyValueNotNull(request, LENS_APERTURE);
            }
        }

        mCollector.expectEquals(""Lens filter density must be present in request if available "" +
                        ""filter densities are present in metadata, and vice-versa."",
                mStaticInfo.areKeysAvailable(CameraCharacteristics.
                        LENS_INFO_AVAILABLE_FILTER_DENSITIES),
                mStaticInfo.areKeysAvailable(CaptureRequest.LENS_FILTER_DENSITY));
        if (mStaticInfo.areKeysAvailable(CameraCharacteristics.
                LENS_INFO_AVAILABLE_FILTER_DENSITIES)) {
            float[] availableFilters =
                    props.get(CameraCharacteristics.LENS_INFO_AVAILABLE_FILTER_DENSITIES);
            if (availableFilters.length > 1) {
                mCollector.expectKeyValueNotNull(request, LENS_FILTER_DENSITY);
            }
        }


        if (!isExternalCamera) {
            float[] availableFocalLen =
                    props.get(CameraCharacteristics.LENS_INFO_AVAILABLE_FOCAL_LENGTHS);
            if (availableFocalLen.length > 1) {
                mCollector.expectKeyValueNotNull(request, LENS_FOCAL_LENGTH);
            }
        }


        mCollector.expectEquals(""Lens optical stabilization must be present in request if "" +
                        ""available optical stabilizations are present in metadata, and vice-versa."",
                mStaticInfo.areKeysAvailable(CameraCharacteristics.
                        LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION),
                mStaticInfo.areKeysAvailable(CaptureRequest.LENS_OPTICAL_STABILIZATION_MODE));
        if (mStaticInfo.areKeysAvailable(CameraCharacteristics.
                LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION)) {
            int[] availableOIS =
                    props.get(CameraCharacteristics.LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION);
            if (availableOIS.length > 1) {
                mCollector.expectKeyValueNotNull(request, LENS_OPTICAL_STABILIZATION_MODE);
            }
        }

        if (mStaticInfo.areKeysAvailable(SENSOR_TEST_PATTERN_MODE)) {
            mCollector.expectKeyValueEquals(request, SENSOR_TEST_PATTERN_MODE,
                    CaptureRequest.SENSOR_TEST_PATTERN_MODE_OFF);
        }

        if (mStaticInfo.areKeysAvailable(BLACK_LEVEL_LOCK)) {
            mCollector.expectKeyValueEquals(request, BLACK_LEVEL_LOCK, false);
        }

        if (mStaticInfo.areKeysAvailable(SENSOR_FRAME_DURATION)) {
            mCollector.expectKeyValueNotNull(request, SENSOR_FRAME_DURATION);
        }

        if (mStaticInfo.areKeysAvailable(SENSOR_EXPOSURE_TIME)) {
            mCollector.expectKeyValueNotNull(request, SENSOR_EXPOSURE_TIME);
        }

        if (mStaticInfo.areKeysAvailable(SENSOR_SENSITIVITY)) {
            mCollector.expectKeyValueNotNull(request, SENSOR_SENSITIVITY);
        }

        // ISP-processing settings.
        if (mStaticInfo.isColorOutputSupported()) {
            mCollector.expectKeyValueEquals(
                    request, STATISTICS_FACE_DETECT_MODE,
                    CaptureRequest.STATISTICS_FACE_DETECT_MODE_OFF);
            mCollector.expectKeyValueEquals(request, FLASH_MODE, CaptureRequest.FLASH_MODE_OFF);
        }

        List<Integer> availableCaps = mStaticInfo.getAvailableCapabilitiesChecked();
        if (mStaticInfo.areKeysAvailable(STATISTICS_LENS_SHADING_MAP_MODE)) {
            // If the device doesn't support RAW, all template should have OFF as default.
            if (!availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                mCollector.expectKeyValueEquals(
                        request, STATISTICS_LENS_SHADING_MAP_MODE,
                        CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE_OFF);
            }
        }

        boolean supportReprocessing =
                availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING) ||
                availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING);


        if (template == CameraDevice.TEMPLATE_STILL_CAPTURE) {

            // Ok with either FAST or HIGH_QUALITY
            if (mStaticInfo.areKeysAvailable(COLOR_CORRECTION_MODE)) {
                mCollector.expectKeyValueNotEquals(
                        request, COLOR_CORRECTION_MODE,
                        CaptureRequest.COLOR_CORRECTION_MODE_TRANSFORM_MATRIX);
            }

            // Edge enhancement, noise reduction and aberration correction modes.
            mCollector.expectEquals(""Edge mode must be present in request if "" +
                            ""available edge modes are present in metadata, and vice-versa."",
                    mStaticInfo.areKeysAvailable(CameraCharacteristics.
                            EDGE_AVAILABLE_EDGE_MODES),
                    mStaticInfo.areKeysAvailable(CaptureRequest.EDGE_MODE));
            if (mStaticInfo.areKeysAvailable(EDGE_MODE)) {
                List<Integer> availableEdgeModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableEdgeModesChecked()));
                // Don't need check fast as fast or high quality must be both present or both not.
                if (availableEdgeModes.contains(CaptureRequest.EDGE_MODE_HIGH_QUALITY)) {
                    mCollector.expectKeyValueEquals(request, EDGE_MODE,
                            CaptureRequest.EDGE_MODE_HIGH_QUALITY);
                } else {
                    mCollector.expectKeyValueEquals(request, EDGE_MODE,
                            CaptureRequest.EDGE_MODE_OFF);
                }
            }
            if (mStaticInfo.areKeysAvailable(SHADING_MODE)) {
                List<Integer> availableShadingModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableShadingModesChecked()));
                mCollector.expectKeyValueEquals(request, SHADING_MODE,
                        CaptureRequest.SHADING_MODE_HIGH_QUALITY);
            }

            mCollector.expectEquals(""Noise reduction mode must be present in request if "" +
                            ""available noise reductions are present in metadata, and vice-versa."",
                    mStaticInfo.areKeysAvailable(CameraCharacteristics.
                            NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES),
                    mStaticInfo.areKeysAvailable(CaptureRequest.NOISE_REDUCTION_MODE));
            if (mStaticInfo.areKeysAvailable(
                    CameraCharacteristics.NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES)) {
                List<Integer> availableNoiseReductionModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableNoiseReductionModesChecked()));
                // Don't need check fast as fast or high quality must be both present or both not.
                if (availableNoiseReductionModes
                        .contains(CaptureRequest.NOISE_REDUCTION_MODE_HIGH_QUALITY)) {
                    mCollector.expectKeyValueEquals(
                            request, NOISE_REDUCTION_MODE,
                            CaptureRequest.NOISE_REDUCTION_MODE_HIGH_QUALITY);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, NOISE_REDUCTION_MODE, CaptureRequest.NOISE_REDUCTION_MODE_OFF);
                }
            }

            mCollector.expectEquals(""Hot pixel mode must be present in request if "" +
                            ""available hot pixel modes are present in metadata, and vice-versa."",
                    mStaticInfo.areKeysAvailable(CameraCharacteristics.
                            HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES),
                    mStaticInfo.areKeysAvailable(CaptureRequest.HOT_PIXEL_MODE));

            if (mStaticInfo.areKeysAvailable(HOT_PIXEL_MODE)) {
                List<Integer> availableHotPixelModes =
                        Arrays.asList(toObject(
                                mStaticInfo.getAvailableHotPixelModesChecked()));
                if (availableHotPixelModes
                        .contains(CaptureRequest.HOT_PIXEL_MODE_HIGH_QUALITY)) {
                    mCollector.expectKeyValueEquals(
                            request, HOT_PIXEL_MODE,
                            CaptureRequest.HOT_PIXEL_MODE_HIGH_QUALITY);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, HOT_PIXEL_MODE, CaptureRequest.HOT_PIXEL_MODE_OFF);
                }
            }

            boolean supportAvailableAberrationModes = mStaticInfo.areKeysAvailable(
                    CameraCharacteristics.COLOR_CORRECTION_AVAILABLE_ABERRATION_MODES);
            boolean supportAberrationRequestKey = mStaticInfo.areKeysAvailable(
                    CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE);
            mCollector.expectEquals(""Aberration correction mode must be present in request if "" +
                    ""available aberration correction reductions are present in metadata, and ""
                    + ""vice-versa."", supportAvailableAberrationModes, supportAberrationRequestKey);
            if (supportAberrationRequestKey) {
                List<Integer> availableAberrationModes = Arrays.asList(
                        toObject(mStaticInfo.getAvailableColorAberrationModesChecked()));
                // Don't need check fast as fast or high quality must be both present or both not.
                if (availableAberrationModes
                        .contains(CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_HIGH_QUALITY)) {
                    mCollector.expectKeyValueEquals(
                            request, COLOR_CORRECTION_ABERRATION_MODE,
                            CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_HIGH_QUALITY);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, COLOR_CORRECTION_ABERRATION_MODE,
                            CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_OFF);
                }
            }
        } else if (template == CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG && supportReprocessing) {
            mCollector.expectKeyValueEquals(request, EDGE_MODE,
                    CaptureRequest.EDGE_MODE_ZERO_SHUTTER_LAG);
            mCollector.expectKeyValueEquals(request, NOISE_REDUCTION_MODE,
                    CaptureRequest.NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG);
        } else if (template == CameraDevice.TEMPLATE_PREVIEW ||
                template == CameraDevice.TEMPLATE_RECORD) {

            // Ok with either FAST or HIGH_QUALITY
            if (mStaticInfo.areKeysAvailable(COLOR_CORRECTION_MODE)) {
                mCollector.expectKeyValueNotEquals(
                        request, COLOR_CORRECTION_MODE,
                        CaptureRequest.COLOR_CORRECTION_MODE_TRANSFORM_MATRIX);
            }

            if (mStaticInfo.areKeysAvailable(EDGE_MODE)) {
                List<Integer> availableEdgeModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableEdgeModesChecked()));
                if (availableEdgeModes.contains(CaptureRequest.EDGE_MODE_FAST)) {
                    mCollector.expectKeyValueEquals(request, EDGE_MODE,
                            CaptureRequest.EDGE_MODE_FAST);
                } else {
                    mCollector.expectKeyValueEquals(request, EDGE_MODE,
                            CaptureRequest.EDGE_MODE_OFF);
                }
            }

            if (mStaticInfo.areKeysAvailable(SHADING_MODE)) {
                List<Integer> availableShadingModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableShadingModesChecked()));
                mCollector.expectKeyValueEquals(request, SHADING_MODE,
                        CaptureRequest.SHADING_MODE_FAST);
            }

            if (mStaticInfo.areKeysAvailable(NOISE_REDUCTION_MODE)) {
                List<Integer> availableNoiseReductionModes =
                        Arrays.asList(toObject(
                                mStaticInfo.getAvailableNoiseReductionModesChecked()));
                if (availableNoiseReductionModes
                        .contains(CaptureRequest.NOISE_REDUCTION_MODE_FAST)) {
                    mCollector.expectKeyValueEquals(
                            request, NOISE_REDUCTION_MODE,
                            CaptureRequest.NOISE_REDUCTION_MODE_FAST);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, NOISE_REDUCTION_MODE, CaptureRequest.NOISE_REDUCTION_MODE_OFF);
                }
            }

            if (mStaticInfo.areKeysAvailable(HOT_PIXEL_MODE)) {
                List<Integer> availableHotPixelModes =
                        Arrays.asList(toObject(
                                mStaticInfo.getAvailableHotPixelModesChecked()));
                if (availableHotPixelModes
                        .contains(CaptureRequest.HOT_PIXEL_MODE_FAST)) {
                    mCollector.expectKeyValueEquals(
                            request, HOT_PIXEL_MODE,
                            CaptureRequest.HOT_PIXEL_MODE_FAST);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, HOT_PIXEL_MODE, CaptureRequest.HOT_PIXEL_MODE_OFF);
                }
            }

            if (mStaticInfo.areKeysAvailable(COLOR_CORRECTION_ABERRATION_MODE)) {
                List<Integer> availableAberrationModes = Arrays.asList(
                        toObject(mStaticInfo.getAvailableColorAberrationModesChecked()));
                if (availableAberrationModes
                        .contains(CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_FAST)) {
                    mCollector.expectKeyValueEquals(
                            request, COLOR_CORRECTION_ABERRATION_MODE,
                            CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_FAST);
                } else {
                    mCollector.expectKeyValueEquals(
                            request, COLOR_CORRECTION_ABERRATION_MODE,
                            CaptureRequest.COLOR_CORRECTION_ABERRATION_MODE_OFF);
                }
            }
        } else {
            if (mStaticInfo.areKeysAvailable(EDGE_MODE)) {
                mCollector.expectKeyValueNotNull(request, EDGE_MODE);
            }

            if (mStaticInfo.areKeysAvailable(NOISE_REDUCTION_MODE)) {
                mCollector.expectKeyValueNotNull(request, NOISE_REDUCTION_MODE);
            }

            if (mStaticInfo.areKeysAvailable(COLOR_CORRECTION_ABERRATION_MODE)) {
                mCollector.expectKeyValueNotNull(request, COLOR_CORRECTION_ABERRATION_MODE);
            }
        }

        // Tone map and lens shading modes.
        if (template == CameraDevice.TEMPLATE_STILL_CAPTURE) {
            mCollector.expectEquals(""Tonemap mode must be present in request if "" +
                            ""available tonemap modes are present in metadata, and vice-versa."",
                    mStaticInfo.areKeysAvailable(CameraCharacteristics.
                            TONEMAP_AVAILABLE_TONE_MAP_MODES),
                    mStaticInfo.areKeysAvailable(CaptureRequest.TONEMAP_MODE));
            if (mStaticInfo.areKeysAvailable(
                    CameraCharacteristics.TONEMAP_AVAILABLE_TONE_MAP_MODES)) {
                List<Integer> availableToneMapModes =
                        Arrays.asList(toObject(mStaticInfo.getAvailableToneMapModesChecked()));
                if (availableToneMapModes.contains(CaptureRequest.TONEMAP_MODE_HIGH_QUALITY)) {
                    mCollector.expectKeyValueEquals(request, TONEMAP_MODE,
                            CaptureRequest.TONEMAP_MODE_HIGH_QUALITY);
                } else {
                    mCollector.expectKeyValueEquals(request, TONEMAP_MODE,
                            CaptureRequest.TONEMAP_MODE_FAST);
                }
            }

            // Still capture template should have android.statistics.lensShadingMapMode ON when
            // RAW capability is supported.
            if (mStaticInfo.areKeysAvailable(STATISTICS_LENS_SHADING_MAP_MODE) &&
                    availableCaps.contains(REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                    mCollector.expectKeyValueEquals(request, STATISTICS_LENS_SHADING_MAP_MODE,
                            STATISTICS_LENS_SHADING_MAP_MODE_ON);
            }
        } else {
            if (mStaticInfo.areKeysAvailable(TONEMAP_MODE)) {
                mCollector.expectKeyValueNotEquals(request, TONEMAP_MODE,
                        CaptureRequest.TONEMAP_MODE_CONTRAST_CURVE);
                mCollector.expectKeyValueNotEquals(request, TONEMAP_MODE,
                        CaptureRequest.TONEMAP_MODE_GAMMA_VALUE);
                mCollector.expectKeyValueNotEquals(request, TONEMAP_MODE,
                        CaptureRequest.TONEMAP_MODE_PRESET_CURVE);
            }
            if (mStaticInfo.areKeysAvailable(STATISTICS_LENS_SHADING_MAP_MODE)) {
                mCollector.expectKeyValueEquals(request, STATISTICS_LENS_SHADING_MAP_MODE,
                        CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE_OFF);
            }
            if (mStaticInfo.areKeysAvailable(STATISTICS_HOT_PIXEL_MAP_MODE)) {
                mCollector.expectKeyValueEquals(request, STATISTICS_HOT_PIXEL_MAP_MODE,
                        false);
            }
        }

        // Enable ZSL
        if (template != CameraDevice.TEMPLATE_STILL_CAPTURE) {
            if (mStaticInfo.areKeysAvailable(CONTROL_ENABLE_ZSL)) {
                    mCollector.expectKeyValueEquals(request, CONTROL_ENABLE_ZSL, false);
            }
        }

        int[] outputFormats = mStaticInfo.getAvailableFormats(
                StaticMetadata.StreamDirection.Output);
        boolean supportRaw = false;
        for (int format : outputFormats) {
            if (format == ImageFormat.RAW_SENSOR || format == ImageFormat.RAW10 ||
                    format == ImageFormat.RAW12 || format == ImageFormat.RAW_PRIVATE) {
                supportRaw = true;
                break;
            }
        }
        if (supportRaw) {
            mCollector.expectKeyValueEquals(request,
                    CONTROL_POST_RAW_SENSITIVITY_BOOST,
                    DEFAULT_POST_RAW_SENSITIVITY_BOOST);
        }

        switch(template) {
            case CameraDevice.TEMPLATE_PREVIEW:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_PREVIEW);
                break;
            case CameraDevice.TEMPLATE_STILL_CAPTURE:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_STILL_CAPTURE);
                break;
            case CameraDevice.TEMPLATE_RECORD:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_VIDEO_RECORD);
                break;
            case CameraDevice.TEMPLATE_VIDEO_SNAPSHOT:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_VIDEO_SNAPSHOT);
                break;
            case CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_ZERO_SHUTTER_LAG);
                break;
            case CameraDevice.TEMPLATE_MANUAL:
                mCollector.expectKeyValueEquals(request, CONTROL_CAPTURE_INTENT,
                        CameraCharacteristics.CONTROL_CAPTURE_INTENT_MANUAL);
                break;
            default:
                // Skip unknown templates here
        }

        // Check distortion correction mode
        if (mStaticInfo.isDistortionCorrectionSupported()) {
            mCollector.expectKeyValueNotEquals(request, DISTORTION_CORRECTION_MODE,
                    CaptureRequest.DISTORTION_CORRECTION_MODE_OFF);
        }

        // Scaler settings
        if (mStaticInfo.areKeysAvailable(
                CameraCharacteristics.SCALER_AVAILABLE_ROTATE_AND_CROP_MODES)) {
            List<Integer> rotateAndCropModes = Arrays.asList(toObject(
                props.get(CameraCharacteristics.SCALER_AVAILABLE_ROTATE_AND_CROP_MODES)));
            if (rotateAndCropModes.contains(SCALER_ROTATE_AND_CROP_AUTO)) {
                mCollector.expectKeyValueEquals(request, SCALER_ROTATE_AND_CROP,
                        CaptureRequest.SCALER_ROTATE_AND_CROP_AUTO);
            }
        }

        // Check JPEG quality
        if (mStaticInfo.isColorOutputSupported()) {
            mCollector.expectKeyValueNotNull(request, JPEG_QUALITY);
        }

        // TODO: use the list of keys from CameraCharacteristics to avoid expecting
        //       keys which are not available by this CameraDevice.
    }

    private void captureTemplateTestByCamera(String cameraId, int template) throws Exception {
        try {
            openDevice(cameraId, mCameraMockListener);

            assertTrue(""Camera template "" + template + "" is out of range!"",
                    template >= CameraDevice.TEMPLATE_PREVIEW
                            && template <= CameraDevice.TEMPLATE_MANUAL);

            mCollector.setCameraId(cameraId);

            try {
                CaptureRequest.Builder request = mCamera.createCaptureRequest(template);
                assertNotNull(""Failed to create capture request for template "" + template, request);

                CameraCharacteristics props = mStaticInfo.getCharacteristics();
                checkRequestForTemplate(request, template, props);
            } catch (IllegalArgumentException e) {
                if (template == CameraDevice.TEMPLATE_MANUAL &&
                        !mStaticInfo.isCapabilitySupported(CameraCharacteristics.
                                REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
                    // OK
                } else if (template == CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG &&
                        !mStaticInfo.isCapabilitySupported(CameraCharacteristics.
                                REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING)) {
                    // OK.
                } else if (sLegacySkipTemplates.contains(template) &&
                        mStaticInfo.isHardwareLevelLegacy()) {
                    // OK
                } else if (template != CameraDevice.TEMPLATE_PREVIEW &&
                        mStaticInfo.isDepthOutputSupported() &&
                        !mStaticInfo.isColorOutputSupported()) {
                    // OK, depth-only devices need only support PREVIEW template
                } else {
                    throw e; // rethrow
                }
            }
        }
        finally {
            try {
                closeSession();
            } finally {
                closeDevice(cameraId, mCameraMockListener);
            }
        }
    }

    /**
     * Start capture with given {@link #CaptureRequest}.
     *
     * @param request The {@link #CaptureRequest} to be captured.
     * @param repeating If the capture is single capture or repeating.
     * @param listener The {@link #CaptureCallback} camera device used to notify callbacks.
     * @param handler The handler camera device used to post callbacks.
     */
    @Override
    protected void startCapture(CaptureRequest request, boolean repeating,
            CameraCaptureSession.CaptureCallback listener, Handler handler)
                    throws CameraAccessException {
        if (VERBOSE) Log.v(TAG, ""Starting capture from session"");

        if (repeating) {
            mSession.setRepeatingRequest(request, listener, handler);
        } else {
            mSession.capture(request, listener, handler);
        }
    }

    /**
     * Start capture with given {@link #CaptureRequest}.
     *
     * @param request The {@link #CaptureRequest} to be captured.
     * @param repeating If the capture is single capture or repeating.
     * @param listener The {@link #CaptureCallback} camera device used to notify callbacks.
     * @param executor The executor used to invoke callbacks.
     */
    protected void startCapture(CaptureRequest request, boolean repeating,
            CameraCaptureSession.CaptureCallback listener, Executor executor)
                    throws CameraAccessException {
        if (VERBOSE) Log.v(TAG, ""Starting capture from session"");

        if (repeating) {
            mSession.setSingleRepeatingRequest(request, executor, listener);
        } else {
            mSession.captureSingleRequest(request, executor, listener);
        }
    }

    /**
     * Close a {@link #CameraCaptureSession capture session}; blocking until
     * the close finishes with a transition to {@link CameraCaptureSession.StateCallback#onClosed}.
     */
    protected void closeSession() {
        if (mSession == null) {
            return;
        }

        mSession.close();
        waitForSessionState(SESSION_CLOSED, SESSION_CLOSE_TIMEOUT_MS);
        mSession = null;

        mSessionMockListener = null;
        mSessionWaiter = null;
    }

    /**
     * A camera capture session listener that keeps all the configured and closed sessions.
     */
    private class MultipleSessionCallback extends CameraCaptureSession.StateCallback {
        public static final int SESSION_CONFIGURED = 0;
        public static final int SESSION_CLOSED = 1;

        final List<CameraCaptureSession> mSessions = new ArrayList<>();
        final Map<CameraCaptureSession, Integer> mSessionStates = new HashMap<>();
        CameraCaptureSession mCurrentConfiguredSession = null;

        final ReentrantLock mLock = new ReentrantLock();
        final Condition mNewStateCond = mLock.newCondition();

        final boolean mFailOnConfigureFailed;

        /**
         * If failOnConfigureFailed is true, it calls fail() when onConfigureFailed() is invoked
         * for any session.
         */
        public MultipleSessionCallback(boolean failOnConfigureFailed) {
            mFailOnConfigureFailed = failOnConfigureFailed;
        }

        @Override
        public void onClosed(CameraCaptureSession session) {
            mLock.lock();
            mSessionStates.put(session, SESSION_CLOSED);
            mNewStateCond.signal();
            mLock.unlock();
        }

        @Override
        public void onConfigured(CameraCaptureSession session) {
            mLock.lock();
            mSessions.add(session);
            mSessionStates.put(session, SESSION_CONFIGURED);
            mNewStateCond.signal();
            mLock.unlock();
        }

        @Override
        public void onConfigureFailed(CameraCaptureSession session) {
            if (mFailOnConfigureFailed) {
                fail(""Configuring a session failed"");
            }
        }

        /**
         * Get a number of sessions that have been configured.
         */
        public List<CameraCaptureSession> getAllSessions(int numSessions, int timeoutMs)
                throws Exception {
            long remainingTime = timeoutMs;
            mLock.lock();
            try {
                while (mSessions.size() < numSessions) {
                    long startTime = SystemClock.elapsedRealtime();
                    boolean ret = mNewStateCond.await(remainingTime, TimeUnit.MILLISECONDS);
                    remainingTime -= (SystemClock.elapsedRealtime() - startTime);
                    ret &= remainingTime > 0;

                    assertTrue(""Get "" + numSessions + "" sessions timed out after "" + timeoutMs +
                            ""ms"", ret);
                }

                return mSessions;
            } finally {
                mLock.unlock();
            }
        }

        /**
         * Wait until a previously-configured sessoin is closed or it times out.
         */
        public void waitForSessionClose(CameraCaptureSession session, int timeoutMs) throws Exception {
            long remainingTime = timeoutMs;
            mLock.lock();
            try {
                while (mSessionStates.get(session).equals(SESSION_CLOSED) == false) {
                    long startTime = SystemClock.elapsedRealtime();
                    boolean ret = mNewStateCond.await(remainingTime, TimeUnit.MILLISECONDS);
                    remainingTime -= (SystemClock.elapsedRealtime() - startTime);
                    ret &= remainingTime > 0;

                    assertTrue(""Wait for session close timed out after "" + timeoutMs + ""ms"", ret);
                }
            } finally {
                mLock.unlock();
            }
        }
    }

    /**
     * Verify audio restrictions are set properly for single CameraDevice usage
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.formats.CameraFormatsActivity"	"setPassFailButtonClickListeners"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/formats/CameraFormatsActivity.java"	""	"public void test/*
 *.
 */
package com.android.cts.verifier.camera.formats;

import com.android.cts.verifier.PassFailButtons;
import com.android.cts.verifier.R;

import android.app.AlertDialog;
import android.graphics.Bitmap;
import android.graphics.Color;
import android.graphics.ColorMatrix;
import android.graphics.ColorMatrixColorFilter;
import android.graphics.ImageFormat;
import android.graphics.Matrix;
import android.graphics.SurfaceTexture;
import android.hardware.Camera;
import android.hardware.Camera.CameraInfo;
import android.os.AsyncTask;
import android.os.Bundle;
import android.os.Handler;
import android.util.Log;
import android.util.SparseArray;
import android.view.Menu;
import android.view.MenuItem;
import android.view.View;
import android.view.Surface;
import android.view.TextureView;
import android.widget.AdapterView;
import android.widget.ArrayAdapter;
import android.widget.Button;
import android.widget.ImageButton;
import android.widget.ImageView;
import android.widget.Spinner;
import android.widget.Toast;

import java.io.IOException;
import java.lang.Math;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.Comparator;
import java.util.List;
import java.util.Optional;
import java.util.TreeSet;

/**
 * Tests for manual verification of the CDD-required camera output formats
 * for preview callbacks
 */
public class CameraFormatsActivity extends PassFailButtons.Activity
        implements TextureView.SurfaceTextureListener, Camera.PreviewCallback {

    private static final String TAG = ""CameraFormats"";

    private TextureView mPreviewView;
    private SurfaceTexture mPreviewTexture;
    private int mPreviewTexWidth;
    private int mPreviewTexHeight;
    private int mPreviewRotation;

    private ImageView mFormatView;

    private Spinner mCameraSpinner;
    private Spinner mFormatSpinner;
    private Spinner mResolutionSpinner;

    private int mCurrentCameraId = -1;
    private Camera mCamera;

    private List<Camera.Size> mPreviewSizes;
    private Camera.Size mNextPreviewSize;
    private Camera.Size mPreviewSize;
    private List<Integer> mPreviewFormats;
    private int mNextPreviewFormat;
    private int mPreviewFormat;
    private SparseArray<String> mPreviewFormatNames;

    private ColorMatrixColorFilter mYuv2RgbFilter;

    private Bitmap mCallbackBitmap;
    private int[] mRgbData;
    private int mRgbWidth;
    private int mRgbHeight;

    private static final int STATE_OFF = 0;
    private static final int STATE_PREVIEW = 1;
    private static final int STATE_NO_CALLBACKS = 2;
    private int mState = STATE_OFF;
    private boolean mProcessInProgress = false;
    private boolean mProcessingFirstFrame = false;

    private final TreeSet<CameraCombination> mTestedCombinations = new TreeSet<>(COMPARATOR);
    private final TreeSet<CameraCombination> mUntestedCombinations = new TreeSet<>(COMPARATOR);

    private int mAllCombinationsSize = 0;

    // Menu to show the test progress
    private static final int MENU_ID_PROGRESS = Menu.FIRST + 1;

    private class CameraCombination {
        private final int mCameraIndex;
        private final int mResolutionIndex;
        private final int mFormatIndex;
        private final int mResolutionWidth;
        private final int mResolutionHeight;
        private final String mFormatName;

        private CameraCombination(int cameraIndex, int resolutionIndex, int formatIndex,
            int resolutionWidth, int resolutionHeight, String formatName) {
            this.mCameraIndex = cameraIndex;
            this.mResolutionIndex = resolutionIndex;
            this.mFormatIndex = formatIndex;
            this.mResolutionWidth = resolutionWidth;
            this.mResolutionHeight = resolutionHeight;
            this.mFormatName = formatName;
        }

        @Override
        public String toString() {
            return String.format(""Camera %d, %dx%d, %s"",
                mCameraIndex, mResolutionWidth, mResolutionHeight, mFormatName);
        }
    }

    private static final Comparator<CameraCombination> COMPARATOR =
        Comparator.<CameraCombination, Integer>comparing(c -> c.mCameraIndex)
            .thenComparing(c -> c.mResolutionIndex)
            .thenComparing(c -> c.mFormatIndex);

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        setContentView(R.layout.cf_main);

        mAllCombinationsSize = calcAllCombinationsSize();

        // disable ""Pass"" button until all combinations are tested
        setPassButtonEnabled(false);

        setPassFailButtonClickListeners();
        setInfoResources(R.string.camera_format, R.string.cf_info, -1);

        mPreviewView = (TextureView) findViewById(R.id.preview_view);
        mFormatView = (ImageView) findViewById(R.id.format_view);

        mPreviewView.setSurfaceTextureListener(this);

        int numCameras = Camera.getNumberOfCameras();
        String[] cameraNames = new String[numCameras];
        for (int i = 0; i < numCameras; i++) {
            cameraNames[i] = ""Camera "" + i;
        }
        mCameraSpinner = (Spinner) findViewById(R.id.cameras_selection);
        mCameraSpinner.setAdapter(
            new ArrayAdapter<String>(
                this, R.layout.camera_list_item, cameraNames));
        mCameraSpinner.setOnItemSelectedListener(mCameraSpinnerListener);

        mFormatSpinner = (Spinner) findViewById(R.id.format_selection);
        mFormatSpinner.setOnItemSelectedListener(mFormatSelectedListener);

        mResolutionSpinner = (Spinner) findViewById(R.id.resolution_selection);
        mResolutionSpinner.setOnItemSelectedListener(mResolutionSelectedListener);

        // Must be kept in sync with android.graphics.ImageFormat manually
        mPreviewFormatNames = new SparseArray(7);
        mPreviewFormatNames.append(ImageFormat.JPEG, ""JPEG"");
        mPreviewFormatNames.append(ImageFormat.NV16, ""NV16"");
        mPreviewFormatNames.append(ImageFormat.NV21, ""NV21"");
        mPreviewFormatNames.append(ImageFormat.RGB_565, ""RGB_565"");
        mPreviewFormatNames.append(ImageFormat.UNKNOWN, ""UNKNOWN"");
        mPreviewFormatNames.append(ImageFormat.YUY2, ""YUY2"");
        mPreviewFormatNames.append(ImageFormat.YV12, ""YV12"");

        // Need YUV->RGB conversion in many cases

        ColorMatrix y2r = new ColorMatrix();
        y2r.setYUV2RGB();
        float[] yuvOffset = new float[] {
            1.f, 0.f, 0.f, 0.f, 0.f,
            0.f, 1.f, 0.f, 0.f, -128.f,
            0.f, 0.f, 1.f, 0.f, -128.f,
            0.f, 0.f, 0.f, 1.f, 0.f
        };

        ColorMatrix yOffset = new ColorMatrix(yuvOffset);

        ColorMatrix yTotal = new ColorMatrix();
        yTotal.setConcat(y2r, yOffset);

        mYuv2RgbFilter = new ColorMatrixColorFilter(yTotal);

        Button mNextButton = findViewById(R.id.next_button);
        mNextButton.setOnClickListener(v -> {
                setUntestedCombination();
                startPreview();
        });
    }

    /**
     * Set an untested combination of resolution and format for the current camera.
     * Triggered by next button click.
     */
    private void setUntestedCombination() {
        Optional<CameraCombination> combination = mUntestedCombinations.stream().filter(
            c -> c.mCameraIndex == mCurrentCameraId).findFirst();
        if (!combination.isPresent()) {
            Toast.makeText(this, ""All Camera "" + mCurrentCameraId + "" tests are done."",
                Toast.LENGTH_SHORT).show();
            return;
        }

        // There is untested combination for the current camera, set the next untested combination.
        int mNextResolutionIndex = combination.get().mResolutionIndex;
        int mNextFormatIndex = combination.get().mFormatIndex;

        mNextPreviewSize = mPreviewSizes.get(mNextResolutionIndex);
        mResolutionSpinner.setSelection(mNextResolutionIndex);
        mNextPreviewFormat = mPreviewFormats.get(mNextFormatIndex);
        mFormatSpinner.setSelection(mNextFormatIndex);
    }

    @Override
    public boolean onCreateOptionsMenu(Menu menu) {
        menu.add(Menu.NONE, MENU_ID_PROGRESS, Menu.NONE, ""Current Progress"");
        return super.onCreateOptionsMenu(menu);
    }

    @Override
    public boolean onOptionsItemSelected(MenuItem item) {
        boolean ret = true;
        switch (item.getItemId()) {
            case MENU_ID_PROGRESS:
                showCombinationsDialog();
                ret = true;
                break;
            default:
                ret = super.onOptionsItemSelected(item);
                break;
        }
        return ret;
    }

    private void showCombinationsDialog() {
        AlertDialog.Builder builder =
                new AlertDialog.Builder(CameraFormatsActivity.this);
        builder.setMessage(getTestDetails())
                .setTitle(""Current Progress"")
                .setPositiveButton(""OK"", null);
        builder.show();
    }

    @Override
    public void onResume() {
        super.onResume();

        setUpCamera(mCameraSpinner.getSelectedItemPosition());
    }

    @Override
    public void onPause() {
        super.onPause();

        shutdownCamera();
        mPreviewTexture = null;
    }

    @Override
    public String getTestDetails() {
        StringBuilder reportBuilder = new StringBuilder();
        reportBuilder.append(""Tested combinations:\n"");
        for (CameraCombination combination: mTestedCombinations) {
            reportBuilder.append(combination);
            reportBuilder.append(""\n"");
        }

        reportBuilder.append(""Untested combinations:\n"");
        for (CameraCombination combination: mUntestedCombinations) {
            reportBuilder.append(combination);
            reportBuilder.append(""\n"");
        }
        return reportBuilder.toString();
    }

    public void onSurfaceTextureAvailable(SurfaceTexture surface,
            int width, int height) {
        mPreviewTexture = surface;
        if (mFormatView.getMeasuredWidth() != width
                || mFormatView.getMeasuredHeight() != height) {
            mPreviewTexWidth = mFormatView.getMeasuredWidth();
            mPreviewTexHeight = mFormatView.getMeasuredHeight();
        } else {
            mPreviewTexWidth = width;
            mPreviewTexHeight = height;
        }

        if (mCamera != null) {
            startPreview();
        }
    }

    public void onSurfaceTextureSizeChanged(SurfaceTexture surface, int width, int height) {
        // Ignored, Camera does all the work for us
    }

    public boolean onSurfaceTextureDestroyed(SurfaceTexture surface) {
        return true;
    }

    public void onSurfaceTextureUpdated(SurfaceTexture surface) {
        // Invoked every time there's a new Camera preview frame
    }

    private AdapterView.OnItemSelectedListener mCameraSpinnerListener =
            new AdapterView.OnItemSelectedListener() {
                public void onItemSelected(AdapterView<?> parent,
                        View view, int pos, long id) {
                    if (mCurrentCameraId != pos) {
                        setUpCamera(pos);
                    }
                }

                public void onNothingSelected(AdapterView parent) {

                }

            };

    private AdapterView.OnItemSelectedListener mResolutionSelectedListener =
            new AdapterView.OnItemSelectedListener() {
                public void onItemSelected(AdapterView<?> parent,
                        View view, int position, long id) {
                    if (mPreviewSizes.get(position) != mPreviewSize) {
                        mNextPreviewSize = mPreviewSizes.get(position);
                        startPreview();
                    }
                }

                public void onNothingSelected(AdapterView parent) {

                }

            };


    private AdapterView.OnItemSelectedListener mFormatSelectedListener =
            new AdapterView.OnItemSelectedListener() {
                public void onItemSelected(AdapterView<?> parent,
                        View view, int position, long id) {
                    if (mPreviewFormats.get(position) != mNextPreviewFormat) {
                        mNextPreviewFormat = mPreviewFormats.get(position);
                        startPreview();
                    }
                }

                public void onNothingSelected(AdapterView parent) {

                }

            };

    private void setUpCamera(int id) {
        shutdownCamera();

        mCurrentCameraId = id;
        mCamera = Camera.open(id);
        Camera.Parameters p = mCamera.getParameters();

        // Get preview resolutions

        List<Camera.Size> unsortedSizes = p.getSupportedPreviewSizes();

        class SizeCompare implements Comparator<Camera.Size> {
            public int compare(Camera.Size lhs, Camera.Size rhs) {
                if (lhs.width < rhs.width) return -1;
                if (lhs.width > rhs.width) return 1;
                if (lhs.height < rhs.height) return -1;
                if (lhs.height > rhs.height) return 1;
                return 0;
            }
        }

        SizeCompare s = new SizeCompare();
        TreeSet<Camera.Size> sortedResolutions = new TreeSet<Camera.Size>(s);
        sortedResolutions.addAll(unsortedSizes);

        mPreviewSizes = new ArrayList<Camera.Size>(sortedResolutions);

        String[] availableSizeNames = new String[mPreviewSizes.size()];
        for (int i = 0; i < mPreviewSizes.size(); i++) {
            availableSizeNames[i] =
                    Integer.toString(mPreviewSizes.get(i).width) + "" x "" +
                    Integer.toString(mPreviewSizes.get(i).height);
        }
        mResolutionSpinner.setAdapter(
            new ArrayAdapter<String>(
                this, R.layout.camera_list_item, availableSizeNames));

        // Get preview formats, removing duplicates

        HashSet<Integer> formatSet = new HashSet<>(p.getSupportedPreviewFormats());
        mPreviewFormats = new ArrayList<Integer>(formatSet);

        String[] availableFormatNames = new String[mPreviewFormats.size()];
        for (int i = 0; i < mPreviewFormats.size(); i++) {
            availableFormatNames[i] =
                    mPreviewFormatNames.get(mPreviewFormats.get(i));
        }
        mFormatSpinner.setAdapter(
            new ArrayAdapter<String>(
                this, R.layout.camera_list_item, availableFormatNames));

        // Update untested entries

        for (int resolutionIndex = 0; resolutionIndex < mPreviewSizes.size(); resolutionIndex++) {
            for (int formatIndex = 0; formatIndex < mPreviewFormats.size(); formatIndex++) {
                CameraCombination combination = new CameraCombination(
                    id, resolutionIndex, formatIndex,
                    mPreviewSizes.get(resolutionIndex).width,
                    mPreviewSizes.get(resolutionIndex).height,
                    mPreviewFormatNames.get(mPreviewFormats.get(formatIndex)));

                if (!mTestedCombinations.contains(combination)) {
                    mUntestedCombinations.add(combination);
                }
            }
        }

        // Set initial values

        mNextPreviewSize = mPreviewSizes.get(0);
        mResolutionSpinner.setSelection(0);

        mNextPreviewFormat = mPreviewFormats.get(0);
        mFormatSpinner.setSelection(0);


        // Set up correct display orientation

        CameraInfo info =
            new CameraInfo();
        Camera.getCameraInfo(id, info);
        int rotation = getWindowManager().getDefaultDisplay().getRotation();
        int degrees = 0;
        switch (rotation) {
            case Surface.ROTATION_0: degrees = 0; break;
            case Surface.ROTATION_90: degrees = 90; break;
            case Surface.ROTATION_180: degrees = 180; break;
            case Surface.ROTATION_270: degrees = 270; break;
        }

        if (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {
            mPreviewRotation = (info.orientation + degrees) % 360;
            mPreviewRotation = (360 - mPreviewRotation) % 360;  // compensate the mirror
        } else {  // back-facing
            mPreviewRotation = (info.orientation - degrees + 360) % 360;
        }
        if (mPreviewRotation != 0 && mPreviewRotation != 180) {
            Log.w(TAG,
                ""Display orientation correction is not 0 or 180, as expected!"");
        }

        mCamera.setDisplayOrientation(mPreviewRotation);

        // Start up preview if display is ready

        if (mPreviewTexture != null) {
            startPreview();
        }

    }

    private void shutdownCamera() {
        if (mCamera != null) {
            mCamera.setPreviewCallback(null);
            mCamera.stopPreview();
            mCamera.release();
            mCamera = null;
            mState = STATE_OFF;
        }
    }

    private void startPreview() {
        if (mState != STATE_OFF) {
            // Stop for a while to drain callbacks
            mCamera.setPreviewCallback(null);
            mCamera.stopPreview();
            mState = STATE_OFF;
            Handler h = new Handler();
            Runnable mDelayedPreview = new Runnable() {
                public void run() {
                    startPreview();
                }
            };
            h.postDelayed(mDelayedPreview, 300);
            return;
        }
        mState = STATE_PREVIEW;

        Matrix transform = new Matrix();
        float widthRatio = mNextPreviewSize.width / (float)mPreviewTexWidth;
        float heightRatio = mNextPreviewSize.height / (float)mPreviewTexHeight;

        if (heightRatio < widthRatio) {
            transform.setScale(1, heightRatio/widthRatio);
            transform.postTranslate(0,
                mPreviewTexHeight * (1 - heightRatio/widthRatio)/2);
        } else {
            transform.setScale(widthRatio/heightRatio, 1);
            transform.postTranslate(mPreviewTexWidth * (1 - widthRatio/heightRatio)/2,
            0);
        }

        mPreviewView.setTransform(transform);

        mPreviewFormat = mNextPreviewFormat;
        mPreviewSize   = mNextPreviewSize;

        Camera.Parameters p = mCamera.getParameters();
        p.setPreviewFormat(mPreviewFormat);
        p.setPreviewSize(mPreviewSize.width, mPreviewSize.height);
        mCamera.setParameters(p);

        mCamera.setPreviewCallback(this);
        switch (mPreviewFormat) {
            case ImageFormat.NV16:
            case ImageFormat.NV21:
            case ImageFormat.YUY2:
            case ImageFormat.YV12:
                mFormatView.setColorFilter(mYuv2RgbFilter);
                break;
            default:
                mFormatView.setColorFilter(null);
                break;
        }

        // Filter out currently untestable formats
        switch (mPreviewFormat) {
            case ImageFormat.NV16:
            case ImageFormat.RGB_565:
            case ImageFormat.UNKNOWN:
            case ImageFormat.JPEG:
                AlertDialog.Builder builder =
                        new AlertDialog.Builder(CameraFormatsActivity.this);
                builder.setMessage(""Unsupported format "" +
                        mPreviewFormatNames.get(mPreviewFormat) +
                        ""; consider this combination as pass. "")
                        .setTitle(""Missing test"" )
                        .setNeutralButton(""Back"", null);
                builder.show();
                mState = STATE_NO_CALLBACKS;
                mCamera.setPreviewCallback(null);
                break;
            default:
                // supported
                break;
        }

        mProcessingFirstFrame = true;
        try {
            mCamera.setPreviewTexture(mPreviewTexture);
            mCamera.startPreview();
        } catch (IOException ioe) {
            // Something bad happened
            Log.e(TAG, ""Unable to start up preview"");
        }
    }

    private class ProcessPreviewDataTask extends AsyncTask<byte[], Void, Boolean> {
        protected Boolean doInBackground(byte[]... datas) {
            byte[] data = datas[0];
            try {
                if (mRgbData == null ||
                        mPreviewSize.width != mRgbWidth ||
                        mPreviewSize.height != mRgbHeight) {

                    mRgbData = new int[mPreviewSize.width * mPreviewSize.height * 4];
                    mRgbWidth = mPreviewSize.width;
                    mRgbHeight = mPreviewSize.height;
                }
                switch(mPreviewFormat) {
                    case ImageFormat.NV21:
                        convertFromNV21(data, mRgbData);
                        break;
                    case ImageFormat.YV12:
                        convertFromYV12(data, mRgbData);
                        break;
                    case ImageFormat.YUY2:
                        convertFromYUY2(data, mRgbData);
                        break;
                    case ImageFormat.NV16:
                    case ImageFormat.RGB_565:
                    case ImageFormat.UNKNOWN:
                    case ImageFormat.JPEG:
                    default:
                        convertFromUnknown(data, mRgbData);
                        break;
                }

                if (mCallbackBitmap == null ||
                        mRgbWidth != mCallbackBitmap.getWidth() ||
                        mRgbHeight != mCallbackBitmap.getHeight() ) {
                    mCallbackBitmap =
                            Bitmap.createBitmap(
                                mRgbWidth, mRgbHeight,
                                Bitmap.Config.ARGB_8888);
                }
                mCallbackBitmap.setPixels(mRgbData, 0, mRgbWidth,
                        0, 0, mRgbWidth, mRgbHeight);
            } catch (OutOfMemoryError o) {
                Log.e(TAG, ""Out of memory trying to process preview data"");
                return false;
            }
            return true;
        }

        protected void onPostExecute(Boolean result) {
            if (result) {
                mFormatView.setImageBitmap(mCallbackBitmap);
                if (mProcessingFirstFrame) {
                    mProcessingFirstFrame = false;

                    CameraCombination combination = new CameraCombination(
                        mCurrentCameraId,
                        mResolutionSpinner.getSelectedItemPosition(),
                        mFormatSpinner.getSelectedItemPosition(),
                        mPreviewSizes.get(mResolutionSpinner.getSelectedItemPosition()).width,
                        mPreviewSizes.get(mResolutionSpinner.getSelectedItemPosition()).height,
                        mPreviewFormatNames.get(
                            mPreviewFormats.get(mFormatSpinner.getSelectedItemPosition())));

                    mUntestedCombinations.remove(combination);
                    mTestedCombinations.add(combination);

                    displayToast(combination.toString());

                    if (mTestedCombinations.size() == mAllCombinationsSize) {
                        setPassButtonEnabled(true);
                    }
                }
            }
            mProcessInProgress = false;
        }

    }

    private void setPassButtonEnabled(boolean enabled) {
        ImageButton pass_button = (ImageButton) findViewById(R.id.pass_button);
        pass_button.setEnabled(enabled);
    }

    private int calcAllCombinationsSize() {
        int allCombinationsSize = 0;
        int numCameras = Camera.getNumberOfCameras();

        for (int i = 0; i<numCameras; i++) {
            // must release a Camera object before a new Camera object is created
            shutdownCamera();

            mCamera = Camera.open(i);
            Camera.Parameters p = mCamera.getParameters();

            HashSet<Integer> formatSet = new HashSet<>(p.getSupportedPreviewFormats());

            allCombinationsSize +=
                    p.getSupportedPreviewSizes().size() *   // resolutions
                    formatSet.size();  // unique formats
        }

        return allCombinationsSize;
    }

    private void displayToast(String combination) {
        Toast.makeText(this, ""\"""" + combination + ""\""\n"" + "" has been tested."", Toast.LENGTH_SHORT)
            .show();
    }

    public void onPreviewFrame(byte[] data, Camera camera) {
        if (mProcessInProgress || mState != STATE_PREVIEW) return;

        int expectedBytes;
        switch (mPreviewFormat) {
            case ImageFormat.YV12:
                // YV12 may have stride != width.
                int w = mPreviewSize.width;
                int h = mPreviewSize.height;
                int yStride = (int)Math.ceil(w / 16.0) * 16;
                int uvStride = (int)Math.ceil(yStride / 2 / 16.0) * 16;
                int ySize = yStride * h;
                int uvSize = uvStride * h / 2;
                expectedBytes = ySize + uvSize * 2;
                break;
            case ImageFormat.NV21:
            case ImageFormat.YUY2:
            default:
                expectedBytes = mPreviewSize.width * mPreviewSize.height *
                        ImageFormat.getBitsPerPixel(mPreviewFormat) / 8;
                break;
        }
        if (expectedBytes != data.length) {
            AlertDialog.Builder builder =
                    new AlertDialog.Builder(CameraFormatsActivity.this);
            builder.setMessage(""Mismatched size of buffer! Expected "" +
                    expectedBytes + "", but got "" +
                    data.length + "" bytes instead!"")
                    .setTitle(""Error trying to use format ""
                            + mPreviewFormatNames.get(mPreviewFormat))
                    .setNeutralButton(""Back"", null);

            builder.show();

            mState = STATE_NO_CALLBACKS;
            mCamera.setPreviewCallback(null);
            return;
        }

        mProcessInProgress = true;
        new ProcessPreviewDataTask().execute(data);
    }

    private void convertFromUnknown(byte[] data, int[] rgbData) {
        int w = mPreviewSize.width;
        int h = mPreviewSize.height;
        // RGBA output
        int rgbInc = 1;
        if (mPreviewRotation == 180) {
            rgbInc = -1;
        }
        int index = 0;
        for (int y = 0; y < h; y++) {
            int rgbIndex = y * w;
            if (mPreviewRotation == 180) {
                rgbIndex = w * (h - y) - 1;
            }
            for (int x = 0; x < mPreviewSize.width/3; x++) {
                int r = data[index + 0] & 0xFF;
                int g = data[index + 1] & 0xFF;
                int b = data[index + 2] & 0xFF;
                rgbData[rgbIndex] = Color.rgb(r,g,b);
                rgbIndex += rgbInc;
                index += 3;
            }
        }
    }

    // NV21 is a semi-planar 4:2:0 format, in the order YVU, which means we have:
    // a W x H-size 1-byte-per-pixel Y plane, then
    // a W/2 x H/2-size 2-byte-per-pixel plane, where each pixel has V then U.
    private void convertFromNV21(byte[] data, int rgbData[]) {
        int w = mPreviewSize.width;
        int h = mPreviewSize.height;
        // RGBA output
        int rgbIndex = 0;
        int rgbInc = 1;
        if (mPreviewRotation == 180) {
            rgbIndex = h * w - 1;
            rgbInc = -1;
        }
        int yIndex = 0;
        int uvRowIndex = w*h;
        int uvRowInc = 0;
        for (int y = 0; y < h; y++) {
            int uvInc = 0;
            int vIndex = uvRowIndex;
            int uIndex = uvRowIndex + 1;

            uvRowIndex += uvRowInc * w;
            uvRowInc = (uvRowInc + 1) & 0x1;

            for (int x = 0; x < w; x++) {
                int yv = data[yIndex] & 0xFF;
                int uv = data[uIndex] & 0xFF;
                int vv = data[vIndex] & 0xFF;
                rgbData[rgbIndex] =
                        Color.rgb(yv, uv, vv);

                rgbIndex += rgbInc;
                yIndex += 1;
                uIndex += uvInc;
                vIndex += uvInc;
                uvInc = (uvInc + 2) & 0x2;
            }
        }
    }

    // YV12 is a planar 4:2:0 format, in the order YVU, which means we have:
    // a W x H-size 1-byte-per-pixel Y plane, then
    // a W/2 x H/2-size 1-byte-per-pixel V plane, then
    // a W/2 x H/2-size 1-byte-per-pixel U plane
    // The stride may not be equal to width, since it has to be a multiple of
    // 16 pixels for both the Y and UV planes.
    private void convertFromYV12(byte[] data, int rgbData[]) {
        int w = mPreviewSize.width;
        int h = mPreviewSize.height;
        // RGBA output
        int rgbIndex = 0;
        int rgbInc = 1;
        if (mPreviewRotation == 180) {
            rgbIndex = h * w - 1;
            rgbInc = -1;
        }

        int yStride = (int)Math.ceil(w / 16.0) * 16;
        int uvStride = (int)Math.ceil(yStride/2/16.0) * 16;
        int ySize = yStride * h;
        int uvSize = uvStride * h / 2;

        int uRowIndex = ySize + uvSize;
        int vRowIndex = ySize;

        int uv_w = w/2;
        for (int y = 0; y < h; y++) {
            int yIndex = yStride * y;
            int uIndex = uRowIndex;
            int vIndex = vRowIndex;

            if ( (y & 0x1) == 1) {
                uRowIndex += uvStride;
                vRowIndex += uvStride;
            }

            int uv = 0, vv = 0;
            for (int x = 0; x < w; x++) {
                if ( (x & 0x1)  == 0) {
                    uv = data[uIndex] & 0xFF;
                    vv = data[vIndex] & 0xFF;
                    uIndex++;
                    vIndex++;
                }
                int yv = data[yIndex] & 0xFF;
                rgbData[rgbIndex] =
                        Color.rgb(yv, uv, vv);

                rgbIndex += rgbInc;
                yIndex += 1;
            }
        }
    }

    // YUY2 is an interleaved 4:2:2 format: YU,YV,YU,YV
    private void convertFromYUY2(byte[] data, int[] rgbData) {
        int w = mPreviewSize.width;
        int h = mPreviewSize.height;
        // RGBA output
        int yIndex = 0;
        int uIndex = 1;
        int vIndex = 3;
        int rgbIndex = 0;
        int rgbInc = 1;
        if (mPreviewRotation == 180) {
            rgbIndex = h * w - 1;
            rgbInc = -1;
        }

        for (int y = 0; y < h; y++) {
            for (int x = 0; x < w; x++) {
                int yv = data[yIndex] & 0xFF;
                int uv = data[uIndex] & 0xFF;
                int vv = data[vIndex] & 0xFF;
                rgbData[rgbIndex] = Color.rgb(yv,uv,vv);
                rgbIndex += rgbInc;
                yIndex += 2;
                if ( (x & 0x1) == 1 ) {
                    uIndex += 4;
                    vIndex += 4;
                }
            }
        }
    }

}"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionImageReaderTest"	"testMultiResolutionImageReaderJpeg"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionImageReaderTest.java"	""	"public void testMultiResolutionImageReaderJpeg() throws Exception {
        testMultiResolutionImageReaderForFormat(ImageFormat.JPEG, /*repeating*/false);
    }"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionImageReaderTest"	"testMultiResolutionImageReaderRepeatingJpeg"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionImageReaderTest.java"	""	"public void testMultiResolutionImageReaderRepeatingJpeg() throws Exception {
        testMultiResolutionImageReaderForFormat(ImageFormat.JPEG, /*repeating*/true);
    }"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.DngCreatorTest"	"testSingleImageThumbnail"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/DngCreatorTest.java"	""	"public void testSingleImageThumbnail() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            String deviceId = mCameraIdsUnderTest[i];
            List<ImageReader> captureReaders = new ArrayList<ImageReader>();
            List<CameraTestUtils.SimpleImageReaderListener> captureListeners =
                    new ArrayList<CameraTestUtils.SimpleImageReaderListener>();
            FileOutputStream fileStream = null;
            ByteArrayOutputStream outputStream = null;
            try {
                if (!mAllStaticInfo.get(deviceId).isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                    Log.i(TAG, ""RAW capability is not supported in camera "" + mCameraIdsUnderTest[i] +
                            "". Skip the test."");
                    continue;
                }

                openDevice(deviceId);
                Size activeArraySize = mStaticInfo.getRawDimensChecked();

                Size[] targetPreviewSizes =
                        mStaticInfo.getAvailableSizesForFormatChecked(ImageFormat.YUV_420_888,
                                StaticMetadata.StreamDirection.Output);
                // Get smallest preview size
                Size previewSize = mOrderedPreviewSizes.get(mOrderedPreviewSizes.size() - 1);

                // Create capture image reader
                CameraTestUtils.SimpleImageReaderListener captureListener
                        = new CameraTestUtils.SimpleImageReaderListener();
                captureReaders.add(createImageReader(activeArraySize, ImageFormat.RAW_SENSOR, 2,
                        captureListener));
                captureListeners.add(captureListener);

                CameraTestUtils.SimpleImageReaderListener previewListener
                        = new CameraTestUtils.SimpleImageReaderListener();

                captureReaders.add(createImageReader(previewSize, ImageFormat.YUV_420_888, 2,
                        previewListener));
                captureListeners.add(previewListener);

                Date beforeCaptureDate = new Date();
                Pair<List<Image>, CaptureResult> resultPair = captureSingleRawShot(activeArraySize,
                        captureReaders, /*waitForAe*/false, captureListeners);
                Date afterCaptureDate = new Date();
                CameraCharacteristics characteristics = mStaticInfo.getCharacteristics();

                if (VERBOSE) {
                    Log.v(TAG, ""Sensor timestamp (ms): "" +
                            resultPair.second.get(CaptureResult.SENSOR_TIMESTAMP) / 1000000);
                    Log.v(TAG, ""SystemClock.elapsedRealtimeNanos (ms): "" +
                            SystemClock.elapsedRealtimeNanos() / 1000000);
                    Log.v(TAG, ""SystemClock.uptimeMillis(): "" + SystemClock.uptimeMillis());
                }
                // Test simple writeImage, no header checks
                DngCreator dngCreator = new DngCreator(characteristics, resultPair.second);
                Location l = new Location(""test"");
                l.reset();
                l.setLatitude(GPS_LATITUDE);
                l.setLongitude(GPS_LONGITUDE);
                l.setTime(GPS_CALENDAR.getTimeInMillis());
                dngCreator.setLocation(l);

                dngCreator.setDescription(""helloworld"");
                dngCreator.setOrientation(ExifInterface.ORIENTATION_FLIP_VERTICAL);
                dngCreator.setThumbnail(resultPair.first.get(1));
                outputStream = new ByteArrayOutputStream();
                dngCreator.writeImage(outputStream, resultPair.first.get(0));

                String filePath = mDebugFileNameBase + ""/camera_thumb_"" + deviceId + ""_"" +
                        DEBUG_DNG_FILE;
                // Write out captured DNG file for the first camera device
                fileStream = new FileOutputStream(filePath);
                fileStream.write(outputStream.toByteArray());
                fileStream.flush();
                fileStream.close();
                if (VERBOSE) {
                    Log.v(TAG, ""Test DNG file for camera "" + deviceId + "" saved to "" + filePath);
                }

                assertTrue(""Generated DNG file does not pass validation"",
                        validateDngNative(outputStream.toByteArray()));

                ExifInterface exifInterface = new ExifInterface(filePath);
                // Verify GPS data.
                float[] latLong = new float[2];
                assertTrue(exifInterface.getLatLong(latLong));
                assertEquals(GPS_LATITUDE, latLong[0], GPS_DIFFERENCE_TOLERANCE);
                assertEquals(GPS_LONGITUDE, latLong[1], GPS_DIFFERENCE_TOLERANCE);
                assertEquals(GPS_DATESTAMP,
                        exifInterface.getAttribute(ExifInterface.TAG_GPS_DATESTAMP));
                assertEquals(GPS_TIMESTAMP,
                        exifInterface.getAttribute(ExifInterface.TAG_GPS_TIMESTAMP));

                // Verify the orientation.
                assertEquals(ExifInterface.ORIENTATION_FLIP_VERTICAL,
                        exifInterface.getAttributeInt(ExifInterface.TAG_ORIENTATION,
                                ExifInterface.ORIENTATION_UNDEFINED));

                // Verify the date/time
                final SimpleDateFormat dngDateTimeStampFormat =
                        new SimpleDateFormat(""yyyy:MM:dd HH:mm:ss"");
                dngDateTimeStampFormat.setLenient(false);

                String dateTimeString =
                        exifInterface.getAttribute(ExifInterface.TAG_DATETIME);
                assertTrue(dateTimeString != null);

                Date dateTime = dngDateTimeStampFormat.parse(dateTimeString);
                long captureTimeMs = dateTime.getTime();

                Log.i(TAG, ""DNG DateTime tag: "" + dateTimeString);
                Log.i(TAG, ""Before capture time: "" + beforeCaptureDate.getTime());
                Log.i(TAG, ""Capture time: "" + captureTimeMs);
                Log.i(TAG, ""After capture time: "" + afterCaptureDate.getTime());

                // Offset beforeCaptureTime by 1 second to account for rounding down of
                // DNG tag
                long beforeCaptureTimeMs = beforeCaptureDate.getTime() - 1000;
                long afterCaptureTimeMs = afterCaptureDate.getTime();
                assertTrue(captureTimeMs >= beforeCaptureTimeMs);
                assertTrue(captureTimeMs <= afterCaptureTimeMs);

                if (!VERBOSE) {
                    // Delete the captured DNG file.
                    File dngFile = new File(filePath);
                    assertTrue(dngFile.delete());
                }
            } finally {
                closeDevice(deviceId);
                for (ImageReader r : captureReaders) {
                    closeImageReader(r);
                }

                if (outputStream != null) {
                    outputStream.close();
                }

                if (fileStream != null) {
                    fileStream.close();
                }
            }
        }
    }

    /**
     * Test basic maximum resolution RAW capture, and ensure that the rendered RAW output is
     * similar to the maximum resolution JPEG created for a similar frame.
     *
     * Since mandatory streams for maximum resolution sensor pixel mode do not guarantee 2 maximum
     * resolution streams we can't capture RAW + JPEG images of the same frame. Therefore, 2
     * sessions are created, one for RAW capture and the other for JPEG capture.
     *
     * <p>
     * This test renders the RAW buffer into an RGB bitmap using a rendering pipeline
     * similar to one in the Adobe DNG validation tool.  JPEGs produced by the vendor hardware may
     * have different tonemapping and saturation applied than the RGB bitmaps produced
     * from this DNG rendering pipeline, and this test allows for fairly wide variations
     * between the histograms for the RAW and JPEG buffers to avoid false positives.
     * </p>
     *
     * <p>
     * To ensure more subtle errors in the colorspace transforms returned for the HAL's RAW
     * metadata, the DNGs and JPEGs produced here should also be manually compared using external
     * DNG rendering tools.  The DNG, rendered RGB bitmap, and JPEG buffer for this test can be
     * dumped to the SD card for further examination by enabling the 'verbose' mode for this test
     * using:
     * adb shell setprop log.tag.DngCreatorTest VERBOSE
     * </p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.DngCreatorTest"	"testRaw16JpegMaximumResolutionConsistency"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/DngCreatorTest.java"	""	"public void testRaw16JpegMaximumResolutionConsistency() throws Exception {
        for (String deviceId : mCameraIdsUnderTest) {
            ImageReader rawImageReader = null;
            ImageReader jpegImageReader = null;
            FileOutputStream fileStream = null;
            FileChannel fileChannel = null;
            try {
                // All ultra high resolution sensors must necessarily support RAW
                if (!mAllStaticInfo.get(deviceId).isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR)) {
                    Log.i(TAG, ""ULTRA_HIGH_RESOLUTION_SENSOR capability is not supported in "" +
                            "" camera "" + deviceId + "". Skip "" +
                            ""testRaw16JpegMaximumResolutionConsistency"");
                    continue;
                }

                CapturedDataMaximumResolution data =
                        captureRawJpegImagePairMaximumResolution(deviceId, rawImageReader,
                                jpegImageReader);
                if (data == null) {
                    continue;
                }
                Image raw = data.raw.first;
                Image jpeg = data.jpeg.first;

                Bitmap rawBitmap = Bitmap.createBitmap(raw.getWidth(), raw.getHeight(),
                        Bitmap.Config.ARGB_8888);

                byte[] rawPlane = new byte[raw.getPlanes()[0].getRowStride() * raw.getHeight()];

                // Render RAW image to a bitmap
                raw.getPlanes()[0].getBuffer().get(rawPlane);
                raw.getPlanes()[0].getBuffer().rewind();

                RawConverter.convertToSRGB(RenderScriptSingleton.getRS(), raw.getWidth(),
                        raw.getHeight(), raw.getPlanes()[0].getRowStride(), rawPlane,
                        data.characteristics, /*captureREsult*/data.raw.second, /*offsetX*/ 0,
                        /*offsetY*/ 0, /*out*/ rawBitmap);

                rawPlane = null;
                System.gc(); // Hint to VM

                if (VERBOSE) {
                    DngDebugParams params = new DngDebugParams();
                    params.deviceId = deviceId;
                    params.characteristics = data.characteristics;
                    params.captureResult = data.raw.second;
                    params.fileStream = fileStream;
                    params.raw = raw;
                    params.jpeg = jpeg;
                    params.fileChannel = fileChannel;
                    params.rawBitmap = rawBitmap;
                    params.intermediateStr = ""maximum_resolution_"";

                    debugDumpDng(params);
                }

                validateRawJpegImagePair(rawBitmap, jpeg, deviceId);
            } finally {
                closeImageReader(rawImageReader);
                closeImageReader(jpegImageReader);

                if (fileChannel != null) {
                    fileChannel.close();
                }
                if (fileStream != null) {
                    fileStream.close();
                }
            }
        }
    }



    /**
     * Test basic RAW capture, and ensure that the rendered RAW output is similar to the JPEG
     * created for the same frame.
     *
     * <p>
     * This test renders the RAW buffer into an RGB bitmap using a rendering pipeline
     * similar to one in the Adobe DNG validation tool.  JPEGs produced by the vendor hardware may
     * have different tonemapping and saturation applied than the RGB bitmaps produced
     * from this DNG rendering pipeline, and this test allows for fairly wide variations
     * between the histograms for the RAW and JPEG buffers to avoid false positives.
     * </p>
     *
     * <p>
     * To ensure more subtle errors in the colorspace transforms returned for the HAL's RAW
     * metadata, the DNGs and JPEGs produced here should also be manually compared using external
     * DNG rendering tools.  The DNG, rendered RGB bitmap, and JPEG buffer for this test can be
     * dumped to the SD card for further examination by enabling the 'verbose' mode for this test
     * using:
     * adb shell setprop log.tag.DngCreatorTest VERBOSE
     * </p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.DngCreatorTest"	"testRaw16JpegConsistency"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/DngCreatorTest.java"	""	"public void testRaw16JpegConsistency() throws Exception {
        for (String deviceId : mCameraIdsUnderTest) {
            List<ImageReader> captureReaders = new ArrayList<>();
            FileOutputStream fileStream = null;
            FileChannel fileChannel = null;
            try {
                CapturedData data = captureRawJpegImagePair(deviceId, captureReaders);
                if (data == null) {
                    continue;
                }
                Image raw = data.imagePair.first.get(0);
                Image jpeg = data.imagePair.first.get(1);

                Bitmap rawBitmap = Bitmap.createBitmap(raw.getWidth(), raw.getHeight(),
                        Bitmap.Config.ARGB_8888);

                byte[] rawPlane = new byte[raw.getPlanes()[0].getRowStride() * raw.getHeight()];

                // Render RAW image to a bitmap
                raw.getPlanes()[0].getBuffer().get(rawPlane);
                raw.getPlanes()[0].getBuffer().rewind();

                RawConverter.convertToSRGB(RenderScriptSingleton.getRS(), raw.getWidth(),
                        raw.getHeight(), raw.getPlanes()[0].getRowStride(), rawPlane,
                        data.characteristics, data.imagePair.second, /*offsetX*/ 0, /*offsetY*/ 0,
                        /*out*/ rawBitmap);

                rawPlane = null;
                System.gc(); // Hint to VM

                if (VERBOSE) {
                    DngDebugParams params = new DngDebugParams();
                    params.deviceId = deviceId;
                    params.characteristics = data.characteristics;
                    params.captureResult = data.imagePair.second;
                    params.fileStream = fileStream;
                    params.raw = raw;
                    params.jpeg = jpeg;
                    params.fileChannel = fileChannel;
                    params.rawBitmap = rawBitmap;
                    params.intermediateStr = """";

                    debugDumpDng(params);
                }

                validateRawJpegImagePair(rawBitmap, jpeg, deviceId);
            } finally {
                for (ImageReader r : captureReaders) {
                    closeImageReader(r);
                }

                if (fileChannel != null) {
                    fileChannel.close();
                }

                if (fileStream != null) {
                    fileStream.close();
                }
            }
        }
    }

    /**
     * Test basic DNG creation, ensure that the DNG image can be rendered by BitmapFactory.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.DngCreatorTest"	"testDngRenderingByBitmapFactor"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/DngCreatorTest.java"	""	"public void testDngRenderingByBitmapFactor() throws Exception {
        for (String deviceId : mCameraIdsUnderTest) {
            List<ImageReader> captureReaders = new ArrayList<>();

            CapturedData data = captureRawJpegImagePair(deviceId, captureReaders);
            if (data == null) {
                continue;
            }
            Image raw = data.imagePair.first.get(0);
            Image jpeg = data.imagePair.first.get(1);

            // Generate DNG file
            DngCreator dngCreator = new DngCreator(data.characteristics, data.imagePair.second);

            // Write DNG to file
            String dngFilePath = mDebugFileNameBase + ""/camera_"" +
                deviceId + ""_"" + TEST_DNG_FILE;

            // Write out captured DNG file for the first camera device if setprop is enabled
            try (FileOutputStream fileStream = new FileOutputStream(dngFilePath)) {
                dngCreator.writeImage(fileStream, raw);

                // Render the DNG file using BitmapFactory.
                Bitmap rawBitmap = BitmapFactory.decodeFile(dngFilePath);
                assertNotNull(rawBitmap);

                validateRawJpegImagePair(rawBitmap, jpeg, deviceId);
            } finally {
                for (ImageReader r : captureReaders) {
                    closeImageReader(r);
                }

                System.gc(); // Hint to VM
            }
        }
    }

    /*
     * Create RAW + JPEG image pair with characteristics info.
     */
    private CapturedData captureRawJpegImagePair(String deviceId, List<ImageReader> captureReaders)
            throws Exception {
        CapturedData data = new CapturedData();
        List<CameraTestUtils.SimpleImageReaderListener> captureListeners = new ArrayList<>();
        try {
            if (!mAllStaticInfo.get(deviceId).isCapabilitySupported(
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                Log.i(TAG, ""RAW capability is not supported in camera "" + deviceId
                        + "". Skip the test."");
                return null;
            }

            openDevice(deviceId);
            Size activeArraySize = mStaticInfo.getRawDimensChecked();

            // Get largest jpeg size
            Size[] targetJpegSizes = mStaticInfo.getAvailableSizesForFormatChecked(
                    ImageFormat.JPEG, StaticMetadata.StreamDirection.Output);

            Size largestJpegSize = Collections.max(Arrays.asList(targetJpegSizes),
                    new CameraTestUtils.SizeComparator());

            // Create raw image reader and capture listener
            CameraTestUtils.SimpleImageReaderListener rawListener =
                    new CameraTestUtils.SimpleImageReaderListener();
            captureReaders.add(createImageReader(activeArraySize, ImageFormat.RAW_SENSOR, 2,
                    rawListener));
            captureListeners.add(rawListener);


            // Create jpeg image reader and capture listener
            CameraTestUtils.SimpleImageReaderListener jpegListener =
                    new CameraTestUtils.SimpleImageReaderListener();
            captureReaders.add(createImageReader(largestJpegSize, ImageFormat.JPEG, 2,
                    jpegListener));
            captureListeners.add(jpegListener);

            data.imagePair = captureSingleRawShot(activeArraySize,
                    captureReaders, /*waitForAe*/ true, captureListeners);
            data.characteristics = mStaticInfo.getCharacteristics();

            Image raw = data.imagePair.first.get(0);
            Size rawBitmapSize = new Size(raw.getWidth(), raw.getHeight());
            assertTrue(""Raw bitmap size must be equal to either pre-correction active array"" +
                    "" size or pixel array size."", rawBitmapSize.equals(activeArraySize));

            return data;
        } finally {
            closeDevice(deviceId);
        }
    }

   private void debugDumpDng(DngDebugParams params) throws Exception {
        // Generate DNG file
        DngCreator dngCreator =
                new DngCreator(params.characteristics, params.captureResult);

        // Write DNG to file
        String dngFilePath = mDebugFileNameBase + ""/camera_"" + params.intermediateStr +
                params.deviceId + ""_"" + DEBUG_DNG_FILE;
        // Write out captured DNG file for the first camera device if setprop is enabled
        params.fileStream = new FileOutputStream(dngFilePath);
        dngCreator.writeImage(params.fileStream, params.raw);
        params.fileStream.flush();
        params.fileStream.close();
        Log.v(TAG, ""Test DNG file for camera "" + params.deviceId + "" saved to "" + dngFilePath);

        // Write JPEG to file
        String jpegFilePath = mDebugFileNameBase + ""/camera_"" + params.intermediateStr  +
                params.deviceId + ""_jpeg.jpg"";
        // Write out captured DNG file for the first camera device if setprop is enabled
        params.fileChannel = new FileOutputStream(jpegFilePath).getChannel();
        ByteBuffer jPlane = params.jpeg.getPlanes()[0].getBuffer();
        params.fileChannel.write(jPlane);
        params.fileChannel.close();
        jPlane.rewind();
        Log.v(TAG, ""Test JPEG file for camera "" + params.deviceId + "" saved to "" +
                jpegFilePath);

        // Write jpeg generated from demosaiced RAW frame to file
        String rawFilePath = mDebugFileNameBase + ""/camera_"" + params.intermediateStr +
                params.deviceId + ""_raw.jpg"";
        // Write out captured DNG file for the first camera device if setprop is enabled
        params.fileStream = new FileOutputStream(rawFilePath);
        params.rawBitmap.compress(Bitmap.CompressFormat.JPEG, 90, params.fileStream);
        params.fileStream.flush();
        params.fileStream.close();
        Log.v(TAG, ""Test converted RAW file for camera "" + params.deviceId + "" saved to "" +
                rawFilePath);
   }

    /*
     * Create RAW + JPEG image pair with characteristics info. Assumes the device supports the RAW
     * capability.
     */
    private CapturedDataMaximumResolution captureRawJpegImagePairMaximumResolution(String deviceId,
            ImageReader rawCaptureReader, ImageReader jpegCaptureReader)
            throws Exception {
        CapturedDataMaximumResolution data = new CapturedDataMaximumResolution();
        try {

            openDevice(deviceId);
            Size activeArraySize = mStaticInfo.getRawDimensChecked(/*maxResolution*/true);

            // Get largest jpeg size
            Size[] targetJpegSizes = mStaticInfo.getAvailableSizesForFormatChecked(
                    ImageFormat.JPEG, StaticMetadata.StreamDirection.Output, /*fastSizes*/ true,
                    /*slowSizes*/ true, /*maxResolution*/true);

            Size largestJpegSize = Collections.max(Arrays.asList(targetJpegSizes),
                    new CameraTestUtils.SizeComparator());

            // Create raw image reader and capture listener
            CameraTestUtils.SimpleImageReaderListener rawCaptureReaderListener =
                    new CameraTestUtils.SimpleImageReaderListener();
            rawCaptureReader = createImageReader(activeArraySize, ImageFormat.RAW_SENSOR, 2,
                    rawCaptureReaderListener);

            // Create jpeg image reader and capture listener
            CameraTestUtils.SimpleImageReaderListener jpegCaptureListener =
                    new CameraTestUtils.SimpleImageReaderListener();
            jpegCaptureReader = createImageReader(largestJpegSize, ImageFormat.JPEG, 2,
                    jpegCaptureListener);

            Pair<Image, CaptureResult> jpegResultPair =
                    captureSingleShotMaximumResolution(activeArraySize,
                             jpegCaptureReader, /*waitForAe*/true, jpegCaptureListener);
            data.jpeg = jpegResultPair;
            data.characteristics = mStaticInfo.getCharacteristics();
            // Create capture image reader
            CameraTestUtils.SimpleImageReaderListener outputRawCaptureReaderListener
                    = new CameraTestUtils.SimpleImageReaderListener();
            CameraTestUtils.SimpleImageReaderListener reprocessReaderListener
                    = new CameraTestUtils.SimpleImageReaderListener();

            ImageReader outputRawCaptureReader = createImageReader(activeArraySize,
                    ImageFormat.RAW_SENSOR, 2, outputRawCaptureReaderListener);
            Pair<Image, CaptureResult> rawResultPair = null;
            if (mAllStaticInfo.get(deviceId).isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_REMOSAIC_REPROCESSING)) {
                rawResultPair =
                        captureReprocessedRawShot(activeArraySize, outputRawCaptureReader,
                                    rawCaptureReader, outputRawCaptureReaderListener,
                                    reprocessReaderListener, /*waitForAe*/ true);
            } else {
                rawResultPair = captureSingleShotMaximumResolution(activeArraySize,
                        rawCaptureReader, /*waitForAe*/true, rawCaptureReaderListener);
            }
            data.raw = rawResultPair;
            Size rawBitmapSize =
                    new Size(rawResultPair.first.getWidth(), rawResultPair.first.getHeight());
            assertTrue(""Raw bitmap size must be equal to either pre-correction active array"" +
                    "" size or pixel array size."", rawBitmapSize.equals(activeArraySize));

            return data;
        } finally {
            closeDevice(deviceId);
        }
    }

    /*
     * Verify the image pair by comparing the center patch.
     */
    private void validateRawJpegImagePair(Bitmap rawBitmap, Image jpeg, String deviceId)
            throws Exception {
        // Decompress JPEG image to a bitmap
        byte[] compressedJpegData = CameraTestUtils.getDataFromImage(jpeg);

        // Get JPEG dimensions without decoding
        BitmapFactory.Options opt0 = new BitmapFactory.Options();
        opt0.inJustDecodeBounds = true;
        BitmapFactory.decodeByteArray(compressedJpegData, /*offset*/0,
                compressedJpegData.length, /*inout*/opt0);
        Rect jpegDimens = new Rect(0, 0, opt0.outWidth, opt0.outHeight);

        // Find square center patch from JPEG and RAW bitmaps
        RectF jpegRect = new RectF(jpegDimens);
        RectF rawRect = new RectF(0, 0, rawBitmap.getWidth(), rawBitmap.getHeight());
        int sideDimen = Math.min(Math.min(Math.min(Math.min(DEFAULT_PATCH_DIMEN,
                jpegDimens.width()), jpegDimens.height()), rawBitmap.getWidth()),
                rawBitmap.getHeight());

        RectF jpegIntermediate = new RectF(0, 0, sideDimen, sideDimen);
        jpegIntermediate.offset(jpegRect.centerX() - jpegIntermediate.centerX(),
                jpegRect.centerY() - jpegIntermediate.centerY());

        RectF rawIntermediate = new RectF(0, 0, sideDimen, sideDimen);
        rawIntermediate.offset(rawRect.centerX() - rawIntermediate.centerX(),
                rawRect.centerY() - rawIntermediate.centerY());
        Rect jpegFinal = new Rect();
        jpegIntermediate.roundOut(jpegFinal);
        Rect rawFinal = new Rect();
        rawIntermediate.roundOut(rawFinal);

        // Get RAW center patch, and free up rest of RAW image
        Bitmap rawPatch = Bitmap.createBitmap(rawBitmap, rawFinal.left, rawFinal.top,
                rawFinal.width(), rawFinal.height());
        rawBitmap.recycle();
        rawBitmap = null;
        System.gc(); // Hint to VM

        BitmapFactory.Options opt = new BitmapFactory.Options();
        opt.inPreferredConfig = Bitmap.Config.ARGB_8888;
        Bitmap jpegPatch = BitmapRegionDecoder.newInstance(compressedJpegData,
                /*offset*/0, compressedJpegData.length, /*isShareable*/true).
                decodeRegion(jpegFinal, opt);

        // Compare center patch from JPEG and rendered RAW bitmap
        double difference = BitmapUtils.calcDifferenceMetric(jpegPatch, rawPatch);
        if (difference > IMAGE_DIFFERENCE_TOLERANCE) {
            FileOutputStream fileStream = null;
            try {
                // Write JPEG patch to file
                String jpegFilePath = mDebugFileNameBase + ""/camera_"" + deviceId +
                        ""_jpeg_patch.jpg"";
                fileStream = new FileOutputStream(jpegFilePath);
                jpegPatch.compress(Bitmap.CompressFormat.JPEG, 90, fileStream);
                fileStream.flush();
                fileStream.close();
                Log.e(TAG, ""Failed JPEG patch file for camera "" + deviceId + "" saved to "" +
                        jpegFilePath);

                // Write RAW patch to file
                String rawFilePath = mDebugFileNameBase + ""/camera_"" + deviceId +
                        ""_raw_patch.jpg"";
                fileStream = new FileOutputStream(rawFilePath);
                rawPatch.compress(Bitmap.CompressFormat.JPEG, 90, fileStream);
                fileStream.flush();
                fileStream.close();
                Log.e(TAG, ""Failed RAW patch file for camera "" + deviceId + "" saved to "" +
                        rawFilePath);

                fail(""Camera "" + deviceId + "": RAW and JPEG image at  for the same "" +
                        ""frame are not similar, center patches have difference metric of "" +
                        difference);
            } finally {
                if (fileStream != null) {
                    fileStream.close();
                }
            }
        }
    }

    private Pair<Image, CaptureResult> captureSingleRawShot(Size s, boolean waitForAe,
            ImageReader captureReader,
            CameraTestUtils.SimpleImageReaderListener captureListener) throws Exception {
        List<ImageReader> readers = new ArrayList<ImageReader>();
        readers.add(captureReader);
        List<CameraTestUtils.SimpleImageReaderListener> listeners =
                new ArrayList<CameraTestUtils.SimpleImageReaderListener>();
        listeners.add(captureListener);
        Pair<List<Image>, CaptureResult> res = captureSingleRawShot(s, readers, waitForAe,
                listeners);
        return new Pair<Image, CaptureResult>(res.first.get(0), res.second);
    }

    private Pair<List<Image>, CaptureResult> captureSingleRawShot(Size s,
            List<ImageReader> captureReaders, boolean waitForAe,
            List<CameraTestUtils.SimpleImageReaderListener> captureListeners) throws Exception {
        return captureRawShots(s, captureReaders, waitForAe, captureListeners, 1,
                /*maxResolution*/false).get(0);
    }

    private Pair<Image, CaptureResult> captureSingleShotMaximumResolution(Size s,
            ImageReader captureReader, boolean waitForAe,
            CameraTestUtils.SimpleImageReaderListener captureListener)
            throws Exception {
        List<ImageReader> readers = new ArrayList<ImageReader>();
        readers.add(captureReader);
        List<CameraTestUtils.SimpleImageReaderListener> listeners =
                new ArrayList<CameraTestUtils.SimpleImageReaderListener>();
        listeners.add(captureListener);
        Pair<List<Image>, CaptureResult> res = captureRawShots(s, readers, waitForAe,
                listeners, /*numShots*/ 1, /*maxResolution*/ true).get(0);
        return new Pair<Image, CaptureResult>(res.first.get(0), res.second);
    }

    private Pair<Image, CaptureResult> captureReprocessedRawShot(Size sz,
            ImageReader inputReader,
            ImageReader reprocessOutputReader,
            CameraTestUtils.SimpleImageReaderListener inputReaderListener,
            CameraTestUtils.SimpleImageReaderListener reprocessReaderListener,
            boolean waitForAe) throws Exception {

        InputConfiguration inputConfig =
            new InputConfiguration(sz.getWidth(), sz.getHeight(), ImageFormat.RAW_SENSOR);
        CameraTestUtils.SimpleCaptureCallback inputCaptureListener =
                new CameraTestUtils.SimpleCaptureCallback();
        CameraTestUtils.SimpleCaptureCallback reprocessOutputCaptureListener =
                new CameraTestUtils.SimpleCaptureCallback();

        inputReader.setOnImageAvailableListener(inputReaderListener, mHandler);
        reprocessOutputReader.setOnImageAvailableListener(reprocessReaderListener, mHandler);

        ArrayList<Surface> outputSurfaces = new ArrayList<Surface>();
        outputSurfaces.add(inputReader.getSurface());
        outputSurfaces.add(reprocessOutputReader.getSurface());
        BlockingSessionCallback sessionListener = new BlockingSessionCallback();
        ImageReader previewReader = null;
        if (waitForAe) {
            // Also setup a small YUV output for AE metering if needed
            Size yuvSize = (mOrderedPreviewSizes.size() == 0) ? null :
                    mOrderedPreviewSizes.get(mOrderedPreviewSizes.size() - 1);
            assertNotNull(""Must support at least one small YUV size."", yuvSize);
            previewReader = createImageReader(yuvSize, ImageFormat.YUV_420_888,
                        /*maxNumImages*/2, new CameraTestUtils.ImageDropperListener());
            outputSurfaces.add(previewReader.getSurface());
        }

        createReprocessableSession(inputConfig, outputSurfaces);

        if (waitForAe) {
            CaptureRequest.Builder precaptureRequest =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
            assertNotNull(""Fail to get captureRequest"", precaptureRequest);
            precaptureRequest.addTarget(previewReader.getSurface());
            precaptureRequest.set(CaptureRequest.CONTROL_MODE,
                    CaptureRequest.CONTROL_MODE_AUTO);
            precaptureRequest.set(CaptureRequest.CONTROL_AE_MODE,
                    CaptureRequest.CONTROL_AE_MODE_ON);

            final ConditionVariable waitForAeCondition = new ConditionVariable(/*isOpen*/false);
            CameraCaptureSession.CaptureCallback captureCallback =
                    new CameraCaptureSession.CaptureCallback() {
                @Override
                public void onCaptureProgressed(CameraCaptureSession session,
                        CaptureRequest request, CaptureResult partialResult) {
                    Integer aeState = partialResult.get(CaptureResult.CONTROL_AE_STATE);
                    if (aeState != null &&
                            (aeState == CaptureRequest.CONTROL_AE_STATE_CONVERGED ||
                             aeState == CaptureRequest.CONTROL_AE_STATE_FLASH_REQUIRED)) {
                        waitForAeCondition.open();
                    }
                }

                @Override
                public void onCaptureCompleted(CameraCaptureSession session,
                        CaptureRequest request, TotalCaptureResult result) {
                    int aeState = result.get(CaptureResult.CONTROL_AE_STATE);
                    if (aeState == CaptureRequest.CONTROL_AE_STATE_CONVERGED ||
                            aeState == CaptureRequest.CONTROL_AE_STATE_FLASH_REQUIRED) {
                        waitForAeCondition.open();
                    }
                }
            };

            startCapture(precaptureRequest.build(), /*repeating*/true, captureCallback, mHandler);

            precaptureRequest.set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,
                    CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_START);
            startCapture(precaptureRequest.build(), /*repeating*/false, captureCallback, mHandler);
            assertTrue(""Timeout out waiting for AE to converge"",
                    waitForAeCondition.block(AE_TIMEOUT_MS));
        }
        ImageWriter inputWriter =
                ImageWriter.newInstance(mCameraSession.getInputSurface(), 1);
        // Prepare a request for reprocess input
        CaptureRequest.Builder builder = mCamera.createCaptureRequest(
                CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG);
        builder.addTarget(inputReader.getSurface());
        // This is a max resolution capture
        builder.set(CaptureRequest.SENSOR_PIXEL_MODE,
                CameraMetadata.SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION);
        CaptureRequest inputRequest = builder.build();
        mCameraSession.capture(inputRequest, inputCaptureListener, mHandler);
        List<CaptureRequest> reprocessCaptureRequests = new ArrayList<>();

        TotalCaptureResult inputResult =
                inputCaptureListener.getTotalCaptureResult(
                        MAX_RESOLUTION_CAPTURE_WAIT_TIMEOUT_MS);
        builder = mCamera.createReprocessCaptureRequest(inputResult);
        inputWriter.queueInputImage(inputReaderListener.getImage(
                        MAX_RESOLUTION_CAPTURE_WAIT_TIMEOUT_MS));
        builder.addTarget(reprocessOutputReader.getSurface());
        reprocessCaptureRequests.add(builder.build());
        mCameraSession.captureBurst(reprocessCaptureRequests, reprocessOutputCaptureListener,
                mHandler);
        TotalCaptureResult result = reprocessOutputCaptureListener.getTotalCaptureResult(
                CAPTURE_WAIT_TIMEOUT_MS);
        return new Pair<Image, CaptureResult>(reprocessReaderListener.getImage(
                MAX_RESOLUTION_CAPTURE_WAIT_TIMEOUT_MS), result);
    }

    /**
     * Capture raw images.
     *
     * <p>Capture raw images for a given size.</p>
     *
     * @param sz The size of the raw image to capture.  Must be one of the available sizes for this
     *          device.
     *
     * @param captureReaders The image readers which are associated with the targets for this
     *        capture.
     *
     * @param waitForAe Whether we should wait for AE to converge before capturing outputs for
     *                  the captureReaders targets
     *
     * @param captureListeners ImageReader listeners which wait on the captured images to be
     *                         available.
     *
     * @param numShots The number of shots to be captured
     *
     * @param maxResolution Whether the target in captureReaders are max resolution captures. If
     *                      this is set to true, captureReaders.size() must be == 1 ( in order to
     *                      satisfy mandatory streams for maximum resolution sensor pixel mode).
     *
     * @return a list of pairs containing a {@link Image} and {@link CaptureResult} used for
     *          each capture.
     */
    private List<Pair<List<Image>, CaptureResult>> captureRawShots(Size sz,
            List<ImageReader> captureReaders, boolean waitForAe,
            List<CameraTestUtils.SimpleImageReaderListener> captureListeners,
            int numShots, boolean maxResolution) throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""captureSingleRawShot - Capturing raw image."");
        }

        int timeoutScale = maxResolution ? MAX_RESOLUTION_CAPTURE_WAIT_TIMEOUT_SCALE : 1;
        Size[] targetCaptureSizes =
                mStaticInfo.getAvailableSizesForFormatChecked(ImageFormat.RAW_SENSOR,
                        StaticMetadata.StreamDirection.Output, /*fastSizes*/ true,
                        /*slowSizes*/ true, maxResolution);

        if (maxResolution) {
            assertTrue(""Maximum number of maximum resolution targets for a session should be 1 as"" +
                "" per the mandatory streams guarantee"", captureReaders.size() == 1);
        }

        // Validate size
        boolean validSize = false;
        for (int i = 0; i < targetCaptureSizes.length; ++i) {
            if (targetCaptureSizes[i].equals(sz)) {
                validSize = true;
                break;
            }
        }
        assertTrue(""Capture size is supported."", validSize);

        // Capture images.
        final List<Surface> outputSurfaces = new ArrayList<Surface>();
        for (ImageReader captureReader : captureReaders) {
            Surface captureSurface = captureReader.getSurface();
            outputSurfaces.add(captureSurface);
        }

        // Set up still capture template targeting JPEG/RAW outputs
        CaptureRequest.Builder request =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        assertNotNull(""Fail to get captureRequest"", request);
        for (Surface surface : outputSurfaces) {
            request.addTarget(surface);
        }

        ImageReader previewReader = null;
        if (waitForAe) {
            // Also setup a small YUV output for AE metering if needed
            Size yuvSize = (mOrderedPreviewSizes.size() == 0) ? null :
                    mOrderedPreviewSizes.get(mOrderedPreviewSizes.size() - 1);
            assertNotNull(""Must support at least one small YUV size."", yuvSize);
            previewReader = createImageReader(yuvSize, ImageFormat.YUV_420_888,
                        /*maxNumImages*/2, new CameraTestUtils.ImageDropperListener());
            outputSurfaces.add(previewReader.getSurface());
        }

        createSession(outputSurfaces);

        if (waitForAe) {
            CaptureRequest.Builder precaptureRequest =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
            assertNotNull(""Fail to get captureRequest"", precaptureRequest);
            precaptureRequest.addTarget(previewReader.getSurface());
            precaptureRequest.set(CaptureRequest.CONTROL_MODE,
                    CaptureRequest.CONTROL_MODE_AUTO);
            precaptureRequest.set(CaptureRequest.CONTROL_AE_MODE,
                    CaptureRequest.CONTROL_AE_MODE_ON);

            final ConditionVariable waitForAeCondition = new ConditionVariable(/*isOpen*/false);
            CameraCaptureSession.CaptureCallback captureCallback =
                    new CameraCaptureSession.CaptureCallback() {
                @Override
                public void onCaptureProgressed(CameraCaptureSession session,
                        CaptureRequest request, CaptureResult partialResult) {
                    Integer aeState = partialResult.get(CaptureResult.CONTROL_AE_STATE);
                    if (aeState != null &&
                            (aeState == CaptureRequest.CONTROL_AE_STATE_CONVERGED ||
                             aeState == CaptureRequest.CONTROL_AE_STATE_FLASH_REQUIRED)) {
                        waitForAeCondition.open();
                    }
                }

                @Override
                public void onCaptureCompleted(CameraCaptureSession session,
                        CaptureRequest request, TotalCaptureResult result) {
                    int aeState = result.get(CaptureResult.CONTROL_AE_STATE);
                    if (aeState == CaptureRequest.CONTROL_AE_STATE_CONVERGED ||
                            aeState == CaptureRequest.CONTROL_AE_STATE_FLASH_REQUIRED) {
                        waitForAeCondition.open();
                    }
                }
            };
            startCapture(precaptureRequest.build(), /*repeating*/true, captureCallback, mHandler);

            precaptureRequest.set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,
                    CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_START);
            startCapture(precaptureRequest.build(), /*repeating*/false, captureCallback, mHandler);
            assertTrue(""Timeout out waiting for AE to converge"",
                    waitForAeCondition.block(AE_TIMEOUT_MS));
        }

        request.set(CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE,
                CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE_ON);
        if (maxResolution) {
            request.set(CaptureRequest.SENSOR_PIXEL_MODE,
                    CaptureRequest.SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION);
        }
        CameraTestUtils.SimpleCaptureCallback resultListener =
                new CameraTestUtils.SimpleCaptureCallback();

        CaptureRequest request1 = request.build();
        for (int i = 0; i < numShots; i++) {
            startCapture(request1, /*repeating*/false, resultListener, mHandler);
        }
        List<Pair<List<Image>, CaptureResult>> ret = new ArrayList<>();
        for (int i = 0; i < numShots; i++) {
            // Verify capture result and images
            CaptureResult result = resultListener.getCaptureResult(CAPTURE_WAIT_TIMEOUT_MS);

            List<Image> resultImages = new ArrayList<Image>();
            for (CameraTestUtils.SimpleImageReaderListener captureListener : captureListeners) {
                Image captureImage =
                        captureListener.getImage(CAPTURE_WAIT_TIMEOUT_MS * timeoutScale);

            /*CameraTestUtils.validateImage(captureImage, s.getWidth(), s.getHeight(),
                    ImageFormat.RAW_SENSOR, null);*/
                resultImages.add(captureImage);
            }
            ret.add(new Pair<List<Image>, CaptureResult>(resultImages, result));
        }
        // Stop capture, delete the streams.
        stopCapture(/*fast*/false);

        return ret;
    }

    /**
     * Use the DNG SDK to validate a DNG file stored in the buffer.
     *
     * Returns false if the DNG has validation errors. Validation warnings/errors
     * will be printed to logcat.
     */
    private static native boolean validateDngNative(byte[] dngBuffer);
}"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.bokeh.CameraBokehActivity"	"setPassFailButtonClickListeners"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/bokeh/CameraBokehActivity.java"	""	"public void test/*
 *.
 */
package com.android.cts.verifier.camera.bokeh;

import com.android.cts.verifier.PassFailButtons;
import com.android.cts.verifier.R;

import com.android.ex.camera2.blocking.BlockingCameraManager;
import com.android.ex.camera2.blocking.BlockingCameraManager.BlockingOpenException;
import com.android.ex.camera2.blocking.BlockingStateCallback;
import com.android.ex.camera2.blocking.BlockingSessionCallback;

import android.app.AlertDialog;
import android.content.res.Configuration;
import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.graphics.Color;
import android.graphics.ColorFilter;
import android.graphics.ColorMatrixColorFilter;
import android.graphics.ImageFormat;
import android.graphics.Matrix;
import android.graphics.RectF;
import android.graphics.SurfaceTexture;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraCharacteristics.Key;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CameraMetadata;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.params.Capability;
import android.hardware.camera2.params.StreamConfigurationMap;
import android.hardware.camera2.TotalCaptureResult;
import android.media.Image;
import android.media.ImageReader;
import android.os.Bundle;
import android.os.Handler;
import android.os.HandlerThread;
import android.util.Log;
import android.util.Size;
import android.util.SparseArray;
import android.view.Menu;
import android.view.MenuItem;
import android.view.View;
import android.view.Surface;
import android.view.TextureView;
import android.widget.AdapterView;
import android.widget.ArrayAdapter;
import android.widget.Button;
import android.widget.ImageButton;
import android.widget.ImageView;
import android.widget.Spinner;
import android.widget.TextView;
import android.widget.Toast;
import android.content.Context;

import java.lang.Math;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Comparator;
import java.util.List;
import java.util.Optional;
import java.util.Set;
import java.util.TreeSet;

/**
 * Tests for manual verification of bokeh modes supported by the camera device.
 */
public class CameraBokehActivity extends PassFailButtons.Activity
        implements TextureView.SurfaceTextureListener,
                   ImageReader.OnImageAvailableListener {

    private static final String TAG = ""CameraBokehActivity"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);
    private static final int SESSION_READY_TIMEOUT_MS = 5000;
    private static final Size FULLHD = new Size(1920, 1080);
    private static final ColorMatrixColorFilter sJFIF_YUVToRGB_Filter =
            new ColorMatrixColorFilter(new float[] {
                        1f,        0f,    1.402f, 0f, -179.456f,
                        1f, -0.34414f, -0.71414f, 0f,   135.46f,
                        1f,    1.772f,        0f, 0f, -226.816f,
                        0f,        0f,        0f, 1f,        0f
                    });

    private TextureView mPreviewView;
    private SurfaceTexture mPreviewTexture;
    private Surface mPreviewSurface;
    private int mPreviewTexWidth, mPreviewTexHeight;

    private ImageView mImageView;
    private ColorFilter mCurrentColorFilter;

    private Spinner mCameraSpinner;
    private TextView mTestLabel;
    private TextView mPreviewLabel;
    private TextView mImageLabel;

    private CameraManager mCameraManager;
    private String[] mCameraIdList;
    private HandlerThread mCameraThread;
    private Handler mCameraHandler;
    private BlockingCameraManager mBlockingCameraManager;
    private CameraCharacteristics mCameraCharacteristics;
    private BlockingStateCallback mCameraListener;

    private BlockingSessionCallback mSessionListener;
    private CaptureRequest.Builder mPreviewRequestBuilder;
    private CaptureRequest mPreviewRequest;
    private CaptureRequest.Builder mStillCaptureRequestBuilder;
    private CaptureRequest mStillCaptureRequest;

    private HashMap<String, ArrayList<Capability>> mTestCases = new HashMap<>();
    private int mCurrentCameraIndex = -1;
    private String mCameraId;
    private CameraCaptureSession mCaptureSession;
    private CameraDevice mCameraDevice;

    SizeComparator mSizeComparator = new SizeComparator();

    private Size mPreviewSize;
    private Size mJpegSize;
    private ImageReader mJpegImageReader;
    private ImageReader mYuvImageReader;

    private SparseArray<String> mModeNames;

    private CameraCombination mNextCombination;
    private Size mMaxBokehStreamingSize;

    private Button mNextButton;

    private final TreeSet<CameraCombination> mTestedCombinations = new TreeSet<>(COMPARATOR);
    private final TreeSet<CameraCombination> mUntestedCombinations = new TreeSet<>(COMPARATOR);
    private final TreeSet<String> mUntestedCameras = new TreeSet<>();

    // Menu to show the test progress
    private static final int MENU_ID_PROGRESS = Menu.FIRST + 1;

    private class CameraCombination {
        private final int mCameraIndex;
        private final int mMode;
        private final Size mPreviewSize;
        private final boolean mIsStillCapture;
        private final String mCameraId;
        private final String mModeName;

        private CameraCombination(int cameraIndex, int mode,
                int streamingWidth, int streamingHeight,
                String cameraId, String modeName,
                boolean isStillCapture) {
            this.mCameraIndex = cameraIndex;
            this.mMode = mode;
            this.mPreviewSize = new Size(streamingWidth, streamingHeight);
            this.mCameraId = cameraId;
            this.mModeName = modeName;
            this.mIsStillCapture = isStillCapture;
        }

        @Override
        public String toString() {
            return String.format(""Camera %s, mode %s, intent %s"",
                    mCameraId, mModeName, mIsStillCapture ? ""PREVIEW"" : ""STILL_CAPTURE"");
        }
    }

    private static final Comparator<CameraCombination> COMPARATOR =
        Comparator.<CameraCombination, Integer>comparing(c -> c.mCameraIndex)
            .thenComparing(c -> c.mMode)
            .thenComparing(c -> c.mIsStillCapture);

    private CameraCaptureSession.CaptureCallback mCaptureCallback =
            new CameraCaptureSession.CaptureCallback() {
        @Override
        public void onCaptureProgressed(CameraCaptureSession session,
                                        CaptureRequest request,
                                        CaptureResult partialResult) {
            // Don't need to do anything here.
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session,
                                       CaptureRequest request,
                                       TotalCaptureResult result) {
            // Don't need to do anything here.
        }
    };

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        setContentView(R.layout.cb_main);

        setPassFailButtonClickListeners();

        mPreviewView = (TextureView) findViewById(R.id.preview_view);
        mImageView = (ImageView) findViewById(R.id.image_view);

        mPreviewView.setSurfaceTextureListener(this);

        mCameraManager = (CameraManager) getSystemService(Context.CAMERA_SERVICE);
        try {
            mCameraIdList = mCameraManager.getCameraIdList();
            for (String id : mCameraIdList) {
                CameraCharacteristics characteristics =
                        mCameraManager.getCameraCharacteristics(id);
                Key<Capability[]> key =
                        CameraCharacteristics.CONTROL_AVAILABLE_EXTENDED_SCENE_MODE_CAPABILITIES;
                Capability[] extendedSceneModeCaps = characteristics.get(key);

                if (extendedSceneModeCaps == null) {
                    continue;
                }

                ArrayList<Capability> nonOffModes = new ArrayList<>();
                for (Capability cap : extendedSceneModeCaps) {
                    int mode = cap.getMode();
                    if (mode == CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE ||
                            mode == CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_CONTINUOUS) {
                        nonOffModes.add(cap);
                    }
                }

                if (nonOffModes.size() > 0) {
                    mUntestedCameras.add(""All combinations for Camera "" + id);
                    mTestCases.put(id, nonOffModes);
                }

            }
        } catch (CameraAccessException e) {
            e.printStackTrace();
        }

        // If no supported bokeh modes, mark the test as pass
        if (mTestCases.size() == 0) {
            setInfoResources(R.string.camera_bokeh_test, R.string.camera_bokeh_no_support, -1);
            setPassButtonEnabled(true);
        } else {
            setInfoResources(R.string.camera_bokeh_test, R.string.camera_bokeh_test_info, -1);
            // disable ""Pass"" button until all combinations are tested
            setPassButtonEnabled(false);
        }

        Set<String> cameraIdSet = mTestCases.keySet();
        String[] cameraNames = new String[cameraIdSet.size()];
        int i = 0;
        for (String id : cameraIdSet) {
            cameraNames[i++] = ""Camera "" + id;
        }
        mCameraSpinner = (Spinner) findViewById(R.id.cameras_selection);
        mCameraSpinner.setAdapter(
            new ArrayAdapter<String>(
                this, R.layout.camera_list_item, cameraNames));
        mCameraSpinner.setOnItemSelectedListener(mCameraSpinnerListener);

        mTestLabel = (TextView) findViewById(R.id.test_label);
        mPreviewLabel = (TextView) findViewById(R.id.preview_label);
        mImageLabel = (TextView) findViewById(R.id.image_label);

        // Must be kept in sync with camera bokeh mode manually
        mModeNames = new SparseArray(2);
        mModeNames.append(
                CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE, ""STILL_CAPTURE"");
        mModeNames.append(
                CameraMetadata.CONTROL_EXTENDED_SCENE_MODE_BOKEH_CONTINUOUS, ""CONTINUOUS"");

        mNextButton = findViewById(R.id.next_button);
        mNextButton.setOnClickListener(v -> {
                if (mNextCombination != null) {
                    mUntestedCombinations.remove(mNextCombination);
                    mTestedCombinations.add(mNextCombination);
                }
                setUntestedCombination();

                if (mNextCombination != null) {
                    if (mNextCombination.mIsStillCapture) {
                        takePicture();
                    } else {
                        if (mCaptureSession != null) {
                            mCaptureSession.close();
                        }
                        startPreview();
                    }
                }
        });

        mBlockingCameraManager = new BlockingCameraManager(mCameraManager);
        mCameraListener = new BlockingStateCallback();
    }

    /**
     * Set an untested combination of resolution and bokeh mode for the current camera.
     * Triggered by next button click.
     */
    private void setUntestedCombination() {
        Optional<CameraCombination> combination = mUntestedCombinations.stream().filter(
            c -> c.mCameraIndex == mCurrentCameraIndex).findFirst();
        if (!combination.isPresent()) {
            Toast.makeText(this, ""All Camera "" + mCurrentCameraIndex + "" tests are done."",
                Toast.LENGTH_SHORT).show();
            mNextCombination = null;

            if (mUntestedCombinations.isEmpty() && mUntestedCameras.isEmpty()) {
                setPassButtonEnabled(true);
            }
            return;
        }

        // There is untested combination for the current camera, set the next untested combination.
        mNextCombination = combination.get();
        int nextMode = mNextCombination.mMode;
        ArrayList<Capability> bokehCaps = mTestCases.get(mCameraId);
        for (Capability cap : bokehCaps) {
            if (cap.getMode() == nextMode) {
                mMaxBokehStreamingSize = cap.getMaxStreamingSize();
            }
        }

        // Update bokeh mode and use case
        String testString = ""Mode: "" + mModeNames.get(mNextCombination.mMode);
        if (mNextCombination.mIsStillCapture) {
            testString += ""\nIntent: Capture"";
        } else {
            testString += ""\nIntent: Preview"";
        }
        testString += ""\n\nPress Next if the bokeh effect works as intended"";
        mTestLabel.setText(testString);

        // Update preview view and image view bokeh expectation
        boolean previewIsBokehCompatible =
                mSizeComparator.compare(mNextCombination.mPreviewSize, mMaxBokehStreamingSize) <= 0;
        String previewLabel = ""Normal preview"";
        if (previewIsBokehCompatible || mNextCombination.mIsStillCapture) {
            previewLabel += "" with bokeh"";
        }
        mPreviewLabel.setText(previewLabel);

        String imageLabel;
        if (mNextCombination.mIsStillCapture) {
            imageLabel = ""JPEG with bokeh"";
        } else {
            imageLabel = ""YUV"";
            if (previewIsBokehCompatible) {
                imageLabel += "" with bokeh"";
            }
        }
        mImageLabel.setText(imageLabel);
    }

    @Override
    public boolean onCreateOptionsMenu(Menu menu) {
        menu.add(Menu.NONE, MENU_ID_PROGRESS, Menu.NONE, ""Current Progress"");
        return super.onCreateOptionsMenu(menu);
    }

    @Override
    public boolean onOptionsItemSelected(MenuItem item) {
        boolean ret = true;
        switch (item.getItemId()) {
            case MENU_ID_PROGRESS:
                showCombinationsDialog();
                ret = true;
                break;
            default:
                ret = super.onOptionsItemSelected(item);
                break;
        }
        return ret;
    }

    private void showCombinationsDialog() {
        AlertDialog.Builder builder =
                new AlertDialog.Builder(CameraBokehActivity.this);
        builder.setMessage(getTestDetails())
                .setTitle(""Current Progress"")
                .setPositiveButton(""OK"", null);
        builder.show();
    }

    @Override
    public void onResume() {
        super.onResume();

        startBackgroundThread();

        int cameraIndex = mCameraSpinner.getSelectedItemPosition();
        if (cameraIndex >= 0) {
            setUpCamera(mCameraSpinner.getSelectedItemPosition());
        }
    }

    @Override
    public void onPause() {
        shutdownCamera();
        stopBackgroundThread();

        super.onPause();
    }

    @Override
    public String getTestDetails() {
        StringBuilder reportBuilder = new StringBuilder();
        reportBuilder.append(""Tested combinations:\n"");
        for (CameraCombination combination: mTestedCombinations) {
            reportBuilder.append(combination);
            reportBuilder.append(""\n"");
        }

        reportBuilder.append(""Untested cameras:\n"");
        for (String untestedCamera : mUntestedCameras) {
            reportBuilder.append(untestedCamera);
            reportBuilder.append(""\n"");
        }
        reportBuilder.append(""Untested combinations:\n"");
        for (CameraCombination combination: mUntestedCombinations) {
            reportBuilder.append(combination);
            reportBuilder.append(""\n"");
        }
        return reportBuilder.toString();
    }

    @Override
    public void onSurfaceTextureAvailable(SurfaceTexture surfaceTexture,
            int width, int height) {
        mPreviewTexture = surfaceTexture;
        mPreviewTexWidth = width;
        mPreviewTexHeight = height;

        mPreviewSurface = new Surface(mPreviewTexture);

        if (mCameraDevice != null) {
            startPreview();
        }
    }

    @Override
    public void onSurfaceTextureSizeChanged(SurfaceTexture surface, int width, int height) {
        // Ignored, Camera does all the work for us
        if (VERBOSE) {
            Log.v(TAG, ""onSurfaceTextureSizeChanged: "" + width + "" x "" + height);
        }
    }

    @Override
    public boolean onSurfaceTextureDestroyed(SurfaceTexture surface) {
        mPreviewTexture = null;
        return true;
    }

    @Override
    public void onSurfaceTextureUpdated(SurfaceTexture surface) {
        // Invoked every time there's a new Camera preview frame
    }

    @Override
    public void onImageAvailable(ImageReader reader) {
        Image img = null;
        try {
            img = reader.acquireNextImage();
            if (img == null) {
                Log.d(TAG, ""Invalid image!"");
                return;
            }
            final int format = img.getFormat();

            Size configuredSize = (format == ImageFormat.YUV_420_888 ? mPreviewSize : mJpegSize);
            Bitmap imgBitmap = null;
            if (format == ImageFormat.YUV_420_888) {
                ByteBuffer yBuffer = img.getPlanes()[0].getBuffer();
                ByteBuffer uBuffer = img.getPlanes()[1].getBuffer();
                ByteBuffer vBuffer = img.getPlanes()[2].getBuffer();
                yBuffer.rewind();
                uBuffer.rewind();
                vBuffer.rewind();
                int w = configuredSize.getWidth();
                int h = configuredSize.getHeight();
                int stride = img.getPlanes()[0].getRowStride();
                int uStride = img.getPlanes()[1].getRowStride();
                int vStride = img.getPlanes()[2].getRowStride();
                int uPStride = img.getPlanes()[1].getPixelStride();
                int vPStride = img.getPlanes()[2].getPixelStride();
                byte[] row = new byte[configuredSize.getWidth()];
                byte[] uRow = new byte[(configuredSize.getWidth()/2-1)*uPStride + 1];
                byte[] vRow = new byte[(configuredSize.getWidth()/2-1)*vPStride + 1];
                int[] imgArray = new int[w * h];
                for (int y = 0, j = 0, rowStart = 0, uRowStart = 0, vRowStart = 0; y < h;
                     y++, rowStart += stride) {
                    yBuffer.position(rowStart);
                    yBuffer.get(row);
                    if (y % 2 == 0) {
                        uBuffer.position(uRowStart);
                        uBuffer.get(uRow);
                        vBuffer.position(vRowStart);
                        vBuffer.get(vRow);
                        uRowStart += uStride;
                        vRowStart += vStride;
                    }
                    for (int x = 0, i = 0; x < w; x++) {
                        int yval = row[i] & 0xFF;
                        int uval = uRow[i/2 * uPStride] & 0xFF;
                        int vval = vRow[i/2 * vPStride] & 0xFF;
                        // Write YUV directly; the ImageView color filter will convert to RGB for us.
                        imgArray[j] = Color.rgb(yval, uval, vval);
                        i++;
                        j++;
                    }
                }
                img.close();
                imgBitmap = Bitmap.createBitmap(imgArray, w, h, Bitmap.Config.ARGB_8888);
            } else if (format == ImageFormat.JPEG) {
                ByteBuffer jpegBuffer = img.getPlanes()[0].getBuffer();
                jpegBuffer.rewind();
                byte[] jpegData = new byte[jpegBuffer.limit()];
                jpegBuffer.get(jpegData);
                imgBitmap = BitmapFactory.decodeByteArray(jpegData, 0, jpegData.length);
                img.close();
            } else {
                Log.i(TAG, ""Unsupported image format: "" + format);
            }
            if (imgBitmap != null) {
                final Bitmap bitmap = imgBitmap;
                runOnUiThread(new Runnable() {
                    @Override
                    public void run() {
                        if (format == ImageFormat.YUV_420_888 && (mCurrentColorFilter == null ||
                                !mCurrentColorFilter.equals(sJFIF_YUVToRGB_Filter))) {
                            mCurrentColorFilter = sJFIF_YUVToRGB_Filter;
                            mImageView.setColorFilter(mCurrentColorFilter);
                        } else if (format == ImageFormat.JPEG && mCurrentColorFilter != null &&
                                mCurrentColorFilter.equals(sJFIF_YUVToRGB_Filter)) {
                            mCurrentColorFilter = null;
                            mImageView.clearColorFilter();
                        }
                        mImageView.setImageBitmap(bitmap);
                    }
                });
            }
        } catch (java.lang.IllegalStateException e) {
            // Swallow exceptions
            e.printStackTrace();
        } finally {
            if (img != null) {
                img.close();
            }
        }
    }

    private AdapterView.OnItemSelectedListener mCameraSpinnerListener =
            new AdapterView.OnItemSelectedListener() {
                public void onItemSelected(AdapterView<?> parent,
                        View view, int pos, long id) {
                    if (mCurrentCameraIndex != pos) {
                        setUpCamera(pos);
                    }
                }

                public void onNothingSelected(AdapterView parent) {
                }
            };

    private class SizeComparator implements Comparator<Size> {
        @Override
        public int compare(Size lhs, Size rhs) {
            long lha = lhs.getWidth() * lhs.getHeight();
            long rha = rhs.getWidth() * rhs.getHeight();
            if (lha == rha) {
                lha = lhs.getWidth();
                rha = rhs.getWidth();
            }
            return (lha < rha) ? -1 : (lha > rha ? 1 : 0);
        }
    }

    private void setUpCamera(int index) {
        shutdownCamera();

        mCurrentCameraIndex = index;
        mCameraId = mCameraIdList[index];
        try {
            mCameraCharacteristics = mCameraManager.getCameraCharacteristics(mCameraId);
            mCameraDevice = mBlockingCameraManager.openCamera(mCameraId,
                    mCameraListener, mCameraHandler);
        } catch (CameraAccessException e) {
            e.printStackTrace();
        } catch (BlockingOpenException e) {
            e.printStackTrace();
        }

        // Update untested cameras
        mUntestedCameras.remove(""All combinations for Camera "" + mCameraId);

        StreamConfigurationMap config =
                mCameraCharacteristics.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        Size[] jpegSizes = config.getOutputSizes(ImageFormat.JPEG);
        Arrays.sort(jpegSizes, mSizeComparator);
        mJpegSize = jpegSizes[jpegSizes.length-1];

        Size[] yuvSizes = config.getOutputSizes(ImageFormat.YUV_420_888);
        Arrays.sort(yuvSizes, mSizeComparator);
        Size maxYuvSize = yuvSizes[yuvSizes.length-1];
        if (mSizeComparator.compare(maxYuvSize, FULLHD) > 1) {
            maxYuvSize = FULLHD;
        }

        // Update untested entries
        ArrayList<Capability> currentTestCase = mTestCases.get(mCameraId);
        for (Capability bokehCap : currentTestCase) {
            Size maxStreamingSize = bokehCap.getMaxStreamingSize();
            Size previewSize;
            if ((maxStreamingSize.getWidth() == 0 && maxStreamingSize.getHeight() == 0) ||
                    (mSizeComparator.compare(maxStreamingSize, maxYuvSize) > 0)) {
                previewSize = maxYuvSize;
            } else {
                previewSize = maxStreamingSize;
            }

            CameraCombination combination = new CameraCombination(
                    index, bokehCap.getMode(), previewSize.getWidth(),
                    previewSize.getHeight(), mCameraId,
                    mModeNames.get(bokehCap.getMode()),
                    /*isStillCapture*/false);

            if (!mTestedCombinations.contains(combination)) {
                mUntestedCombinations.add(combination);
            }

            // For BOKEH_MODE_STILL_CAPTURE, add 2 combinations: one streaming, one still capture.
            if (bokehCap.getMode() ==
                    CaptureRequest.CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE) {
                CameraCombination combination2 = new CameraCombination(
                        index, bokehCap.getMode(), previewSize.getWidth(),
                        previewSize.getHeight(), mCameraId,
                        mModeNames.get(bokehCap.getMode()),
                        /*isStillCapture*/true);

                if (!mTestedCombinations.contains(combination2)) {
                    mUntestedCombinations.add(combination2);
                }
            }
        }

        mJpegImageReader = ImageReader.newInstance(
                mJpegSize.getWidth(), mJpegSize.getHeight(), ImageFormat.JPEG, 1);
        mJpegImageReader.setOnImageAvailableListener(this, mCameraHandler);

        setUntestedCombination();

        if (mPreviewTexture != null) {
            startPreview();
        }
    }

    private void shutdownCamera() {
        if (null != mCaptureSession) {
            mCaptureSession.close();
            mCaptureSession = null;
        }
        if (null != mCameraDevice) {
            mCameraDevice.close();
            mCameraDevice = null;
        }
        if (null != mJpegImageReader) {
            mJpegImageReader.close();
            mJpegImageReader = null;
        }
        if (null != mYuvImageReader) {
            mYuvImageReader.close();
            mYuvImageReader = null;
        }
    }

    private void configurePreviewTextureTransform() {
        int rotation = getWindowManager().getDefaultDisplay().getRotation();
        Configuration config = getResources().getConfiguration();
        int degrees = 0;
        switch (rotation) {
            case Surface.ROTATION_0: degrees = 0; break;
            case Surface.ROTATION_90: degrees = 90; break;
            case Surface.ROTATION_180: degrees = 180; break;
            case Surface.ROTATION_270: degrees = 270; break;
        }
        Matrix matrix = mPreviewView.getTransform(null);
        int deviceOrientation = Configuration.ORIENTATION_PORTRAIT;
        if ((degrees % 180 == 0 && config.orientation == Configuration.ORIENTATION_LANDSCAPE) ||
                (degrees % 180 == 90 && config.orientation == Configuration.ORIENTATION_PORTRAIT)) {
            deviceOrientation = Configuration.ORIENTATION_LANDSCAPE;
        }
        int effectiveWidth = mPreviewSize.getWidth();
        int effectiveHeight = mPreviewSize.getHeight();
        if (deviceOrientation == Configuration.ORIENTATION_PORTRAIT) {
            int temp = effectiveWidth;
            effectiveWidth = effectiveHeight;
            effectiveHeight = temp;
        }

        RectF viewRect = new RectF(0, 0, mPreviewTexWidth, mPreviewTexHeight);
        RectF bufferRect = new RectF(0, 0, effectiveWidth, effectiveHeight);
        float centerX = viewRect.centerX();
        float centerY = viewRect.centerY();
        bufferRect.offset(centerX - bufferRect.centerX(),
                centerY - bufferRect.centerY());

        matrix.setRectToRect(viewRect, bufferRect, Matrix.ScaleToFit.FILL);

        matrix.postRotate((360 - degrees) % 360, centerX, centerY);
        if ((degrees % 180) == 90) {
            int temp = effectiveWidth;
            effectiveWidth = effectiveHeight;
            effectiveHeight = temp;
        }
        // Scale to fit view, avoiding any crop
        float scale = Math.min(mPreviewTexWidth / (float) effectiveWidth,
                mPreviewTexHeight / (float) effectiveHeight);
        matrix.postScale(scale, scale, centerX, centerY);

        mPreviewView.setTransform(matrix);
    }
    /**
     * Starts a background thread and its {@link Handler}.
     */
    private void startBackgroundThread() {
        mCameraThread = new HandlerThread(""CameraBokehBackground"");
        mCameraThread.start();
        mCameraHandler = new Handler(mCameraThread.getLooper());
    }

    /**
     * Stops the background thread and its {@link Handler}.
     */
    private void stopBackgroundThread() {
        mCameraThread.quitSafely();
        try {
            mCameraThread.join();
            mCameraThread = null;
            mCameraHandler = null;
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }

    private void startPreview() {
        try {
            if (mPreviewSize == null || !mPreviewSize.equals(mNextCombination.mPreviewSize)) {
                mPreviewSize = mNextCombination.mPreviewSize;

                mYuvImageReader = ImageReader.newInstance(mPreviewSize.getWidth(),
                        mPreviewSize.getHeight(), ImageFormat.YUV_420_888, 1);
                mYuvImageReader.setOnImageAvailableListener(this, mCameraHandler);
            };

            mPreviewTexture.setDefaultBufferSize(mPreviewSize.getWidth(), mPreviewSize.getHeight());
            mPreviewRequestBuilder =
                    mCameraDevice.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
            mPreviewRequestBuilder.addTarget(mPreviewSurface);
            mPreviewRequestBuilder.addTarget(mYuvImageReader.getSurface());

            mStillCaptureRequestBuilder =
                    mCameraDevice.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
            mStillCaptureRequestBuilder.addTarget(mPreviewSurface);
            mStillCaptureRequestBuilder.addTarget(mJpegImageReader.getSurface());

            mSessionListener = new BlockingSessionCallback();
            List<Surface> outputSurfaces = new ArrayList<Surface>(/*capacity*/3);
            outputSurfaces.add(mPreviewSurface);
            outputSurfaces.add(mYuvImageReader.getSurface());
            outputSurfaces.add(mJpegImageReader.getSurface());
            mCameraDevice.createCaptureSession(outputSurfaces, mSessionListener, mCameraHandler);
            mCaptureSession = mSessionListener.waitAndGetSession(/*timeoutMs*/3000);

            configurePreviewTextureTransform();

            /* Set bokeh mode and start streaming */
            int bokehMode = mNextCombination.mMode;
            mPreviewRequestBuilder.set(CaptureRequest.CONTROL_EXTENDED_SCENE_MODE, bokehMode);
            mStillCaptureRequestBuilder.set(CaptureRequest.CONTROL_EXTENDED_SCENE_MODE, bokehMode);
            mPreviewRequest = mPreviewRequestBuilder.build();
            mStillCaptureRequest = mStillCaptureRequestBuilder.build();

            mCaptureSession.setRepeatingRequest(mPreviewRequest, mCaptureCallback, mCameraHandler);
        } catch (CameraAccessException e) {
            e.printStackTrace();
        }
    }

    private void takePicture() {
        try {
            mCaptureSession.stopRepeating();
            mSessionListener.getStateWaiter().waitForState(
                    BlockingSessionCallback.SESSION_READY, SESSION_READY_TIMEOUT_MS);

            mCaptureSession.capture(mStillCaptureRequest, mCaptureCallback, mCameraHandler);
        } catch (CameraAccessException e) {
            e.printStackTrace();
        }
    }

    private void setPassButtonEnabled(boolean enabled) {
        ImageButton pass_button = (ImageButton) findViewById(R.id.pass_button);
        pass_button.setEnabled(enabled);
    }
}"	""	""	"JPEG 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.its.ItsService"	"doCheckSensorExistence"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/its/ItsService.java"	""	"public void test/*
 *.
 */

package com.android.cts.verifier.camera.its;

import android.app.Notification;
import android.app.NotificationChannel;
import android.app.NotificationManager;
import android.app.Service;
import android.content.Context;
import android.content.Intent;
import android.content.pm.ServiceInfo;
import android.graphics.ImageFormat;
import android.graphics.Rect;
import android.hardware.SensorPrivacyManager;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CaptureFailure;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.DngCreator;
import android.hardware.camera2.TotalCaptureResult;
import android.hardware.camera2.cts.PerformanceTest;
import android.hardware.camera2.params.InputConfiguration;
import android.hardware.camera2.params.MeteringRectangle;
import android.hardware.camera2.params.OutputConfiguration;
import android.hardware.camera2.params.SessionConfiguration;
import android.hardware.Sensor;
import android.hardware.SensorEvent;
import android.hardware.SensorEventListener;
import android.hardware.SensorManager;
import android.media.AudioAttributes;
import android.media.Image;
import android.media.ImageReader;
import android.media.ImageWriter;
import android.media.Image.Plane;
import android.net.Uri;
import android.os.Build;
import android.os.Bundle;
import android.os.ConditionVariable;
import android.os.Handler;
import android.os.HandlerThread;
import android.os.IBinder;
import android.os.Message;
import android.os.SystemClock;
import android.os.Vibrator;
import android.util.Log;
import android.util.Rational;
import android.util.Size;
import android.util.SparseArray;
import android.view.Surface;

import androidx.test.InstrumentationRegistry;

import com.android.ex.camera2.blocking.BlockingCameraManager;
import com.android.ex.camera2.blocking.BlockingCameraManager.BlockingOpenException;
import com.android.ex.camera2.blocking.BlockingStateCallback;
import com.android.ex.camera2.blocking.BlockingSessionCallback;

import com.android.compatibility.common.util.ReportLog.Metric;
import com.android.cts.verifier.camera.its.StatsImage;
import com.android.cts.verifier.camera.performance.CameraTestInstrumentation;
import com.android.cts.verifier.camera.performance.CameraTestInstrumentation.MetricListener;
import com.android.cts.verifier.R;

import org.json.JSONArray;
import org.json.JSONObject;
import org.junit.runner.JUnitCore;
import org.junit.runner.Request;
import org.junit.runner.Result;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.io.PrintWriter;
import java.math.BigInteger;
import java.net.ServerSocket;
import java.net.Socket;
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.nio.FloatBuffer;
import java.nio.charset.Charset;
import java.security.MessageDigest;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Executor;
import java.util.concurrent.LinkedBlockingDeque;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.Semaphore;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;

public class ItsService extends Service implements SensorEventListener {
    public static final String TAG = ItsService.class.getSimpleName();

    // Version number to keep host/server communication in sync
    // This string must be in sync with python side device.py
    // Updated when interface between script and ItsService is changed
    private final String ITS_SERVICE_VERSION = ""1.0"";

    private final int SERVICE_NOTIFICATION_ID = 37; // random int that is unique within app
    private NotificationChannel mChannel;

    // Timeouts, in seconds.
    private static final int TIMEOUT_CALLBACK = 20;
    private static final int TIMEOUT_3A = 10;

    // Time given for background requests to warm up pipeline
    private static final long PIPELINE_WARMUP_TIME_MS = 2000;

    // State transition timeouts, in ms.
    private static final long TIMEOUT_IDLE_MS = 2000;
    private static final long TIMEOUT_STATE_MS = 500;
    private static final long TIMEOUT_SESSION_CLOSE = 3000;

    // Timeout to wait for a capture result after the capture buffer has arrived, in ms.
    private static final long TIMEOUT_CAP_RES = 2000;

    private static final int MAX_CONCURRENT_READER_BUFFERS = 10;

    // Supports at most RAW+YUV+JPEG, one surface each, plus optional background stream
    private static final int MAX_NUM_OUTPUT_SURFACES = 4;

    // Performance class R version number
    private static final int PERFORMANCE_CLASS_R = Build.VERSION_CODES.R;
    // Performance class S version number
    private static final int PERFORMANCE_CLASS_S = Build.VERSION_CODES.R + 1;

    public static final int SERVERPORT = 6000;

    public static final String REGION_KEY = ""regions"";
    public static final String REGION_AE_KEY = ""ae"";
    public static final String REGION_AWB_KEY = ""awb"";
    public static final String REGION_AF_KEY = ""af"";
    public static final String LOCK_AE_KEY = ""aeLock"";
    public static final String LOCK_AWB_KEY = ""awbLock"";
    public static final String TRIGGER_KEY = ""triggers"";
    public static final String PHYSICAL_ID_KEY = ""physicalId"";
    public static final String TRIGGER_AE_KEY = ""ae"";
    public static final String TRIGGER_AF_KEY = ""af"";
    public static final String VIB_PATTERN_KEY = ""pattern"";
    public static final String EVCOMP_KEY = ""evComp"";
    public static final String AUDIO_RESTRICTION_MODE_KEY = ""mode"";

    private CameraManager mCameraManager = null;
    private HandlerThread mCameraThread = null;
    private Handler mCameraHandler = null;
    private BlockingCameraManager mBlockingCameraManager = null;
    private BlockingStateCallback mCameraListener = null;
    private CameraDevice mCamera = null;
    private CameraCaptureSession mSession = null;
    private ImageReader[] mOutputImageReaders = null;
    private SparseArray<String> mPhysicalStreamMap = new SparseArray<String>();
    private ImageReader mInputImageReader = null;
    private CameraCharacteristics mCameraCharacteristics = null;
    private HashMap<String, CameraCharacteristics> mPhysicalCameraChars =
            new HashMap<String, CameraCharacteristics>();
    private ItsUtils.ItsCameraIdList mItsCameraIdList = null;

    private Vibrator mVibrator = null;

    private HandlerThread mSaveThreads[] = new HandlerThread[MAX_NUM_OUTPUT_SURFACES];
    private Handler mSaveHandlers[] = new Handler[MAX_NUM_OUTPUT_SURFACES];
    private HandlerThread mResultThread = null;
    private Handler mResultHandler = null;

    private volatile boolean mThreadExitFlag = false;

    private volatile ServerSocket mSocket = null;
    private volatile SocketRunnable mSocketRunnableObj = null;
    private Semaphore mSocketQueueQuota = null;
    private int mMemoryQuota = -1;
    private LinkedList<Integer> mInflightImageSizes = new LinkedList<>();
    private volatile BlockingQueue<ByteBuffer> mSocketWriteQueue =
            new LinkedBlockingDeque<ByteBuffer>();
    private final Object mSocketWriteEnqueueLock = new Object();
    private final Object mSocketWriteDrainLock = new Object();

    private volatile BlockingQueue<Object[]> mSerializerQueue =
            new LinkedBlockingDeque<Object[]>();

    private AtomicInteger mCountCallbacksRemaining = new AtomicInteger();
    private AtomicInteger mCountRawOrDng = new AtomicInteger();
    private AtomicInteger mCountRaw10 = new AtomicInteger();
    private AtomicInteger mCountRaw12 = new AtomicInteger();
    private AtomicInteger mCountJpg = new AtomicInteger();
    private AtomicInteger mCountYuv = new AtomicInteger();
    private AtomicInteger mCountCapRes = new AtomicInteger();
    private boolean mCaptureRawIsDng;
    private boolean mCaptureRawIsStats;
    private int mCaptureStatsGridWidth;
    private int mCaptureStatsGridHeight;
    private CaptureResult mCaptureResults[] = null;

    private volatile ConditionVariable mInterlock3A = new ConditionVariable(true);

    final Object m3AStateLock = new Object();
    private volatile boolean mConvergedAE = false;
    private volatile boolean mConvergedAF = false;
    private volatile boolean mConvergedAWB = false;
    private volatile boolean mLockedAE = false;
    private volatile boolean mLockedAWB = false;
    private volatile boolean mNeedsLockedAE = false;
    private volatile boolean mNeedsLockedAWB = false;

    class MySensorEvent {
        public Sensor sensor;
        public int accuracy;
        public long timestamp;
        public float values[];
    }

    // For capturing motion sensor traces.
    private SensorManager mSensorManager = null;
    private Sensor mAccelSensor = null;
    private Sensor mMagSensor = null;
    private Sensor mGyroSensor = null;
    private volatile LinkedList<MySensorEvent> mEvents = null;
    private volatile Object mEventLock = new Object();
    private volatile boolean mEventsEnabled = false;
    private HandlerThread mSensorThread = null;
    private Handler mSensorHandler = null;

    private SensorPrivacyManager mSensorPrivacyManager;

    // Camera test instrumentation
    private CameraTestInstrumentation mCameraInstrumentation;
    // Camera PerformanceTest metric
    private final ArrayList<Metric> mResults = new ArrayList<Metric>();

    private static final int SERIALIZER_SURFACES_ID = 2;
    private static final int SERIALIZER_PHYSICAL_METADATA_ID = 3;

    public interface CaptureCallback {
        void onCaptureAvailable(Image capture, String physicalCameraId);
    }

    public abstract class CaptureResultListener extends CameraCaptureSession.CaptureCallback {}

    @Override
    public IBinder onBind(Intent intent) {
        return null;
    }

    @Override
    public void onCreate() {
        try {
            mThreadExitFlag = false;

            // Get handle to camera manager.
            mCameraManager = (CameraManager) this.getSystemService(Context.CAMERA_SERVICE);
            if (mCameraManager == null) {
                throw new ItsException(""Failed to connect to camera manager"");
            }
            mBlockingCameraManager = new BlockingCameraManager(mCameraManager);
            mCameraListener = new BlockingStateCallback();

            // Register for motion events.
            mEvents = new LinkedList<MySensorEvent>();
            mSensorManager = (SensorManager)getSystemService(Context.SENSOR_SERVICE);
            mAccelSensor = mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER);
            mMagSensor = mSensorManager.getDefaultSensor(Sensor.TYPE_MAGNETIC_FIELD);
            mGyroSensor = mSensorManager.getDefaultSensor(Sensor.TYPE_GYROSCOPE);
            mSensorThread = new HandlerThread(""SensorThread"");
            mSensorThread.start();
            mSensorHandler = new Handler(mSensorThread.getLooper());
            mSensorManager.registerListener(this, mAccelSensor,
                    /*100hz*/ 10000, mSensorHandler);
            mSensorManager.registerListener(this, mMagSensor,
                    SensorManager.SENSOR_DELAY_NORMAL, mSensorHandler);
            mSensorManager.registerListener(this, mGyroSensor,
                    /*200hz*/5000, mSensorHandler);

            // Get a handle to the system vibrator.
            mVibrator = (Vibrator)getSystemService(Context.VIBRATOR_SERVICE);

            // Create threads to receive images and save them.
            for (int i = 0; i < MAX_NUM_OUTPUT_SURFACES; i++) {
                mSaveThreads[i] = new HandlerThread(""SaveThread"" + i);
                mSaveThreads[i].start();
                mSaveHandlers[i] = new Handler(mSaveThreads[i].getLooper());
            }

            // Create a thread to handle object serialization.
            (new Thread(new SerializerRunnable())).start();;

            // Create a thread to receive capture results and process them.
            mResultThread = new HandlerThread(""ResultThread"");
            mResultThread.start();
            mResultHandler = new Handler(mResultThread.getLooper());

            // Create a thread for the camera device.
            mCameraThread = new HandlerThread(""ItsCameraThread"");
            mCameraThread.start();
            mCameraHandler = new Handler(mCameraThread.getLooper());

            // Create a thread to process commands, listening on a TCP socket.
            mSocketRunnableObj = new SocketRunnable();
            (new Thread(mSocketRunnableObj)).start();
        } catch (ItsException e) {
            Logt.e(TAG, ""Service failed to start: "", e);
        }

        NotificationManager notificationManager =
                (NotificationManager) getSystemService(Context.NOTIFICATION_SERVICE);
        mChannel = new NotificationChannel(
                ""ItsServiceChannel"", ""ItsService"", NotificationManager.IMPORTANCE_LOW);
        // Configure the notification channel.
        mChannel.setDescription(""ItsServiceChannel"");
        mChannel.enableVibration(false);
        notificationManager.createNotificationChannel(mChannel);

        mSensorPrivacyManager = getSystemService(SensorPrivacyManager.class);
    }

    @Override
    public int onStartCommand(Intent intent, int flags, int startId) {
        try {
            // Just log a message indicating that the service is running and is able to accept
            // socket connections.
            while (!mThreadExitFlag && mSocket==null) {
                Thread.sleep(1);
            }
            if (!mThreadExitFlag){
                Logt.i(TAG, ""ItsService ready"");
            } else {
                Logt.e(TAG, ""Starting ItsService in bad state"");
            }

            Notification notification = new Notification.Builder(this, mChannel.getId())
                    .setContentTitle(""CameraITS Service"")
                    .setContentText(""CameraITS Service is running"")
                    .setSmallIcon(R.drawable.icon)
                    .setOngoing(true).build();
            startForeground(SERVICE_NOTIFICATION_ID, notification,
                    ServiceInfo.FOREGROUND_SERVICE_TYPE_CAMERA);
        } catch (java.lang.InterruptedException e) {
            Logt.e(TAG, ""Error starting ItsService (interrupted)"", e);
        }
        return START_STICKY;
    }

    @Override
    public void onDestroy() {
        mThreadExitFlag = true;
        for (int i = 0; i < MAX_NUM_OUTPUT_SURFACES; i++) {
            if (mSaveThreads[i] != null) {
                mSaveThreads[i].quit();
                mSaveThreads[i] = null;
            }
        }
        if (mSensorThread != null) {
            mSensorThread.quitSafely();
            mSensorThread = null;
        }
        if (mResultThread != null) {
            mResultThread.quitSafely();
            mResultThread = null;
        }
        if (mCameraThread != null) {
            mCameraThread.quitSafely();
            mCameraThread = null;
        }
    }

    public void openCameraDevice(String cameraId) throws ItsException {
        Logt.i(TAG, String.format(""Opening camera %s"", cameraId));

        try {
            if (mMemoryQuota == -1) {
                // Initialize memory quota on this device
                if (mItsCameraIdList == null) {
                    mItsCameraIdList = ItsUtils.getItsCompatibleCameraIds(mCameraManager);
                }
                if (mItsCameraIdList.mCameraIds.size() == 0) {
                    throw new ItsException(""No camera devices"");
                }
                for (String camId : mItsCameraIdList.mCameraIds) {
                    CameraCharacteristics chars =  mCameraManager.getCameraCharacteristics(camId);
                    Size maxYuvSize = ItsUtils.getMaxOutputSize(
                            chars, ImageFormat.YUV_420_888);
                    // 4 bytes per pixel for RGBA8888 Bitmap and at least 3 Bitmaps per CDD
                    int quota = maxYuvSize.getWidth() * maxYuvSize.getHeight() * 4 * 3;
                    if (quota > mMemoryQuota) {
                        mMemoryQuota = quota;
                    }
                }
            }
        } catch (CameraAccessException e) {
            throw new ItsException(""Failed to get device ID list"", e);
        }

        try {
            mCamera = mBlockingCameraManager.openCamera(cameraId, mCameraListener, mCameraHandler);
            mCameraCharacteristics = mCameraManager.getCameraCharacteristics(cameraId);

            boolean isLogicalCamera = hasCapability(
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_LOGICAL_MULTI_CAMERA);
            if (isLogicalCamera) {
                Set<String> physicalCameraIds = mCameraCharacteristics.getPhysicalCameraIds();
                for (String id : physicalCameraIds) {
                    mPhysicalCameraChars.put(id, mCameraManager.getCameraCharacteristics(id));
                }
            }
            mSocketQueueQuota = new Semaphore(mMemoryQuota, true);
        } catch (CameraAccessException e) {
            throw new ItsException(""Failed to open camera"", e);
        } catch (BlockingOpenException e) {
            throw new ItsException(""Failed to open camera (after blocking)"", e);
        }
        mSocketRunnableObj.sendResponse(""cameraOpened"", """");
    }

    public void closeCameraDevice() throws ItsException {
        try {
            if (mCamera != null) {
                Logt.i(TAG, ""Closing camera"");
                mCamera.close();
                mCamera = null;
            }
        } catch (Exception e) {
            throw new ItsException(""Failed to close device"");
        }
        mSocketRunnableObj.sendResponse(""cameraClosed"", """");
    }

    class SerializerRunnable implements Runnable {
        // Use a separate thread to perform JSON serialization (since this can be slow due to
        // the reflection).
        @Override
        public void run() {
            Logt.i(TAG, ""Serializer thread starting"");
            while (! mThreadExitFlag) {
                try {
                    Object objs[] = mSerializerQueue.take();
                    JSONObject jsonObj = new JSONObject();
                    String tag = null;
                    for (int i = 0; i < objs.length; i++) {
                        Object obj = objs[i];
                        if (obj instanceof String) {
                            if (tag != null) {
                                throw new ItsException(""Multiple tags for socket response"");
                            }
                            tag = (String)obj;
                        } else if (obj instanceof CameraCharacteristics) {
                            jsonObj.put(""cameraProperties"", ItsSerializer.serialize(
                                    (CameraCharacteristics)obj));
                        } else if (obj instanceof CaptureRequest) {
                            jsonObj.put(""captureRequest"", ItsSerializer.serialize(
                                    (CaptureRequest)obj));
                        } else if (obj instanceof CaptureResult) {
                            jsonObj.put(""captureResult"", ItsSerializer.serialize(
                                    (CaptureResult)obj));
                        } else if (obj instanceof JSONArray) {
                            if (tag == ""captureResults"") {
                                if (i == SERIALIZER_SURFACES_ID) {
                                    jsonObj.put(""outputs"", (JSONArray)obj);
                                } else if (i == SERIALIZER_PHYSICAL_METADATA_ID) {
                                    jsonObj.put(""physicalResults"", (JSONArray)obj);
                                } else {
                                    throw new ItsException(
                                            ""Unsupported JSONArray for captureResults"");
                                }
                            } else {
                                jsonObj.put(""outputs"", (JSONArray)obj);
                            }
                        } else {
                            throw new ItsException(""Invalid object received for serialization"");
                        }
                    }
                    if (tag == null) {
                        throw new ItsException(""No tag provided for socket response"");
                    }
                    mSocketRunnableObj.sendResponse(tag, null, jsonObj, null);
                    Logt.i(TAG, String.format(""Serialized %s"", tag));
                } catch (org.json.JSONException e) {
                    Logt.e(TAG, ""Error serializing object"", e);
                    break;
                } catch (ItsException e) {
                    Logt.e(TAG, ""Error serializing object"", e);
                    break;
                } catch (java.lang.InterruptedException e) {
                    Logt.e(TAG, ""Error serializing object (interrupted)"", e);
                    break;
                }
            }
            Logt.i(TAG, ""Serializer thread terminated"");
        }
    }

    class SocketWriteRunnable implements Runnable {

        // Use a separate thread to service a queue of objects to be written to the socket,
        // writing each sequentially in order. This is needed since different handler functions
        // (called on different threads) will need to send data back to the host script.

        public Socket mOpenSocket = null;
        private Thread mThread = null;

        public SocketWriteRunnable(Socket openSocket) {
            mOpenSocket = openSocket;
        }

        public void setOpenSocket(Socket openSocket) {
            mOpenSocket = openSocket;
        }

        @Override
        public void run() {
            Logt.i(TAG, ""Socket writer thread starting"");
            while (true) {
                try {
                    ByteBuffer b = mSocketWriteQueue.take();
                    synchronized(mSocketWriteDrainLock) {
                        if (mOpenSocket == null) {
                            Logt.e(TAG, ""No open socket connection!"");
                            continue;
                        }
                        if (b.hasArray()) {
                            mOpenSocket.getOutputStream().write(b.array(), 0, b.capacity());
                        } else {
                            byte[] barray = new byte[b.capacity()];
                            b.get(barray);
                            mOpenSocket.getOutputStream().write(barray);
                        }
                        mOpenSocket.getOutputStream().flush();
                        Logt.i(TAG, String.format(""Wrote to socket: %d bytes"", b.capacity()));
                        Integer imgBufSize = mInflightImageSizes.peek();
                        if (imgBufSize != null && imgBufSize == b.capacity()) {
                            mInflightImageSizes.removeFirst();
                            if (mSocketQueueQuota != null) {
                                mSocketQueueQuota.release(imgBufSize);
                            }
                        }
                    }
                } catch (IOException e) {
                    Logt.e(TAG, ""Error writing to socket"", e);
                    mOpenSocket = null;
                    break;
                } catch (java.lang.InterruptedException e) {
                    Logt.e(TAG, ""Error writing to socket (interrupted)"", e);
                    mOpenSocket = null;
                    break;
                }
            }
            Logt.i(TAG, ""Socket writer thread terminated"");
        }

        public synchronized void checkAndStartThread() {
            if (mThread == null || mThread.getState() == Thread.State.TERMINATED) {
                mThread = new Thread(this);
            }
            if (mThread.getState() == Thread.State.NEW) {
                mThread.start();
            }
        }

    }

    class SocketRunnable implements Runnable {

        // Format of sent messages (over the socket):
        // * Serialized JSON object on a single line (newline-terminated)
        // * For byte buffers, the binary data then follows
        //
        // Format of received messages (from the socket):
        // * Serialized JSON object on a single line (newline-terminated)

        private Socket mOpenSocket = null;
        private SocketWriteRunnable mSocketWriteRunnable = null;

        @Override
        public void run() {
            Logt.i(TAG, ""Socket thread starting"");
            try {
                mSocket = new ServerSocket(SERVERPORT);
            } catch (IOException e) {
                Logt.e(TAG, ""Failed to create socket"", e);
            }

            // Create a new thread to handle writes to this socket.
            mSocketWriteRunnable = new SocketWriteRunnable(null);

            while (!mThreadExitFlag) {
                // Receive the socket-open request from the host.
                try {
                    Logt.i(TAG, ""Waiting for client to connect to socket"");
                    mOpenSocket = mSocket.accept();
                    if (mOpenSocket == null) {
                        Logt.e(TAG, ""Socket connection error"");
                        break;
                    }
                    mSocketWriteQueue.clear();
                    mInflightImageSizes.clear();
                    mSocketWriteRunnable.setOpenSocket(mOpenSocket);
                    mSocketWriteRunnable.checkAndStartThread();
                    Logt.i(TAG, ""Socket connected"");
                } catch (IOException e) {
                    Logt.e(TAG, ""Socket open error: "", e);
                    break;
                }

                // Process commands over the open socket.
                while (!mThreadExitFlag) {
                    try {
                        BufferedReader input = new BufferedReader(
                                new InputStreamReader(mOpenSocket.getInputStream()));
                        if (input == null) {
                            Logt.e(TAG, ""Failed to get socket input stream"");
                            break;
                        }
                        String line = input.readLine();
                        if (line == null) {
                            Logt.i(TAG, ""Socket readline returned null (host disconnected)"");
                            break;
                        }
                        processSocketCommand(line);
                    } catch (IOException e) {
                        Logt.e(TAG, ""Socket read error: "", e);
                        break;
                    } catch (ItsException e) {
                        Logt.e(TAG, ""Script error: "", e);
                        break;
                    }
                }

                // Close socket and go back to waiting for a new connection.
                try {
                    synchronized(mSocketWriteDrainLock) {
                        mSocketWriteQueue.clear();
                        mInflightImageSizes.clear();
                        mOpenSocket.close();
                        mOpenSocket = null;
                        mSocketWriteRunnable.setOpenSocket(null);
                        Logt.i(TAG, ""Socket disconnected"");
                    }
                } catch (java.io.IOException e) {
                    Logt.e(TAG, ""Exception closing socket"");
                }
            }

            // It's an overall error state if the code gets here; no recevery.
            // Try to do some cleanup, but the service probably needs to be restarted.
            Logt.i(TAG, ""Socket server loop exited"");
            mThreadExitFlag = true;
            try {
                synchronized(mSocketWriteDrainLock) {
                    if (mOpenSocket != null) {
                        mOpenSocket.close();
                        mOpenSocket = null;
                        mSocketWriteRunnable.setOpenSocket(null);
                    }
                }
            } catch (java.io.IOException e) {
                Logt.w(TAG, ""Exception closing socket"");
            }
            try {
                if (mSocket != null) {
                    mSocket.close();
                    mSocket = null;
                }
            } catch (java.io.IOException e) {
                Logt.w(TAG, ""Exception closing socket"");
            }
        }

        public void processSocketCommand(String cmd)
                throws ItsException {
            // Default locale must be set to ""en-us""
            Locale locale = Locale.getDefault();
            if (!Locale.US.equals(locale)) {
                Logt.e(TAG, ""Default language is not set to "" + Locale.US + ""!"");
                stopSelf();
            }

            // Each command is a serialized JSON object.
            try {
                JSONObject cmdObj = new JSONObject(cmd);
                Logt.i(TAG, ""Start processing command"" + cmdObj.getString(""cmdName""));
                if (""open"".equals(cmdObj.getString(""cmdName""))) {
                    String cameraId = cmdObj.getString(""cameraId"");
                    openCameraDevice(cameraId);
                } else if (""close"".equals(cmdObj.getString(""cmdName""))) {
                    closeCameraDevice();
                } else if (""getCameraProperties"".equals(cmdObj.getString(""cmdName""))) {
                    doGetProps();
                } else if (""getCameraPropertiesById"".equals(cmdObj.getString(""cmdName""))) {
                    doGetPropsById(cmdObj);
                } else if (""startSensorEvents"".equals(cmdObj.getString(""cmdName""))) {
                    doStartSensorEvents();
                } else if (""checkSensorExistence"".equals(cmdObj.getString(""cmdName""))) {
                    doCheckSensorExistence();
                } else if (""getSensorEvents"".equals(cmdObj.getString(""cmdName""))) {
                    doGetSensorEvents();
                } else if (""do3A"".equals(cmdObj.getString(""cmdName""))) {
                    do3A(cmdObj);
                } else if (""doCapture"".equals(cmdObj.getString(""cmdName""))) {
                    doCapture(cmdObj);
                } else if (""doVibrate"".equals(cmdObj.getString(""cmdName""))) {
                    doVibrate(cmdObj);
                } else if (""setAudioRestriction"".equals(cmdObj.getString(""cmdName""))) {
                    doSetAudioRestriction(cmdObj);
                } else if (""getCameraIds"".equals(cmdObj.getString(""cmdName""))) {
                    doGetCameraIds();
                } else if (""doReprocessCapture"".equals(cmdObj.getString(""cmdName""))) {
                    doReprocessCapture(cmdObj);
                } else if (""getItsVersion"".equals(cmdObj.getString(""cmdName""))) {
                    mSocketRunnableObj.sendResponse(""ItsVersion"", ITS_SERVICE_VERSION);
                } else if (""isStreamCombinationSupported"".equals(cmdObj.getString(""cmdName""))) {
                    doCheckStreamCombination(cmdObj);
                } else if (""isCameraPrivacyModeSupported"".equals(cmdObj.getString(""cmdName""))) {
                    doCheckCameraPrivacyModeSupport();
                } else if (""isPerformanceClassPrimaryCamera"".equals(cmdObj.getString(""cmdName""))) {
                    String cameraId = cmdObj.getString(""cameraId"");
                    doCheckPerformanceClassPrimaryCamera(cameraId);
                } else if (""measureCameraLaunchMs"".equals(cmdObj.getString(""cmdName""))) {
                    String cameraId = cmdObj.getString(""cameraId"");
                    doMeasureCameraLaunchMs(cameraId);
                } else if (""measureCamera1080pJpegCaptureMs"".equals(cmdObj.getString(""cmdName""))) {
                    String cameraId = cmdObj.getString(""cameraId"");
                    doMeasureCamera1080pJpegCaptureMs(cameraId);
                } else {
                    throw new ItsException(""Unknown command: "" + cmd);
                }
                Logt.i(TAG, ""Finish processing command"" + cmdObj.getString(""cmdName""));
            } catch (org.json.JSONException e) {
                Logt.e(TAG, ""Invalid command: "", e);
            }
        }

        public void sendResponse(String tag, String str, JSONObject obj, ByteBuffer bbuf)
                throws ItsException {
            try {
                JSONObject jsonObj = new JSONObject();
                jsonObj.put(""tag"", tag);
                if (str != null) {
                    jsonObj.put(""strValue"", str);
                }
                if (obj != null) {
                    jsonObj.put(""objValue"", obj);
                }
                if (bbuf != null) {
                    jsonObj.put(""bufValueSize"", bbuf.capacity());
                }
                ByteBuffer bstr = ByteBuffer.wrap(
                        (jsonObj.toString()+""\n"").getBytes(Charset.defaultCharset()));
                synchronized(mSocketWriteEnqueueLock) {
                    if (bstr != null) {
                        mSocketWriteQueue.put(bstr);
                    }
                    if (bbuf != null) {
                        mInflightImageSizes.add(bbuf.capacity());
                        mSocketWriteQueue.put(bbuf);
                    }
                }
            } catch (org.json.JSONException e) {
                throw new ItsException(""JSON error: "", e);
            } catch (java.lang.InterruptedException e) {
                throw new ItsException(""Socket error: "", e);
            }
        }

        public void sendResponse(String tag, String str)
                throws ItsException {
            sendResponse(tag, str, null, null);
        }

        public void sendResponse(String tag, JSONObject obj)
                throws ItsException {
            sendResponse(tag, null, obj, null);
        }

        public void sendResponseCaptureBuffer(String tag, ByteBuffer bbuf)
                throws ItsException {
            sendResponse(tag, null, null, bbuf);
        }

        public void sendResponse(LinkedList<MySensorEvent> events)
                throws ItsException {
            Logt.i(TAG, ""Sending "" + events.size() + "" sensor events"");
            try {
                JSONArray accels = new JSONArray();
                JSONArray mags = new JSONArray();
                JSONArray gyros = new JSONArray();
                for (MySensorEvent event : events) {
                    JSONObject obj = new JSONObject();
                    obj.put(""time"", event.timestamp);
                    obj.put(""x"", event.values[0]);
                    obj.put(""y"", event.values[1]);
                    obj.put(""z"", event.values[2]);
                    if (event.sensor.getType() == Sensor.TYPE_ACCELEROMETER) {
                        accels.put(obj);
                    } else if (event.sensor.getType() == Sensor.TYPE_MAGNETIC_FIELD) {
                        mags.put(obj);
                    } else if (event.sensor.getType() == Sensor.TYPE_GYROSCOPE) {
                        gyros.put(obj);
                    }
                }
                JSONObject obj = new JSONObject();
                obj.put(""accel"", accels);
                obj.put(""mag"", mags);
                obj.put(""gyro"", gyros);
                sendResponse(""sensorEvents"", null, obj, null);
            } catch (org.json.JSONException e) {
                throw new ItsException(""JSON error: "", e);
            }
            Logt.i(TAG, ""Sent sensor events"");
        }

        public void sendResponse(CameraCharacteristics props)
                throws ItsException {
            try {
                Object objs[] = new Object[2];
                objs[0] = ""cameraProperties"";
                objs[1] = props;
                mSerializerQueue.put(objs);
            } catch (InterruptedException e) {
                throw new ItsException(""Interrupted: "", e);
            }
        }

        public void sendResponseCaptureResult(CameraCharacteristics props,
                                              CaptureRequest request,
                                              TotalCaptureResult result,
                                              ImageReader[] readers)
                throws ItsException {
            try {
                JSONArray jsonSurfaces = new JSONArray();
                for (int i = 0; i < readers.length; i++) {
                    JSONObject jsonSurface = new JSONObject();
                    jsonSurface.put(""width"", readers[i].getWidth());
                    jsonSurface.put(""height"", readers[i].getHeight());
                    int format = readers[i].getImageFormat();
                    if (format == ImageFormat.RAW_SENSOR) {
                        if (mCaptureRawIsStats) {
                            int aaw = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .width();
                            int aah = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .height();
                            jsonSurface.put(""format"", ""rawStats"");
                            jsonSurface.put(""width"", aaw/mCaptureStatsGridWidth);
                            jsonSurface.put(""height"", aah/mCaptureStatsGridHeight);
                        } else if (mCaptureRawIsDng) {
                            jsonSurface.put(""format"", ""dng"");
                        } else {
                            jsonSurface.put(""format"", ""raw"");
                        }
                    } else if (format == ImageFormat.RAW10) {
                        jsonSurface.put(""format"", ""raw10"");
                    } else if (format == ImageFormat.RAW12) {
                        jsonSurface.put(""format"", ""raw12"");
                    } else if (format == ImageFormat.JPEG) {
                        jsonSurface.put(""format"", ""jpeg"");
                    } else if (format == ImageFormat.YUV_420_888) {
                        jsonSurface.put(""format"", ""yuv"");
                    } else if (format == ImageFormat.Y8) {
                        jsonSurface.put(""format"", ""y8"");
                    } else {
                        throw new ItsException(""Invalid format"");
                    }
                    jsonSurfaces.put(jsonSurface);
                }

                Map<String, CaptureResult> physicalMetadata =
                        result.getPhysicalCameraResults();
                JSONArray jsonPhysicalMetadata = new JSONArray();
                for (Map.Entry<String, CaptureResult> pair : physicalMetadata.entrySet()) {
                    JSONObject jsonOneMetadata = new JSONObject();
                    jsonOneMetadata.put(pair.getKey(), ItsSerializer.serialize(pair.getValue()));
                    jsonPhysicalMetadata.put(jsonOneMetadata);
                }
                Object objs[] = new Object[4];
                objs[0] = ""captureResults"";
                objs[1] = result;
                objs[SERIALIZER_SURFACES_ID] = jsonSurfaces;
                objs[SERIALIZER_PHYSICAL_METADATA_ID] = jsonPhysicalMetadata;
                mSerializerQueue.put(objs);
            } catch (org.json.JSONException e) {
                throw new ItsException(""JSON error: "", e);
            } catch (InterruptedException e) {
                throw new ItsException(""Interrupted: "", e);
            }
        }
    }

    public ImageReader.OnImageAvailableListener
            createAvailableListener(final CaptureCallback listener) {
        return new ImageReader.OnImageAvailableListener() {
            @Override
            public void onImageAvailable(ImageReader reader) {
                Image i = null;
                try {
                    i = reader.acquireNextImage();
                    String physicalCameraId = new String();
                    for (int idx = 0; idx < mOutputImageReaders.length; idx++) {
                        if (mOutputImageReaders[idx] == reader) {
                            physicalCameraId = mPhysicalStreamMap.get(idx);
                        }
                    }
                    listener.onCaptureAvailable(i, physicalCameraId);
                } finally {
                    if (i != null) {
                        i.close();
                    }
                }
            }
        };
    }

    private ImageReader.OnImageAvailableListener
            createAvailableListenerDropper() {
        return new ImageReader.OnImageAvailableListener() {
            @Override
            public void onImageAvailable(ImageReader reader) {
                Image i = reader.acquireNextImage();
                i.close();
            }
        };
    }

    private void doStartSensorEvents() throws ItsException {
        synchronized(mEventLock) {
            mEventsEnabled = true;
        }
        mSocketRunnableObj.sendResponse(""sensorEventsStarted"", """");
    }

    private void doCheckSensorExistence() throws ItsException {
        try {
            JSONObject obj = new JSONObject();
            obj.put(""accel"", mAccelSensor != null);
            obj.put(""mag"", mMagSensor != null);
            obj.put(""gyro"", mGyroSensor != null);
            obj.put(""vibrator"", mVibrator.hasVibrator());
            mSocketRunnableObj.sendResponse(""sensorExistence"", null, obj, null);
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        }
    }

    private void doGetSensorEvents() throws ItsException {
        synchronized(mEventLock) {
            mSocketRunnableObj.sendResponse(mEvents);
            mEvents.clear();
            mEventsEnabled = false;
        }
    }

    private void doGetProps() throws ItsException {
        mSocketRunnableObj.sendResponse(mCameraCharacteristics);
    }

    private void doGetPropsById(JSONObject params) throws ItsException {
        String[] devices;
        try {
            // Intentionally not using ItsUtils.getItsCompatibleCameraIds here so it's possible to
            // write some simple script to query camera characteristics even for devices exempted
            // from ITS today.
            devices = mCameraManager.getCameraIdList();
            if (devices == null || devices.length == 0) {
                throw new ItsException(""No camera devices"");
            }
        } catch (CameraAccessException e) {
            throw new ItsException(""Failed to get device ID list"", e);
        }

        try {
            String cameraId = params.getString(""cameraId"");
            CameraCharacteristics characteristics =
                    mCameraManager.getCameraCharacteristics(cameraId);
            mSocketRunnableObj.sendResponse(characteristics);
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        } catch (IllegalArgumentException e) {
            throw new ItsException(""Illegal argument error:"", e);
        } catch (CameraAccessException e) {
            throw new ItsException(""Access error: "", e);
        }
    }

    private void doGetCameraIds() throws ItsException {
        if (mItsCameraIdList == null) {
            mItsCameraIdList = ItsUtils.getItsCompatibleCameraIds(mCameraManager);
        }
        if (mItsCameraIdList.mCameraIdCombos.size() == 0) {
            throw new ItsException(""No camera devices"");
        }

        try {
            JSONObject obj = new JSONObject();
            JSONArray array = new JSONArray();
            for (String id : mItsCameraIdList.mCameraIdCombos) {
                array.put(id);
            }
            obj.put(""cameraIdArray"", array);
            mSocketRunnableObj.sendResponse(""cameraIds"", obj);
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        }
    }

    private static class HandlerExecutor implements Executor {
        private final Handler mHandler;

        public HandlerExecutor(Handler handler) {
            mHandler = handler;
        }

        @Override
        public void execute(Runnable runCmd) {
            mHandler.post(runCmd);
        }
    }

    private void doCheckStreamCombination(JSONObject params) throws ItsException {
        try {
            JSONObject obj = new JSONObject();
            JSONArray jsonOutputSpecs = ItsUtils.getOutputSpecs(params);
            prepareImageReadersWithOutputSpecs(jsonOutputSpecs, /*inputSize*/null,
                    /*inputFormat*/0, /*maxInputBuffers*/0, /*backgroundRequest*/false);
            int numSurfaces = mOutputImageReaders.length;
            List<OutputConfiguration> outputConfigs =
                    new ArrayList<OutputConfiguration>(numSurfaces);
            for (int i = 0; i < numSurfaces; i++) {
                OutputConfiguration config = new OutputConfiguration(
                        mOutputImageReaders[i].getSurface());
                if (mPhysicalStreamMap.get(i) != null) {
                    config.setPhysicalCameraId(mPhysicalStreamMap.get(i));
                }
                outputConfigs.add(config);
            }

            BlockingSessionCallback sessionListener = new BlockingSessionCallback();
            SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_REGULAR, outputConfigs,
                new HandlerExecutor(mCameraHandler), sessionListener);
            boolean supported = mCamera.isSessionConfigurationSupported(sessionConfig);

            String supportString = supported ? ""supportedCombination"" : ""unsupportedCombination"";
            mSocketRunnableObj.sendResponse(""streamCombinationSupport"", supportString);

        } catch (UnsupportedOperationException e) {
            mSocketRunnableObj.sendResponse(""streamCombinationSupport"", ""unsupportedOperation"");
        } catch (IllegalArgumentException e) {
            throw new ItsException(""Error checking stream combination"", e);
        } catch (CameraAccessException e) {
            throw new ItsException(""Error checking stream combination"", e);
        }
    }

    private void doCheckCameraPrivacyModeSupport() throws ItsException {
        boolean hasPrivacySupport = mSensorPrivacyManager
                .supportsSensorToggle(SensorPrivacyManager.Sensors.CAMERA);
        mSocketRunnableObj.sendResponse(""cameraPrivacyModeSupport"",
                hasPrivacySupport ? ""true"" : ""false"");
    }

    private void doCheckPerformanceClassPrimaryCamera(String cameraId) throws ItsException {
        boolean  isPerfClass = (Build.VERSION.MEDIA_PERFORMANCE_CLASS == PERFORMANCE_CLASS_S
                || Build.VERSION.MEDIA_PERFORMANCE_CLASS == PERFORMANCE_CLASS_R);

        if (mItsCameraIdList == null) {
            mItsCameraIdList = ItsUtils.getItsCompatibleCameraIds(mCameraManager);
        }
        if (mItsCameraIdList.mCameraIds.size() == 0) {
            throw new ItsException(""No camera devices"");
        }
        if (!mItsCameraIdList.mCameraIds.contains(cameraId)) {
            throw new ItsException(""Invalid cameraId "" + cameraId);
        }

        boolean isPrimaryCamera = false;
        try {
            CameraCharacteristics c = mCameraManager.getCameraCharacteristics(cameraId);
            Integer cameraFacing = c.get(CameraCharacteristics.LENS_FACING);
            for (String id : mItsCameraIdList.mCameraIds) {
                c = mCameraManager.getCameraCharacteristics(id);
                Integer facing = c.get(CameraCharacteristics.LENS_FACING);
                if (cameraFacing.equals(facing)) {
                    if (cameraId.equals(id)) {
                        isPrimaryCamera = true;
                    } else {
                        isPrimaryCamera = false;
                    }
                    break;
                }
            }
        } catch (CameraAccessException e) {
            throw new ItsException(""Failed to get camera characteristics"", e);
        }

        mSocketRunnableObj.sendResponse(""performanceClassPrimaryCamera"",
                (isPerfClass && isPrimaryCamera) ? ""true"" : ""false"");
    }

    private double invokeCameraPerformanceTest(Class testClass, String testName,
            String cameraId, String metricName) throws ItsException {
        mResults.clear();
        mCameraInstrumentation = new CameraTestInstrumentation();
        MetricListener metricListener = new MetricListener() {
            @Override
            public void onResultMetric(Metric metric) {
                mResults.add(metric);
            }
        };
        mCameraInstrumentation.initialize(this, metricListener);

        Bundle bundle = new Bundle();
        bundle.putString(""camera-id"", cameraId);
        bundle.putString(""perf-measure"", ""on"");
        bundle.putString(""perf-class-test"", ""on"");
        InstrumentationRegistry.registerInstance(mCameraInstrumentation, bundle);

        JUnitCore testRunner = new JUnitCore();
        Log.v(TAG, String.format(""Execute Test: %s#%s"", testClass.getSimpleName(), testName));
        Request request = Request.method(testClass, testName);
        Result runResult = testRunner.run(request);
        if (!runResult.wasSuccessful()) {
            throw new ItsException(""Camera PerformanceTest "" + testClass.getSimpleName() +
                    ""#"" + testName + "" failed"");
        }

        for (Metric m : mResults) {
            if (m.getMessage().equals(metricName) && m.getValues().length == 1) {
                return m.getValues()[0];
            }
        }

        throw new ItsException(""Failed to look up "" + metricName +
                "" in Camera PerformanceTest result!"");
    }

    private void doMeasureCameraLaunchMs(String cameraId) throws ItsException {
        double launchMs = invokeCameraPerformanceTest(PerformanceTest.class,
                ""testCameraLaunch"", cameraId, ""camera_launch_average_time_for_all_cameras"");
        mSocketRunnableObj.sendResponse(""cameraLaunchMs"", Double.toString(launchMs));
    }

    private void doMeasureCamera1080pJpegCaptureMs(String cameraId) throws ItsException {
        double jpegCaptureMs = invokeCameraPerformanceTest(PerformanceTest.class,
                ""testSingleCapture"", cameraId,
                ""camera_capture_average_latency_for_all_cameras_jpeg"");
        mSocketRunnableObj.sendResponse(""camera1080pJpegCaptureMs"", Double.toString(jpegCaptureMs));
    }

    private void prepareImageReaders(Size[] outputSizes, int[] outputFormats, Size inputSize,
            int inputFormat, int maxInputBuffers) {
        closeImageReaders();
        mOutputImageReaders = new ImageReader[outputSizes.length];
        for (int i = 0; i < outputSizes.length; i++) {
            // Check if the output image reader can be shared with the input image reader.
            if (outputSizes[i].equals(inputSize) && outputFormats[i] == inputFormat) {
                mOutputImageReaders[i] = ImageReader.newInstance(outputSizes[i].getWidth(),
                        outputSizes[i].getHeight(), outputFormats[i],
                        MAX_CONCURRENT_READER_BUFFERS + maxInputBuffers);
                mInputImageReader = mOutputImageReaders[i];
            } else {
                mOutputImageReaders[i] = ImageReader.newInstance(outputSizes[i].getWidth(),
                        outputSizes[i].getHeight(), outputFormats[i],
                        MAX_CONCURRENT_READER_BUFFERS);
            }
        }

        if (inputSize != null && mInputImageReader == null) {
            mInputImageReader = ImageReader.newInstance(inputSize.getWidth(), inputSize.getHeight(),
                    inputFormat, maxInputBuffers);
        }
    }

    private void closeImageReaders() {
        if (mOutputImageReaders != null) {
            for (int i = 0; i < mOutputImageReaders.length; i++) {
                if (mOutputImageReaders[i] != null) {
                    mOutputImageReaders[i].close();
                    mOutputImageReaders[i] = null;
                }
            }
        }
        if (mInputImageReader != null) {
            mInputImageReader.close();
            mInputImageReader = null;
        }
    }

    private void do3A(JSONObject params) throws ItsException {
        ThreeAResultListener threeAListener = new ThreeAResultListener();
        try {
            // Start a 3A action, and wait for it to converge.
            // Get the converged values for each ""A"", and package into JSON result for caller.

            // Configure streams on physical sub-camera if PHYSICAL_ID_KEY is specified.
            String physicalId = null;
            CameraCharacteristics c = mCameraCharacteristics;
            if (params.has(PHYSICAL_ID_KEY)) {
                physicalId = params.getString(PHYSICAL_ID_KEY);
                c = mPhysicalCameraChars.get(physicalId);
            }

            // 3A happens on full-res frames.
            Size sizes[] = ItsUtils.getYuvOutputSizes(c);
            int outputFormats[] = new int[1];
            outputFormats[0] = ImageFormat.YUV_420_888;
            Size[] outputSizes = new Size[1];
            outputSizes[0] = sizes[0];
            int width = outputSizes[0].getWidth();
            int height = outputSizes[0].getHeight();

            prepareImageReaders(outputSizes, outputFormats, /*inputSize*/null, /*inputFormat*/0,
                    /*maxInputBuffers*/0);

            List<OutputConfiguration> outputConfigs = new ArrayList<OutputConfiguration>(1);
            OutputConfiguration config =
                    new OutputConfiguration(mOutputImageReaders[0].getSurface());
            if (physicalId != null) {
                config.setPhysicalCameraId(physicalId);
            }
            outputConfigs.add(config);
            BlockingSessionCallback sessionListener = new BlockingSessionCallback();
            mCamera.createCaptureSessionByOutputConfigurations(
                    outputConfigs, sessionListener, mCameraHandler);
            mSession = sessionListener.waitAndGetSession(TIMEOUT_IDLE_MS);

            // Add a listener that just recycles buffers; they aren't saved anywhere.
            ImageReader.OnImageAvailableListener readerListener =
                    createAvailableListenerDropper();
            mOutputImageReaders[0].setOnImageAvailableListener(readerListener, mSaveHandlers[0]);

            // Get the user-specified regions for AE, AWB, AF.
            // Note that the user specifies normalized [x,y,w,h], which is converted below
            // to an [x0,y0,x1,y1] region in sensor coords. The capture request region
            // also has a fifth ""weight"" element: [x0,y0,x1,y1,w].
            // Use logical camera's active array size for 3A regions.
            Rect activeArray = mCameraCharacteristics.get(
                    CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE);
            int aaWidth = activeArray.right - activeArray.left;
            int aaHeight = activeArray.bottom - activeArray.top;
            MeteringRectangle[] regionAE = new MeteringRectangle[]{
                    new MeteringRectangle(0,0,aaWidth,aaHeight,1)};
            MeteringRectangle[] regionAF = new MeteringRectangle[]{
                    new MeteringRectangle(0,0,aaWidth,aaHeight,1)};
            MeteringRectangle[] regionAWB = new MeteringRectangle[]{
                    new MeteringRectangle(0,0,aaWidth,aaHeight,1)};
            if (params.has(REGION_KEY)) {
                JSONObject regions = params.getJSONObject(REGION_KEY);
                if (regions.has(REGION_AE_KEY)) {
                    regionAE = ItsUtils.getJsonWeightedRectsFromArray(
                            regions.getJSONArray(REGION_AE_KEY), true, aaWidth, aaHeight);
                }
                if (regions.has(REGION_AF_KEY)) {
                    regionAF = ItsUtils.getJsonWeightedRectsFromArray(
                            regions.getJSONArray(REGION_AF_KEY), true, aaWidth, aaHeight);
                }
                if (regions.has(REGION_AWB_KEY)) {
                    regionAWB = ItsUtils.getJsonWeightedRectsFromArray(
                            regions.getJSONArray(REGION_AWB_KEY), true, aaWidth, aaHeight);
                }
            }

            // An EV compensation can be specified as part of AE convergence.
            int evComp = params.optInt(EVCOMP_KEY, 0);
            if (evComp != 0) {
                Logt.i(TAG, String.format(""Running 3A with AE exposure compensation value: %d"", evComp));
            }

            // By default, AE and AF both get triggered, but the user can optionally override this.
            // Also, AF won't get triggered if the lens is fixed-focus.
            boolean doAE = true;
            boolean doAF = true;
            if (params.has(TRIGGER_KEY)) {
                JSONObject triggers = params.getJSONObject(TRIGGER_KEY);
                if (triggers.has(TRIGGER_AE_KEY)) {
                    doAE = triggers.getBoolean(TRIGGER_AE_KEY);
                }
                if (triggers.has(TRIGGER_AF_KEY)) {
                    doAF = triggers.getBoolean(TRIGGER_AF_KEY);
                }
            }
            Float minFocusDistance = c.get(
                    CameraCharacteristics.LENS_INFO_MINIMUM_FOCUS_DISTANCE);
            boolean isFixedFocusLens = minFocusDistance != null && minFocusDistance == 0.0;
            if (doAF && isFixedFocusLens) {
                // Send a fake result back for the code that is waiting for this message to see
                // that AF has converged.
                Logt.i(TAG, ""Ignoring request for AF on fixed-focus camera"");
                mSocketRunnableObj.sendResponse(""afResult"", ""0.0"");
                doAF = false;
            }

            mInterlock3A.open();
            synchronized(m3AStateLock) {
                // If AE or AWB lock is specified, then the 3A will converge first and then lock these
                // values, waiting until the HAL has reported that the lock was successful.
                mNeedsLockedAE = params.optBoolean(LOCK_AE_KEY, false);
                mNeedsLockedAWB = params.optBoolean(LOCK_AWB_KEY, false);
                mConvergedAE = false;
                mConvergedAWB = false;
                mConvergedAF = false;
                mLockedAE = false;
                mLockedAWB = false;
            }
            long tstart = System.currentTimeMillis();
            boolean triggeredAE = false;
            boolean triggeredAF = false;

            Logt.i(TAG, String.format(""Initiating 3A: AE:%d, AF:%d, AWB:1, AELOCK:%d, AWBLOCK:%d"",
                    doAE?1:0, doAF?1:0, mNeedsLockedAE?1:0, mNeedsLockedAWB?1:0));

            // Keep issuing capture requests until 3A has converged.
            while (true) {

                // Block until can take the next 3A frame. Only want one outstanding frame
                // at a time, to simplify the logic here.
                if (!mInterlock3A.block(TIMEOUT_3A * 1000) ||
                        System.currentTimeMillis() - tstart > TIMEOUT_3A * 1000) {
                    throw new ItsException(
                            ""3A failed to converge after "" + TIMEOUT_3A + "" seconds.\n"" +
                            ""AE converge state: "" + mConvergedAE + "", \n"" +
                            ""AF convergence state: "" + mConvergedAF + "", \n"" +
                            ""AWB convergence state: "" + mConvergedAWB + ""."");
                }
                mInterlock3A.close();

                synchronized(m3AStateLock) {
                    // If not converged yet, issue another capture request.
                    if (       (doAE && (!triggeredAE || !mConvergedAE))
                            || !mConvergedAWB
                            || (doAF && (!triggeredAF || !mConvergedAF))
                            || (doAE && mNeedsLockedAE && !mLockedAE)
                            || (mNeedsLockedAWB && !mLockedAWB)) {

                        // Baseline capture request for 3A.
                        CaptureRequest.Builder req = mCamera.createCaptureRequest(
                                CameraDevice.TEMPLATE_PREVIEW);
                        req.set(CaptureRequest.FLASH_MODE, CaptureRequest.FLASH_MODE_OFF);
                        req.set(CaptureRequest.CONTROL_MODE, CaptureRequest.CONTROL_MODE_AUTO);
                        req.set(CaptureRequest.CONTROL_CAPTURE_INTENT,
                                CaptureRequest.CONTROL_CAPTURE_INTENT_PREVIEW);
                        req.set(CaptureRequest.CONTROL_AE_MODE,
                                CaptureRequest.CONTROL_AE_MODE_ON);
                        req.set(CaptureRequest.CONTROL_AE_EXPOSURE_COMPENSATION, 0);
                        req.set(CaptureRequest.CONTROL_AE_LOCK, false);
                        req.set(CaptureRequest.CONTROL_AE_REGIONS, regionAE);
                        req.set(CaptureRequest.CONTROL_AF_MODE,
                                CaptureRequest.CONTROL_AF_MODE_AUTO);
                        req.set(CaptureRequest.CONTROL_AF_REGIONS, regionAF);
                        req.set(CaptureRequest.CONTROL_AWB_MODE,
                                CaptureRequest.CONTROL_AWB_MODE_AUTO);
                        req.set(CaptureRequest.CONTROL_AWB_LOCK, false);
                        req.set(CaptureRequest.CONTROL_AWB_REGIONS, regionAWB);
                        // ITS only turns OIS on when it's explicitly requested
                        req.set(CaptureRequest.LENS_OPTICAL_STABILIZATION_MODE,
                                CaptureRequest.LENS_OPTICAL_STABILIZATION_MODE_OFF);

                        if (evComp != 0) {
                            req.set(CaptureRequest.CONTROL_AE_EXPOSURE_COMPENSATION, evComp);
                        }

                        if (mConvergedAE && mNeedsLockedAE) {
                            req.set(CaptureRequest.CONTROL_AE_LOCK, true);
                        }
                        if (mConvergedAWB && mNeedsLockedAWB) {
                            req.set(CaptureRequest.CONTROL_AWB_LOCK, true);
                        }

                        boolean triggering = false;
                        // Trigger AE first.
                        if (doAE && !triggeredAE) {
                            Logt.i(TAG, ""Triggering AE"");
                            req.set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,
                                    CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_START);
                            triggeredAE = true;
                            triggering = true;
                        }

                        // After AE has converged, trigger AF.
                        if (doAF && !triggeredAF && (!doAE || (triggeredAE && mConvergedAE))) {
                            Logt.i(TAG, ""Triggering AF"");
                            req.set(CaptureRequest.CONTROL_AF_TRIGGER,
                                    CaptureRequest.CONTROL_AF_TRIGGER_START);
                            triggeredAF = true;
                            triggering = true;
                        }

                        req.addTarget(mOutputImageReaders[0].getSurface());

                        if (triggering) {
                            // Send single request for AE/AF trigger
                            mSession.capture(req.build(),
                                    threeAListener, mResultHandler);
                        } else {
                            // Use repeating request for non-trigger requests
                            mSession.setRepeatingRequest(req.build(),
                                    threeAListener, mResultHandler);
                        }
                    } else {
                        mSocketRunnableObj.sendResponse(""3aConverged"", """");
                        Logt.i(TAG, ""3A converged"");
                        break;
                    }
                }
            }
        } catch (android.hardware.camera2.CameraAccessException e) {
            throw new ItsException(""Access error: "", e);
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        } finally {
            mSocketRunnableObj.sendResponse(""3aDone"", """");
            // stop listener from updating 3A states
            threeAListener.stop();
            if (mSession != null) {
                mSession.close();
            }
        }
    }

    private void doVibrate(JSONObject params) throws ItsException {
        try {
            if (mVibrator == null) {
                throw new ItsException(""Unable to start vibrator"");
            }
            JSONArray patternArray = params.getJSONArray(VIB_PATTERN_KEY);
            int len = patternArray.length();
            long pattern[] = new long[len];
            for (int i = 0; i < len; i++) {
                pattern[i] = patternArray.getLong(i);
            }
            Logt.i(TAG, String.format(""Starting vibrator, pattern length %d"",len));

            // Mark the vibrator as alarm to test the audio restriction API
            // TODO: consider making this configurable
            AudioAttributes audioAttributes = new AudioAttributes.Builder()
                    .setUsage(AudioAttributes.USAGE_ALARM).build();
            mVibrator.vibrate(pattern, -1, audioAttributes);
            mSocketRunnableObj.sendResponse(""vibrationStarted"", """");
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        }
    }

    private void doSetAudioRestriction(JSONObject params) throws ItsException {
        try {
            if (mCamera == null) {
                throw new ItsException(""Camera is closed"");
            }
            int mode = params.getInt(AUDIO_RESTRICTION_MODE_KEY);
            mCamera.setCameraAudioRestriction(mode);
            Logt.i(TAG, String.format(""Set audio restriction mode to %d"", mode));

            mSocketRunnableObj.sendResponse(""audioRestrictionSet"", """");
        } catch (org.json.JSONException e) {
            throw new ItsException(""JSON error: "", e);
        } catch (android.hardware.camera2.CameraAccessException e) {
            throw new ItsException(""Access error: "", e);
        }
    }

    /**
     * Parse jsonOutputSpecs to get output surface sizes and formats. Create input and output
     * image readers for the parsed output surface sizes, output formats, and the given input
     * size and format.
     */
    private void prepareImageReadersWithOutputSpecs(JSONArray jsonOutputSpecs, Size inputSize,
            int inputFormat, int maxInputBuffers, boolean backgroundRequest) throws ItsException {
        Size outputSizes[];
        int outputFormats[];
        int numSurfaces = 0;
        mPhysicalStreamMap.clear();

        if (jsonOutputSpecs != null) {
            try {
                numSurfaces = jsonOutputSpecs.length();
                if (backgroundRequest) {
                    numSurfaces += 1;
                }
                if (numSurfaces > MAX_NUM_OUTPUT_SURFACES) {
                    throw new ItsException(""Too many output surfaces"");
                }

                outputSizes = new Size[numSurfaces];
                outputFormats = new int[numSurfaces];
                for (int i = 0; i < numSurfaces; i++) {
                    // Append optional background stream at the end
                    if (backgroundRequest && i == numSurfaces - 1) {
                        outputFormats[i] = ImageFormat.YUV_420_888;
                        outputSizes[i] = new Size(640, 480);
                        continue;
                    }
                    // Get the specified surface.
                    JSONObject surfaceObj = jsonOutputSpecs.getJSONObject(i);
                    String physicalCameraId = surfaceObj.optString(""physicalCamera"");
                    CameraCharacteristics cameraCharacteristics =  mCameraCharacteristics;
                    mPhysicalStreamMap.put(i, physicalCameraId);
                    if (!physicalCameraId.isEmpty()) {
                        cameraCharacteristics = mPhysicalCameraChars.get(physicalCameraId);
                    }

                    String sformat = surfaceObj.optString(""format"");
                    Size sizes[];
                    if (""yuv"".equals(sformat) || """".equals(sformat)) {
                        // Default to YUV if no format is specified.
                        outputFormats[i] = ImageFormat.YUV_420_888;
                        sizes = ItsUtils.getYuvOutputSizes(cameraCharacteristics);
                    } else if (""jpg"".equals(sformat) || ""jpeg"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.JPEG;
                        sizes = ItsUtils.getJpegOutputSizes(cameraCharacteristics);
                    } else if (""raw"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.RAW_SENSOR;
                        sizes = ItsUtils.getRaw16OutputSizes(cameraCharacteristics);
                    } else if (""raw10"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.RAW10;
                        sizes = ItsUtils.getRaw10OutputSizes(cameraCharacteristics);
                    } else if (""raw12"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.RAW12;
                        sizes = ItsUtils.getRaw12OutputSizes(cameraCharacteristics);
                    } else if (""dng"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.RAW_SENSOR;
                        sizes = ItsUtils.getRaw16OutputSizes(cameraCharacteristics);
                        mCaptureRawIsDng = true;
                    } else if (""rawStats"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.RAW_SENSOR;
                        sizes = ItsUtils.getRaw16OutputSizes(cameraCharacteristics);
                        mCaptureRawIsStats = true;
                        mCaptureStatsGridWidth = surfaceObj.optInt(""gridWidth"");
                        mCaptureStatsGridHeight = surfaceObj.optInt(""gridHeight"");
                    } else if (""y8"".equals(sformat)) {
                        outputFormats[i] = ImageFormat.Y8;
                        sizes = ItsUtils.getY8OutputSizes(cameraCharacteristics);
                    } else {
                        throw new ItsException(""Unsupported format: "" + sformat);
                    }
                    // If the size is omitted, then default to the largest allowed size for the
                    // format.
                    int width = surfaceObj.optInt(""width"");
                    int height = surfaceObj.optInt(""height"");
                    if (width <= 0) {
                        if (sizes == null || sizes.length == 0) {
                            throw new ItsException(String.format(
                                    ""Zero stream configs available for requested format: %s"",
                                    sformat));
                        }
                        width = ItsUtils.getMaxSize(sizes).getWidth();
                    }
                    if (height <= 0) {
                        height = ItsUtils.getMaxSize(sizes).getHeight();
                    }
                    // The stats computation only applies to the active array region.
                    int aaw = ItsUtils.getActiveArrayCropRegion(cameraCharacteristics).width();
                    int aah = ItsUtils.getActiveArrayCropRegion(cameraCharacteristics).height();
                    if (mCaptureStatsGridWidth <= 0 || mCaptureStatsGridWidth > aaw) {
                        mCaptureStatsGridWidth = aaw;
                    }
                    if (mCaptureStatsGridHeight <= 0 || mCaptureStatsGridHeight > aah) {
                        mCaptureStatsGridHeight = aah;
                    }

                    outputSizes[i] = new Size(width, height);
                }
            } catch (org.json.JSONException e) {
                throw new ItsException(""JSON error"", e);
            }
        } else {
            // No surface(s) specified at all.
            // Default: a single output surface which is full-res YUV.
            Size maxYuvSize = ItsUtils.getMaxOutputSize(
                    mCameraCharacteristics, ImageFormat.YUV_420_888);
            numSurfaces = backgroundRequest ? 2 : 1;

            outputSizes = new Size[numSurfaces];
            outputFormats = new int[numSurfaces];
            outputSizes[0] = maxYuvSize;
            outputFormats[0] = ImageFormat.YUV_420_888;
            if (backgroundRequest) {
                outputSizes[1] = new Size(640, 480);
                outputFormats[1] = ImageFormat.YUV_420_888;
            }
        }

        prepareImageReaders(outputSizes, outputFormats, inputSize, inputFormat, maxInputBuffers);
    }

    /**
     * Wait until mCountCallbacksRemaining is 0 or a specified amount of time has elapsed between
     * each callback.
     */
    private void waitForCallbacks(long timeoutMs) throws ItsException {
        synchronized(mCountCallbacksRemaining) {
            int currentCount = mCountCallbacksRemaining.get();
            while (currentCount > 0) {
                try {
                    mCountCallbacksRemaining.wait(timeoutMs);
                } catch (InterruptedException e) {
                    throw new ItsException(""Waiting for callbacks was interrupted."", e);
                }

                int newCount = mCountCallbacksRemaining.get();
                if (newCount == currentCount) {
                    throw new ItsException(""No callback received within timeout "" +
                            timeoutMs + ""ms"");
                }
                currentCount = newCount;
            }
        }
    }

    private void doCapture(JSONObject params) throws ItsException {
        try {
            // Parse the JSON to get the list of capture requests.
            List<CaptureRequest.Builder> requests = ItsSerializer.deserializeRequestList(
                    mCamera, params, ""captureRequests"");

            // optional background preview requests
            List<CaptureRequest.Builder> backgroundRequests = ItsSerializer.deserializeRequestList(
                    mCamera, params, ""repeatRequests"");
            boolean backgroundRequest = backgroundRequests.size() > 0;

            int numSurfaces = 0;
            int numCaptureSurfaces = 0;
            BlockingSessionCallback sessionListener = new BlockingSessionCallback();
            try {
                mCountRawOrDng.set(0);
                mCountJpg.set(0);
                mCountYuv.set(0);
                mCountRaw10.set(0);
                mCountRaw12.set(0);
                mCountCapRes.set(0);
                mCaptureRawIsDng = false;
                mCaptureRawIsStats = false;
                mCaptureResults = new CaptureResult[requests.size()];

                JSONArray jsonOutputSpecs = ItsUtils.getOutputSpecs(params);

                prepareImageReadersWithOutputSpecs(jsonOutputSpecs, /*inputSize*/null,
                        /*inputFormat*/0, /*maxInputBuffers*/0, backgroundRequest);
                numSurfaces = mOutputImageReaders.length;
                numCaptureSurfaces = numSurfaces - (backgroundRequest ? 1 : 0);

                List<OutputConfiguration> outputConfigs =
                        new ArrayList<OutputConfiguration>(numSurfaces);
                for (int i = 0; i < numSurfaces; i++) {
                    OutputConfiguration config = new OutputConfiguration(
                            mOutputImageReaders[i].getSurface());
                    if (mPhysicalStreamMap.get(i) != null) {
                        config.setPhysicalCameraId(mPhysicalStreamMap.get(i));
                    }
                    outputConfigs.add(config);
                }
                mCamera.createCaptureSessionByOutputConfigurations(outputConfigs,
                        sessionListener, mCameraHandler);
                mSession = sessionListener.waitAndGetSession(TIMEOUT_IDLE_MS);

                for (int i = 0; i < numSurfaces; i++) {
                    ImageReader.OnImageAvailableListener readerListener;
                    if (backgroundRequest && i == numSurfaces - 1) {
                        readerListener = createAvailableListenerDropper();
                    } else {
                        readerListener = createAvailableListener(mCaptureCallback);
                    }
                    mOutputImageReaders[i].setOnImageAvailableListener(readerListener,
                            mSaveHandlers[i]);
                }

                // Plan for how many callbacks need to be received throughout the duration of this
                // sequence of capture requests. There is one callback per image surface, and one
                // callback for the CaptureResult, for each capture.
                int numCaptures = requests.size();
                mCountCallbacksRemaining.set(numCaptures * (numCaptureSurfaces + 1));

            } catch (CameraAccessException e) {
                throw new ItsException(""Error configuring outputs"", e);
            }

            // Start background requests and let it warm up pipeline
            if (backgroundRequest) {
                List<CaptureRequest> bgRequestList =
                        new ArrayList<CaptureRequest>(backgroundRequests.size());
                for (int i = 0; i < backgroundRequests.size(); i++) {
                    CaptureRequest.Builder req = backgroundRequests.get(i);
                    req.addTarget(mOutputImageReaders[numCaptureSurfaces].getSurface());
                    bgRequestList.add(req.build());
                }
                mSession.setRepeatingBurst(bgRequestList, null, null);
                // warm up the pipeline
                Thread.sleep(PIPELINE_WARMUP_TIME_MS);
            }

            // Initiate the captures.
            long maxExpTimeNs = -1;
            List<CaptureRequest> requestList =
                    new ArrayList<>(requests.size());
            for (int i = 0; i < requests.size(); i++) {
                CaptureRequest.Builder req = requests.get(i);
                // For DNG captures, need the LSC map to be available.
                if (mCaptureRawIsDng) {
                    req.set(CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE, 1);
                }
                Long expTimeNs = req.get(CaptureRequest.SENSOR_EXPOSURE_TIME);
                if (expTimeNs != null && expTimeNs > maxExpTimeNs) {
                    maxExpTimeNs = expTimeNs;
                }

                for (int j = 0; j < numCaptureSurfaces; j++) {
                    req.addTarget(mOutputImageReaders[j].getSurface());
                }
                requestList.add(req.build());
            }
            mSession.captureBurst(requestList, mCaptureResultListener, mResultHandler);

            long timeout = TIMEOUT_CALLBACK * 1000;
            if (maxExpTimeNs > 0) {
                timeout += maxExpTimeNs / 1000000; // ns to ms
            }
            // Make sure all callbacks have been hit (wait until captures are done).
            // If no timeouts are received after a timeout, then fail.
            waitForCallbacks(timeout);

            // Close session and wait until session is fully closed
            mSession.close();
            sessionListener.getStateWaiter().waitForState(
                    BlockingSessionCallback.SESSION_CLOSED, TIMEOUT_SESSION_CLOSE);

        } catch (android.hardware.camera2.CameraAccessException e) {
            throw new ItsException(""Access error: "", e);
        } catch (InterruptedException e) {
            throw new ItsException(""Unexpected InterruptedException: "", e);
        }
    }

    /**
     * Perform reprocess captures.
     *
     * It takes captureRequests in a JSON object and perform capture requests in two steps:
     * regular capture request to get reprocess input and reprocess capture request to get
     * reprocess outputs.
     *
     * Regular capture requests:
     *   1. For each capture request in the JSON object, create a full-size capture request with
     *      the settings in the JSON object.
     *   2. Remember and clear noise reduction, edge enhancement, and effective exposure factor
     *      from the regular capture requests. (Those settings will be used for reprocess requests.)
     *   3. Submit the regular capture requests.
     *
     * Reprocess capture requests:
     *   4. Wait for the regular capture results and use them to create reprocess capture requests.
     *   5. Wait for the regular capture output images and queue them to the image writer.
     *   6. Set the noise reduction, edge enhancement, and effective exposure factor from #2.
     *   7. Submit the reprocess capture requests.
     *
     * The output images and results for the regular capture requests won't be written to socket.
     * The output images and results for the reprocess capture requests will be written to socket.
     */
    private void doReprocessCapture(JSONObject params) throws ItsException {
        ImageWriter imageWriter = null;
        ArrayList<Integer> noiseReductionModes = new ArrayList<>();
        ArrayList<Integer> edgeModes = new ArrayList<>();
        ArrayList<Float> effectiveExposureFactors = new ArrayList<>();

        mCountRawOrDng.set(0);
        mCountJpg.set(0);
        mCountYuv.set(0);
        mCountRaw10.set(0);
        mCountRaw12.set(0);
        mCountCapRes.set(0);
        mCaptureRawIsDng = false;
        mCaptureRawIsStats = false;

        try {
            // Parse the JSON to get the list of capture requests.
            List<CaptureRequest.Builder> inputRequests =
                    ItsSerializer.deserializeRequestList(mCamera, params, ""captureRequests"");

            // Prepare the image readers for reprocess input and reprocess outputs.
            int inputFormat = getReprocessInputFormat(params);
            Size inputSize = ItsUtils.getMaxOutputSize(mCameraCharacteristics, inputFormat);
            JSONArray jsonOutputSpecs = ItsUtils.getOutputSpecs(params);
            prepareImageReadersWithOutputSpecs(jsonOutputSpecs, inputSize, inputFormat,
                    inputRequests.size(), /*backgroundRequest*/false);

            // Prepare a reprocessable session.
            int numOutputSurfaces = mOutputImageReaders.length;
            InputConfiguration inputConfig = new InputConfiguration(inputSize.getWidth(),
                    inputSize.getHeight(), inputFormat);
            List<Surface> outputSurfaces = new ArrayList<Surface>();
            boolean addSurfaceForInput = true;
            for (int i = 0; i < numOutputSurfaces; i++) {
                outputSurfaces.add(mOutputImageReaders[i].getSurface());
                if (mOutputImageReaders[i] == mInputImageReader) {
                    // If input and one of the outputs share the same image reader, avoid
                    // adding the same surfaces twice.
                    addSurfaceForInput = false;
                }
            }

            if (addSurfaceForInput) {
                // Besides the output surfaces specified in JSON object, add an additional one
                // for reprocess input.
                outputSurfaces.add(mInputImageReader.getSurface());
            }

            BlockingSessionCallback sessionListener = new BlockingSessionCallback();
            mCamera.createReprocessableCaptureSession(inputConfig, outputSurfaces, sessionListener,
                    mCameraHandler);
            mSession = sessionListener.waitAndGetSession(TIMEOUT_IDLE_MS);

            // Create an image writer for reprocess input.
            Surface inputSurface = mSession.getInputSurface();
            imageWriter = ImageWriter.newInstance(inputSurface, inputRequests.size());

            // Set up input reader listener and capture callback listener to get
            // reprocess input buffers and the results in order to create reprocess capture
            // requests.
            ImageReaderListenerWaiter inputReaderListener = new ImageReaderListenerWaiter();
            mInputImageReader.setOnImageAvailableListener(inputReaderListener, mSaveHandlers[0]);

            CaptureCallbackWaiter captureCallbackWaiter = new CaptureCallbackWaiter();
            // Prepare the reprocess input request
            for (CaptureRequest.Builder inputReqest : inputRequests) {
                // Remember and clear noise reduction, edge enhancement, and effective exposure
                // factors.
                noiseReductionModes.add(inputReqest.get(CaptureRequest.NOISE_REDUCTION_MODE));
                edgeModes.add(inputReqest.get(CaptureRequest.EDGE_MODE));
                effectiveExposureFactors.add(inputReqest.get(
                        CaptureRequest.REPROCESS_EFFECTIVE_EXPOSURE_FACTOR));

                inputReqest.set(CaptureRequest.NOISE_REDUCTION_MODE,
                        CaptureRequest.NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG);
                inputReqest.set(CaptureRequest.EDGE_MODE, CaptureRequest.EDGE_MODE_ZERO_SHUTTER_LAG);
                inputReqest.set(CaptureRequest.REPROCESS_EFFECTIVE_EXPOSURE_FACTOR, null);
                inputReqest.addTarget(mInputImageReader.getSurface());
                mSession.capture(inputReqest.build(), captureCallbackWaiter, mResultHandler);
            }

            // Wait for reprocess input images
            ArrayList<CaptureRequest.Builder> reprocessOutputRequests = new ArrayList<>();
            for (int i = 0; i < inputRequests.size(); i++) {
                TotalCaptureResult result =
                        captureCallbackWaiter.getResult(TIMEOUT_CALLBACK * 1000);
                reprocessOutputRequests.add(mCamera.createReprocessCaptureRequest(result));
                imageWriter.queueInputImage(inputReaderListener.getImage(TIMEOUT_CALLBACK * 1000));
            }

            // Start performing reprocess captures.

            mCaptureResults = new CaptureResult[inputRequests.size()];

            // Prepare reprocess capture requests.
            for (int i = 0; i < numOutputSurfaces; i++) {
                ImageReader.OnImageAvailableListener outputReaderListener =
                        createAvailableListener(mCaptureCallback);
                mOutputImageReaders[i].setOnImageAvailableListener(outputReaderListener,
                        mSaveHandlers[i]);
            }

            // Plan for how many callbacks need to be received throughout the duration of this
            // sequence of capture requests. There is one callback per image surface, and one
            // callback for the CaptureResult, for each capture.
            int numCaptures = reprocessOutputRequests.size();
            mCountCallbacksRemaining.set(numCaptures * (numOutputSurfaces + 1));

            // Initiate the captures.
            for (int i = 0; i < reprocessOutputRequests.size(); i++) {
                CaptureRequest.Builder req = reprocessOutputRequests.get(i);
                for (ImageReader outputImageReader : mOutputImageReaders) {
                    req.addTarget(outputImageReader.getSurface());
                }

                req.set(CaptureRequest.NOISE_REDUCTION_MODE, noiseReductionModes.get(i));
                req.set(CaptureRequest.EDGE_MODE, edgeModes.get(i));
                req.set(CaptureRequest.REPROCESS_EFFECTIVE_EXPOSURE_FACTOR,
                        effectiveExposureFactors.get(i));

                mSession.capture(req.build(), mCaptureResultListener, mResultHandler);
            }

            // Make sure all callbacks have been hit (wait until captures are done).
            // If no timeouts are received after a timeout, then fail.
            waitForCallbacks(TIMEOUT_CALLBACK * 1000);
        } catch (android.hardware.camera2.CameraAccessException e) {
            throw new ItsException(""Access error: "", e);
        } finally {
            closeImageReaders();
            if (mSession != null) {
                mSession.close();
                mSession = null;
            }
            if (imageWriter != null) {
                imageWriter.close();
            }
        }
    }

    @Override
    public final void onAccuracyChanged(Sensor sensor, int accuracy) {
        Logt.i(TAG, ""Sensor "" + sensor.getName() + "" accuracy changed to "" + accuracy);
    }

    @Override
    public final void onSensorChanged(SensorEvent event) {
        synchronized(mEventLock) {
            if (mEventsEnabled) {
                MySensorEvent ev2 = new MySensorEvent();
                ev2.sensor = event.sensor;
                ev2.accuracy = event.accuracy;
                ev2.timestamp = event.timestamp;
                ev2.values = new float[event.values.length];
                System.arraycopy(event.values, 0, ev2.values, 0, event.values.length);
                mEvents.add(ev2);
            }
        }
    }

    private final CaptureCallback mCaptureCallback = new CaptureCallback() {
        @Override
        public void onCaptureAvailable(Image capture, String physicalCameraId) {
            try {
                int format = capture.getFormat();
                if (format == ImageFormat.JPEG) {
                    Logt.i(TAG, ""Received JPEG capture"");
                    byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                    ByteBuffer buf = ByteBuffer.wrap(img);
                    int count = mCountJpg.getAndIncrement();
                    mSocketRunnableObj.sendResponseCaptureBuffer(""jpegImage""+physicalCameraId, buf);
                } else if (format == ImageFormat.YUV_420_888) {
                    Logt.i(TAG, ""Received YUV capture"");
                    byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                    ByteBuffer buf = ByteBuffer.wrap(img);
                    mSocketRunnableObj.sendResponseCaptureBuffer(
                            ""yuvImage""+physicalCameraId, buf);
                } else if (format == ImageFormat.RAW10) {
                    Logt.i(TAG, ""Received RAW10 capture"");
                    byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                    ByteBuffer buf = ByteBuffer.wrap(img);
                    int count = mCountRaw10.getAndIncrement();
                    mSocketRunnableObj.sendResponseCaptureBuffer(
                            ""raw10Image""+physicalCameraId, buf);
                } else if (format == ImageFormat.RAW12) {
                    Logt.i(TAG, ""Received RAW12 capture"");
                    byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                    ByteBuffer buf = ByteBuffer.wrap(img);
                    int count = mCountRaw12.getAndIncrement();
                    mSocketRunnableObj.sendResponseCaptureBuffer(""raw12Image""+physicalCameraId, buf);
                } else if (format == ImageFormat.RAW_SENSOR) {
                    Logt.i(TAG, ""Received RAW16 capture"");
                    int count = mCountRawOrDng.getAndIncrement();
                    if (! mCaptureRawIsDng) {
                        byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                        if (! mCaptureRawIsStats) {
                            ByteBuffer buf = ByteBuffer.wrap(img);
                            mSocketRunnableObj.sendResponseCaptureBuffer(
                                    ""rawImage"" + physicalCameraId, buf);
                        } else {
                            // Compute the requested stats on the raw frame, and return the results
                            // in a new ""stats image"".
                            long startTimeMs = SystemClock.elapsedRealtime();
                            int w = capture.getWidth();
                            int h = capture.getHeight();
                            int aaw = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .width();
                            int aah = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .height();
                            int aax = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .left;
                            int aay = ItsUtils.getActiveArrayCropRegion(mCameraCharacteristics)
                                              .top;

                            if (w == aaw) {
                                aax = 0;
                            }
                            if (h == aah) {
                                aay = 0;
                            }

                            int gw = mCaptureStatsGridWidth;
                            int gh = mCaptureStatsGridHeight;
                            float[] stats = StatsImage.computeStatsImage(
                                                             img, w, h, aax, aay, aaw, aah, gw, gh);
                            long endTimeMs = SystemClock.elapsedRealtime();
                            Log.e(TAG, ""Raw stats computation takes "" + (endTimeMs - startTimeMs) + "" ms"");
                            int statsImgSize = stats.length * 4;
                            if (mSocketQueueQuota != null) {
                                mSocketQueueQuota.release(img.length);
                                mSocketQueueQuota.acquire(statsImgSize);
                            }
                            ByteBuffer bBuf = ByteBuffer.allocate(statsImgSize);
                            bBuf.order(ByteOrder.nativeOrder());
                            FloatBuffer fBuf = bBuf.asFloatBuffer();
                            fBuf.put(stats);
                            fBuf.position(0);
                            mSocketRunnableObj.sendResponseCaptureBuffer(
                                    ""rawStatsImage""+physicalCameraId, bBuf);
                        }
                    } else {
                        // Wait until the corresponding capture result is ready, up to a timeout.
                        long t0 = android.os.SystemClock.elapsedRealtime();
                        while (! mThreadExitFlag
                                && android.os.SystemClock.elapsedRealtime()-t0 < TIMEOUT_CAP_RES) {
                            if (mCaptureResults[count] != null) {
                                Logt.i(TAG, ""Writing capture as DNG"");
                                DngCreator dngCreator = new DngCreator(
                                        mCameraCharacteristics, mCaptureResults[count]);
                                ByteArrayOutputStream dngStream = new ByteArrayOutputStream();
                                dngCreator.writeImage(dngStream, capture);
                                byte[] dngArray = dngStream.toByteArray();
                                if (mSocketQueueQuota != null) {
                                    // Ideally we should acquire before allocating memory, but
                                    // here the DNG size is unknown before toByteArray call, so
                                    // we have to register the size afterward. This should still
                                    // works most of the time since all DNG images are handled by
                                    // the same handler thread, so we are at most one buffer over
                                    // the quota.
                                    mSocketQueueQuota.acquire(dngArray.length);
                                }
                                ByteBuffer dngBuf = ByteBuffer.wrap(dngArray);
                                mSocketRunnableObj.sendResponseCaptureBuffer(""dngImage"", dngBuf);
                                break;
                            } else {
                                Thread.sleep(1);
                            }
                        }
                    }
                } else if (format == ImageFormat.Y8) {
                    Logt.i(TAG, ""Received Y8 capture"");
                    byte[] img = ItsUtils.getDataFromImage(capture, mSocketQueueQuota);
                    ByteBuffer buf = ByteBuffer.wrap(img);
                    mSocketRunnableObj.sendResponseCaptureBuffer(
                            ""y8Image""+physicalCameraId, buf);
                } else {
                    throw new ItsException(""Unsupported image format: "" + format);
                }

                synchronized(mCountCallbacksRemaining) {
                    mCountCallbacksRemaining.decrementAndGet();
                    mCountCallbacksRemaining.notify();
                }
            } catch (IOException e) {
                Logt.e(TAG, ""Script error: "", e);
            } catch (InterruptedException e) {
                Logt.e(TAG, ""Script error: "", e);
            } catch (ItsException e) {
                Logt.e(TAG, ""Script error: "", e);
            }
        }
    };

    private static float r2f(Rational r) {
        return (float)r.getNumerator() / (float)r.getDenominator();
    }

    private boolean hasCapability(int capability) throws ItsException {
        int[] capabilities = mCameraCharacteristics.get(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
        if (capabilities == null) {
            throw new ItsException(""Failed to get capabilities"");
        }
        for (int c : capabilities) {
            if (c == capability) {
                return true;
            }
        }
        return false;
    }

    private String buildLogString(CaptureResult result) throws ItsException {
        StringBuilder logMsg = new StringBuilder();
        logMsg.append(String.format(
                ""Capt result: AE=%d, AF=%d, AWB=%d, "",
                result.get(CaptureResult.CONTROL_AE_STATE),
                result.get(CaptureResult.CONTROL_AF_STATE),
                result.get(CaptureResult.CONTROL_AWB_STATE)));

        boolean readSensorSettings = hasCapability(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS);

        if (readSensorSettings) {
            logMsg.append(String.format(
                    ""sens=%d, exp=%.1fms, dur=%.1fms, "",
                    result.get(CaptureResult.SENSOR_SENSITIVITY),
                    result.get(CaptureResult.SENSOR_EXPOSURE_TIME).longValue() / 1000000.0f,
                    result.get(CaptureResult.SENSOR_FRAME_DURATION).longValue() /
                                1000000.0f));
        }
        if (result.get(CaptureResult.COLOR_CORRECTION_GAINS) != null) {
            logMsg.append(String.format(
                    ""gains=[%.1f, %.1f, %.1f, %.1f], "",
                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getRed(),
                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getGreenEven(),
                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getGreenOdd(),
                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getBlue()));
        } else {
            logMsg.append(""gains=[], "");
        }
        if (result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM) != null) {
            logMsg.append(String.format(
                    ""xform=[%.1f, %.1f, %.1f, %.1f, %.1f, %.1f, %.1f, %.1f, %.1f], "",
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(0,0)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(1,0)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(2,0)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(0,1)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(1,1)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(2,1)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(0,2)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(1,2)),
                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).getElement(2,2))));
        } else {
            logMsg.append(""xform=[], "");
        }
        logMsg.append(String.format(
                ""foc=%.1f"",
                result.get(CaptureResult.LENS_FOCUS_DISTANCE)));
        return logMsg.toString();
    }

    private class ThreeAResultListener extends CaptureResultListener {
        private volatile boolean stopped = false;
        private boolean aeResultSent = false;
        private boolean awbResultSent = false;
        private boolean afResultSent = false;

        @Override
        public void onCaptureStarted(CameraCaptureSession session, CaptureRequest request,
                long timestamp, long frameNumber) {
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                TotalCaptureResult result) {
            try {
                if (stopped) {
                    return;
                }

                if (request == null || result == null) {
                    throw new ItsException(""Request/result is invalid"");
                }

                Logt.i(TAG, buildLogString(result));

                synchronized(m3AStateLock) {
                    if (result.get(CaptureResult.CONTROL_AE_STATE) != null) {
                        mConvergedAE = result.get(CaptureResult.CONTROL_AE_STATE) ==
                                                  CaptureResult.CONTROL_AE_STATE_CONVERGED ||
                                       result.get(CaptureResult.CONTROL_AE_STATE) ==
                                                  CaptureResult.CONTROL_AE_STATE_FLASH_REQUIRED ||
                                       result.get(CaptureResult.CONTROL_AE_STATE) ==
                                                  CaptureResult.CONTROL_AE_STATE_LOCKED;
                        mLockedAE = result.get(CaptureResult.CONTROL_AE_STATE) ==
                                               CaptureResult.CONTROL_AE_STATE_LOCKED;
                    }
                    if (result.get(CaptureResult.CONTROL_AF_STATE) != null) {
                        mConvergedAF = result.get(CaptureResult.CONTROL_AF_STATE) ==
                                                  CaptureResult.CONTROL_AF_STATE_FOCUSED_LOCKED;
                    }
                    if (result.get(CaptureResult.CONTROL_AWB_STATE) != null) {
                        mConvergedAWB = result.get(CaptureResult.CONTROL_AWB_STATE) ==
                                                   CaptureResult.CONTROL_AWB_STATE_CONVERGED ||
                                        result.get(CaptureResult.CONTROL_AWB_STATE) ==
                                                   CaptureResult.CONTROL_AWB_STATE_LOCKED;
                        mLockedAWB = result.get(CaptureResult.CONTROL_AWB_STATE) ==
                                                CaptureResult.CONTROL_AWB_STATE_LOCKED;
                    }

                    if (mConvergedAE && (!mNeedsLockedAE || mLockedAE) && !aeResultSent) {
                        aeResultSent = true;
                        if (result.get(CaptureResult.SENSOR_SENSITIVITY) != null
                                && result.get(CaptureResult.SENSOR_EXPOSURE_TIME) != null) {
                            mSocketRunnableObj.sendResponse(""aeResult"", String.format(""%d %d"",
                                    result.get(CaptureResult.SENSOR_SENSITIVITY).intValue(),
                                    result.get(CaptureResult.SENSOR_EXPOSURE_TIME).intValue()
                                    ));
                        } else {
                            Logt.i(TAG, String.format(
                                    ""AE converged but NULL exposure values, sensitivity:%b, expTime:%b"",
                                    result.get(CaptureResult.SENSOR_SENSITIVITY) == null,
                                    result.get(CaptureResult.SENSOR_EXPOSURE_TIME) == null));
                        }
                    }

                    if (mConvergedAF && !afResultSent) {
                        afResultSent = true;
                        if (result.get(CaptureResult.LENS_FOCUS_DISTANCE) != null) {
                            mSocketRunnableObj.sendResponse(""afResult"", String.format(""%f"",
                                    result.get(CaptureResult.LENS_FOCUS_DISTANCE)
                                    ));
                        } else {
                            Logt.i(TAG, ""AF converged but NULL focus distance values"");
                        }
                    }

                    if (mConvergedAWB && (!mNeedsLockedAWB || mLockedAWB) && !awbResultSent) {
                        awbResultSent = true;
                        if (result.get(CaptureResult.COLOR_CORRECTION_GAINS) != null
                                && result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM) != null) {
                            mSocketRunnableObj.sendResponse(""awbResult"", String.format(
                                    ""%f %f %f %f %f %f %f %f %f %f %f %f %f"",
                                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getRed(),
                                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getGreenEven(),
                                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getGreenOdd(),
                                    result.get(CaptureResult.COLOR_CORRECTION_GAINS).getBlue(),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(0,0)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(1,0)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(2,0)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(0,1)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(1,1)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(2,1)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(0,2)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(1,2)),
                                    r2f(result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM).
                                            getElement(2,2))));
                        } else {
                            Logt.i(TAG, String.format(
                                    ""AWB converged but NULL color correction values, gains:%b, ccm:%b"",
                                    result.get(CaptureResult.COLOR_CORRECTION_GAINS) == null,
                                    result.get(CaptureResult.COLOR_CORRECTION_TRANSFORM) == null));
                        }
                    }
                }

                mInterlock3A.open();
            } catch (ItsException e) {
                Logt.e(TAG, ""Script error: "", e);
            } catch (Exception e) {
                Logt.e(TAG, ""Script error: "", e);
            }
        }

        @Override
        public void onCaptureFailed(CameraCaptureSession session, CaptureRequest request,
                CaptureFailure failure) {
            Logt.e(TAG, ""Script error: capture failed"");
        }

        public void stop() {
            stopped = true;
        }
    }

    private final CaptureResultListener mCaptureResultListener = new CaptureResultListener() {
        @Override
        public void onCaptureStarted(CameraCaptureSession session, CaptureRequest request,
                long timestamp, long frameNumber) {
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                TotalCaptureResult result) {
            try {
                if (request == null || result == null) {
                    throw new ItsException(""Request/result is invalid"");
                }

                Logt.i(TAG, buildLogString(result));

                int count = mCountCapRes.getAndIncrement();
                mCaptureResults[count] = result;
                mSocketRunnableObj.sendResponseCaptureResult(mCameraCharacteristics,
                        request, result, mOutputImageReaders);
                synchronized(mCountCallbacksRemaining) {
                    mCountCallbacksRemaining.decrementAndGet();
                    mCountCallbacksRemaining.notify();
                }
            } catch (ItsException e) {
                Logt.e(TAG, ""Script error: "", e);
            } catch (Exception e) {
                Logt.e(TAG, ""Script error: "", e);
            }
        }

        @Override
        public void onCaptureFailed(CameraCaptureSession session, CaptureRequest request,
                CaptureFailure failure) {
            Logt.e(TAG, ""Script error: capture failed"");
        }
    };

    private class CaptureCallbackWaiter extends CameraCaptureSession.CaptureCallback {
        private final LinkedBlockingQueue<TotalCaptureResult> mResultQueue =
                new LinkedBlockingQueue<>();

        @Override
        public void onCaptureStarted(CameraCaptureSession session, CaptureRequest request,
                long timestamp, long frameNumber) {
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                TotalCaptureResult result) {
            try {
                mResultQueue.put(result);
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onImageAvailable"");
            }
        }

        @Override
        public void onCaptureFailed(CameraCaptureSession session, CaptureRequest request,
                CaptureFailure failure) {
            Logt.e(TAG, ""Script error: capture failed"");
        }

        public TotalCaptureResult getResult(long timeoutMs) throws ItsException {
            TotalCaptureResult result;
            try {
                result = mResultQueue.poll(timeoutMs, TimeUnit.MILLISECONDS);
            } catch (InterruptedException e) {
                throw new ItsException(e);
            }

            if (result == null) {
                throw new ItsException(""Getting an image timed out after "" + timeoutMs +
                        ""ms"");
            }

            return result;
        }
    }

    private static class ImageReaderListenerWaiter implements ImageReader.OnImageAvailableListener {
        private final LinkedBlockingQueue<Image> mImageQueue = new LinkedBlockingQueue<>();

        @Override
        public void onImageAvailable(ImageReader reader) {
            try {
                mImageQueue.put(reader.acquireNextImage());
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onImageAvailable"");
            }
        }

        public Image getImage(long timeoutMs) throws ItsException {
            Image image;
            try {
                image = mImageQueue.poll(timeoutMs, TimeUnit.MILLISECONDS);
            } catch (InterruptedException e) {
                throw new ItsException(e);
            }

            if (image == null) {
                throw new ItsException(""Getting an image timed out after "" + timeoutMs +
                        ""ms"");
            }
            return image;
        }
    }

    private int getReprocessInputFormat(JSONObject params) throws ItsException {
        String reprocessFormat;
        try {
            reprocessFormat = params.getString(""reprocessFormat"");
        } catch (org.json.JSONException e) {
            throw new ItsException(""Error parsing reprocess format: "" + e);
        }

        if (reprocessFormat.equals(""yuv"")) {
            return ImageFormat.YUV_420_888;
        } else if (reprocessFormat.equals(""private"")) {
            return ImageFormat.PRIVATE;
        }

        throw new ItsException(""Uknown reprocess format: "" + reprocessFormat);
    }
}"	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraExtensionCharacteristicsTest"	"testExtensionSizes"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CameraExtensionCharacteristicsTest.java"	""	"public void testExtensionSizes() throws Exception {
        for (String id : mTestRule.getCameraIdsUnderTest()) {
            StaticMetadata staticMeta =
                    new StaticMetadata(mTestRule.getCameraManager().getCameraCharacteristics(id));
            if (!staticMeta.isColorOutputSupported()) {
                continue;
            }
            CameraExtensionCharacteristics extensionChars =
                    mTestRule.getCameraManager().getCameraExtensionCharacteristics(id);
            List<Integer> supportedExtensions = extensionChars.getSupportedExtensions();
            for (Integer extension : supportedExtensions) {
                verifySupportedSizes(extensionChars, id, extension, SurfaceTexture.class);
                verifySupportedSizes(extensionChars, id, extension, ImageFormat.JPEG);
            }
        }
    }"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraExtensionCharacteristicsTest"	"testExtensionLatencyRanges"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CameraExtensionCharacteristicsTest.java"	""	"public void testExtensionLatencyRanges() throws Exception {
        final int testFormat = ImageFormat.JPEG;
        for (String id : mTestRule.getCameraIdsUnderTest()) {
            StaticMetadata staticMeta =
                    new StaticMetadata(mTestRule.getCameraManager().getCameraCharacteristics(id));
            if (!staticMeta.isColorOutputSupported()) {
                continue;
            }

            CameraExtensionCharacteristics chars =
                    mTestRule.getCameraManager().getCameraExtensionCharacteristics(id);
            List<Integer> supportedExtensions = chars.getSupportedExtensions();
            for (Integer extension : supportedExtensions) {
                List<Size> extensionSizes = chars.getExtensionSupportedSizes(extension, testFormat);
                for (Size sz : extensionSizes) {
                    Range<Long> latencyRange = chars.getEstimatedCaptureLatencyRangeMillis(
                            extension, sz, testFormat);
                    if (latencyRange != null) {
                        assertTrue(""Negative range surface type"", (latencyRange.getLower() > 0) &&
                                (latencyRange.getUpper() > 0));
                        assertTrue(""Lower range value must be smaller compared to the upper"",
                                (latencyRange.getLower() < latencyRange.getUpper()));
                    }
                }
            }
        }
    }
}"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RobustnessTest"	"testMandatoryMaximumResolutionOutputCombinations"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RobustnessTest.java"	""	"public void testMandatoryMaximumResolutionOutputCombinations() throws Exception {
        testMandatoryOutputCombinations(/*maxResolution*/ true);
    }

    private void testMandatoryStreamCombination(String cameraId, StaticMetadata staticInfo,
            String physicalCameraId, MandatoryStreamCombination combination) throws Exception {
        // Check whether substituting YUV_888 format with Y8 format
        boolean substituteY8 = false;
        if (staticInfo.isMonochromeWithY8()) {
            List<MandatoryStreamInformation> streamsInfo = combination.getStreamsInformation();
            for (MandatoryStreamInformation streamInfo : streamsInfo) {
                if (streamInfo.getFormat() == ImageFormat.YUV_420_888) {
                    substituteY8 = true;
                    break;
                }
            }
        }

        // Check whether substituting JPEG format with HEIC format
        boolean substituteHeic = false;
        if (staticInfo.isHeicSupported()) {
            List<MandatoryStreamInformation> streamsInfo = combination.getStreamsInformation();
            for (MandatoryStreamInformation streamInfo : streamsInfo) {
                if (streamInfo.getFormat() == ImageFormat.JPEG) {
                    substituteHeic = true;
                    break;
                }
            }
        }

        // Test camera output combination
        String log = ""Testing mandatory stream combination: "" + combination.getDescription() +
                "" on camera: "" + cameraId;
        if (physicalCameraId != null) {
            log += "", physical sub-camera: "" + physicalCameraId;
        }
        Log.i(TAG, log);
        testMandatoryStreamCombination(cameraId, staticInfo, physicalCameraId, combination,
                /*substituteY8*/false, /*substituteHeic*/false, /*maxResolution*/false);

        if (substituteY8) {
            Log.i(TAG, log + "" with Y8"");
            testMandatoryStreamCombination(cameraId, staticInfo, physicalCameraId, combination,
                    /*substituteY8*/true, /*substituteHeic*/false, /*maxResolution*/false);
        }

        if (substituteHeic) {
            Log.i(TAG, log + "" with HEIC"");
            testMandatoryStreamCombination(cameraId, staticInfo, physicalCameraId, combination,
                    /*substituteY8*/false, /*substituteHeic*/true, /**maxResolution*/ false);
        }
    }

    private void testMandatoryStreamCombination(String cameraId,
            StaticMetadata staticInfo, String physicalCameraId,
            MandatoryStreamCombination combination,
            boolean substituteY8, boolean substituteHeic, boolean ultraHighResolution)
            throws Exception {

        // Timeout is relaxed by 1 second for LEGACY devices to reduce false positive rate in CTS
        // TODO: This needs to be adjusted based on feedback
        final int TIMEOUT_MULTIPLIER = ultraHighResolution ? 2 : 1;
        final int TIMEOUT_FOR_RESULT_MS =
                ((staticInfo.isHardwareLevelLegacy()) ? 2000 : 1000) * TIMEOUT_MULTIPLIER;
        final int MIN_RESULT_COUNT = 3;

        // Set up outputs
        List<OutputConfiguration> outputConfigs = new ArrayList<>();
        List<Surface> outputSurfaces = new ArrayList<Surface>();
        List<Surface> uhOutputSurfaces = new ArrayList<Surface>();
        StreamCombinationTargets targets = new StreamCombinationTargets();

        CameraTestUtils.setupConfigurationTargets(combination.getStreamsInformation(),
                targets, outputConfigs, outputSurfaces, uhOutputSurfaces, MIN_RESULT_COUNT,
                substituteY8, substituteHeic, physicalCameraId, /*multiResStreamConfig*/null,
                mHandler);

        boolean haveSession = false;
        try {
            CaptureRequest.Builder requestBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
            CaptureRequest.Builder uhRequestBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);

            for (Surface s : outputSurfaces) {
                requestBuilder.addTarget(s);
            }

            for (Surface s : uhOutputSurfaces) {
                uhRequestBuilder.addTarget(s);
            }
            // We need to explicitly set the sensor pixel mode to default since we're mixing default
            // and max resolution requests in the same capture session.
            requestBuilder.set(CaptureRequest.SENSOR_PIXEL_MODE,
                    CameraMetadata.SENSOR_PIXEL_MODE_DEFAULT);
            if (ultraHighResolution) {
                uhRequestBuilder.set(CaptureRequest.SENSOR_PIXEL_MODE,
                        CameraMetadata.SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION);
            }
            CameraCaptureSession.CaptureCallback mockCaptureCallback =
                    mock(CameraCaptureSession.CaptureCallback.class);

            if (physicalCameraId == null) {
                checkSessionConfigurationSupported(mCamera, mHandler, outputConfigs,
                        /*inputConfig*/ null, SessionConfiguration.SESSION_REGULAR,
                        true/*defaultSupport*/, String.format(
                        ""Session configuration query from combination: %s failed"",
                        combination.getDescription()));
            } else {
                SessionConfigSupport sessionConfigSupport = isSessionConfigSupported(
                        mCamera, mHandler, outputConfigs, /*inputConfig*/ null,
                        SessionConfiguration.SESSION_REGULAR, false/*defaultSupport*/);
                assertTrue(
                        String.format(""Session configuration query from combination: %s failed"",
                        combination.getDescription()), !sessionConfigSupport.error);
                if (!sessionConfigSupport.callSupported) {
                    return;
                }
                assertTrue(
                        String.format(""Session configuration must be supported for combination: "" +
                        ""%s"", combination.getDescription()), sessionConfigSupport.configSupported);
            }

            createSessionByConfigs(outputConfigs);
            haveSession = true;
            CaptureRequest request = requestBuilder.build();
            CaptureRequest uhRequest = uhRequestBuilder.build();
            mCameraSession.setRepeatingRequest(request, mockCaptureCallback, mHandler);
            if (ultraHighResolution) {
                mCameraSession.capture(uhRequest, mockCaptureCallback, mHandler);
            }
            verify(mockCaptureCallback,
                    timeout(TIMEOUT_FOR_RESULT_MS * MIN_RESULT_COUNT).atLeast(MIN_RESULT_COUNT))
                    .onCaptureCompleted(
                        eq(mCameraSession),
                        eq(request),
                        isA(TotalCaptureResult.class));
           if (ultraHighResolution) {
                verify(mockCaptureCallback,
                        timeout(TIMEOUT_FOR_RESULT_MS).atLeast(1))
                        .onCaptureCompleted(
                            eq(mCameraSession),
                            eq(uhRequest),
                            isA(TotalCaptureResult.class));
            }

            verify(mockCaptureCallback, never()).
                    onCaptureFailed(
                        eq(mCameraSession),
                        eq(request),
                        isA(CaptureFailure.class));

        } catch (Throwable e) {
            mCollector.addMessage(String.format(""Mandatory stream combination: %s failed due: %s"",
                    combination.getDescription(), e.getMessage()));
        }
        if (haveSession) {
            try {
                Log.i(TAG, String.format(""Done with camera %s, combination: %s, closing session"",
                                cameraId, combination.getDescription()));
                stopCapture(/*fast*/false);
            } catch (Throwable e) {
                mCollector.addMessage(
                    String.format(""Closing down for combination: %s failed due to: %s"",
                            combination.getDescription(), e.getMessage()));
            }
        }

        targets.close();
    }

    /**
     * Test for making sure the required reprocess input/output combinations for each hardware
     * level and capability work as expected.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RobustnessTest"	"testMandatoryMaximumResolutionReprocessConfigurations"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RobustnessTest.java"	""	"public void testMandatoryMaximumResolutionReprocessConfigurations() throws Exception {
        testMandatoryReprocessConfigurations(/*maxResolution*/true);
    }

    /**
     * Test for making sure the required reprocess input/output combinations for each hardware
     * level and capability work as expected.
     */
    public void testMandatoryReprocessConfigurations(boolean maxResolution) throws Exception {
        for (String id : mCameraIdsUnderTest) {
            openDevice(id);
            CameraCharacteristics chars = mStaticInfo.getCharacteristics();
            if (maxResolution && !CameraTestUtils.hasCapability(
                  chars, CameraMetadata.REQUEST_AVAILABLE_CAPABILITIES_REMOSAIC_REPROCESSING)) {
                Log.i(TAG, ""Camera id "" + id + ""doesn't support REMOSAIC_REPROCESSING, skip test"");
                closeDevice(id);
                continue;
            }
            CameraCharacteristics.Key<MandatoryStreamCombination []> ck =
                    CameraCharacteristics.SCALER_MANDATORY_STREAM_COMBINATIONS;

            if (maxResolution) {
                ck = CameraCharacteristics.SCALER_MANDATORY_MAXIMUM_RESOLUTION_STREAM_COMBINATIONS;
            }

            MandatoryStreamCombination[] combinations = chars.get(ck);
            if (combinations == null) {
                Log.i(TAG, ""No mandatory stream combinations for camera: "" + id + "" skip test"");
                closeDevice(id);
                continue;
            }

            try {
                for (MandatoryStreamCombination combination : combinations) {
                    if (combination.isReprocessable()) {
                        Log.i(TAG, ""Testing mandatory reprocessable stream combination: "" +
                                combination.getDescription() + "" on camera: "" + id);
                        testMandatoryReprocessableStreamCombination(id, combination, maxResolution);
                    }
                }
            } finally {
                closeDevice(id);
            }
        }
    }

    private void testMandatoryReprocessableStreamCombination(String cameraId,
            MandatoryStreamCombination combination, boolean maxResolution)  throws Exception {
        // Test reprocess stream combination
        testMandatoryReprocessableStreamCombination(cameraId, combination,
                /*substituteY8*/false, /*substituteHeic*/false, maxResolution/*maxResolution*/);
        if (maxResolution) {
            // Maximum resolution mode doesn't guarantee HEIC and Y8 streams.
            return;
        }

        // Test substituting YUV_888 format with Y8 format in reprocess stream combination.
        if (mStaticInfo.isMonochromeWithY8()) {
            List<MandatoryStreamInformation> streamsInfo = combination.getStreamsInformation();
            boolean substituteY8 = false;
            for (MandatoryStreamInformation streamInfo : streamsInfo) {
                if (streamInfo.getFormat() == ImageFormat.YUV_420_888) {
                    substituteY8 = true;
                }
            }
            if (substituteY8) {
                testMandatoryReprocessableStreamCombination(cameraId, combination,
                        /*substituteY8*/true, /*substituteHeic*/false, false/*maxResolution*/);
            }
        }

        if (mStaticInfo.isHeicSupported()) {
            List<MandatoryStreamInformation> streamsInfo = combination.getStreamsInformation();
            boolean substituteHeic = false;
            for (MandatoryStreamInformation streamInfo : streamsInfo) {
                if (streamInfo.getFormat() == ImageFormat.JPEG) {
                    substituteHeic = true;
                }
            }
            if (substituteHeic) {
                testMandatoryReprocessableStreamCombination(cameraId, combination,
                        /*substituteY8*/false, /*substituteHeic*/true, false/*maxResolution*/);
            }
        }
    }

    private void testMandatoryReprocessableStreamCombination(String cameraId,
            MandatoryStreamCombination combination, boolean substituteY8,
            boolean substituteHeic, boolean maxResolution) throws Exception {

        final int TIMEOUT_MULTIPLIER = maxResolution ? 2 : 1;
        final int TIMEOUT_FOR_RESULT_MS = 5000 * TIMEOUT_MULTIPLIER;
        final int NUM_REPROCESS_CAPTURES_PER_CONFIG = 3;

        StreamCombinationTargets targets = new StreamCombinationTargets();
        ArrayList<Surface> defaultOutputSurfaces = new ArrayList<>();
        ArrayList<Surface> allOutputSurfaces = new ArrayList<>();
        List<OutputConfiguration> outputConfigs = new ArrayList<>();
        List<Surface> uhOutputSurfaces = new ArrayList<Surface>();
        ImageReader inputReader = null;
        ImageWriter inputWriter = null;
        SimpleImageReaderListener inputReaderListener = new SimpleImageReaderListener();
        SimpleCaptureCallback inputCaptureListener = new SimpleCaptureCallback();
        SimpleCaptureCallback reprocessOutputCaptureListener = new SimpleCaptureCallback();

        List<MandatoryStreamInformation> streamInfo = combination.getStreamsInformation();
        assertTrue(""Reprocessable stream combinations should have at least 3 or more streams"",
                (streamInfo != null) && (streamInfo.size() >= 3));

        assertTrue(""The first mandatory stream information in a reprocessable combination must "" +
                ""always be input"", streamInfo.get(0).isInput());

        List<Size> inputSizes = streamInfo.get(0).getAvailableSizes();
        int inputFormat = streamInfo.get(0).getFormat();
        if (substituteY8 && (inputFormat == ImageFormat.YUV_420_888)) {
            inputFormat = ImageFormat.Y8;
        }

        Log.i(TAG, ""testMandatoryReprocessableStreamCombination: "" +
                combination.getDescription() + "", substituteY8 = "" + substituteY8 +
                "", substituteHeic = "" + substituteHeic);
        try {
            // The second stream information entry is the ZSL stream, which is configured
            // separately.
            List<MandatoryStreamInformation> mandatoryStreamInfos = null;
            mandatoryStreamInfos = new ArrayList<MandatoryStreamInformation>();
            mandatoryStreamInfos = streamInfo.subList(2, streamInfo.size());
            CameraTestUtils.setupConfigurationTargets(mandatoryStreamInfos, targets,
                    outputConfigs, defaultOutputSurfaces, uhOutputSurfaces,
                    NUM_REPROCESS_CAPTURES_PER_CONFIG,
                    substituteY8, substituteHeic, null/*overridePhysicalCameraId*/,
                    /*multiResStreamConfig*/null, mHandler);
            allOutputSurfaces.addAll(defaultOutputSurfaces);
            allOutputSurfaces.addAll(uhOutputSurfaces);
            InputConfiguration inputConfig = new InputConfiguration(inputSizes.get(0).getWidth(),
                    inputSizes.get(0).getHeight(), inputFormat);

            // For each config, YUV and JPEG outputs will be tested. (For YUV/Y8 reprocessing,
            // the YUV/Y8 ImageReader for input is also used for output.)
            final boolean inputIsYuv = inputConfig.getFormat() == ImageFormat.YUV_420_888;
            final boolean inputIsY8 = inputConfig.getFormat() == ImageFormat.Y8;
            final boolean useYuv = inputIsYuv || targets.mYuvTargets.size() > 0;
            final boolean useY8 = inputIsY8 || targets.mY8Targets.size() > 0;
            final int totalNumReprocessCaptures =  NUM_REPROCESS_CAPTURES_PER_CONFIG *
                    (maxResolution ? 1 : (((inputIsYuv || inputIsY8) ? 1 : 0) +
                    (substituteHeic ? targets.mHeicTargets.size() : targets.mJpegTargets.size()) +
                    (useYuv ? targets.mYuvTargets.size() : targets.mY8Targets.size())));

            // It needs 1 input buffer for each reprocess capture + the number of buffers
            // that will be used as outputs.
            inputReader = ImageReader.newInstance(inputConfig.getWidth(), inputConfig.getHeight(),
                    inputConfig.getFormat(),
                    totalNumReprocessCaptures + NUM_REPROCESS_CAPTURES_PER_CONFIG);
            inputReader.setOnImageAvailableListener(inputReaderListener, mHandler);
            allOutputSurfaces.add(inputReader.getSurface());

            checkSessionConfigurationWithSurfaces(mCamera, mHandler, allOutputSurfaces,
                    inputConfig, SessionConfiguration.SESSION_REGULAR, /*defaultSupport*/ true,
                    String.format(""Session configuration query %s failed"",
                    combination.getDescription()));

            // Verify we can create a reprocessable session with the input and all outputs.
            BlockingSessionCallback sessionListener = new BlockingSessionCallback();
            CameraCaptureSession session = configureReprocessableCameraSession(mCamera,
                    inputConfig, allOutputSurfaces, sessionListener, mHandler);
            inputWriter = ImageWriter.newInstance(session.getInputSurface(),
                    totalNumReprocessCaptures);

            // Prepare a request for reprocess input
            CaptureRequest.Builder builder = mCamera.createCaptureRequest(
                    CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG);
            builder.addTarget(inputReader.getSurface());
            if (maxResolution) {
                builder.set(CaptureRequest.SENSOR_PIXEL_MODE,
                        CameraMetadata.SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION);
            }

            for (int i = 0; i < totalNumReprocessCaptures; i++) {
                session.capture(builder.build(), inputCaptureListener, mHandler);
            }

            List<CaptureRequest> reprocessRequests = new ArrayList<>();
            List<Surface> reprocessOutputs = new ArrayList<>();

            if (maxResolution) {
                if (uhOutputSurfaces.size() == 0) { // RAW -> RAW reprocessing
                    reprocessOutputs.add(inputReader.getSurface());
                } else {
                    for (Surface surface : uhOutputSurfaces) {
                        reprocessOutputs.add(surface);
                    }
                }
            } else {
                if (inputIsYuv || inputIsY8) {
                    reprocessOutputs.add(inputReader.getSurface());
                }

                for (ImageReader reader : targets.mJpegTargets) {
                    reprocessOutputs.add(reader.getSurface());
                }

                for (ImageReader reader : targets.mHeicTargets) {
                    reprocessOutputs.add(reader.getSurface());
                }

                for (ImageReader reader : targets.mYuvTargets) {
                    reprocessOutputs.add(reader.getSurface());
                }

                for (ImageReader reader : targets.mY8Targets) {
                    reprocessOutputs.add(reader.getSurface());
                }
            }

            for (int i = 0; i < NUM_REPROCESS_CAPTURES_PER_CONFIG; i++) {
                for (Surface output : reprocessOutputs) {
                    TotalCaptureResult result = inputCaptureListener.getTotalCaptureResult(
                            TIMEOUT_FOR_RESULT_MS);
                    builder =  mCamera.createReprocessCaptureRequest(result);
                    inputWriter.queueInputImage(
                            inputReaderListener.getImage(TIMEOUT_FOR_RESULT_MS));
                    builder.addTarget(output);
                    reprocessRequests.add(builder.build());
                }
            }

            session.captureBurst(reprocessRequests, reprocessOutputCaptureListener, mHandler);

            for (int i = 0; i < reprocessOutputs.size() * NUM_REPROCESS_CAPTURES_PER_CONFIG; i++) {
                TotalCaptureResult result = reprocessOutputCaptureListener.getTotalCaptureResult(
                        TIMEOUT_FOR_RESULT_MS);
            }
        } catch (Throwable e) {
            mCollector.addMessage(String.format(""Reprocess stream combination %s failed due to: %s"",
                    combination.getDescription(), e.getMessage()));
        } finally {
            inputReaderListener.drain();
            reprocessOutputCaptureListener.drain();
            targets.close();

            if (inputReader != null) {
                inputReader.close();
            }

            if (inputWriter != null) {
                inputWriter.close();
            }
        }
    }"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RobustnessTest"	"testVerifyMandatoryOutputCombinationTables"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RobustnessTest.java"	""	"public void testVerifyMandatoryOutputCombinationTables() throws Exception {
       final int[][] LEGACY_COMBINATIONS = {
            // Simple preview, GPU video processing, or no-preview video recording
            {PRIV, MAXIMUM},
            // No-viewfinder still image capture
            {JPEG, MAXIMUM},
            // In-application video/image processing
            {YUV,  MAXIMUM},
            // Standard still imaging.
            {PRIV, PREVIEW,  JPEG, MAXIMUM},
            // In-app processing plus still capture.
            {YUV,  PREVIEW,  JPEG, MAXIMUM},
            // Standard recording.
            {PRIV, PREVIEW,  PRIV, PREVIEW},
            // Preview plus in-app processing.
            {PRIV, PREVIEW,  YUV,  PREVIEW},
            // Still capture plus in-app processing.
            {PRIV, PREVIEW,  YUV,  PREVIEW,  JPEG, MAXIMUM}
        };

        final int[][] LIMITED_COMBINATIONS = {
            // High-resolution video recording with preview.
            {PRIV, PREVIEW,  PRIV, RECORD },
            // High-resolution in-app video processing with preview.
            {PRIV, PREVIEW,  YUV , RECORD },
            // Two-input in-app video processing.
            {YUV , PREVIEW,  YUV , RECORD },
            // High-resolution recording with video snapshot.
            {PRIV, PREVIEW,  PRIV, RECORD,   JPEG, RECORD  },
            // High-resolution in-app processing with video snapshot.
            {PRIV, PREVIEW,  YUV,  RECORD,   JPEG, RECORD  },
            // Two-input in-app processing with still capture.
            {YUV , PREVIEW,  YUV,  PREVIEW,  JPEG, MAXIMUM }
        };

        final int[][] BURST_COMBINATIONS = {
            // Maximum-resolution GPU processing with preview.
            {PRIV, PREVIEW,  PRIV, MAXIMUM },
            // Maximum-resolution in-app processing with preview.
            {PRIV, PREVIEW,  YUV,  MAXIMUM },
            // Maximum-resolution two-input in-app processing.
            {YUV,  PREVIEW,  YUV,  MAXIMUM },
        };

        final int[][] FULL_COMBINATIONS = {
            // Video recording with maximum-size video snapshot.
            {PRIV, PREVIEW,  PRIV, PREVIEW,  JPEG, MAXIMUM },
            // Standard video recording plus maximum-resolution in-app processing.
            {YUV,  VGA,      PRIV, PREVIEW,  YUV,  MAXIMUM },
            // Preview plus two-input maximum-resolution in-app processing.
            {YUV,  VGA,      YUV,  PREVIEW,  YUV,  MAXIMUM }
        };

        final int[][] RAW_COMBINATIONS = {
            // No-preview DNG capture.
            {RAW,  MAXIMUM },
            // Standard DNG capture.
            {PRIV, PREVIEW,  RAW,  MAXIMUM },
            // In-app processing plus DNG capture.
            {YUV,  PREVIEW,  RAW,  MAXIMUM },
            // Video recording with DNG capture.
            {PRIV, PREVIEW,  PRIV, PREVIEW,  RAW, MAXIMUM},
            // Preview with in-app processing and DNG capture.
            {PRIV, PREVIEW,  YUV,  PREVIEW,  RAW, MAXIMUM},
            // Two-input in-app processing plus DNG capture.
            {YUV,  PREVIEW,  YUV,  PREVIEW,  RAW, MAXIMUM},
            // Still capture with simultaneous JPEG and DNG.
            {PRIV, PREVIEW,  JPEG, MAXIMUM,  RAW, MAXIMUM},
            // In-app processing with simultaneous JPEG and DNG.
            {YUV,  PREVIEW,  JPEG, MAXIMUM,  RAW, MAXIMUM}
        };

        final int[][] LEVEL_3_COMBINATIONS = {
            // In-app viewfinder analysis with dynamic selection of output format
            {PRIV, PREVIEW, PRIV, VGA, YUV, MAXIMUM, RAW, MAXIMUM},
            // In-app viewfinder analysis with dynamic selection of output format
            {PRIV, PREVIEW, PRIV, VGA, JPEG, MAXIMUM, RAW, MAXIMUM}
        };

        final int[][][] TABLES =
                { LEGACY_COMBINATIONS, LIMITED_COMBINATIONS, BURST_COMBINATIONS, FULL_COMBINATIONS,
                  RAW_COMBINATIONS, LEVEL_3_COMBINATIONS };

        validityCheckConfigurationTables(TABLES);

        for (String id : mCameraIdsUnderTest) {
            openDevice(id);
            MandatoryStreamCombination[] combinations =
                    mStaticInfo.getCharacteristics().get(
                            CameraCharacteristics.SCALER_MANDATORY_STREAM_COMBINATIONS);
            if ((combinations == null) || (combinations.length == 0)) {
                Log.i(TAG, ""No mandatory stream combinations for camera: "" + id + "" skip test"");
                closeDevice(id);
                continue;
            }

            MaxStreamSizes maxSizes = new MaxStreamSizes(mStaticInfo, id, mContext);
            try {
                if (mStaticInfo.isColorOutputSupported()) {
                    for (int[] c : LEGACY_COMBINATIONS) {
                        assertTrue(String.format(""Expected static stream combination: %s not "" +
                                    ""found among the available mandatory combinations"",
                                    maxSizes.combinationToString(c)),
                                isMandatoryCombinationAvailable(c, maxSizes, combinations));
                    }
                }

                if (!mStaticInfo.isHardwareLevelLegacy()) {
                    if (mStaticInfo.isColorOutputSupported()) {
                        for (int[] c : LIMITED_COMBINATIONS) {
                            assertTrue(String.format(""Expected static stream combination: %s not "" +
                                        ""found among the available mandatory combinations"",
                                        maxSizes.combinationToString(c)),
                                    isMandatoryCombinationAvailable(c, maxSizes, combinations));
                        }
                    }

                    if (mStaticInfo.isCapabilitySupported(
                            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_BURST_CAPTURE)) {
                        for (int[] c : BURST_COMBINATIONS) {
                            assertTrue(String.format(""Expected static stream combination: %s not "" +
                                        ""found among the available mandatory combinations"",
                                        maxSizes.combinationToString(c)),
                                    isMandatoryCombinationAvailable(c, maxSizes, combinations));
                        }
                    }

                    if (mStaticInfo.isHardwareLevelAtLeastFull()) {
                        for (int[] c : FULL_COMBINATIONS) {
                            assertTrue(String.format(""Expected static stream combination: %s not "" +
                                        ""found among the available mandatory combinations"",
                                        maxSizes.combinationToString(c)),
                                    isMandatoryCombinationAvailable(c, maxSizes, combinations));
                        }
                    }

                    if (mStaticInfo.isCapabilitySupported(
                            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                        for (int[] c : RAW_COMBINATIONS) {
                            assertTrue(String.format(""Expected static stream combination: %s not "" +
                                        ""found among the available mandatory combinations"",
                                        maxSizes.combinationToString(c)),
                                    isMandatoryCombinationAvailable(c, maxSizes, combinations));
                        }
                    }

                    if (mStaticInfo.isHardwareLevelAtLeast(
                            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_3)) {
                        for (int[] c: LEVEL_3_COMBINATIONS) {
                            assertTrue(String.format(""Expected static stream combination: %s not "" +
                                        ""found among the available mandatory combinations"",
                                        maxSizes.combinationToString(c)),
                                    isMandatoryCombinationAvailable(c, maxSizes, combinations));
                        }
                    }
                }
            } finally {
                closeDevice(id);
            }
        }
    }

    /**
     * Test for making sure that all expected reprocessable mandatory stream combinations are
     * present and advertised accordingly.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RobustnessTest"	"testVerifyReprocessMandatoryOutputCombinationTables"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RobustnessTest.java"	""	"public void testVerifyReprocessMandatoryOutputCombinationTables() throws Exception {
        final int[][] LIMITED_COMBINATIONS = {
            // Input           Outputs
            {PRIV, MAXIMUM,    JPEG, MAXIMUM},
            {YUV , MAXIMUM,    JPEG, MAXIMUM},
            {PRIV, MAXIMUM,    PRIV, PREVIEW, JPEG, MAXIMUM},
            {YUV , MAXIMUM,    PRIV, PREVIEW, JPEG, MAXIMUM},
            {PRIV, MAXIMUM,    YUV , PREVIEW, JPEG, MAXIMUM},
            {YUV , MAXIMUM,    YUV , PREVIEW, JPEG, MAXIMUM},
            {PRIV, MAXIMUM,    YUV , PREVIEW, YUV , PREVIEW, JPEG, MAXIMUM},
            {YUV,  MAXIMUM,    YUV , PREVIEW, YUV , PREVIEW, JPEG, MAXIMUM},
        };

        final int[][] FULL_COMBINATIONS = {
            // Input           Outputs
            {YUV , MAXIMUM,    PRIV, PREVIEW},
            {YUV , MAXIMUM,    YUV , PREVIEW},
            {PRIV, MAXIMUM,    PRIV, PREVIEW, YUV , RECORD},
            {YUV , MAXIMUM,    PRIV, PREVIEW, YUV , RECORD},
            {PRIV, MAXIMUM,    PRIV, PREVIEW, YUV , MAXIMUM},
            {PRIV, MAXIMUM,    YUV , PREVIEW, YUV , MAXIMUM},
            {PRIV, MAXIMUM,    PRIV, PREVIEW, YUV , PREVIEW, JPEG, MAXIMUM},
            {YUV , MAXIMUM,    PRIV, PREVIEW, YUV , PREVIEW, JPEG, MAXIMUM},
        };

        final int[][] RAW_COMBINATIONS = {
            // Input           Outputs
            {PRIV, MAXIMUM,    YUV , PREVIEW, RAW , MAXIMUM},
            {YUV , MAXIMUM,    YUV , PREVIEW, RAW , MAXIMUM},
            {PRIV, MAXIMUM,    PRIV, PREVIEW, YUV , PREVIEW, RAW , MAXIMUM},
            {YUV , MAXIMUM,    PRIV, PREVIEW, YUV , PREVIEW, RAW , MAXIMUM},
            {PRIV, MAXIMUM,    YUV , PREVIEW, YUV , PREVIEW, RAW , MAXIMUM},
            {YUV , MAXIMUM,    YUV , PREVIEW, YUV , PREVIEW, RAW , MAXIMUM},
            {PRIV, MAXIMUM,    PRIV, PREVIEW, JPEG, MAXIMUM, RAW , MAXIMUM},
            {YUV , MAXIMUM,    PRIV, PREVIEW, JPEG, MAXIMUM, RAW , MAXIMUM},
            {PRIV, MAXIMUM,    YUV , PREVIEW, JPEG, MAXIMUM, RAW , MAXIMUM},
            {YUV , MAXIMUM,    YUV , PREVIEW, JPEG, MAXIMUM, RAW , MAXIMUM},
        };

        final int[][] LEVEL_3_COMBINATIONS = {
            // Input          Outputs
            // In-app viewfinder analysis with YUV->YUV ZSL and RAW
            {YUV , MAXIMUM,   PRIV, PREVIEW, PRIV, VGA, RAW, MAXIMUM},
            // In-app viewfinder analysis with PRIV->JPEG ZSL and RAW
            {PRIV, MAXIMUM,   PRIV, PREVIEW, PRIV, VGA, RAW, MAXIMUM, JPEG, MAXIMUM},
            // In-app viewfinder analysis with YUV->JPEG ZSL and RAW
            {YUV , MAXIMUM,   PRIV, PREVIEW, PRIV, VGA, RAW, MAXIMUM, JPEG, MAXIMUM},
        };

        final int[][][] TABLES =
                { LIMITED_COMBINATIONS, FULL_COMBINATIONS, RAW_COMBINATIONS, LEVEL_3_COMBINATIONS };

        validityCheckConfigurationTables(TABLES);

        for (String id : mCameraIdsUnderTest) {
            openDevice(id);
            MandatoryStreamCombination[] cs = mStaticInfo.getCharacteristics().get(
                    CameraCharacteristics.SCALER_MANDATORY_STREAM_COMBINATIONS);
            if ((cs == null) || (cs.length == 0)) {
                Log.i(TAG, ""No mandatory stream combinations for camera: "" + id + "" skip test"");
                closeDevice(id);
                continue;
            }

            boolean supportYuvReprocess = mStaticInfo.isCapabilitySupported(
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING);
            boolean supportOpaqueReprocess = mStaticInfo.isCapabilitySupported(
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING);
            if (!supportYuvReprocess && !supportOpaqueReprocess) {
                Log.i(TAG, ""No reprocess support for camera: "" + id + "" skip test"");
                closeDevice(id);
                continue;
            }

            MaxStreamSizes maxSizes = new MaxStreamSizes(mStaticInfo, id, mContext);
            try {
                for (int[] c : LIMITED_COMBINATIONS) {
                    assertTrue(String.format(""Expected static reprocessable stream combination:"" +
                                ""%s not found among the available mandatory combinations"",
                                maxSizes.reprocessCombinationToString(c)),
                            isMandatoryCombinationAvailable(c, maxSizes, /*isInput*/ true, cs));
                }

                if (mStaticInfo.isHardwareLevelAtLeastFull()) {
                    for (int[] c : FULL_COMBINATIONS) {
                        assertTrue(String.format(
                                    ""Expected static reprocessable stream combination:"" +
                                    ""%s not found among the available mandatory combinations"",
                                    maxSizes.reprocessCombinationToString(c)),
                                isMandatoryCombinationAvailable(c, maxSizes, /*isInput*/ true, cs));
                    }
                }

                if (mStaticInfo.isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                    for (int[] c : RAW_COMBINATIONS) {
                        assertTrue(String.format(
                                    ""Expected static reprocessable stream combination:"" +
                                    ""%s not found among the available mandatory combinations"",
                                    maxSizes.reprocessCombinationToString(c)),
                                isMandatoryCombinationAvailable(c, maxSizes, /*isInput*/ true, cs));
                    }
                }

                if (mStaticInfo.isHardwareLevelAtLeast(
                            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_3)) {
                    for (int[] c : LEVEL_3_COMBINATIONS) {
                        assertTrue(String.format(
                                    ""Expected static reprocessable stream combination:"" +
                                    ""%s not found among the available mandatory combinations"",
                                    maxSizes.reprocessCombinationToString(c)),
                                isMandatoryCombinationAvailable(c, maxSizes, /*isInput*/ true, cs));
                    }
                }
            } finally {
                closeDevice(id);
            }
        }
    }

    private boolean isMandatoryCombinationAvailable(final int[] combination,
            final MaxStreamSizes maxSizes,
            final MandatoryStreamCombination[] availableCombinations) {
        return isMandatoryCombinationAvailable(combination, maxSizes, /*isInput*/ false,
                availableCombinations);
    }

    private boolean isMandatoryCombinationAvailable(final int[] combination,
            final MaxStreamSizes maxSizes, boolean isInput,
            final MandatoryStreamCombination[] availableCombinations) {
        boolean supportYuvReprocess = mStaticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING);
        boolean supportOpaqueReprocess = mStaticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING);
        // Static combinations to be verified can be composed of multiple entries
        // that have the following layout (format, size). In case ""isInput"" is set,
        // the first stream configuration entry will contain the input format and size
        // as well as the first matching output.
        int streamCount = combination.length / 2;

        List<Pair<Pair<Integer, Boolean>, Size>> currentCombination =
                new ArrayList<Pair<Pair<Integer, Boolean>, Size>>(streamCount);
        for (int i = 0; i < combination.length; i += 2) {
            if (isInput && (i == 0)) {
                // Skip the combination if the format is not supported for reprocessing.
                if ((combination[i] == YUV && !supportYuvReprocess) ||
                        (combination[i] == PRIV && !supportOpaqueReprocess)) {
                    return true;
                }
                Size sz = maxSizes.getMaxInputSizeForFormat(combination[i]);
                currentCombination.add(Pair.create(Pair.create(new Integer(combination[i]),
                            new Boolean(true)), sz));
                currentCombination.add(Pair.create(Pair.create(new Integer(combination[i]),
                            new Boolean(false)), sz));
            } else {
                Size sz = maxSizes.getOutputSizeForFormat(combination[i], combination[i+1]);
                currentCombination.add(Pair.create(Pair.create(new Integer(combination[i]),
                            new Boolean(false)), sz));
            }
        }

        for (MandatoryStreamCombination c : availableCombinations) {
            List<MandatoryStreamInformation> streamInfoList = c.getStreamsInformation();
            if ((streamInfoList.size() == currentCombination.size()) &&
                    (isInput == c.isReprocessable())) {
                ArrayList<Pair<Pair<Integer, Boolean>, Size>> expected =
                        new ArrayList<Pair<Pair<Integer, Boolean>, Size>>(currentCombination);

                for (MandatoryStreamInformation streamInfo : streamInfoList) {
                    Size maxSize = CameraTestUtils.getMaxSize(
                            streamInfo.getAvailableSizes().toArray(new Size[0]));
                    Pair p = Pair.create(Pair.create(new Integer(streamInfo.getFormat()),
                            new Boolean(streamInfo.isInput())), maxSize);
                    if (expected.contains(p)) {
                        expected.remove(p);
                    }
                }

                if (expected.isEmpty()) {
                    return true;
                }
            }
        }

        return false;
    }

    /**
     * Verify correctness of the configuration tables.
     */
    private void validityCheckConfigurationTables(final int[][][] tables) throws Exception {
        int tableIdx = 0;
        for (int[][] table : tables) {
            int rowIdx = 0;
            for (int[] row : table) {
                assertTrue(String.format(""Odd number of entries for table %d row %d: %s "",
                                tableIdx, rowIdx, Arrays.toString(row)),
                        (row.length % 2) == 0);
                for (int i = 0; i < row.length; i += 2) {
                    int format = row[i];
                    int maxSize = row[i + 1];
                    assertTrue(String.format(""table %d row %d index %d format not valid: %d"",
                                    tableIdx, rowIdx, i, format),
                            format == PRIV || format == JPEG || format == YUV || format == RAW);
                    assertTrue(String.format(""table %d row %d index %d max size not valid: %d"",
                                    tableIdx, rowIdx, i + 1, maxSize),
                            maxSize == PREVIEW || maxSize == RECORD ||
                            maxSize == MAXIMUM || maxSize == VGA);
                }
                rowIdx++;
            }
            tableIdx++;
        }
    }

    /**
     * Simple holder for resolutions to use for different camera outputs and size limits.
     */
    static class MaxStreamSizes {
        // Format shorthands
        static final int PRIV = ImageFormat.PRIVATE;
        static final int JPEG = ImageFormat.JPEG;
        static final int YUV  = ImageFormat.YUV_420_888;
        static final int RAW  = ImageFormat.RAW_SENSOR;
        static final int Y8   = ImageFormat.Y8;
        static final int HEIC = ImageFormat.HEIC;

        // Max resolution indices
        static final int PREVIEW = 0;
        static final int RECORD  = 1;
        static final int MAXIMUM = 2;
        static final int VGA = 3;
        static final int VGA_FULL_FOV = 4;
        static final int MAX_30FPS = 5;
        static final int RESOLUTION_COUNT = 6;

        static final long FRAME_DURATION_30FPS_NSEC = (long) 1e9 / 30;

        public MaxStreamSizes(StaticMetadata sm, String cameraId, Context context) {
            Size[] privSizes = sm.getAvailableSizesForFormatChecked(ImageFormat.PRIVATE,
                    StaticMetadata.StreamDirection.Output, /*fastSizes*/true, /*slowSizes*/false);
            Size[] yuvSizes = sm.getAvailableSizesForFormatChecked(ImageFormat.YUV_420_888,
                    StaticMetadata.StreamDirection.Output, /*fastSizes*/true, /*slowSizes*/false);

            Size[] y8Sizes = sm.getAvailableSizesForFormatChecked(ImageFormat.Y8,
                    StaticMetadata.StreamDirection.Output, /*fastSizes*/true, /*slowSizes*/false);
            Size[] jpegSizes = sm.getAvailableSizesForFormatChecked(ImageFormat.JPEG,
                    StaticMetadata.StreamDirection.Output, /*fastSizes*/true, /*slowSizes*/false);
            Size[] rawSizes = sm.getAvailableSizesForFormatChecked(ImageFormat.RAW_SENSOR,
                    StaticMetadata.StreamDirection.Output, /*fastSizes*/true, /*slowSizes*/false);
            Size[] heicSizes = sm.getAvailableSizesForFormatChecked(ImageFormat.HEIC,
                    StaticMetadata.StreamDirection.Output, /*fastSizes*/true, /*slowSizes*/false);

            Size maxPreviewSize = getMaxPreviewSize(context, cameraId);

            maxRawSize = (rawSizes.length != 0) ? CameraTestUtils.getMaxSize(rawSizes) : null;

            StreamConfigurationMap configs = sm.getCharacteristics().get(
                    CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            if (sm.isColorOutputSupported()) {
                maxPrivSizes[PREVIEW] = getMaxSize(privSizes, maxPreviewSize);
                maxYuvSizes[PREVIEW]  = getMaxSize(yuvSizes, maxPreviewSize);
                maxJpegSizes[PREVIEW] = getMaxSize(jpegSizes, maxPreviewSize);

                if (sm.isExternalCamera()) {
                    maxPrivSizes[RECORD] = getMaxExternalRecordingSize(cameraId, configs);
                    maxYuvSizes[RECORD]  = getMaxExternalRecordingSize(cameraId, configs);
                    maxJpegSizes[RECORD] = getMaxExternalRecordingSize(cameraId, configs);
                } else {
                    maxPrivSizes[RECORD] = getMaxRecordingSize(cameraId);
                    maxYuvSizes[RECORD]  = getMaxRecordingSize(cameraId);
                    maxJpegSizes[RECORD] = getMaxRecordingSize(cameraId);
                }

                maxPrivSizes[MAXIMUM] = CameraTestUtils.getMaxSize(privSizes);
                maxYuvSizes[MAXIMUM] = CameraTestUtils.getMaxSize(yuvSizes);
                maxJpegSizes[MAXIMUM] = CameraTestUtils.getMaxSize(jpegSizes);

                // Must always be supported, add unconditionally
                final Size vgaSize = new Size(640, 480);
                maxPrivSizes[VGA] = vgaSize;
                maxYuvSizes[VGA] = vgaSize;
                maxJpegSizes[VGA] = vgaSize;

                if (sm.isMonochromeWithY8()) {
                    maxY8Sizes[PREVIEW]  = getMaxSize(y8Sizes, maxPreviewSize);
                    if (sm.isExternalCamera()) {
                        maxY8Sizes[RECORD]  = getMaxExternalRecordingSize(cameraId, configs);
                    } else {
                        maxY8Sizes[RECORD]  = getMaxRecordingSize(cameraId);
                    }
                    maxY8Sizes[MAXIMUM] = CameraTestUtils.getMaxSize(y8Sizes);
                    maxY8Sizes[VGA] = vgaSize;
                }

                if (sm.isHeicSupported()) {
                    maxHeicSizes[PREVIEW] = getMaxSize(heicSizes, maxPreviewSize);
                    maxHeicSizes[RECORD] = getMaxRecordingSize(cameraId);
                    maxHeicSizes[MAXIMUM] = CameraTestUtils.getMaxSize(heicSizes);
                    maxHeicSizes[VGA] = vgaSize;
                }
            }
            if (sm.isColorOutputSupported() && !sm.isHardwareLevelLegacy()) {
                // VGA resolution, but with aspect ratio matching full res FOV
                float fullFovAspect = maxYuvSizes[MAXIMUM].getWidth() /
                    (float) maxYuvSizes[MAXIMUM].getHeight();
                Size vgaFullFovSize = new Size(640, (int) (640 / fullFovAspect));

                maxPrivSizes[VGA_FULL_FOV] = vgaFullFovSize;
                maxYuvSizes[VGA_FULL_FOV] = vgaFullFovSize;
                maxJpegSizes[VGA_FULL_FOV] = vgaFullFovSize;
                if (sm.isMonochromeWithY8()) {
                    maxY8Sizes[VGA_FULL_FOV] = vgaFullFovSize;
                }

                // Max resolution that runs at 30fps

                Size maxPriv30fpsSize = null;
                Size maxYuv30fpsSize = null;
                Size maxY830fpsSize = null;
                Size maxJpeg30fpsSize = null;
                Comparator<Size> comparator = new SizeComparator();
                for (Map.Entry<Size, Long> e :
                             sm.getAvailableMinFrameDurationsForFormatChecked(ImageFormat.PRIVATE).
                             entrySet()) {
                    Size s = e.getKey();
                    Long minDuration = e.getValue();
                    Log.d(TAG, String.format(""Priv Size: %s, duration %d limit %d"", s, minDuration,
                                FRAME_DURATION_30FPS_NSEC));
                    if (minDuration <= FRAME_DURATION_30FPS_NSEC) {
                        if (maxPriv30fpsSize == null ||
                                comparator.compare(maxPriv30fpsSize, s) < 0) {
                            maxPriv30fpsSize = s;
                        }
                    }
                }
                assertTrue(""No PRIVATE resolution available at 30fps!"", maxPriv30fpsSize != null);

                for (Map.Entry<Size, Long> e :
                             sm.getAvailableMinFrameDurationsForFormatChecked(
                                     ImageFormat.YUV_420_888).
                             entrySet()) {
                    Size s = e.getKey();
                    Long minDuration = e.getValue();
                    Log.d(TAG, String.format(""YUV Size: %s, duration %d limit %d"", s, minDuration,
                                FRAME_DURATION_30FPS_NSEC));
                    if (minDuration <= FRAME_DURATION_30FPS_NSEC) {
                        if (maxYuv30fpsSize == null ||
                                comparator.compare(maxYuv30fpsSize, s) < 0) {
                            maxYuv30fpsSize = s;
                        }
                    }
                }
                assertTrue(""No YUV_420_888 resolution available at 30fps!"",
                        maxYuv30fpsSize != null);

                if (sm.isMonochromeWithY8()) {
                    for (Map.Entry<Size, Long> e :
                                 sm.getAvailableMinFrameDurationsForFormatChecked(
                                         ImageFormat.Y8).
                                 entrySet()) {
                        Size s = e.getKey();
                        Long minDuration = e.getValue();
                        Log.d(TAG, String.format(""Y8 Size: %s, duration %d limit %d"",
                                s, minDuration, FRAME_DURATION_30FPS_NSEC));
                        if (minDuration <= FRAME_DURATION_30FPS_NSEC) {
                            if (maxY830fpsSize == null ||
                                    comparator.compare(maxY830fpsSize, s) < 0) {
                                maxY830fpsSize = s;
                            }
                        }
                    }
                    assertTrue(""No Y8 resolution available at 30fps!"", maxY830fpsSize != null);
                }

                for (Map.Entry<Size, Long> e :
                             sm.getAvailableMinFrameDurationsForFormatChecked(ImageFormat.JPEG).
                             entrySet()) {
                    Size s = e.getKey();
                    Long minDuration = e.getValue();
                    Log.d(TAG, String.format(""JPEG Size: %s, duration %d limit %d"", s, minDuration,
                                FRAME_DURATION_30FPS_NSEC));
                    if (minDuration <= FRAME_DURATION_30FPS_NSEC) {
                        if (maxJpeg30fpsSize == null ||
                                comparator.compare(maxJpeg30fpsSize, s) < 0) {
                            maxJpeg30fpsSize = s;
                        }
                    }
                }
                assertTrue(""No JPEG resolution available at 30fps!"", maxJpeg30fpsSize != null);

                maxPrivSizes[MAX_30FPS] = maxPriv30fpsSize;
                maxYuvSizes[MAX_30FPS] = maxYuv30fpsSize;
                maxY8Sizes[MAX_30FPS] = maxY830fpsSize;
                maxJpegSizes[MAX_30FPS] = maxJpeg30fpsSize;
            }

            Size[] privInputSizes = configs.getInputSizes(ImageFormat.PRIVATE);
            maxInputPrivSize = privInputSizes != null ?
                    CameraTestUtils.getMaxSize(privInputSizes) : null;
            Size[] yuvInputSizes = configs.getInputSizes(ImageFormat.YUV_420_888);
            maxInputYuvSize = yuvInputSizes != null ?
                    CameraTestUtils.getMaxSize(yuvInputSizes) : null;
            Size[] y8InputSizes = configs.getInputSizes(ImageFormat.Y8);
            maxInputY8Size = y8InputSizes != null ?
                    CameraTestUtils.getMaxSize(y8InputSizes) : null;
        }

        private final Size[] maxPrivSizes = new Size[RESOLUTION_COUNT];
        private final Size[] maxJpegSizes = new Size[RESOLUTION_COUNT];
        private final Size[] maxYuvSizes = new Size[RESOLUTION_COUNT];
        private final Size[] maxY8Sizes = new Size[RESOLUTION_COUNT];
        private final Size[] maxHeicSizes = new Size[RESOLUTION_COUNT];
        private final Size maxRawSize;
        // TODO: support non maximum reprocess input.
        private final Size maxInputPrivSize;
        private final Size maxInputYuvSize;
        private final Size maxInputY8Size;

        public final Size getOutputSizeForFormat(int format, int resolutionIndex) {
            if (resolutionIndex >= RESOLUTION_COUNT) {
                return new Size(0, 0);
            }

            switch (format) {
                case PRIV:
                    return maxPrivSizes[resolutionIndex];
                case YUV:
                    return maxYuvSizes[resolutionIndex];
                case JPEG:
                    return maxJpegSizes[resolutionIndex];
                case Y8:
                    return maxY8Sizes[resolutionIndex];
                case HEIC:
                    return maxHeicSizes[resolutionIndex];
                case RAW:
                    return maxRawSize;
                default:
                    return new Size(0, 0);
            }
        }

        public final Size getMaxInputSizeForFormat(int format) {
            switch (format) {
                case PRIV:
                    return maxInputPrivSize;
                case YUV:
                    return maxInputYuvSize;
                case Y8:
                    return maxInputY8Size;
                default:
                    return new Size(0, 0);
            }
        }

        static public String combinationToString(int[] combination) {
            StringBuilder b = new StringBuilder(""{ "");
            for (int i = 0; i < combination.length; i += 2) {
                int format = combination[i];
                int sizeLimit = combination[i + 1];

                appendFormatSize(b, format, sizeLimit);
                b.append("" "");
            }
            b.append(""}"");
            return b.toString();
        }

        static public String reprocessCombinationToString(int[] reprocessCombination) {
            // reprocessConfig[0..1] is the input configuration
            StringBuilder b = new StringBuilder(""Input: "");
            appendFormatSize(b, reprocessCombination[0], reprocessCombination[1]);

            // reprocessCombnation[0..1] is also output combination to be captured as reprocess
            // input.
            b.append("", Outputs: { "");
            for (int i = 0; i < reprocessCombination.length; i += 2) {
                int format = reprocessCombination[i];
                int sizeLimit = reprocessCombination[i + 1];

                appendFormatSize(b, format, sizeLimit);
                b.append("" "");
            }
            b.append(""}"");
            return b.toString();
        }

        static private void appendFormatSize(StringBuilder b, int format, int Size) {
            switch (format) {
                case PRIV:
                    b.append(""[PRIV, "");
                    break;
                case JPEG:
                    b.append(""[JPEG, "");
                    break;
                case YUV:
                    b.append(""[YUV, "");
                    break;
                case Y8:
                    b.append(""[Y8, "");
                    break;
                case RAW:
                    b.append(""[RAW, "");
                    break;
                default:
                    b.append(""[UNK, "");
                    break;
            }

            switch (Size) {
                case PREVIEW:
                    b.append(""PREVIEW]"");
                    break;
                case RECORD:
                    b.append(""RECORD]"");
                    break;
                case MAXIMUM:
                    b.append(""MAXIMUM]"");
                    break;
                case VGA:
                    b.append(""VGA]"");
                    break;
                case VGA_FULL_FOV:
                    b.append(""VGA_FULL_FOV]"");
                    break;
                case MAX_30FPS:
                    b.append(""MAX_30FPS]"");
                    break;
                default:
                    b.append(""UNK]"");
                    break;
            }
        }
    }

    private static Size getMaxRecordingSize(String cameraId) {
        int id = Integer.valueOf(cameraId);

        int quality =
                CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_2160P) ?
                    CamcorderProfile.QUALITY_2160P :
                CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_1080P) ?
                    CamcorderProfile.QUALITY_1080P :
                CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_720P) ?
                    CamcorderProfile.QUALITY_720P :
                CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_480P) ?
                    CamcorderProfile.QUALITY_480P :
                CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_QVGA) ?
                    CamcorderProfile.QUALITY_QVGA :
                CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_CIF) ?
                    CamcorderProfile.QUALITY_CIF :
                CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_QCIF) ?
                    CamcorderProfile.QUALITY_QCIF :
                    -1;

        assertTrue(""No recording supported for camera id "" + cameraId, quality != -1);

        CamcorderProfile maxProfile = CamcorderProfile.get(id, quality);
        return new Size(maxProfile.videoFrameWidth, maxProfile.videoFrameHeight);
    }

    private static Size getMaxExternalRecordingSize(
            String cameraId, StreamConfigurationMap config) {
        final Size FULLHD = new Size(1920, 1080);

        Size[] videoSizeArr = config.getOutputSizes(android.media.MediaRecorder.class);
        List<Size> sizes = new ArrayList<Size>();
        for (Size sz: videoSizeArr) {
            if (sz.getWidth() <= FULLHD.getWidth() && sz.getHeight() <= FULLHD.getHeight()) {
                sizes.add(sz);
            }
        }
        List<Size> videoSizes = getAscendingOrderSizes(sizes, /*ascending*/false);
        for (Size sz : videoSizes) {
            long minFrameDuration = config.getOutputMinFrameDuration(
                    android.media.MediaRecorder.class, sz);
            // Give some margin for rounding error
            if (minFrameDuration > (1e9 / 30.1)) {
                Log.i(TAG, ""External camera "" + cameraId + "" has max video size:"" + sz);
                return sz;
            }
        }
        fail(""Camera "" + cameraId + "" does not support any 30fps video output"");
        return FULLHD; // doesn't matter what size is returned here
    }

    /**
     * Get maximum size in list that's equal or smaller to than the bound.
     * Returns null if no size is smaller than or equal to the bound.
     */
    private static Size getMaxSize(Size[] sizes, Size bound) {
        if (sizes == null || sizes.length == 0) {
            throw new IllegalArgumentException(""sizes was empty"");
        }

        Size sz = null;
        for (Size size : sizes) {
            if (size.getWidth() <= bound.getWidth() && size.getHeight() <= bound.getHeight()) {

                if (sz == null) {
                    sz = size;
                } else {
                    long curArea = sz.getWidth() * (long) sz.getHeight();
                    long newArea = size.getWidth() * (long) size.getHeight();
                    if ( newArea > curArea ) {
                        sz = size;
                    }
                }
            }
        }

        assertTrue(""No size under bound found: "" + Arrays.toString(sizes) + "" bound "" + bound,
                sz != null);

        return sz;
    }

    private static Size getMaxPreviewSize(Context context, String cameraId) {
        try {
            WindowManager windowManager =
                (WindowManager) context.getSystemService(Context.WINDOW_SERVICE);
            Display display = windowManager.getDefaultDisplay();

            int width = display.getWidth();
            int height = display.getHeight();

            if (height > width) {
                height = width;
                width = display.getHeight();
            }

            CameraManager camMgr =
                (CameraManager) context.getSystemService(Context.CAMERA_SERVICE);
            List<Size> orderedPreviewSizes = CameraTestUtils.getSupportedPreviewSizes(
                cameraId, camMgr, PREVIEW_SIZE_BOUND);

            if (orderedPreviewSizes != null) {
                for (Size size : orderedPreviewSizes) {
                    if (width >= size.getWidth() &&
                        height >= size.getHeight())
                        return size;
                }
            }
        } catch (Exception e) {
            Log.e(TAG, ""getMaxPreviewSize Failed. ""+e.toString());
        }
        return PREVIEW_SIZE_BOUND;
    }
}"	""	""	"JPEG 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionReprocessCaptureTest"	"testMultiResolutionReprocessCharacteristics"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionReprocessCaptureTest.java"	""	"public void testMultiResolutionReprocessCharacteristics() {
        for (String id : mCameraIdsUnderTest) {
            if (VERBOSE) {
                Log.v(TAG, ""Testing multi-resolution reprocess characteristics for Camera "" + id);
            }
            StaticMetadata info = mAllStaticInfo.get(id);
            CameraCharacteristics c = info.getCharacteristics();
            StreamConfigurationMap config = c.get(
                    CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            int[] inputFormats = config.getInputFormats();
            int[] capabilities = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
            boolean isLogicalCamera = CameraTestUtils.contains(capabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_LOGICAL_MULTI_CAMERA);
            boolean isUltraHighResCamera = info.isUltraHighResolutionSensor();
            Set<String> physicalCameraIds = c.getPhysicalCameraIds();

            MultiResolutionStreamConfigurationMap multiResolutionMap = c.get(
                    CameraCharacteristics.SCALER_MULTI_RESOLUTION_STREAM_CONFIGURATION_MAP);
            if (multiResolutionMap == null) {
                Log.i(TAG, ""Camera "" + id + "" doesn't support multi-resolution reprocessing."");
                continue;
            }
            if (VERBOSE) {
                Log.v(TAG, ""MULTI_RESOLUTION_STREAM_CONFIGURATION_MAP: ""
                        + multiResolutionMap.toString());
            }

            // Find multi-resolution input and output formats
            int[] multiResolutionInputFormats = multiResolutionMap.getInputFormats();
            int[] multiResolutionOutputFormats = multiResolutionMap.getOutputFormats();

            assertTrue(""Camera "" + id + "" must be a logical multi-camera or ultra high res camera ""
                    + ""to support multi-resolution reprocessing."",
                    isLogicalCamera || isUltraHighResCamera);

            for (int format : multiResolutionInputFormats) {
                assertTrue(String.format(""Camera %s: multi-resolution input format %d ""
                        + ""isn't a supported format"", id, format),
                        CameraTestUtils.contains(inputFormats, format));

                Collection<MultiResolutionStreamInfo> multiResolutionStreams =
                        multiResolutionMap.getInputInfo(format);
                assertTrue(String.format(""Camera %s supports %d multi-resolution ""
                        + ""input stream info, expected at least 2"", id,
                        multiResolutionStreams.size()),
                        multiResolutionStreams.size() >= 2);

                // Make sure that each multi-resolution input stream info has the maximum size
                // for that format.
                for (MultiResolutionStreamInfo streamInfo : multiResolutionStreams) {
                    String physicalCameraId = streamInfo.getPhysicalCameraId();
                    Size streamSize = new Size(streamInfo.getWidth(), streamInfo.getHeight());
                    if (!isLogicalCamera) {
                        assertTrue(""Camera "" + id + "" is ultra high resolution camera, but ""
                                + ""the multi-resolution reprocessing stream info camera Id ""
                                + physicalCameraId + "" doesn't match"",
                                physicalCameraId.equals(id));
                    } else {
                        assertTrue(""Camera "" + id + ""'s multi-resolution input info ""
                                + ""physical camera id "" + physicalCameraId + "" isn't valid"",
                                physicalCameraIds.contains(physicalCameraId));
                    }

                    StaticMetadata pInfo = mAllStaticInfo.get(physicalCameraId);
                    CameraCharacteristics pChar = pInfo.getCharacteristics();
                    StreamConfigurationMap pConfig = pChar.get(
                            CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
                    Size[] sizes = pConfig.getInputSizes(format);

                    assertTrue(String.format(""Camera %s must ""
                            + ""support at least one input size for multi-resolution input ""
                            + ""format %d."", physicalCameraId, format),
                             sizes != null && sizes.length > 0);

                    List<Size> maxSizes = new ArrayList<Size>();
                    maxSizes.add(CameraTestUtils.getMaxSize(sizes));
                    StreamConfigurationMap pMaxResConfig = pChar.get(CameraCharacteristics.
                            SCALER_STREAM_CONFIGURATION_MAP_MAXIMUM_RESOLUTION);
                    if (pMaxResConfig != null) {
                        Size[] maxResSizes = pMaxResConfig.getInputSizes(format);
                        if (maxResSizes != null && maxResSizes.length > 0) {
                            maxSizes.add(CameraTestUtils.getMaxSize(maxResSizes));
                        }
                    }

                    assertTrue(String.format(""Camera %s's supported multi-resolution""
                           + "" input size %s for physical camera %s is not one of the largest ""
                           + ""supported input sizes %s for format %d"", id, streamSize,
                           physicalCameraId, maxSizes, format), maxSizes.contains(streamSize));
                }
            }

            // YUV reprocessing capabilities check
            if (CameraTestUtils.contains(multiResolutionOutputFormats, ImageFormat.YUV_422_888) &&
                    CameraTestUtils.contains(multiResolutionInputFormats,
                    ImageFormat.YUV_420_888)) {
                assertTrue(""The camera device must have YUV_REPROCESSING capability if it ""
                        + ""supports multi-resolution YUV input and YUV output"",
                        CameraTestUtils.contains(capabilities,
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING));

                assertTrue(""The camera device must supports multi-resolution JPEG output if ""
                        + ""supports multi-resolution YUV input and YUV output"",
                        CameraTestUtils.contains(multiResolutionOutputFormats, ImageFormat.JPEG));
            }

            // OPAQUE reprocessing capabilities check
            if (CameraTestUtils.contains(multiResolutionOutputFormats, ImageFormat.PRIVATE) &&
                    CameraTestUtils.contains(multiResolutionInputFormats, ImageFormat.PRIVATE)) {
                assertTrue(""The camera device must have PRIVATE_REPROCESSING capability if it ""
                        + ""supports multi-resolution PRIVATE input and PRIVATE output"",
                        CameraTestUtils.contains(capabilities,
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING));

                assertTrue(""The camera device must supports multi-resolution JPEG output if ""
                        + ""supports multi-resolution PRIVATE input and PRIVATE output"",
                        CameraTestUtils.contains(multiResolutionOutputFormats, ImageFormat.JPEG));
                assertTrue(""The camera device must supports multi-resolution YUV output if ""
                        + ""supports multi-resolution PRIVATE input and PRIVATE output"",
                        CameraTestUtils.contains(multiResolutionOutputFormats,
                        ImageFormat.YUV_420_888));
            }
        }
    }

    /**
     * Test YUV_420_888 -> YUV_420_888 multi-resolution reprocessing
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionReprocessCaptureTest"	"testMultiResolutionYuvToYuvReprocessing"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionReprocessCaptureTest.java"	""	"public void testMultiResolutionYuvToYuvReprocessing() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            testMultiResolutionReprocessing(id, ImageFormat.YUV_420_888, ImageFormat.YUV_420_888);
        }
    }

    /**
     * Test YUV_420_888 -> JPEG multi-resolution reprocessing
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionReprocessCaptureTest"	"testMultiResolutionYuvToJpegReprocessing"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionReprocessCaptureTest.java"	""	"public void testMultiResolutionYuvToJpegReprocessing() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            testMultiResolutionReprocessing(id, ImageFormat.YUV_420_888, ImageFormat.JPEG);
        }
    }

    /**
     * Test OPAQUE -> YUV_420_888 multi-resolution reprocessing
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionReprocessCaptureTest"	"testMultiResolutionOpaqueToYuvReprocessing"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionReprocessCaptureTest.java"	""	"public void testMultiResolutionOpaqueToYuvReprocessing() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            // Opaque -> YUV_420_888 must be supported.
            testMultiResolutionReprocessing(id, ImageFormat.PRIVATE, ImageFormat.YUV_420_888);
        }
    }

    /**
     * Test OPAQUE -> JPEG multi-resolution reprocessing
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionReprocessCaptureTest"	"testMultiResolutionOpaqueToJpegReprocessing"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionReprocessCaptureTest.java"	""	"public void testMultiResolutionOpaqueToJpegReprocessing() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            // OPAQUE -> JPEG must be supported.
            testMultiResolutionReprocessing(id, ImageFormat.PRIVATE, ImageFormat.JPEG);
        }
    }

    /**
     * Test for making sure the mandatory stream combinations work for multi-resolution
     * reprocessing.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.MultiResolutionReprocessCaptureTest"	"testMultiResolutionMandatoryStreamCombinationTest"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/MultiResolutionReprocessCaptureTest.java"	""	"public void testMultiResolutionMandatoryStreamCombinationTest() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            StaticMetadata info = mAllStaticInfo.get(id);
            CameraCharacteristics c = info.getCharacteristics();
            MandatoryStreamCombination[] combinations = c.get(
                            CameraCharacteristics.SCALER_MANDATORY_STREAM_COMBINATIONS);
            if (combinations == null) {
                Log.i(TAG, ""No mandatory stream combinations for camera: "" + id + "" skip test"");
                continue;
            }
            MultiResolutionStreamConfigurationMap multiResolutionMap = c.get(
                    CameraCharacteristics.SCALER_MULTI_RESOLUTION_STREAM_CONFIGURATION_MAP);
            if (multiResolutionMap == null) {
                Log.i(TAG, ""Camera "" + id + "" doesn't support multi-resolution capture."");
                continue;
            }
            int[] multiResolutionInputFormats = multiResolutionMap.getInputFormats();
            int[] multiResolutionOutputFormats = multiResolutionMap.getOutputFormats();
            if (multiResolutionInputFormats.length == 0
                    || multiResolutionOutputFormats.length == 0) {
                Log.i(TAG, ""Camera "" + id + "" doesn't support multi-resolution reprocess ""
                        + ""input/output."");
                continue;
            }

            try {
                openDevice(id);
                for (MandatoryStreamCombination combination : combinations) {
                    if (!combination.isReprocessable()) {
                        continue;
                    }

                    MandatoryStreamCombination.MandatoryStreamInformation firstStreamInfo =
                            combination.getStreamsInformation().get(0);
                    int inputFormat = firstStreamInfo.getFormat();
                    boolean supportMultiResReprocess = firstStreamInfo.isInput() &&
                            CameraTestUtils.contains(multiResolutionOutputFormats, inputFormat) &&
                            CameraTestUtils.contains(multiResolutionInputFormats, inputFormat);
                    if (!supportMultiResReprocess)  {
                        continue;
                    }

                    testMultiResolutionMandatoryStreamCombination(id, info, combination,
                            multiResolutionMap);
                }
            } finally {
                closeDevice(id);
            }
        }
    }

    private void testMultiResolutionMandatoryStreamCombination(String cameraId,
            StaticMetadata staticInfo, MandatoryStreamCombination combination,
            MultiResolutionStreamConfigurationMap multiResStreamConfig) throws Exception {
        String log = ""Testing multi-resolution mandatory stream combination: "" +
                combination.getDescription() + "" on camera: "" + cameraId;
        Log.i(TAG, log);

        final int TIMEOUT_FOR_RESULT_MS = 5000;
        final int NUM_REPROCESS_CAPTURES_PER_CONFIG = 3;

        // Set up outputs
        List<OutputConfiguration> outputConfigs = new ArrayList<OutputConfiguration>();
        List<Surface> outputSurfaces = new ArrayList<Surface>();
        StreamCombinationTargets targets = new StreamCombinationTargets();
        MultiResolutionImageReader inputReader = null;
        ImageWriter inputWriter = null;
        SimpleImageReaderListener inputReaderListener = new SimpleImageReaderListener();
        SimpleCaptureCallback inputCaptureListener = new SimpleCaptureCallback();
        SimpleCaptureCallback reprocessOutputCaptureListener = new SimpleCaptureCallback();

        List<MandatoryStreamInformation> streamInfo = combination.getStreamsInformation();
        assertTrue(""Reprocessable stream combinations should have at least 3 or more streams"",
                    (streamInfo != null) && (streamInfo.size() >= 3));
        assertTrue(""The first mandatory stream information in a reprocessable combination must "" +
                ""always be input"", streamInfo.get(0).isInput());

        int inputFormat = streamInfo.get(0).getFormat();

        CameraTestUtils.setupConfigurationTargets(streamInfo.subList(2, streamInfo.size()),
                targets, outputConfigs, outputSurfaces, NUM_REPROCESS_CAPTURES_PER_CONFIG,
                /*substituteY8*/false, /*substituteHeic*/false, /*physicalCameraId*/null,
                multiResStreamConfig, mHandler);

        Collection<MultiResolutionStreamInfo> multiResInputs =
                multiResStreamConfig.getInputInfo(inputFormat);
        InputConfiguration inputConfig = new InputConfiguration(multiResInputs, inputFormat);

        try {
            // For each config, YUV and JPEG outputs will be tested. (For YUV reprocessing,
            // the YUV ImageReader for input is also used for output.)
            final boolean inputIsYuv = inputConfig.getFormat() == ImageFormat.YUV_420_888;
            final boolean useYuv = inputIsYuv || targets.mYuvTargets.size() > 0 ||
                    targets.mYuvMultiResTargets.size() > 0;
            final int totalNumReprocessCaptures =  NUM_REPROCESS_CAPTURES_PER_CONFIG * (
                    (inputIsYuv ? 1 : 0) + targets.mJpegMultiResTargets.size() +
                    targets.mJpegTargets.size() +
                    (useYuv ? targets.mYuvMultiResTargets.size() + targets.mYuvTargets.size() : 0));

            // It needs 1 input buffer for each reprocess capture + the number of buffers
            // that will be used as outputs.
            inputReader = new MultiResolutionImageReader(multiResInputs, inputFormat,
                    totalNumReprocessCaptures + NUM_REPROCESS_CAPTURES_PER_CONFIG);
            inputReader.setOnImageAvailableListener(
                    inputReaderListener, new HandlerExecutor(mHandler));
            outputConfigs.addAll(
                    OutputConfiguration.createInstancesForMultiResolutionOutput(inputReader));
            outputSurfaces.add(inputReader.getSurface());

            CameraCaptureSession.CaptureCallback mockCaptureCallback =
                    mock(CameraCaptureSession.CaptureCallback.class);

            checkSessionConfigurationSupported(mCamera, mHandler, outputConfigs,
                    inputConfig, SessionConfiguration.SESSION_REGULAR,
                    true/*defaultSupport*/, String.format(
                    ""Session configuration query for multi-res combination: %s failed"",
                    combination.getDescription()));

            // Verify we can create a reprocessable session with the input and all outputs.
            BlockingSessionCallback sessionListener = new BlockingSessionCallback();
            CameraCaptureSession session = configureReprocessableCameraSessionWithConfigurations(
                    mCamera, inputConfig, outputConfigs, sessionListener, mHandler);
            inputWriter = ImageWriter.newInstance(
                    session.getInputSurface(), totalNumReprocessCaptures);

            // Prepare a request for reprocess input
            CaptureRequest.Builder builder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG);
            builder.addTarget(inputReader.getSurface());

            for (int i = 0; i < totalNumReprocessCaptures; i++) {
                session.capture(builder.build(), inputCaptureListener, mHandler);
            }

            List<CaptureRequest> reprocessRequests = new ArrayList<>();
            List<Surface> reprocessOutputs = new ArrayList<>();

            if (inputIsYuv) {
                reprocessOutputs.add(inputReader.getSurface());
            }
            for (MultiResolutionImageReader reader : targets.mJpegMultiResTargets) {
                reprocessOutputs.add(reader.getSurface());
            }
            for (ImageReader reader : targets.mJpegTargets) {
                reprocessOutputs.add(reader.getSurface());
            }
            for (MultiResolutionImageReader reader : targets.mYuvMultiResTargets) {
                reprocessOutputs.add(reader.getSurface());
            }
            for (ImageReader reader : targets.mYuvTargets) {
                reprocessOutputs.add(reader.getSurface());
            }

            for (int i = 0; i < NUM_REPROCESS_CAPTURES_PER_CONFIG; i++) {
                for (Surface output : reprocessOutputs) {
                    TotalCaptureResult result = inputCaptureListener.getTotalCaptureResult(
                            TIMEOUT_FOR_RESULT_MS);
                    Map<String, TotalCaptureResult> physicalResults =
                            result.getPhysicalCameraTotalResults();
                    for (Map.Entry<String, TotalCaptureResult> entry : physicalResults.entrySet()) {
                        String physicalCameraId = entry.getKey();
                        TotalCaptureResult physicalResult = entry.getValue();
                        String activePhysicalId = physicalResult.get(
                                CaptureResult.LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID);
                        mCollector.expectEquals(String.format(
                                ""Physical camera result metadata must contain activePhysicalId "" +
                                ""(%s) matching with physical camera Id (%s)."", activePhysicalId,
                                physicalCameraId), physicalCameraId, activePhysicalId);
                    }

                    String activePhysicalCameraId = result.get(
                            CaptureResult.LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID);
                    if (activePhysicalCameraId != null) {
                        result = physicalResults.get(activePhysicalCameraId);
                    }

                    builder = mCamera.createReprocessCaptureRequest(result);
                    inputWriter.queueInputImage(
                            inputReaderListener.getImage(TIMEOUT_FOR_RESULT_MS));
                    builder.addTarget(output);
                    reprocessRequests.add(builder.build());
                }
            }

            session.captureBurst(reprocessRequests, reprocessOutputCaptureListener, mHandler);

            for (int i = 0; i < reprocessOutputs.size() * NUM_REPROCESS_CAPTURES_PER_CONFIG; i++) {
                TotalCaptureResult result = reprocessOutputCaptureListener.getTotalCaptureResult(
                        TIMEOUT_FOR_RESULT_MS);
            }
        } catch (Throwable e) {
            mCollector.addMessage(
                    String.format(""Mandatory multi-res stream combination: %s failed due: %s"",
                    combination.getDescription(), e.getMessage()));
        } finally {
            inputReaderListener.drain();
            reprocessOutputCaptureListener.drain();
            targets.close();

            if (inputReader != null) {
                inputReader.close();
            }

            if (inputWriter != null) {
                inputWriter.close();
            }
        }
    }

    /**
     * Test multi-resolution reprocessing from the input format to the output format
     */
    private void testMultiResolutionReprocessing(String cameraId, int inputFormat,
            int outputFormat) throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""testMultiResolutionReprocessing: cameraId: "" + cameraId + "" inputFormat: ""
                    + inputFormat + "" outputFormat: "" + outputFormat);
        }

        Collection<MultiResolutionStreamInfo> inputStreamInfo =
                getMultiResReprocessInfo(cameraId, inputFormat, /*input*/ true);
        Collection<MultiResolutionStreamInfo> regularOutputStreamInfo =
                getMultiResReprocessInfo(cameraId, inputFormat, /*input*/ false);
        Collection<MultiResolutionStreamInfo> reprocessOutputStreamInfo =
                getMultiResReprocessInfo(cameraId, outputFormat, /*input*/ false);
        if (inputStreamInfo == null || regularOutputStreamInfo == null ||
                reprocessOutputStreamInfo == null) {
            return;
        }
        assertTrue(""The multi-resolution stream info for format "" + inputFormat
                + "" must be equal between input and output"",
                inputStreamInfo.containsAll(regularOutputStreamInfo)
                && regularOutputStreamInfo.containsAll(inputStreamInfo));

        try {
            openDevice(cameraId);

            testMultiResolutionReprocessWithStreamInfo(cameraId, inputFormat, inputStreamInfo,
                    outputFormat, reprocessOutputStreamInfo);
        } finally {
            closeDevice(cameraId);
        }
    }

    /**
     * Test multi-resolution reprocess with multi-resolution stream info lists for a particular
     * format combination.
     */
    private void testMultiResolutionReprocessWithStreamInfo(String cameraId,
            int inputFormat, Collection<MultiResolutionStreamInfo> inputInfo,
            int outputFormat, Collection<MultiResolutionStreamInfo> outputInfo)
            throws Exception {
        try {
            setupMultiResImageReaders(inputFormat, inputInfo, outputFormat, outputInfo,
                    /*maxImages*/1);
            setupReprocessableSession(inputFormat, inputInfo, outputInfo,
                    /*numImageWriterImages*/1);

            List<Float> zoomRatioList = CameraTestUtils.getCandidateZoomRatios(mStaticInfo);
            for (Float zoomRatio :  zoomRatioList) {
                ImageResultSizeHolder imageResultSizeHolder = null;

                try {
                    imageResultSizeHolder = doMultiResReprocessCapture(zoomRatio);
                    Image reprocessedImage = imageResultSizeHolder.getImage();
                    Size outputSize = imageResultSizeHolder.getExpectedSize();
                    TotalCaptureResult result = imageResultSizeHolder.getTotalCaptureResult();

                    mCollector.expectImageProperties(""testMultiResolutionReprocess"",
                            reprocessedImage, outputFormat, outputSize,
                            result.get(CaptureResult.SENSOR_TIMESTAMP));

                    if (DEBUG) {
                        Log.d(TAG, String.format(""camera %s %d zoom %f out %dx%d %d"",
                                cameraId, inputFormat, zoomRatio,
                                outputSize.getWidth(), outputSize.getHeight(),
                                outputFormat));

                        dumpImage(reprocessedImage,
                                ""/testMultiResolutionReprocess_camera"" + cameraId
                                + ""_"" + mDumpFrameCount);
                        mDumpFrameCount++;
                    }
                } finally {
                    if (imageResultSizeHolder != null) {
                        imageResultSizeHolder.getImage().close();
                    }
                }
            }
        } finally {
            closeReprossibleSession();
            closeMultiResImageReaders();
        }
    }

    /**
     * Set up multi-resolution image readers for regular and reprocess output
     *
     * <p>If the reprocess input format is equal to output format, share one multi-resolution
     * image reader.</p>
     */
    private void setupMultiResImageReaders(int inputFormat,
            Collection<MultiResolutionStreamInfo> inputInfo, int outputFormat,
            Collection<MultiResolutionStreamInfo> outputInfo, int maxImages) {

        mShareOneReader = false;
        // If the regular output and reprocess output have the same format,
        // they can share one MultiResolutionImageReader.
        if (inputFormat == outputFormat) {
            maxImages *= 2;
            mShareOneReader = true;
        }

        // create an MultiResolutionImageReader for the regular capture
        mMultiResImageReader = new MultiResolutionImageReader(inputInfo,
                inputFormat, maxImages);
        mMultiResImageReaderListener = new SimpleMultiResolutionImageReaderListener(
                mMultiResImageReader, 1, /*repeating*/false);
        mMultiResImageReader.setOnImageAvailableListener(mMultiResImageReaderListener,
                new HandlerExecutor(mHandler));

        if (!mShareOneReader) {
            // create an MultiResolutionImageReader for the reprocess capture
            mSecondMultiResImageReader = new MultiResolutionImageReader(
                    outputInfo, outputFormat, maxImages);
            mSecondMultiResImageReaderListener = new SimpleMultiResolutionImageReaderListener(
                    mSecondMultiResImageReader, maxImages, /*repeating*/ false);
            mSecondMultiResImageReader.setOnImageAvailableListener(
                    mSecondMultiResImageReaderListener, new HandlerExecutor(mHandler));
        }
    }

    /**
     * Close two multi-resolution image readers.
     */
    private void closeMultiResImageReaders() {
        mMultiResImageReader.close();
        mMultiResImageReader = null;

        if (!mShareOneReader) {
            mSecondMultiResImageReader.close();
            mSecondMultiResImageReader = null;
        }
    }

    /**
     * Get the MultiResolutionImageReader for reprocess output.
     */
    private MultiResolutionImageReader getOutputMultiResImageReader() {
        if (mShareOneReader) {
            return mMultiResImageReader;
        } else {
            return mSecondMultiResImageReader;
        }
    }

    /**
     * Get the MultiResolutionImageReaderListener for reprocess output.
     */
    private SimpleMultiResolutionImageReaderListener getOutputMultiResImageReaderListener() {
        if (mShareOneReader) {
            return mMultiResImageReaderListener;
        } else {
            return mSecondMultiResImageReaderListener;
        }
    }

    /**
     * Set up a reprocessable session and create an ImageWriter with the session's input surface.
     */
    private void setupReprocessableSession(int inputFormat,
            Collection<MultiResolutionStreamInfo> inputInfo,
            Collection<MultiResolutionStreamInfo> outputInfo,
            int numImageWriterImages) throws Exception {
        // create a reprocessable capture session
        Collection<OutputConfiguration> outConfigs =
                OutputConfiguration.createInstancesForMultiResolutionOutput(
                        mMultiResImageReader);
        ArrayList<OutputConfiguration> outputConfigsList = new ArrayList<OutputConfiguration>(
                outConfigs);

        if (!mShareOneReader) {
            Collection<OutputConfiguration> secondOutputConfigs =
                    OutputConfiguration.createInstancesForMultiResolutionOutput(
                            mSecondMultiResImageReader);
            outputConfigsList.addAll(secondOutputConfigs);
        }

        InputConfiguration inputConfig = new InputConfiguration(inputInfo, inputFormat);
        if (VERBOSE) {
            String inputConfigString = inputConfig.toString();
            Log.v(TAG, ""InputConfiguration: "" + inputConfigString);
        }

        mCameraSessionListener = new BlockingSessionCallback();
        mCameraSession = configureReprocessableCameraSessionWithConfigurations(
                mCamera, inputConfig, outputConfigsList, mCameraSessionListener, mHandler);

        // create an ImageWriter
        mInputSurface = mCameraSession.getInputSurface();
        mImageWriter = ImageWriter.newInstance(mInputSurface,
                numImageWriterImages);

        mImageWriterListener = new SimpleImageWriterListener(mImageWriter);
        mImageWriter.setOnImageReleasedListener(mImageWriterListener, mHandler);
    }

    /**
     * Close the reprocessable session and ImageWriter.
     */
    private void closeReprossibleSession() {
        mInputSurface = null;

        if (mCameraSession != null) {
            mCameraSession.close();
            mCameraSession = null;
        }

        if (mImageWriter != null) {
            mImageWriter.close();
            mImageWriter = null;
        }
    }

    /**
     * Do one multi-resolution reprocess capture for the specified zoom ratio
     */
    private ImageResultSizeHolder doMultiResReprocessCapture(float zoomRatio) throws Exception {
        // submit a regular capture and get the result
        TotalCaptureResult totalResult = submitCaptureRequest(
                zoomRatio, mMultiResImageReader.getSurface(), /*inputResult*/null);
        Map<String, TotalCaptureResult> physicalResults =
                totalResult.getPhysicalCameraTotalResults();

        ImageAndMultiResStreamInfo inputImageAndInfo =
                mMultiResImageReaderListener.getAnyImageAndInfoAvailable(CAPTURE_TIMEOUT_MS);
        assertNotNull(""Failed to capture input image"", inputImageAndInfo);
        Image inputImage = inputImageAndInfo.image;
        MultiResolutionStreamInfo inputStreamInfo = inputImageAndInfo.streamInfo;
        TotalCaptureResult inputSettings =
                physicalResults.get(inputStreamInfo.getPhysicalCameraId());
        assertTrue(""Regular capture's TotalCaptureResult doesn't contain capture result for ""
                + ""physical camera id "" + inputStreamInfo.getPhysicalCameraId(),
                inputSettings != null);

        // Submit a reprocess capture and get the result
        mImageWriter.queueInputImage(inputImage);

        TotalCaptureResult finalResult = submitCaptureRequest(zoomRatio,
                getOutputMultiResImageReader().getSurface(), inputSettings);

        ImageAndMultiResStreamInfo outputImageAndInfo =
                getOutputMultiResImageReaderListener().getAnyImageAndInfoAvailable(
                CAPTURE_TIMEOUT_MS);
        Image outputImage = outputImageAndInfo.image;
        MultiResolutionStreamInfo outputStreamInfo = outputImageAndInfo.streamInfo;

        assertTrue(""The regular output and reprocess output's stream info must be the same"",
                outputStreamInfo.equals(inputStreamInfo));

        ImageResultSizeHolder holder = new ImageResultSizeHolder(outputImageAndInfo.image,
                finalResult, new Size(outputStreamInfo.getWidth(), outputStreamInfo.getHeight()));

        return holder;
    }

    /**
     * Issue a capture request and return the result for a particular zoom ratio.
     *
     * <p>If inputResult is null, it's a regular request. Otherwise, it's a reprocess request.</p>
     */
    private TotalCaptureResult submitCaptureRequest(float zoomRatio,
            Surface output, TotalCaptureResult inputResult) throws Exception {

        SimpleCaptureCallback captureCallback = new SimpleCaptureCallback();

        // Prepare a list of capture requests. Whether it's a regular or reprocess capture request
        // is based on inputResult.
        CaptureRequest.Builder builder;
        boolean isReprocess = (inputResult != null);
        if (isReprocess) {
            builder = mCamera.createReprocessCaptureRequest(inputResult);
        } else {
            builder = mCamera.createCaptureRequest(CAPTURE_TEMPLATE);
            builder.set(CaptureRequest.CONTROL_ZOOM_RATIO, zoomRatio);
        }
        builder.addTarget(output);
        CaptureRequest request = builder.build();
        assertTrue(""Capture request reprocess type "" + request.isReprocess() + "" is wrong."",
            request.isReprocess() == isReprocess);

        mCameraSession.capture(request, captureCallback, mHandler);

        TotalCaptureResult result = captureCallback.getTotalCaptureResultForRequest(
                request, CAPTURE_TIMEOUT_FRAMES);

        // make sure all input surfaces are released.
        if (isReprocess) {
            mImageWriterListener.waitForImageReleased(CAPTURE_TIMEOUT_MS);
        }

        return result;
    }

    private Size getMaxSize(int format, StaticMetadata.StreamDirection direction) {
        Size[] sizes = mStaticInfo.getAvailableSizesForFormatChecked(format, direction);
        return getAscendingOrderSizes(Arrays.asList(sizes), /*ascending*/false).get(0);
    }

    private Collection<MultiResolutionStreamInfo> getMultiResReprocessInfo(String cameraId,
            int format, boolean input) throws Exception {
        StaticMetadata staticInfo = mAllStaticInfo.get(cameraId);
        CameraCharacteristics characteristics = staticInfo.getCharacteristics();
        MultiResolutionStreamConfigurationMap configs = characteristics.get(
                CameraCharacteristics.SCALER_MULTI_RESOLUTION_STREAM_CONFIGURATION_MAP);
        if (configs == null) {
            Log.i(TAG, ""Camera "" + cameraId + "" doesn't support multi-resolution streams"");
            return null;
        }

        String streamType = input ? ""input"" : ""output"";
        int[] formats = input ? configs.getInputFormats() :
                configs.getOutputFormats();
        if (!CameraTestUtils.contains(formats, format)) {
            Log.i(TAG, ""Camera "" + cameraId + "" doesn't support multi-resolution ""
                    + streamType + "" stream for format "" + format + "". Supported formats are ""
                    + Arrays.toString(formats));
            return null;
        }
        Collection<MultiResolutionStreamInfo> streams =
                input ? configs.getInputInfo(format) : configs.getOutputInfo(format);
        mCollector.expectTrue(String.format(""Camera %s supported 0 multi-resolution ""
                + streamType + "" stream info, expected at least 1"", cameraId),
                streams.size() > 0);

        return streams;
    }

    private void dumpImage(Image image, String name) {
        String filename = mDebugFileNameBase + name;
        switch(image.getFormat()) {
            case ImageFormat.JPEG:
                filename += "".jpg"";
                break;
            case ImageFormat.YUV_420_888:
                filename += "".yuv"";
                break;
            default:
                filename += ""."" + image.getFormat();
                break;
        }

        Log.d(TAG, ""dumping an image to "" + filename);
        dumpFile(filename , getDataFromImage(image));
    }

    /**
     * A class that holds an Image, a TotalCaptureResult, and expected image size.
     */
    public static class ImageResultSizeHolder {
        private final Image mImage;
        private final TotalCaptureResult mResult;
        private final Size mExpectedSize;

        public ImageResultSizeHolder(Image image, TotalCaptureResult result, Size expectedSize) {
            mImage = image;
            mResult = result;
            mExpectedSize = expectedSize;
        }

        public Image getImage() {
            return mImage;
        }

        public TotalCaptureResult getTotalCaptureResult() {
            return mResult;
        }

        public Size getExpectedSize() {
            return mExpectedSize;
        }
    }

}"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.HeifWriterTest"	"testHeif"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/HeifWriterTest.java"	""	"public void testHeif() throws Exception {
        final int NUM_SINGLE_CAPTURE_TESTED = 3;
        final int NUM_HEIC_CAPTURE_TESTED = 2;
        final int SESSION_WARMUP_MS = 1000;
        final int HEIF_STOP_TIMEOUT = 3000 * NUM_SINGLE_CAPTURE_TESTED;

        if (!canEncodeHeic()) {
            MediaUtils.skipTest(""heic encoding is not supported on this device"");
            return;
        }

        boolean sessionFailure = false;
        Integer[] sessionStates = {BlockingSessionCallback.SESSION_READY,
                BlockingSessionCallback.SESSION_CONFIGURE_FAILED};
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.v(TAG, ""Testing HEIF capture for Camera "" + id);
                openDevice(id);

                Size[] availableSizes = mStaticInfo.getAvailableSizesForFormatChecked(
                        ImageFormat.PRIVATE,
                        StaticMetadata.StreamDirection.Output);

                // for each resolution, test imageReader:
                for (Size sz : availableSizes) {
                    HeifWriter heifWriter = null;
                    OutputConfiguration outConfig = null;
                    Surface latestSurface = null;
                    CaptureRequest.Builder reqStill = null;
                    int width = sz.getWidth();
                    int height = sz.getHeight();
                    for (int cap = 0; cap < NUM_HEIC_CAPTURE_TESTED; cap++) {
                        if (VERBOSE) {
                            Log.v(TAG, ""Testing size "" + sz.toString() + "" format PRIVATE""
                                    + "" for camera "" + mCamera.getId() + "". Iteration:"" + cap);
                        }

                        try {
                            TestConfig.Builder builder = new TestConfig.Builder(/*useGrid*/false);
                            builder.setNumImages(NUM_SINGLE_CAPTURE_TESTED);
                            builder.setSize(sz);
                            String filename = ""Cam"" + id + ""_"" + width + ""x"" + height +
                                    ""_"" + cap + "".heic"";
                            builder.setOutputPath(
                                    new File(mFilePath, filename).getAbsolutePath());
                            TestConfig config = builder.build();

                            try {
                                heifWriter = new HeifWriter.Builder(
                                        config.mOutputPath,
                                        width, height, INPUT_MODE_SURFACE)
                                    .setGridEnabled(config.mUseGrid)
                                    .setMaxImages(config.mMaxNumImages)
                                    .setQuality(config.mQuality)
                                    .setPrimaryIndex(config.mNumImages - 1)
                                    .setHandler(mHandler)
                                    .build();
                            } catch (IOException e) {
                                // Continue in case the size is not supported
                                sessionFailure = true;
                                Log.i(TAG, ""Skip due to heifWriter creation failure: ""
                                        + e.getMessage());
                                continue;
                            }

                            // First capture. Start capture session
                            latestSurface = heifWriter.getInputSurface();
                            outConfig = new OutputConfiguration(latestSurface);
                            List<OutputConfiguration> configs =
                                new ArrayList<OutputConfiguration>();
                            configs.add(outConfig);

                            SurfaceTexture preview = new SurfaceTexture(/*random int*/ 1);
                            Surface previewSurface = new Surface(preview);
                            preview.setDefaultBufferSize(640, 480);
                            configs.add(new OutputConfiguration(previewSurface));

                            CaptureRequest.Builder reqPreview = mCamera.createCaptureRequest(
                                    CameraDevice.TEMPLATE_PREVIEW);
                            reqPreview.addTarget(previewSurface);

                            reqStill = mCamera.createCaptureRequest(
                                    CameraDevice.TEMPLATE_STILL_CAPTURE);
                            reqStill.addTarget(previewSurface);
                            reqStill.addTarget(latestSurface);

                            // Start capture session and preview
                            createSessionByConfigs(configs);
                            int state = mCameraSessionListener.getStateWaiter().waitForAnyOfStates(
                                    Arrays.asList(sessionStates), SESSION_CONFIGURE_TIMEOUT_MS);
                            if (state == BlockingSessionCallback.SESSION_CONFIGURE_FAILED) {
                                // session configuration failure. Bail out due to known issue of
                                // HeifWriter INPUT_SURFACE mode support for camera. b/79699819
                                sessionFailure = true;
                                break;
                            }
                            startCapture(reqPreview.build(), /*repeating*/true, null, null);

                            SystemClock.sleep(SESSION_WARMUP_MS);

                            heifWriter.start();

                            // Start capture.
                            CaptureRequest request = reqStill.build();
                            SimpleCaptureCallback listener = new SimpleCaptureCallback();

                            int numImages = config.mNumImages;

                            for (int i = 0; i < numImages; i++) {
                                startCapture(request, /*repeating*/false, listener, mHandler);
                            }

                            // Validate capture result.
                            CaptureResult result = validateCaptureResult(
                                    ImageFormat.PRIVATE, sz, listener, numImages);

                            // TODO: convert capture results into EXIF and send to heifwriter

                            heifWriter.stop(HEIF_STOP_TIMEOUT);

                            verifyResult(config.mOutputPath, width, height,
                                    config.mRotation, config.mUseGrid,
                                    Math.min(numImages, config.mMaxNumImages));
                        } finally {
                            if (heifWriter != null) {
                                heifWriter.close();
                                heifWriter = null;
                            }
                            if (!sessionFailure) {
                                stopCapture(/*fast*/false);
                            }
                        }
                    }

                    if (sessionFailure) {
                        break;
                    }
                }
            } finally {
                closeDevice(id);
            }
        }
    }

    private static boolean canEncodeHeic() {
        return MediaUtils.hasEncoder(MediaFormat.MIMETYPE_VIDEO_HEVC)
            || MediaUtils.hasEncoder(MediaFormat.MIMETYPE_IMAGE_ANDROID_HEIC);
    }

    private static class TestConfig {
        final boolean mUseGrid;
        final int mMaxNumImages;
        final int mNumImages;
        final int mWidth;
        final int mHeight;
        final int mRotation;
        final int mQuality;
        final String mOutputPath;

        TestConfig(boolean useGrid, int maxNumImages, int numImages,
                   int width, int height, int rotation, int quality,
                   String outputPath) {
            mUseGrid = useGrid;
            mMaxNumImages = maxNumImages;
            mNumImages = numImages;
            mWidth = width;
            mHeight = height;
            mRotation = rotation;
            mQuality = quality;
            mOutputPath = outputPath;
        }

        static class Builder {
            final boolean mUseGrid;
            int mMaxNumImages;
            int mNumImages;
            int mWidth;
            int mHeight;
            int mRotation;
            final int mQuality;
            String mOutputPath;

            Builder(boolean useGrids) {
                mUseGrid = useGrids;
                mMaxNumImages = mNumImages = 4;
                mWidth = 1920;
                mHeight = 1080;
                mRotation = 0;
                mQuality = 100;
                mOutputPath = new File(Environment.getExternalStorageDirectory(),
                        OUTPUT_FILENAME).getAbsolutePath();
            }

            Builder setNumImages(int numImages) {
                mMaxNumImages = mNumImages = numImages;
                return this;
            }

            Builder setRotation(int rotation) {
                mRotation = rotation;
                return this;
            }

            Builder setSize(Size sz) {
                mWidth = sz.getWidth();
                mHeight = sz.getHeight();
                return this;
            }

            Builder setOutputPath(String path) {
                mOutputPath = path;
                return this;
            }

            private void cleanupStaleOutputs() {
                File outputFile = new File(mOutputPath);
                if (outputFile.exists()) {
                    outputFile.delete();
                }
            }

            TestConfig build() {
                cleanupStaleOutputs();
                return new TestConfig(mUseGrid, mMaxNumImages, mNumImages,
                        mWidth, mHeight, mRotation, mQuality, mOutputPath);
            }
        }

        @Override
        public String toString() {
            return ""TestConfig""
                    + "": mUseGrid "" + mUseGrid
                    + "", mMaxNumImages "" + mMaxNumImages
                    + "", mNumImages "" + mNumImages
                    + "", mWidth "" + mWidth
                    + "", mHeight "" + mHeight
                    + "", mRotation "" + mRotation
                    + "", mQuality "" + mQuality
                    + "", mOutputPath "" + mOutputPath;
        }
    }

    private void verifyResult(
            String filename, int width, int height, int rotation, boolean useGrid, int numImages)
            throws Exception {
        MediaMetadataRetriever retriever = new MediaMetadataRetriever();
        retriever.setDataSource(filename);
        String hasImage = retriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_HAS_IMAGE);
        if (!""yes"".equals(hasImage)) {
            throw new Exception(""No images found in file "" + filename);
        }
        assertEquals(""Wrong image count"", numImages,
                Integer.parseInt(retriever.extractMetadata(
                    MediaMetadataRetriever.METADATA_KEY_IMAGE_COUNT)));
        assertEquals(""Wrong width"", width,
                Integer.parseInt(retriever.extractMetadata(
                    MediaMetadataRetriever.METADATA_KEY_IMAGE_WIDTH)));
        assertEquals(""Wrong height"", height,
                Integer.parseInt(retriever.extractMetadata(
                    MediaMetadataRetriever.METADATA_KEY_IMAGE_HEIGHT)));
        assertEquals(""Wrong rotation"", rotation,
                Integer.parseInt(retriever.extractMetadata(
                    MediaMetadataRetriever.METADATA_KEY_IMAGE_ROTATION)));
        retriever.release();

        if (useGrid) {
            MediaExtractor extractor = new MediaExtractor();
            extractor.setDataSource(filename);
            MediaFormat format = extractor.getTrackFormat(0);
            int tileWidth = format.getInteger(MediaFormat.KEY_TILE_WIDTH);
            int tileHeight = format.getInteger(MediaFormat.KEY_TILE_HEIGHT);
            int gridRows = format.getInteger(MediaFormat.KEY_GRID_ROWS);
            int gridCols = format.getInteger(MediaFormat.KEY_GRID_COLUMNS);
            assertTrue(""Wrong tile width or grid cols"",
                    ((width + tileWidth - 1) / tileWidth) == gridCols);
            assertTrue(""Wrong tile height or grid rows"",
                    ((height + tileHeight - 1) / tileHeight) == gridRows);
            extractor.release();
        }
    }

    /**
     * Validate capture results.
     *
     * @param format The format of this capture.
     * @param size The capture size.
     * @param listener The capture listener to get capture result callbacks.
     * @return the last verified CaptureResult
     */
    private CaptureResult validateCaptureResult(
            int format, Size size, SimpleCaptureCallback listener, int numFrameVerified) {
        CaptureResult result = null;
        for (int i = 0; i < numFrameVerified; i++) {
            result = listener.getCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);
            if (mStaticInfo.isCapabilitySupported(
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS)) {
                Long exposureTime = getValueNotNull(result, CaptureResult.SENSOR_EXPOSURE_TIME);
                Integer sensitivity = getValueNotNull(result, CaptureResult.SENSOR_SENSITIVITY);
                mCollector.expectInRange(
                        String.format(
                                ""Capture for format %d, size %s exposure time is invalid."",
                                format, size.toString()),
                        exposureTime,
                        mStaticInfo.getExposureMinimumOrDefault(),
                        mStaticInfo.getExposureMaximumOrDefault()
                );
                mCollector.expectInRange(
                        String.format(""Capture for format %d, size %s sensitivity is invalid."",
                                format, size.toString()),
                        sensitivity,
                        mStaticInfo.getSensitivityMinimumOrDefault(),
                        mStaticInfo.getSensitivityMaximumOrDefault()
                );
            }
            // TODO: add more key validations.
        }
        return result;
    }
}"	""	""	"1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CaptureResultTest"	"testResultTimestamps"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/CaptureResultTest.java"	""	"public void testResultTimestamps() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            ImageReader previewReader = null;
            ImageReader jpegReader = null;

            CaptureResult resultForNdk = null;

            SimpleImageReaderListener jpegListener = new SimpleImageReaderListener();
            SimpleImageReaderListener prevListener = new SimpleImageReaderListener();
            try {
                if (!mAllStaticInfo.get(id).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" does not support color outputs, skipping"");
                    continue;
                }

                openDevice(id);
                CaptureRequest.Builder previewBuilder =
                        mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
                CaptureRequest.Builder multiBuilder =
                        mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);

                // Create image reader and surface.
                Size previewSize = mOrderedPreviewSizes.get(0);
                Size jpegSize = mOrderedStillSizes.get(0);

                // Create ImageReaders.
                previewReader = makeImageReader(previewSize, ImageFormat.YUV_420_888,
                        MAX_NUM_IMAGES, prevListener, mHandler);
                jpegReader = makeImageReader(jpegSize, ImageFormat.JPEG,
                        MAX_NUM_IMAGES, jpegListener, mHandler);

                // Configure output streams with preview and jpeg streams.
                List<Surface> outputSurfaces = new ArrayList<>(Arrays.asList(
                        previewReader.getSurface(), jpegReader.getSurface()));

                SessionListener mockSessionListener = getMockSessionListener();

                CameraCaptureSession session = configureAndVerifySession(mockSessionListener,
                        mCamera, outputSurfaces, mHandler);

                // Configure the requests.
                previewBuilder.addTarget(previewReader.getSurface());
                multiBuilder.addTarget(previewReader.getSurface());
                multiBuilder.addTarget(jpegReader.getSurface());

                if (mStaticInfo.isEnableZslSupported()) {
                    // Turn off ZSL to ensure timestamps are increasing
                    previewBuilder.set(CaptureRequest.CONTROL_ENABLE_ZSL, false);
                    multiBuilder.set(CaptureRequest.CONTROL_ENABLE_ZSL, false);
                }

                CaptureCallback mockCaptureCallback = getMockCaptureListener();

                // Capture targeting only preview
                Pair<TotalCaptureResult, Long> result = captureAndVerifyResult(mockCaptureCallback,
                        session, previewBuilder.build(), mHandler);

                // Check if all timestamps are the same
                Image prevImage = prevListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                validateTimestamps(""Result 1"", result.first,
                        prevImage, result.second);
                prevImage.close();

                // Capture targeting both jpeg and preview
                Pair<TotalCaptureResult, Long> result2 = captureAndVerifyResult(mockCaptureCallback,
                        session, multiBuilder.build(), mHandler);

                // Check if all timestamps are the same
                prevImage = prevListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                Image jpegImage = jpegListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                validateTimestamps(""Result 2 Preview"", result2.first,
                        prevImage, result2.second);
                validateTimestamps(""Result 2 Jpeg"", result2.first,
                        jpegImage, result2.second);
                prevImage.close();
                jpegImage.close();

                // Check if timestamps are increasing
                mCollector.expectGreater(""Timestamps must be increasing."", result.second,
                        result2.second);

                // Capture two preview frames
                long startTime = SystemClock.elapsedRealtimeNanos();
                Pair<TotalCaptureResult, Long> result3 = captureAndVerifyResult(mockCaptureCallback,
                        session, previewBuilder.build(), mHandler);
                Pair<TotalCaptureResult, Long> result4 = captureAndVerifyResult(mockCaptureCallback,
                        session, previewBuilder.build(), mHandler);
                long clockDiff = SystemClock.elapsedRealtimeNanos() - startTime;
                long resultDiff = result4.second - result3.second;

                // Check if all timestamps are the same
                prevImage = prevListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                validateTimestamps(""Result 3"", result3.first,
                        prevImage, result3.second);
                prevImage.close();
                prevImage = prevListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                validateTimestamps(""Result 4"", result4.first,
                        prevImage, result4.second);
                prevImage.close();

                // Check that the timestamps monotonically increase at a reasonable rate
                mCollector.expectGreaterOrEqual(""Timestamps increase faster than system clock."",
                        resultDiff, clockDiff);
                mCollector.expectGreater(""Timestamps must be increasing."", result3.second,
                        result4.second);

                resultForNdk = result.first;
            } finally {
                closeDevice(id);
                closeImageReader(previewReader);
                closeImageReader(jpegReader);
            }

            mCollector.expectTrue(
                ""validateACameraMetadataFromCameraMetadataCriticalTagsNative failed"",
                validateACameraMetadataFromCameraMetadataCriticalTagsNative(resultForNdk,
                        resultForNdk.get(CaptureResult.SENSOR_TIMESTAMP)));

            long timestamp = resultForNdk.get(CaptureResult.SENSOR_TIMESTAMP);
            mCollector.expectTrue(
                ""stashACameraMetadataFromCameraMetadataNative failed"",
                stashACameraMetadataFromCameraMetadataNative(resultForNdk));

            // Try to drop the Java side object here
            resultForNdk = null;
            int[] block = null;
            final int count = 9;
            for (int i = 0; i < count + 1; i++) {
                block = new int[1000000];
                block[1000 + i] = i;

                Runtime.getRuntime().gc();
                Runtime.getRuntime().runFinalization();

                mCollector.expectTrue(""This should never fail"", block[1000 + i] == i);
            }
            mCollector.expectTrue(
                ""validateStashedACameraMetadataFromCameraMetadataNative failed"",
                validateStashedACameraMetadataFromCameraMetadataNative(timestamp));
            mCollector.expectTrue(""This should never fail"", block[1000 + count] == count);
        }
    }

    private void validateTimestamps(String msg, TotalCaptureResult result, Image resultImage,
                                    long captureTime) {
        mCollector.expectKeyValueEquals(result, CaptureResult.SENSOR_TIMESTAMP, captureTime);
        mCollector.expectEquals(msg + "": Capture timestamp must be same as resultImage timestamp"",
                resultImage.getTimestamp(), captureTime);
    }

    public static void validateCaptureResult(CameraErrorCollector errorCollector,
            SimpleCaptureCallback captureListener, StaticMetadata staticInfo,
            Map<String, StaticMetadata> allStaticInfo, List<String> requestedPhysicalIds,
            CaptureRequest.Builder requestBuilder, int numFramesVerified) throws Exception {
        // List that includes all public keys from CaptureResult
        List<CaptureResult.Key<?>> allKeys = getAllCaptureResultKeys();
        // Get the waived keys for current camera device
        List<CaptureResult.Key<?>> waiverKeys = getWaiverKeysForCamera(staticInfo);
        if (requestedPhysicalIds == null) {
            requestedPhysicalIds = new ArrayList<String>();
        }

        HashMap<String, List<CaptureResult.Key<?>>> physicalWaiverKeys = new HashMap<>();
        for (String physicalId : requestedPhysicalIds) {
            StaticMetadata physicalStaticInfo = allStaticInfo.get(physicalId);
            physicalWaiverKeys.put(physicalId, getWaiverKeysForCamera(physicalStaticInfo));
        }

        TotalCaptureResult result = null;
        // List of (frameNumber, physical camera Id) pairs
        ArrayList<Pair<Long, String>> droppedPhysicalResults = new ArrayList<>();
        for (int i = 0; i < numFramesVerified; i++) {
            result = captureListener.getTotalCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);

            Map<String, CaptureResult> physicalCaptureResults = result.getPhysicalCameraResults();
            ArrayList<String> droppedIds = new ArrayList<String>(requestedPhysicalIds);
            droppedIds.removeAll(physicalCaptureResults.keySet());
            for (String droppedId : droppedIds) {
                droppedPhysicalResults.add(
                        new Pair<Long, String>(result.getFrameNumber(), droppedId));
            }

            validateOneCaptureResult(errorCollector, staticInfo, waiverKeys, allKeys,
                    requestBuilder, result, null/*cameraId*/, i);
            for (String physicalId : physicalCaptureResults.keySet()) {
                StaticMetadata physicalStaticInfo = allStaticInfo.get(physicalId);
                validateOneCaptureResult(errorCollector, physicalStaticInfo,
                        physicalWaiverKeys.get(physicalId),
                        allKeys, null/*requestBuilder*/, physicalCaptureResults.get(physicalId),
                        physicalId, i);
            }
        }

        // Verify that all dropped physical camera results are notified via capture failure.
        while (captureListener.hasMoreFailures()) {
            ArrayList<CaptureFailure> failures =
                    captureListener.getCaptureFailures(/*maxNumFailures*/ 1);
            for (CaptureFailure failure : failures) {
                String failedPhysicalId = failure.getPhysicalCameraId();
                Long failedFrameNumber = failure.getFrameNumber();
                if (failedPhysicalId != null) {
                    droppedPhysicalResults.removeIf(
                            n -> n.equals(
                            new Pair<Long, String>(failedFrameNumber, failedPhysicalId)));
                }
            }
        }
        errorCollector.expectTrue(""Not all dropped results for physical cameras are notified"",
                droppedPhysicalResults.isEmpty());
    }

    private static void validateOneCaptureResult(CameraErrorCollector errorCollector,
            StaticMetadata staticInfo, List<CaptureResult.Key<?>> skippedKeys,
            List<CaptureResult.Key<?>> allKeys,
            CaptureRequest.Builder requestBuilder, CaptureResult result, String cameraId,
            int resultCount) throws Exception {
        String failMsg = ""Failed capture result "" + resultCount + "" test"";
        String cameraIdString = "" "";
        if (cameraId != null) {
            cameraIdString += ""for physical camera "" + cameraId;
        }
        boolean verifyMatchRequest = (requestBuilder != null);
        for (CaptureResult.Key<?> key : allKeys) {
            if (!skippedKeys.contains(key)) {
                /**
                 * Check the critical tags here.
                 * TODO: Can use the same key for request and result when request/result
                 * becomes symmetric (b/14059883). Then below check can be wrapped into
                 * a generic function.
                 */
                String msg = failMsg + cameraIdString + ""for key "" + key.getName();
                if (verifyMatchRequest) {
                    if (key.equals(CaptureResult.CONTROL_AE_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.CONTROL_AE_MODE),
                                result.get(CaptureResult.CONTROL_AE_MODE));
                    } else if (key.equals(CaptureResult.CONTROL_AF_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.CONTROL_AF_MODE),
                                result.get(CaptureResult.CONTROL_AF_MODE));
                    } else if (key.equals(CaptureResult.CONTROL_AWB_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.CONTROL_AWB_MODE),
                                result.get(CaptureResult.CONTROL_AWB_MODE));
                    } else if (key.equals(CaptureResult.CONTROL_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.CONTROL_MODE),
                                result.get(CaptureResult.CONTROL_MODE));
                    } else if (key.equals(CaptureResult.STATISTICS_FACE_DETECT_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.STATISTICS_FACE_DETECT_MODE),
                                result.get(CaptureResult.STATISTICS_FACE_DETECT_MODE));
                    } else if (key.equals(CaptureResult.NOISE_REDUCTION_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.NOISE_REDUCTION_MODE),
                                result.get(CaptureResult.NOISE_REDUCTION_MODE));
                    } else if (key.equals(CaptureResult.NOISE_REDUCTION_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.NOISE_REDUCTION_MODE),
                                result.get(CaptureResult.NOISE_REDUCTION_MODE));
                    } else if (key.equals(CaptureResult.REQUEST_PIPELINE_DEPTH)) {

                    } else if (key.equals(CaptureResult.STATISTICS_OIS_DATA_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.STATISTICS_OIS_DATA_MODE),
                                result.get(CaptureResult.STATISTICS_OIS_DATA_MODE));
                    } else if (key.equals(CaptureResult.DISTORTION_CORRECTION_MODE)) {
                        errorCollector.expectEquals(msg,
                                requestBuilder.get(CaptureRequest.DISTORTION_CORRECTION_MODE),
                                result.get(CaptureResult.DISTORTION_CORRECTION_MODE));
                    } else if (key.equals(CaptureResult.SENSOR_DYNAMIC_BLACK_LEVEL)) {
                        float[] blackLevel = errorCollector.expectKeyValueNotNull(
                                result, CaptureResult.SENSOR_DYNAMIC_BLACK_LEVEL);
                        if (blackLevel != null && staticInfo.isMonochromeCamera()) {
                            errorCollector.expectEquals(
                                    ""Monochrome camera dynamic blacklevel must be 2x2"",
                                    blackLevel.length, 4);
                            for (int index = 1; index < blackLevel.length; index++) {
                                errorCollector.expectEquals(
                                    ""Monochrome camera 2x2 channels blacklevel value must be the same."",
                                    blackLevel[index], blackLevel[0]);
                            }
                        }
                    } else {
                        // Only do non-null check for the rest of keys.
                        errorCollector.expectKeyValueNotNull(failMsg, result, key);
                    }
                } else {
                    // Only do non-null check for the rest of keys.
                    errorCollector.expectKeyValueNotNull(failMsg, result, key);
                }
            } else {
                // These keys should always be null
                if (key.equals(CaptureResult.CONTROL_AE_REGIONS)) {
                    errorCollector.expectNull(
                            ""Capture result contains AE regions but aeMaxRegions is 0""
                            + cameraIdString,
                            result.get(CaptureResult.CONTROL_AE_REGIONS));
                } else if (key.equals(CaptureResult.CONTROL_AWB_REGIONS)) {
                    errorCollector.expectNull(
                            ""Capture result contains AWB regions but awbMaxRegions is 0""
                            + cameraIdString,
                            result.get(CaptureResult.CONTROL_AWB_REGIONS));
                } else if (key.equals(CaptureResult.CONTROL_AF_REGIONS)) {
                    errorCollector.expectNull(
                            ""Capture result contains AF regions but afMaxRegions is 0""
                            + cameraIdString,
                            result.get(CaptureResult.CONTROL_AF_REGIONS));
                }
            }
        }
    }

    /*
     * Add waiver keys per camera device hardware level and capability.
     *
     * Must be called after camera device is opened.
     */
    private static List<CaptureResult.Key<?>> getWaiverKeysForCamera(StaticMetadata staticInfo) {
        List<CaptureResult.Key<?>> waiverKeys = new ArrayList<>();

        // Global waiver keys
        waiverKeys.add(CaptureResult.JPEG_GPS_LOCATION);
        waiverKeys.add(CaptureResult.JPEG_ORIENTATION);
        waiverKeys.add(CaptureResult.JPEG_QUALITY);
        waiverKeys.add(CaptureResult.JPEG_THUMBNAIL_QUALITY);
        waiverKeys.add(CaptureResult.JPEG_THUMBNAIL_SIZE);

        if (!staticInfo.isUltraHighResolutionSensor()) {
            waiverKeys.add(CaptureResult.SENSOR_PIXEL_MODE);
            waiverKeys.add(CaptureResult.SENSOR_RAW_BINNING_FACTOR_USED);
        }

        // Keys only present when corresponding control is on are being
        // verified in its own functional test
        // Only present in certain tonemap mode. Test in CaptureRequestTest.
        waiverKeys.add(CaptureResult.TONEMAP_CURVE);
        waiverKeys.add(CaptureResult.TONEMAP_GAMMA);
        waiverKeys.add(CaptureResult.TONEMAP_PRESET_CURVE);
        // Only present when test pattern mode is SOLID_COLOR.
        // TODO: verify this key in test pattern test later
        waiverKeys.add(CaptureResult.SENSOR_TEST_PATTERN_DATA);
        // Only present when STATISTICS_LENS_SHADING_MAP_MODE is ON
        waiverKeys.add(CaptureResult.STATISTICS_LENS_SHADING_CORRECTION_MAP);
        // Only present when STATISTICS_INFO_AVAILABLE_HOT_PIXEL_MAP_MODES is ON
        waiverKeys.add(CaptureResult.STATISTICS_HOT_PIXEL_MAP);
        // Only present when face detection is on
        waiverKeys.add(CaptureResult.STATISTICS_FACES);
        // Only present in reprocessing capture result.
        waiverKeys.add(CaptureResult.REPROCESS_EFFECTIVE_EXPOSURE_FACTOR);

        // LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID not required if key is not supported.
        if (!staticInfo.isLogicalMultiCamera() ||
                !staticInfo.isActivePhysicalCameraIdSupported()) {
            waiverKeys.add(CaptureResult.LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID);
        }

        //Keys not required if RAW is not supported
        if (!staticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
            waiverKeys.add(CaptureResult.SENSOR_NEUTRAL_COLOR_POINT);
            waiverKeys.add(CaptureResult.SENSOR_GREEN_SPLIT);
            waiverKeys.add(CaptureResult.SENSOR_NOISE_PROFILE);
        } else if (staticInfo.isMonochromeCamera()) {
            waiverKeys.add(CaptureResult.SENSOR_NEUTRAL_COLOR_POINT);
            waiverKeys.add(CaptureResult.SENSOR_GREEN_SPLIT);
        }

        boolean calibrationReported = staticInfo.areKeysAvailable(
                CameraCharacteristics.LENS_POSE_ROTATION,
                CameraCharacteristics.LENS_POSE_TRANSLATION,
                CameraCharacteristics.LENS_INTRINSIC_CALIBRATION);

        // If any of distortion coefficients is reported in CameraCharacteristics, HAL must
        // also report (one of) them in CaptureResult
        boolean distortionReported = 
                staticInfo.areKeysAvailable(
                        CameraCharacteristics.LENS_RADIAL_DISTORTION) || 
                staticInfo.areKeysAvailable(
                        CameraCharacteristics.LENS_DISTORTION);

        //Keys for lens distortion correction
        boolean distortionCorrectionSupported = staticInfo.isDistortionCorrectionSupported();
        if (!distortionCorrectionSupported) {
            waiverKeys.add(CaptureResult.DISTORTION_CORRECTION_MODE);
        }

        boolean mustReportDistortion = true;
        // These keys must present on either DEPTH or distortion correction devices
        if (!staticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_DEPTH_OUTPUT) &&
                !distortionCorrectionSupported &&
                !distortionReported) {
            mustReportDistortion = false;
            waiverKeys.add(CaptureResult.LENS_RADIAL_DISTORTION);
            waiverKeys.add(CaptureResult.LENS_DISTORTION);
        } else {
            // Radial distortion doesn't need to be present for new devices, or old devices that
            // opt in the new lens distortion tag.
            CameraCharacteristics c = staticInfo.getCharacteristics();
            if (Build.VERSION.DEVICE_INITIAL_SDK_INT > Build.VERSION_CODES.O_MR1 ||
                    c.get(CameraCharacteristics.LENS_DISTORTION) != null) {
                waiverKeys.add(CaptureResult.LENS_RADIAL_DISTORTION);
            }
        }

        // Calibration keys must exist for
        //   - DEPTH capable devices
        //   - Devices that reports calibration keys in static metadata
        //   - Devices that reports lens distortion keys in static metadata
        if (!staticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_DEPTH_OUTPUT) &&
                !calibrationReported && !mustReportDistortion) {
            waiverKeys.add(CaptureResult.LENS_POSE_ROTATION);
            waiverKeys.add(CaptureResult.LENS_POSE_TRANSLATION);
            waiverKeys.add(CaptureResult.LENS_INTRINSIC_CALIBRATION);
        }

        // Waived if RAW output is not supported
        int[] outputFormats = staticInfo.getAvailableFormats(
                StaticMetadata.StreamDirection.Output);
        boolean supportRaw = false;
        for (int format : outputFormats) {
            if (format == ImageFormat.RAW_SENSOR || format == ImageFormat.RAW10 ||
                    format == ImageFormat.RAW12 || format == ImageFormat.RAW_PRIVATE) {
                supportRaw = true;
                break;
            }
        }
        if (!supportRaw) {
            waiverKeys.add(CaptureResult.CONTROL_POST_RAW_SENSITIVITY_BOOST);
        }

        // Waived if MONOCHROME capability
        if (staticInfo.isMonochromeCamera()) {
            waiverKeys.add(CaptureResult.COLOR_CORRECTION_MODE);
            waiverKeys.add(CaptureResult.COLOR_CORRECTION_TRANSFORM);
            waiverKeys.add(CaptureResult.COLOR_CORRECTION_GAINS);
        }

        if (staticInfo.getAeMaxRegionsChecked() == 0) {
            waiverKeys.add(CaptureResult.CONTROL_AE_REGIONS);
        }
        if (staticInfo.getAwbMaxRegionsChecked() == 0) {
            waiverKeys.add(CaptureResult.CONTROL_AWB_REGIONS);
        }
        if (staticInfo.getAfMaxRegionsChecked() == 0) {
            waiverKeys.add(CaptureResult.CONTROL_AF_REGIONS);
        }

        // Keys for dynamic black/white levels
        if (!staticInfo.isOpticalBlackRegionSupported()) {
            waiverKeys.add(CaptureResult.SENSOR_DYNAMIC_BLACK_LEVEL);
            waiverKeys.add(CaptureResult.SENSOR_DYNAMIC_WHITE_LEVEL);
        }

        if (!staticInfo.isEnableZslSupported()) {
            waiverKeys.add(CaptureResult.CONTROL_ENABLE_ZSL);
        }

        if (!staticInfo.isAfSceneChangeSupported()) {
            waiverKeys.add(CaptureResult.CONTROL_AF_SCENE_CHANGE);
        }

        if (!staticInfo.isOisDataModeSupported()) {
            waiverKeys.add(CaptureResult.STATISTICS_OIS_DATA_MODE);
            waiverKeys.add(CaptureResult.STATISTICS_OIS_SAMPLES);
        }

        if (staticInfo.getAvailableExtendedSceneModeCapsChecked().length == 0) {
            waiverKeys.add(CaptureResult.CONTROL_EXTENDED_SCENE_MODE);
        }

        if (!staticInfo.isRotateAndCropSupported()) {
            waiverKeys.add(CaptureResult.SCALER_ROTATE_AND_CROP);
        }

        if (staticInfo.isHardwareLevelAtLeastFull()) {
            return waiverKeys;
        }

        /*
         * Hardware Level = LIMITED or LEGACY
         */
        // Key not present if certain control is not supported
        if (!staticInfo.isColorCorrectionSupported()) {
            waiverKeys.add(CaptureResult.COLOR_CORRECTION_GAINS);
            waiverKeys.add(CaptureResult.COLOR_CORRECTION_MODE);
            waiverKeys.add(CaptureResult.COLOR_CORRECTION_TRANSFORM);
        }

        if (!staticInfo.isManualColorAberrationControlSupported()) {
            waiverKeys.add(CaptureResult.COLOR_CORRECTION_ABERRATION_MODE);
        }

        if (!staticInfo.isManualToneMapSupported()) {
            waiverKeys.add(CaptureResult.TONEMAP_MODE);
        }

        if (!staticInfo.isEdgeModeControlSupported()) {
            waiverKeys.add(CaptureResult.EDGE_MODE);
        }

        if (!staticInfo.isHotPixelMapModeControlSupported()) {
            waiverKeys.add(CaptureResult.HOT_PIXEL_MODE);
        }

        if (!staticInfo.isNoiseReductionModeControlSupported()) {
            waiverKeys.add(CaptureResult.NOISE_REDUCTION_MODE);
        }

        if (!staticInfo.isManualLensShadingMapSupported()) {
            waiverKeys.add(CaptureResult.SHADING_MODE);
        }

        //Keys not required if neither MANUAL_SENSOR nor READ_SENSOR_SETTINGS is supported
        if (!staticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR) &&
            !staticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS)) {
            waiverKeys.add(CaptureResult.SENSOR_EXPOSURE_TIME);
            waiverKeys.add(CaptureResult.SENSOR_SENSITIVITY);
            waiverKeys.add(CaptureResult.LENS_FOCUS_DISTANCE);
            waiverKeys.add(CaptureResult.LENS_APERTURE);
        }

        if (!staticInfo.isCapabilitySupported(
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
            waiverKeys.add(CaptureResult.SENSOR_FRAME_DURATION);
            waiverKeys.add(CaptureResult.BLACK_LEVEL_LOCK);
            waiverKeys.add(CaptureResult.LENS_FOCUS_RANGE);
            waiverKeys.add(CaptureResult.LENS_STATE);
            waiverKeys.add(CaptureResult.LENS_FILTER_DENSITY);
        }

        if (staticInfo.isHardwareLevelLimited() && staticInfo.isColorOutputSupported()) {
            return waiverKeys;
        }

        /*
         * Hardware Level = EXTERNAL
         */
        if (staticInfo.isExternalCamera()) {
            waiverKeys.add(CaptureResult.LENS_FOCAL_LENGTH);
            waiverKeys.add(CaptureResult.SENSOR_TEST_PATTERN_MODE);
            waiverKeys.add(CaptureResult.SENSOR_ROLLING_SHUTTER_SKEW);
        }

        if (staticInfo.isExternalCamera() && staticInfo.isColorOutputSupported()) {
            return waiverKeys;
        }

        /*
         * Hardware Level = LEGACY or no regular output is supported
         */
        waiverKeys.add(CaptureResult.CONTROL_AE_PRECAPTURE_TRIGGER);
        waiverKeys.add(CaptureResult.CONTROL_AE_STATE);
        waiverKeys.add(CaptureResult.CONTROL_AWB_STATE);
        waiverKeys.add(CaptureResult.FLASH_STATE);
        waiverKeys.add(CaptureResult.LENS_OPTICAL_STABILIZATION_MODE);
        waiverKeys.add(CaptureResult.SENSOR_ROLLING_SHUTTER_SKEW);
        waiverKeys.add(CaptureResult.STATISTICS_LENS_SHADING_MAP_MODE);
        waiverKeys.add(CaptureResult.STATISTICS_SCENE_FLICKER);
        waiverKeys.add(CaptureResult.STATISTICS_HOT_PIXEL_MAP_MODE);
        waiverKeys.add(CaptureResult.CONTROL_AE_TARGET_FPS_RANGE);
        waiverKeys.add(CaptureResult.CONTROL_AF_TRIGGER);

        if (staticInfo.isHardwareLevelLegacy()) {
            return waiverKeys;
        }

        /*
         * Regular output not supported, only depth, waive color-output-related keys
         */
        waiverKeys.add(CaptureResult.CONTROL_SCENE_MODE);
        waiverKeys.add(CaptureResult.CONTROL_EFFECT_MODE);
        waiverKeys.add(CaptureResult.CONTROL_VIDEO_STABILIZATION_MODE);
        waiverKeys.add(CaptureResult.SENSOR_TEST_PATTERN_MODE);
        waiverKeys.add(CaptureResult.NOISE_REDUCTION_MODE);
        waiverKeys.add(CaptureResult.COLOR_CORRECTION_ABERRATION_MODE);
        waiverKeys.add(CaptureResult.CONTROL_AE_ANTIBANDING_MODE);
        waiverKeys.add(CaptureResult.CONTROL_AE_EXPOSURE_COMPENSATION);
        waiverKeys.add(CaptureResult.CONTROL_AE_LOCK);
        waiverKeys.add(CaptureResult.CONTROL_AE_MODE);
        waiverKeys.add(CaptureResult.CONTROL_AF_MODE);
        waiverKeys.add(CaptureResult.CONTROL_AWB_MODE);
        waiverKeys.add(CaptureResult.CONTROL_AWB_LOCK);
        waiverKeys.add(CaptureResult.CONTROL_ZOOM_RATIO);
        waiverKeys.add(CaptureResult.STATISTICS_FACE_DETECT_MODE);
        waiverKeys.add(CaptureResult.FLASH_MODE);
        waiverKeys.add(CaptureResult.SCALER_CROP_REGION);
        waiverKeys.add(CaptureResult.SCALER_ROTATE_AND_CROP);

        return waiverKeys;
    }

    /**
     * A capture listener implementation for collecting both partial and total results.
     *
     * <p> This is not a full-blown class and has some implicit assumptions. The class groups
     * capture results by capture request, so the user must guarantee each request this listener
     * is listening is unique. This class is not thread safe, so don't attach an instance object
     * with multiple handlers.</p>
     * */
    private static class TotalAndPartialResultListener
            extends CameraCaptureSession.CaptureCallback {
        static final int ERROR_DUPLICATED_REQUEST = 1 << 0;
        static final int ERROR_WRONG_CALLBACK_ORDER = 1 << 1;

        private final LinkedBlockingQueue<Pair<TotalCaptureResult, List<CaptureResult>> > mQueue =
                new LinkedBlockingQueue<>();
        private final HashMap<CaptureRequest, List<CaptureResult>> mPartialResultsMap =
                new HashMap<CaptureRequest, List<CaptureResult>>();
        private final HashSet<CaptureRequest> completedRequests = new HashSet<>();
        private int errorCode = 0;

        @Override
        public void onCaptureStarted(
            CameraCaptureSession session, CaptureRequest request, long timestamp, long frameNumber)
        {
            checkCallbackOrder(request);
            createMapEntryIfNecessary(request);
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                TotalCaptureResult result) {
            try {
                List<CaptureResult> partialResultsList = mPartialResultsMap.get(request);
                if (partialResultsList == null) {
                    Log.w(TAG, ""onCaptureCompleted: unknown request"");
                }
                mQueue.put(new Pair<TotalCaptureResult, List<CaptureResult>>(
                        result, partialResultsList));
                mPartialResultsMap.remove(request);
                boolean newEntryAdded = completedRequests.add(request);
                if (!newEntryAdded) {
                    Integer frame = (Integer) request.getTag();
                    Log.e(TAG, ""Frame "" + frame + ""ERROR_DUPLICATED_REQUEST"");
                    errorCode |= ERROR_DUPLICATED_REQUEST;
                }
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureCompleted"");
            }
        }

        @Override
        public void onCaptureProgressed(CameraCaptureSession session, CaptureRequest request,
                CaptureResult partialResult) {
            createMapEntryIfNecessary(request);
            List<CaptureResult> partialResultsList = mPartialResultsMap.get(request);
            partialResultsList.add(partialResult);
        }

        private void createMapEntryIfNecessary(CaptureRequest request) {
            if (!mPartialResultsMap.containsKey(request)) {
                // create a new entry in the map
                mPartialResultsMap.put(request, new ArrayList<CaptureResult>());
            }
        }

        private void checkCallbackOrder(CaptureRequest request) {
            if (completedRequests.contains(request)) {
                Integer frame = (Integer) request.getTag();
                Log.e(TAG, ""Frame "" + frame + ""ERROR_WRONG_CALLBACK_ORDER"");
                errorCode |= ERROR_WRONG_CALLBACK_ORDER;
            }
        }

        public Pair<TotalCaptureResult, List<CaptureResult>> getCaptureResultPairs(long timeout) {
            try {
                Pair<TotalCaptureResult, List<CaptureResult>> result =
                        mQueue.poll(timeout, TimeUnit.MILLISECONDS);
                assertNotNull(""Wait for a capture result timed out in "" + timeout + ""ms"", result);
                return result;
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }
        }

        public int getErrorCode() {
            return errorCode;
        }
    }

    // Returns true if `result` has timestamp `sensorTimestamp` when queried from the NDK via
    // ACameraMetadata_fromCameraMetadata().
    private static native boolean validateACameraMetadataFromCameraMetadataCriticalTagsNative(
        CaptureResult result, long sensorTimestamp);

    // First stash a native ACameraMetadata created from a capture result, then compare the stored value
    // to the passed-in timestamp.
    private static native boolean stashACameraMetadataFromCameraMetadataNative(CaptureResult result);
    private static native boolean validateStashedACameraMetadataFromCameraMetadataNative(long timestamp);

    /**
     * TODO: Use CameraCharacteristics.getAvailableCaptureResultKeys() once we can filter out
     * @hide keys.
     *
     */

    /*@O~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~
     * The key entries below this point are generated from metadata
     * definitions in /system/media/camera/docs. Do not modify by hand or
     * modify the comment blocks at the start or end.
     *~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~*/

    private static List<CaptureResult.Key<?>> getAllCaptureResultKeys() {
        ArrayList<CaptureResult.Key<?>> resultKeys = new ArrayList<CaptureResult.Key<?>>();
        resultKeys.add(CaptureResult.COLOR_CORRECTION_MODE);
        resultKeys.add(CaptureResult.COLOR_CORRECTION_TRANSFORM);
        resultKeys.add(CaptureResult.COLOR_CORRECTION_GAINS);
        resultKeys.add(CaptureResult.COLOR_CORRECTION_ABERRATION_MODE);
        resultKeys.add(CaptureResult.CONTROL_AE_ANTIBANDING_MODE);
        resultKeys.add(CaptureResult.CONTROL_AE_EXPOSURE_COMPENSATION);
        resultKeys.add(CaptureResult.CONTROL_AE_LOCK);
        resultKeys.add(CaptureResult.CONTROL_AE_MODE);
        resultKeys.add(CaptureResult.CONTROL_AE_REGIONS);
        resultKeys.add(CaptureResult.CONTROL_AE_TARGET_FPS_RANGE);
        resultKeys.add(CaptureResult.CONTROL_AE_PRECAPTURE_TRIGGER);
        resultKeys.add(CaptureResult.CONTROL_AF_MODE);
        resultKeys.add(CaptureResult.CONTROL_AF_REGIONS);
        resultKeys.add(CaptureResult.CONTROL_AF_TRIGGER);
        resultKeys.add(CaptureResult.CONTROL_AWB_LOCK);
        resultKeys.add(CaptureResult.CONTROL_AWB_MODE);
        resultKeys.add(CaptureResult.CONTROL_AWB_REGIONS);
        resultKeys.add(CaptureResult.CONTROL_CAPTURE_INTENT);
        resultKeys.add(CaptureResult.CONTROL_EFFECT_MODE);
        resultKeys.add(CaptureResult.CONTROL_MODE);
        resultKeys.add(CaptureResult.CONTROL_SCENE_MODE);
        resultKeys.add(CaptureResult.CONTROL_VIDEO_STABILIZATION_MODE);
        resultKeys.add(CaptureResult.CONTROL_AE_STATE);
        resultKeys.add(CaptureResult.CONTROL_AF_STATE);
        resultKeys.add(CaptureResult.CONTROL_AWB_STATE);
        resultKeys.add(CaptureResult.CONTROL_POST_RAW_SENSITIVITY_BOOST);
        resultKeys.add(CaptureResult.CONTROL_ENABLE_ZSL);
        resultKeys.add(CaptureResult.CONTROL_AF_SCENE_CHANGE);
        resultKeys.add(CaptureResult.CONTROL_EXTENDED_SCENE_MODE);
        resultKeys.add(CaptureResult.CONTROL_ZOOM_RATIO);
        resultKeys.add(CaptureResult.EDGE_MODE);
        resultKeys.add(CaptureResult.FLASH_MODE);
        resultKeys.add(CaptureResult.FLASH_STATE);
        resultKeys.add(CaptureResult.HOT_PIXEL_MODE);
        resultKeys.add(CaptureResult.JPEG_GPS_LOCATION);
        resultKeys.add(CaptureResult.JPEG_ORIENTATION);
        resultKeys.add(CaptureResult.JPEG_QUALITY);
        resultKeys.add(CaptureResult.JPEG_THUMBNAIL_QUALITY);
        resultKeys.add(CaptureResult.JPEG_THUMBNAIL_SIZE);
        resultKeys.add(CaptureResult.LENS_APERTURE);
        resultKeys.add(CaptureResult.LENS_FILTER_DENSITY);
        resultKeys.add(CaptureResult.LENS_FOCAL_LENGTH);
        resultKeys.add(CaptureResult.LENS_FOCUS_DISTANCE);
        resultKeys.add(CaptureResult.LENS_OPTICAL_STABILIZATION_MODE);
        resultKeys.add(CaptureResult.LENS_POSE_ROTATION);
        resultKeys.add(CaptureResult.LENS_POSE_TRANSLATION);
        resultKeys.add(CaptureResult.LENS_FOCUS_RANGE);
        resultKeys.add(CaptureResult.LENS_STATE);
        resultKeys.add(CaptureResult.LENS_INTRINSIC_CALIBRATION);
        resultKeys.add(CaptureResult.LENS_RADIAL_DISTORTION);
        resultKeys.add(CaptureResult.LENS_DISTORTION);
        resultKeys.add(CaptureResult.NOISE_REDUCTION_MODE);
        resultKeys.add(CaptureResult.REQUEST_PIPELINE_DEPTH);
        resultKeys.add(CaptureResult.SCALER_CROP_REGION);
        resultKeys.add(CaptureResult.SCALER_ROTATE_AND_CROP);
        resultKeys.add(CaptureResult.SENSOR_EXPOSURE_TIME);
        resultKeys.add(CaptureResult.SENSOR_FRAME_DURATION);
        resultKeys.add(CaptureResult.SENSOR_SENSITIVITY);
        resultKeys.add(CaptureResult.SENSOR_TIMESTAMP);
        resultKeys.add(CaptureResult.SENSOR_NEUTRAL_COLOR_POINT);
        resultKeys.add(CaptureResult.SENSOR_NOISE_PROFILE);
        resultKeys.add(CaptureResult.SENSOR_GREEN_SPLIT);
        resultKeys.add(CaptureResult.SENSOR_TEST_PATTERN_DATA);
        resultKeys.add(CaptureResult.SENSOR_TEST_PATTERN_MODE);
        resultKeys.add(CaptureResult.SENSOR_ROLLING_SHUTTER_SKEW);
        resultKeys.add(CaptureResult.SENSOR_DYNAMIC_BLACK_LEVEL);
        resultKeys.add(CaptureResult.SENSOR_DYNAMIC_WHITE_LEVEL);
        resultKeys.add(CaptureResult.SENSOR_PIXEL_MODE);
        resultKeys.add(CaptureResult.SENSOR_RAW_BINNING_FACTOR_USED);
        resultKeys.add(CaptureResult.SHADING_MODE);
        resultKeys.add(CaptureResult.STATISTICS_FACE_DETECT_MODE);
        resultKeys.add(CaptureResult.STATISTICS_HOT_PIXEL_MAP_MODE);
        resultKeys.add(CaptureResult.STATISTICS_FACES);
        resultKeys.add(CaptureResult.STATISTICS_LENS_SHADING_CORRECTION_MAP);
        resultKeys.add(CaptureResult.STATISTICS_SCENE_FLICKER);
        resultKeys.add(CaptureResult.STATISTICS_HOT_PIXEL_MAP);
        resultKeys.add(CaptureResult.STATISTICS_LENS_SHADING_MAP_MODE);
        resultKeys.add(CaptureResult.STATISTICS_OIS_DATA_MODE);
        resultKeys.add(CaptureResult.STATISTICS_OIS_SAMPLES);
        resultKeys.add(CaptureResult.TONEMAP_CURVE);
        resultKeys.add(CaptureResult.TONEMAP_MODE);
        resultKeys.add(CaptureResult.TONEMAP_GAMMA);
        resultKeys.add(CaptureResult.TONEMAP_PRESET_CURVE);
        resultKeys.add(CaptureResult.BLACK_LEVEL_LOCK);
        resultKeys.add(CaptureResult.REPROCESS_EFFECTIVE_EXPOSURE_FACTOR);
        resultKeys.add(CaptureResult.LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID);
        resultKeys.add(CaptureResult.DISTORTION_CORRECTION_MODE);

        return resultKeys;
    }

    /*~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~
     * End generated code
     *~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~@~O@*/
}"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.FastBasicsTest"	"testCamera2"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/FastBasicsTest.java"	""	"public void testCamera2() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing camera2 API for camera device "" + mCameraIdsUnderTest[i]);

                if (!mAllStaticInfo.get(mCameraIdsUnderTest[i]).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }

                openDevice(mCameraIdsUnderTest[i]);
                camera2TestByCamera();
            } finally {
                closeDevice();
            }
        }
    }

    public void camera2TestByCamera() throws Exception {
        CaptureRequest.Builder previewRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureRequest.Builder stillCaptureRequest =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
        Size previewSize = mOrderedPreviewSizes.get(0);
        Size stillSize = mOrderedStillSizes.get(0);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        SimpleImageReaderListener imageListener = new SimpleImageReaderListener();

        prepareStillCaptureAndStartPreview(previewRequest, stillCaptureRequest,
                previewSize, stillSize, resultListener, imageListener, false /*isHeic*/);

        CaptureResult result = resultListener.getCaptureResult(WAIT_FOR_FRAMES_TIMEOUT_MS);

        Long timestamp = result.get(CaptureResult.SENSOR_TIMESTAMP);
        assertNotNull(""Can't read a capture result timestamp"", timestamp);

        CaptureResult result2 = resultListener.getCaptureResult(WAIT_FOR_FRAMES_TIMEOUT_MS);

        Long timestamp2 = result2.get(CaptureResult.SENSOR_TIMESTAMP);
        assertNotNull(""Can't read a capture result 2 timestamp"", timestamp2);

        assertTrue(""Bad timestamps"", timestamp2 > timestamp);

        // If EnableZsl is supported, disable ZSL in order to compare preview and still timestamps.
        if (mStaticInfo.isEnableZslSupported()) {
            stillCaptureRequest.set(CaptureRequest.CONTROL_ENABLE_ZSL, false);
        }

        CaptureRequest capture = stillCaptureRequest.build();
        mSession.capture(capture, resultListener, mHandler);

        CaptureResult stillResult =
                resultListener.getTotalCaptureResultForRequest(capture, FRAMES_TO_WAIT_FOR_CAPTURE);

        Long timestamp3 = stillResult.get(CaptureResult.SENSOR_TIMESTAMP);
        assertNotNull(""Can't read a still capture result timestamp"", timestamp3);

        assertTrue(""Bad timestamps"", timestamp3 > timestamp2);

        Image img = imageListener.getImage(WAIT_FOR_PICTURE_TIMEOUT_MS);

        ByteBuffer jpegBuffer = img.getPlanes()[0].getBuffer();
        byte[] jpegData = new byte[jpegBuffer.remaining()];
        jpegBuffer.get(jpegData);

        Bitmap b = BitmapFactory.decodeByteArray(jpegData, 0, jpegData.length);

        assertNotNull(""Unable to decode still capture JPEG"", b);

        closeImageReader();
    }

    private class Camera1Listener
            implements SurfaceTexture.OnFrameAvailableListener, Camera.PictureCallback {

        private Object mFrameSignal = new Object();
        private boolean mGotFrame = false;

        public boolean waitForFrame() {
            synchronized(mFrameSignal) {
                boolean waited = false;
                while (!waited) {
                    try {
                        mFrameSignal.wait(WAIT_FOR_FRAMES_TIMEOUT_MS);
                        waited = true;
                    } catch (InterruptedException e) {
                    }
                }
                return mGotFrame;
            }
        }

        public void onFrameAvailable(SurfaceTexture s) {
            synchronized(mFrameSignal) {
                mGotFrame = true;
                mFrameSignal.notifyAll();
            }
        }

        private Object mPictureSignal = new Object();
        private boolean mGotPicture = false;
        private byte[] mPictureData = null;

        public byte[] waitForPicture() {
            Log.i(TAG, ""waitForPicture called"");
            synchronized(mPictureSignal) {
                boolean waited = false;
                while (!waited) {
                    try {
                        mPictureSignal.wait(WAIT_FOR_PICTURE_TIMEOUT_MS);
                        waited = true;
                        Log.i(TAG, ""waitForPicture returned with mGotPicture = "" + mGotPicture);
                    } catch (InterruptedException e) {
                        Log.e(TAG, ""waitForPicture gets interrupted exception!"");
                    }
                }
                return mPictureData;
            }
        }

        public void onPictureTaken(byte[] data, Camera camera) {
            Log.i(TAG, ""onPictureTaken called"");
            synchronized(mPictureSignal) {
                mPictureData = data;
                mGotPicture = true;
                mPictureSignal.notifyAll();
            }
        }
    }

    @Presubmit"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.camera.cts.api31test.SPerfClassTest"	"getCameraIdList"	"CtsCameraApi31TestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/api31test/src/android/camera/cts/api31test/SPerfClassTest.java"	""	"public void test/*
 *.
 */

package android.camera.cts.api31test;

import static android.hardware.camera2.cts.CameraTestUtils.*;

import android.content.Context;
import android.graphics.ImageFormat;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CaptureFailure;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.cts.CameraTestUtils;
import android.hardware.camera2.cts.CameraTestUtils.SimpleImageReaderListener;
import android.hardware.camera2.cts.helpers.CameraErrorCollector;
import android.hardware.camera2.cts.helpers.StaticMetadata;
import android.hardware.camera2.cts.helpers.StaticMetadata.CheckLevel;
import android.hardware.camera2.params.OutputConfiguration;
import android.hardware.camera2.params.SessionConfiguration;
import android.hardware.camera2.TotalCaptureResult;
import android.media.Image;
import android.media.ImageReader;
import android.os.Handler;
import android.os.HandlerThread;
import android.test.AndroidTestCase;
import android.util.Log;
import android.util.Size;
import android.view.Surface;

import com.android.compatibility.common.util.CddTest;
import com.android.ex.camera2.blocking.BlockingSessionCallback;

import java.util.ArrayList;
import java.util.List;

import org.junit.Test;

import static junit.framework.Assert.*;
import static org.mockito.Mockito.*;

public class SPerfClassTest extends AndroidTestCase {
    private static final String TAG = ""SPerfClassTest"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);

    private static final Size FULLHD = new Size(1920, 1080);
    private static final Size VGA = new Size(640, 480);
    private static final int CONFIGURE_TIMEOUT = 5000; //ms
    private static final int CAPTURE_TIMEOUT = 1500; //ms

    private CameraManager mCameraManager;
    private String[] mCameraIds;
    private Handler mHandler;
    private HandlerThread mHandlerThread;
    private CameraErrorCollector mCollector;

    @Override
    public void setContext(Context context) {
        super.setContext(context);
        mCameraManager = context.getSystemService(CameraManager.class);
        assertNotNull(""Can't connect to camera manager!"", mCameraManager);
    }

    @Override
    protected void setUp() throws Exception {
        super.setUp();

        mCameraIds = mCameraManager.getCameraIdList();
        assertNotNull(""Camera ids shouldn't be null"", mCameraIds);

        mHandlerThread = new HandlerThread(TAG);
        mHandlerThread.start();
        mHandler = new Handler(mHandlerThread.getLooper());

        mCollector = new CameraErrorCollector();
    }

    @Override
    protected void tearDown() throws Exception {
        mHandlerThread.quitSafely();
        mHandler = null;

        try {
            mCollector.verify();
        } catch (Throwable e) {
            // When new Exception(e) is used, exception info will be printed twice.
            throw new Exception(e.getMessage());
        } finally {
            super.tearDown();
        }
    }

    // Verify primary camera devices's supported JPEG sizes are at least 1080p.
    private void testSPerfClassJpegSizesByCamera(String cameraId) throws Exception {
        boolean isPrimaryRear = CameraTestUtils.isPrimaryRearFacingCamera(
                mCameraManager, cameraId);
        boolean isPrimaryFront = CameraTestUtils.isPrimaryFrontFacingCamera(
                mCameraManager, cameraId);
        if (!isPrimaryRear && !isPrimaryFront) {
            return;
        }

        CameraCharacteristics c = mCameraManager.getCameraCharacteristics(cameraId);
        StaticMetadata staticInfo = new StaticMetadata(c, CheckLevel.ASSERT, mCollector);

        Size[] jpegSizes = staticInfo.getJpegOutputSizesChecked();
        assertTrue(""Primary cameras must support JPEG formats"",
                jpegSizes != null && jpegSizes.length > 0);
        for (Size jpegSize : jpegSizes) {
            mCollector.expectTrue(
                    ""Primary camera's JPEG size must be at least 1080p, but is "" +
                    jpegSize,
                    jpegSize.getWidth() >= FULLHD.getWidth() &&
                    jpegSize.getHeight() >= FULLHD.getHeight());
        }

        CameraDevice camera = null;
        ImageReader jpegTarget = null;
        Image image = null;
        try {
            camera = CameraTestUtils.openCamera(mCameraManager, cameraId,
                    /*listener*/null, mHandler);

            List<OutputConfiguration> outputConfigs = new ArrayList<>();
            SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
            jpegTarget = CameraTestUtils.makeImageReader(VGA,
                    ImageFormat.JPEG, 1 /*maxNumImages*/, imageListener, mHandler);
            Surface jpegSurface = jpegTarget.getSurface();
            outputConfigs.add(new OutputConfiguration(jpegSurface));

            // isSessionConfigurationSupported will return true for JPEG sizes smaller
            // than 1080P, due to framework rouding up to closest supported size (1080p).
            SessionConfigSupport sessionConfigSupport = isSessionConfigSupported(
                    camera, mHandler, outputConfigs, /*inputConfig*/ null,
                    SessionConfiguration.SESSION_REGULAR, true/*defaultSupport*/);
            mCollector.expectTrue(""isSessionConfiguration fails with error"",
                    !sessionConfigSupport.error);
            mCollector.expectTrue(""isSessionConfiguration returns false for JPEG < 1080p"",
                    sessionConfigSupport.configSupported);

            // Session creation for JPEG sizes smaller than 1080p will succeed, and the
            // result JPEG image dimension is rounded up to closest supported size (1080p).
            CaptureRequest.Builder request =
                    camera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
            request.addTarget(jpegSurface);


            CameraCaptureSession.StateCallback sessionListener =
                    mock(CameraCaptureSession.StateCallback.class);
            CameraCaptureSession session = configureCameraSessionWithConfig(
                    camera, outputConfigs, sessionListener, mHandler);

            verify(sessionListener, timeout(CONFIGURE_TIMEOUT).atLeastOnce()).
                    onConfigured(any(CameraCaptureSession.class));
            verify(sessionListener, timeout(CONFIGURE_TIMEOUT).atLeastOnce()).
                    onReady(any(CameraCaptureSession.class));
            verify(sessionListener, never()).onConfigureFailed(any(CameraCaptureSession.class));
            verify(sessionListener, never()).onActive(any(CameraCaptureSession.class));
            verify(sessionListener, never()).onClosed(any(CameraCaptureSession.class));

            CameraCaptureSession.CaptureCallback captureListener =
                    mock(CameraCaptureSession.CaptureCallback.class);
            session.capture(request.build(), captureListener, mHandler);

            verify(captureListener, timeout(CAPTURE_TIMEOUT).atLeastOnce()).
                    onCaptureCompleted(any(CameraCaptureSession.class),
                            any(CaptureRequest.class), any(TotalCaptureResult.class));
            verify(captureListener, never()).onCaptureFailed(any(CameraCaptureSession.class),
                    any(CaptureRequest.class), any(CaptureFailure.class));

            image = imageListener.getImage(CAPTURE_TIMEOUT);
            assertNotNull(""Image must be valid"", image);
            assertEquals(""Image format isn't JPEG"", image.getFormat(), ImageFormat.JPEG);

            byte[] data = CameraTestUtils.getDataFromImage(image);
            assertTrue(""Invalid image data"", data != null && data.length > 0);

            CameraTestUtils.validateJpegData(data, FULLHD.getWidth(), FULLHD.getHeight(),
                    null /*filePath*/);
        } finally {
            if (camera != null) {
                camera.close();
            }
            if (jpegTarget != null) {
                jpegTarget.close();
            }
            if (image != null) {
                image.close();
            }
        }
    }

    /**
     * Check camera S Performance class requirement for JPEG sizes.
     */
    @CddTest(requirement=""7.5/H-1-8"")"	""	""	"JPEG 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.camera.cts.api31test.SPerfClassTest"	"testSPerfClassJpegSizes"	"CtsCameraApi31TestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/api31test/src/android/camera/cts/api31test/SPerfClassTest.java"	""	"public void testSPerfClassJpegSizes() throws Exception {
        boolean isSPerfClass = CameraTestUtils.isSPerfClass();
        if (!isSPerfClass) {
            return;
        }

        for (int i = 0; i < mCameraIds.length; i++) {
            testSPerfClassJpegSizesByCamera(mCameraIds[i]);
        }
    }
}"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ReprocessCaptureTest"	"testBasicYuvToYuvReprocessing"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ReprocessCaptureTest.java"	""	"public void testBasicYuvToYuvReprocessing() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            if (!isYuvReprocessSupported(id)) {
                continue;
            }

            // YUV_420_888 -> YUV_420_888 must be supported.
            testBasicReprocessing(id, ImageFormat.YUV_420_888, ImageFormat.YUV_420_888);
        }
    }

    /**
     * Test YUV_420_888 -> JPEG with maximal supported sizes
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ReprocessCaptureTest"	"testBasicYuvToJpegReprocessing"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ReprocessCaptureTest.java"	""	"public void testBasicYuvToJpegReprocessing() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            if (!isYuvReprocessSupported(id)) {
                continue;
            }

            // YUV_420_888 -> JPEG must be supported.
            testBasicReprocessing(id, ImageFormat.YUV_420_888, ImageFormat.JPEG);
        }
    }

    /**
     * Test YUV_420_888 -> HEIC with maximal supported sizes
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ReprocessCaptureTest"	"testBasicOpaqueToYuvReprocessing"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ReprocessCaptureTest.java"	""	"public void testBasicOpaqueToYuvReprocessing() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            if (!isOpaqueReprocessSupported(id)) {
                continue;
            }

            // Opaque -> YUV_420_888 must be supported.
            testBasicReprocessing(id, ImageFormat.PRIVATE, ImageFormat.YUV_420_888);
        }
    }

    /**
     * Test OPAQUE -> JPEG with maximal supported sizes
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ReprocessCaptureTest"	"testBasicOpaqueToJpegReprocessing"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ReprocessCaptureTest.java"	""	"public void testBasicOpaqueToJpegReprocessing() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            if (!isOpaqueReprocessSupported(id)) {
                continue;
            }

            // OPAQUE -> JPEG must be supported.
            testBasicReprocessing(id, ImageFormat.PRIVATE, ImageFormat.JPEG);
        }
    }

    /**
     * Test OPAQUE -> HEIC with maximal supported sizes
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ReprocessCaptureTest"	"testCrossSessionCaptureException"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ReprocessCaptureTest.java"	""	"public void testCrossSessionCaptureException() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            // Test one supported input format -> JPEG
            int inputFormat;
            int reprocessOutputFormat = ImageFormat.JPEG;

            if (isOpaqueReprocessSupported(id)) {
                inputFormat = ImageFormat.PRIVATE;
            } else if (isYuvReprocessSupported(id)) {
                inputFormat = ImageFormat.YUV_420_888;
            } else {
                continue;
            }

            openDevice(id);

            // Test the largest sizes
            Size inputSize =
                    getMaxSize(inputFormat, StaticMetadata.StreamDirection.Input);
            Size reprocessOutputSize =
                    getMaxSize(reprocessOutputFormat, StaticMetadata.StreamDirection.Output);

            try {
                if (VERBOSE) {
                    Log.v(TAG, ""testCrossSessionCaptureException: cameraId: "" + id +
                            "" inputSize: "" + inputSize + "" inputFormat: "" + inputFormat +
                            "" reprocessOutputSize: "" + reprocessOutputSize +
                            "" reprocessOutputFormat: "" + reprocessOutputFormat);
                }

                setupImageReaders(inputSize, inputFormat, reprocessOutputSize,
                        reprocessOutputFormat, /*maxImages*/1);
                setupReprocessableSession(/*previewSurface*/null, /*numImageWriterImages*/1);

                TotalCaptureResult result = submitCaptureRequest(mFirstImageReader.getSurface(),
                        /*inputResult*/null);
                Image image = mFirstImageReaderListener.getImage(CAPTURE_TIMEOUT_MS);

                // queue the image to image writer
                mImageWriter.queueInputImage(image);

                // recreate the session
                closeReprossibleSession();
                setupReprocessableSession(/*previewSurface*/null, /*numImageWriterImages*/1);
                try {
                    TotalCaptureResult reprocessResult;
                    // issue and wait on reprocess capture request
                    reprocessResult = submitCaptureRequest(
                            getReprocessOutputImageReader().getSurface(), result);
                    fail(""Camera "" + id + "": should get IllegalArgumentException for cross "" +
                            ""session reprocess captrue."");
                } catch (IllegalArgumentException e) {
                    // expected
                    if (DEBUG) {
                        Log.d(TAG, ""Camera "" + id + "": get IllegalArgumentException for cross "" +
                                ""session reprocess capture as expected: "" + e.getMessage());
                    }
                }
            } finally {
                closeReprossibleSession();
                closeImageReaders();
                closeDevice();
            }
        }
    }

    /**
     * Verify queued input images are cleared in new reprocessable capture session.
     *
     * This tests the case where an application receives onCaptureBufferLost() for an
     * output stream, resulting in pending input buffers not having corresponding request.
     *
     * For subsequent new reprocessable capture session, ImageWriter.queueInputBuffer may become
     * stuck due to stale buffers from previous session.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ReprocessCaptureTest"	"testQueueImageWithoutRequest"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ReprocessCaptureTest.java"	""	"public void testQueueImageWithoutRequest() throws Exception {
        final int MAX_IMAGES = 1;
        final int ITERATIONS = MAX_IMAGES + 3;
        for (String id : mCameraIdsUnderTest) {
            // Test one supported input format -> JPEG
            int inputFormat;
            int reprocessOutputFormat = ImageFormat.JPEG;

            if (isOpaqueReprocessSupported(id)) {
                inputFormat = ImageFormat.PRIVATE;
            } else if (isYuvReprocessSupported(id)) {
                inputFormat = ImageFormat.YUV_420_888;
            } else {
                continue;
            }

            openDevice(id);

            // Test the largest sizes
            Size inputSize =
                    getMaxSize(inputFormat, StaticMetadata.StreamDirection.Input);
            Size reprocessOutputSize =
                    getMaxSize(reprocessOutputFormat, StaticMetadata.StreamDirection.Output);

            try {
                if (VERBOSE) {
                    Log.v(TAG, ""testQueueImageWithoutRequest: cameraId: "" + id +
                            "" inputSize: "" + inputSize + "" inputFormat: "" + inputFormat +
                            "" reprocessOutputSize: "" + reprocessOutputSize +
                            "" reprocessOutputFormat: "" + reprocessOutputFormat);
                }

                setupImageReaders(inputSize, inputFormat, reprocessOutputSize,
                        reprocessOutputFormat, MAX_IMAGES);

                for (int i = 0; i < ITERATIONS; i++) {
                    setupReprocessableSession(/*previewSurface*/null, /*numImageWriterImages*/1);

                    TotalCaptureResult result = submitCaptureRequest(mFirstImageReader.getSurface(),
                            /*inputResult*/null);
                    Image image = mFirstImageReaderListener.getImage(CAPTURE_TIMEOUT_MS);

                    // queue the image to image writer
                    mImageWriter.queueInputImage(image);

                    mInputSurface = null;
                    mImageWriter.close();
                    mImageWriter = null;
                }
            } finally {
                closeReprossibleSession();
                closeImageReaders();
                closeDevice();
            }
        }
    }

    /**
     * Test burst reprocessing captures with and without preview.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ReprocessCaptureTest"	"testReprocessTimestamps"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ReprocessCaptureTest.java"	""	"public void testReprocessTimestamps() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            if (!isYuvReprocessSupported(id) && !isOpaqueReprocessSupported(id)) {
                continue;
            }

            try {
                // open Camera device
                openDevice(id);

                int[] supportedInputFormats =
                    mStaticInfo.getAvailableFormats(StaticMetadata.StreamDirection.Input);
                for (int inputFormat : supportedInputFormats) {
                    int[] supportedReprocessOutputFormats =
                            mStaticInfo.getValidOutputFormatsForInput(inputFormat);
                    for (int reprocessOutputFormat : supportedReprocessOutputFormats) {
                        testReprocessingMaxSizes(id, inputFormat, reprocessOutputFormat,
                                /*previewSize*/null, CaptureTestCase.TIMESTAMPS);
                    }
                }
            } finally {
                closeDevice();
            }
        }
    }

    /**
     * Test reprocess jpeg output's exif data for the largest input and output sizes for each
     * supported format.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ReprocessCaptureTest"	"testReprocessJpegExif"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ReprocessCaptureTest.java"	""	"public void testReprocessJpegExif() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            if (!isYuvReprocessSupported(id) && !isOpaqueReprocessSupported(id)) {
                continue;
            }

            try {
                // open Camera device
                openDevice(id);

                int[] supportedInputFormats =
                    mStaticInfo.getAvailableFormats(StaticMetadata.StreamDirection.Input);

                for (int inputFormat : supportedInputFormats) {
                    int[] supportedReprocessOutputFormats =
                            mStaticInfo.getValidOutputFormatsForInput(inputFormat);

                    for (int reprocessOutputFormat : supportedReprocessOutputFormats) {
                        if (reprocessOutputFormat == ImageFormat.JPEG) {
                            testReprocessingMaxSizes(id, inputFormat, ImageFormat.JPEG,
                                    /*previewSize*/null, CaptureTestCase.JPEG_EXIF);
                        }
                    }
                }
            } finally {
                closeDevice();
            }
        }
    }"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	4
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ReprocessCaptureTest"	"testReprocessRequestKeys"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ReprocessCaptureTest.java"	""	"public void testReprocessRequestKeys() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            if (!isYuvReprocessSupported(id) && !isOpaqueReprocessSupported(id)) {
                continue;
            }

            try {
                // open Camera device
                openDevice(id);

                int[] supportedInputFormats =
                    mStaticInfo.getAvailableFormats(StaticMetadata.StreamDirection.Input);
                for (int inputFormat : supportedInputFormats) {
                    int[] supportedReprocessOutputFormats =
                            mStaticInfo.getValidOutputFormatsForInput(inputFormat);
                    for (int reprocessOutputFormat : supportedReprocessOutputFormats) {
                        testReprocessingMaxSizes(id, inputFormat, reprocessOutputFormat,
                                /*previewSize*/null, CaptureTestCase.REQUEST_KEYS);
                    }
                }
            } finally {
                closeDevice();
            }
        }
    }

    /**
     * Test the input format and output format with the largest input and output sizes.
     */
    private void testBasicReprocessing(String cameraId, int inputFormat,
            int reprocessOutputFormat) throws Exception {
        try {
            openDevice(cameraId);

            testReprocessingMaxSizes(cameraId, inputFormat, reprocessOutputFormat,
                    /* previewSize */null, CaptureTestCase.SINGLE_SHOT);
        } finally {
            closeDevice();
        }
    }

    /**
     * Test the input format and output format with the largest input and output sizes for a
     * certain test case.
     */
    private void testReprocessingMaxSizes(String cameraId, int inputFormat,
            int reprocessOutputFormat, Size previewSize, CaptureTestCase captureTestCase)
            throws Exception {
        Size maxInputSize = getMaxSize(inputFormat, StaticMetadata.StreamDirection.Input);
        Size maxReprocessOutputSize =
                getMaxSize(reprocessOutputFormat, StaticMetadata.StreamDirection.Output);

        switch (captureTestCase) {
            case SINGLE_SHOT:
                testReprocess(cameraId, maxInputSize, inputFormat, maxReprocessOutputSize,
                        reprocessOutputFormat, previewSize, NUM_REPROCESS_CAPTURES);
                break;
            case ABORT_CAPTURE:
                testReprocessAbort(cameraId, maxInputSize, inputFormat, maxReprocessOutputSize,
                        reprocessOutputFormat);
                break;
            case TIMESTAMPS:
                testReprocessTimestamps(cameraId, maxInputSize, inputFormat, maxReprocessOutputSize,
                        reprocessOutputFormat);
                break;
            case JPEG_EXIF:
                testReprocessJpegExif(cameraId, maxInputSize, inputFormat, maxReprocessOutputSize);
                break;
            case REQUEST_KEYS:
                testReprocessRequestKeys(cameraId, maxInputSize, inputFormat,
                        maxReprocessOutputSize, reprocessOutputFormat);
                break;
            default:
                throw new IllegalArgumentException(""Invalid test case"");
        }
    }

    /**
     * Test all input format, input size, output format, and output size combinations.
     */
    private void testReprocessingAllCombinations(String cameraId, Size previewSize,
            CaptureTestCase captureTestCase) throws Exception {

        Size QCIF = new Size(176, 144);
        Size VGA = new Size(640, 480);
        Size FULL_HD = new Size(1920, 1080);
        int[] supportedInputFormats =
                mStaticInfo.getAvailableFormats(StaticMetadata.StreamDirection.Input);
        for (int inputFormat : supportedInputFormats) {
            Size[] supportedInputSizes =
                    mStaticInfo.getAvailableSizesForFormatChecked(inputFormat,
                    StaticMetadata.StreamDirection.Input);

            for (Size inputSize : supportedInputSizes) {
                int[] supportedReprocessOutputFormats =
                        mStaticInfo.getValidOutputFormatsForInput(inputFormat);

                for (int reprocessOutputFormat : supportedReprocessOutputFormats) {
                    Size[] supportedReprocessOutputSizes =
                            mStaticInfo.getAvailableSizesForFormatChecked(reprocessOutputFormat,
                            StaticMetadata.StreamDirection.Output);

                    for (Size reprocessOutputSize : supportedReprocessOutputSizes) {
                        // Handle QCIF exceptions
                        if (reprocessOutputSize.equals(QCIF) &&
                                ((inputSize.getWidth() > FULL_HD.getWidth()) ||
                                 (inputSize.getHeight() > FULL_HD.getHeight()))) {
                            continue;
                        }
                        if (inputSize.equals(QCIF) &&
                                ((reprocessOutputSize.getWidth() > FULL_HD.getWidth()) ||
                                 (reprocessOutputSize.getHeight() > FULL_HD.getHeight()))) {
                            continue;
                        }
                        if ((previewSize != null) &&
                                ((previewSize.getWidth() > FULL_HD.getWidth()) || (
                                  previewSize.getHeight() > FULL_HD.getHeight())) &&
                                (inputSize.equals(QCIF) || reprocessOutputSize.equals(QCIF))) {
                            previewSize = VGA;
                        }

                        switch (captureTestCase) {
                            case SINGLE_SHOT:
                                testReprocess(cameraId, inputSize, inputFormat,
                                        reprocessOutputSize, reprocessOutputFormat, previewSize,
                                        NUM_REPROCESS_CAPTURES);
                                break;
                            case BURST:
                                testReprocessBurst(cameraId, inputSize, inputFormat,
                                        reprocessOutputSize, reprocessOutputFormat, previewSize,
                                        NUM_REPROCESS_BURST);
                                break;
                            case MIXED_BURST:
                                testReprocessMixedBurst(cameraId, inputSize, inputFormat,
                                        reprocessOutputSize, reprocessOutputFormat, previewSize,
                                        NUM_REPROCESS_BURST);
                                break;
                            default:
                                throw new IllegalArgumentException(""Invalid test case"");
                        }
                    }
                }
            }
        }
    }

    /**
     * Test burst that is mixed with regular and reprocess capture requests.
     */
    private void testReprocessMixedBurst(String cameraId, Size inputSize, int inputFormat,
            Size reprocessOutputSize, int reprocessOutputFormat, Size previewSize,
            int numBurst) throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""testReprocessMixedBurst: cameraId: "" + cameraId + "" inputSize: "" +
                    inputSize + "" inputFormat: "" + inputFormat + "" reprocessOutputSize: "" +
                    reprocessOutputSize + "" reprocessOutputFormat: "" + reprocessOutputFormat +
                    "" previewSize: "" + previewSize + "" numBurst: "" + numBurst);
        }

        boolean enablePreview = (previewSize != null);
        ImageResultHolder[] imageResultHolders = new ImageResultHolder[0];

        try {
            // totalNumBurst = number of regular burst + number of reprocess burst.
            int totalNumBurst = numBurst * 2;

            if (enablePreview) {
                updatePreviewSurface(previewSize);
            } else {
                mPreviewSurface = null;
            }

            setupImageReaders(inputSize, inputFormat, reprocessOutputSize, reprocessOutputFormat,
                totalNumBurst);
            setupReprocessableSession(mPreviewSurface, /*numImageWriterImages*/numBurst);

            if (enablePreview) {
                startPreview(mPreviewSurface);
            }

            // Prepare an array of booleans indicating each capture's type (regular or reprocess)
            boolean[] isReprocessCaptures = new boolean[totalNumBurst];
            for (int i = 0; i < totalNumBurst; i++) {
                if ((i & 1) == 0) {
                    isReprocessCaptures[i] = true;
                } else {
                    isReprocessCaptures[i] = false;
                }
            }

            imageResultHolders = doMixedReprocessBurstCapture(isReprocessCaptures);
            for (ImageResultHolder holder : imageResultHolders) {
                Image reprocessedImage = holder.getImage();
                TotalCaptureResult result = holder.getTotalCaptureResult();

                mCollector.expectImageProperties(""testReprocessMixedBurst"", reprocessedImage,
                            reprocessOutputFormat, reprocessOutputSize,
                            result.get(CaptureResult.SENSOR_TIMESTAMP));

                if (DEBUG) {
                    Log.d(TAG, String.format(""camera %s in %dx%d %d out %dx%d %d"",
                            cameraId, inputSize.getWidth(), inputSize.getHeight(), inputFormat,
                            reprocessOutputSize.getWidth(), reprocessOutputSize.getHeight(),
                            reprocessOutputFormat));
                    dumpImage(reprocessedImage,
                            ""/testReprocessMixedBurst_camera"" + cameraId + ""_"" + mDumpFrameCount);
                    mDumpFrameCount++;
                }
            }
        } finally {
            for (ImageResultHolder holder : imageResultHolders) {
                holder.getImage().close();
            }
            closeReprossibleSession();
            closeImageReaders();
        }
    }

    /**
     * Test burst of reprocess capture requests.
     */
    private void testReprocessBurst(String cameraId, Size inputSize, int inputFormat,
            Size reprocessOutputSize, int reprocessOutputFormat, Size previewSize,
            int numBurst) throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""testReprocessBurst: cameraId: "" + cameraId + "" inputSize: "" +
                    inputSize + "" inputFormat: "" + inputFormat + "" reprocessOutputSize: "" +
                    reprocessOutputSize + "" reprocessOutputFormat: "" + reprocessOutputFormat +
                    "" previewSize: "" + previewSize + "" numBurst: "" + numBurst);
        }

        boolean enablePreview = (previewSize != null);
        ImageResultHolder[] imageResultHolders = new ImageResultHolder[0];

        try {
            if (enablePreview) {
                updatePreviewSurface(previewSize);
            } else {
                mPreviewSurface = null;
            }

            setupImageReaders(inputSize, inputFormat, reprocessOutputSize, reprocessOutputFormat,
                numBurst);
            setupReprocessableSession(mPreviewSurface, numBurst);

            if (enablePreview) {
                startPreview(mPreviewSurface);
            }

            imageResultHolders = doReprocessBurstCapture(numBurst);
            for (ImageResultHolder holder : imageResultHolders) {
                Image reprocessedImage = holder.getImage();
                TotalCaptureResult result = holder.getTotalCaptureResult();

                mCollector.expectImageProperties(""testReprocessBurst"", reprocessedImage,
                            reprocessOutputFormat, reprocessOutputSize,
                            result.get(CaptureResult.SENSOR_TIMESTAMP));

                if (DEBUG) {
                    Log.d(TAG, String.format(""camera %s in %dx%d %d out %dx%d %d"",
                            cameraId, inputSize.getWidth(), inputSize.getHeight(), inputFormat,
                            reprocessOutputSize.getWidth(), reprocessOutputSize.getHeight(),
                            reprocessOutputFormat));
                    dumpImage(reprocessedImage,
                            ""/testReprocessBurst_camera"" + cameraId + ""_"" + mDumpFrameCount);
                    mDumpFrameCount++;
                }
            }
        } finally {
            for (ImageResultHolder holder : imageResultHolders) {
                holder.getImage().close();
            }
            closeReprossibleSession();
            closeImageReaders();
        }
    }

    /**
     * Test a sequences of reprocess capture requests.
     */
    private void testReprocess(String cameraId, Size inputSize, int inputFormat,
            Size reprocessOutputSize, int reprocessOutputFormat, Size previewSize,
            int numReprocessCaptures) throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""testReprocess: cameraId: "" + cameraId + "" inputSize: "" +
                    inputSize + "" inputFormat: "" + inputFormat + "" reprocessOutputSize: "" +
                    reprocessOutputSize + "" reprocessOutputFormat: "" + reprocessOutputFormat +
                    "" previewSize: "" + previewSize);
        }

        boolean enablePreview = (previewSize != null);

        try {
            if (enablePreview) {
                updatePreviewSurface(previewSize);
            } else {
                mPreviewSurface = null;
            }

            setupImageReaders(inputSize, inputFormat, reprocessOutputSize, reprocessOutputFormat,
                    /*maxImages*/1);
            setupReprocessableSession(mPreviewSurface, /*numImageWriterImages*/1);

            if (enablePreview) {
                startPreview(mPreviewSurface);
            }

            for (int i = 0; i < numReprocessCaptures; i++) {
                ImageResultHolder imageResultHolder = null;

                try {
                    imageResultHolder = doReprocessCapture();
                    Image reprocessedImage = imageResultHolder.getImage();
                    TotalCaptureResult result = imageResultHolder.getTotalCaptureResult();

                    mCollector.expectImageProperties(""testReprocess"", reprocessedImage,
                            reprocessOutputFormat, reprocessOutputSize,
                            result.get(CaptureResult.SENSOR_TIMESTAMP));

                    if (DEBUG) {
                        Log.d(TAG, String.format(""camera %s in %dx%d %d out %dx%d %d"",
                                cameraId, inputSize.getWidth(), inputSize.getHeight(), inputFormat,
                                reprocessOutputSize.getWidth(), reprocessOutputSize.getHeight(),
                                reprocessOutputFormat));

                        dumpImage(reprocessedImage,
                                ""/testReprocess_camera"" + cameraId + ""_"" + mDumpFrameCount);
                        mDumpFrameCount++;
                    }
                } finally {
                    if (imageResultHolder != null) {
                        imageResultHolder.getImage().close();
                    }
                }
            }
        } finally {
            closeReprossibleSession();
            closeImageReaders();
        }
    }

    /**
     * Test aborting a burst reprocess capture and multiple single reprocess captures.
     */
    private void testReprocessAbort(String cameraId, Size inputSize, int inputFormat,
            Size reprocessOutputSize, int reprocessOutputFormat) throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""testReprocessAbort: cameraId: "" + cameraId + "" inputSize: "" +
                    inputSize + "" inputFormat: "" + inputFormat + "" reprocessOutputSize: "" +
                    reprocessOutputSize + "" reprocessOutputFormat: "" + reprocessOutputFormat);
        }

        try {
            setupImageReaders(inputSize, inputFormat, reprocessOutputSize, reprocessOutputFormat,
                    NUM_REPROCESS_CAPTURES);
            setupReprocessableSession(/*previewSurface*/null, NUM_REPROCESS_CAPTURES);

            // Wait for session READY state after session creation
            mSessionListener.getStateWaiter().waitForState(
                    BlockingSessionCallback.SESSION_READY, SESSION_CLOSE_TIMEOUT_MS);

            // Test two cases: submitting reprocess requests one by one and in a burst.
            boolean submitInBursts[] = {false, true};
            for (boolean submitInBurst : submitInBursts) {
                // Prepare reprocess capture requests.
                ArrayList<CaptureRequest> reprocessRequests =
                        new ArrayList<>(NUM_REPROCESS_CAPTURES);

                for (int i = 0; i < NUM_REPROCESS_CAPTURES; i++) {
                    TotalCaptureResult result = submitCaptureRequest(mFirstImageReader.getSurface(),
                            /*inputResult*/null);

                    // Wait and drain the READY state for each reprocessing input output.
                    mSessionListener.getStateWaiter().waitForState(
                            BlockingSessionCallback.SESSION_READY, SESSION_CLOSE_TIMEOUT_MS);

                    mImageWriter.queueInputImage(
                            mFirstImageReaderListener.getImage(CAPTURE_TIMEOUT_MS));
                    CaptureRequest.Builder builder = mCamera.createReprocessCaptureRequest(result);
                    builder.addTarget(getReprocessOutputImageReader().getSurface());
                    reprocessRequests.add(builder.build());
                }

                SimpleCaptureCallback captureCallback = new SimpleCaptureCallback();

                // Submit reprocess capture requests.
                if (submitInBurst) {
                    mSession.captureBurst(reprocessRequests, captureCallback, mHandler);
                } else {
                    for (CaptureRequest request : reprocessRequests) {
                        mSession.capture(request, captureCallback, mHandler);
                    }
                }

                // Abort after getting the first result
                TotalCaptureResult reprocessResult =
                        captureCallback.getTotalCaptureResultForRequest(reprocessRequests.get(0),
                        CAPTURE_TIMEOUT_FRAMES);
                mSession.abortCaptures();

                // Wait until the session is ready again.
                mSessionListener.getStateWaiter().waitForState(
                        BlockingSessionCallback.SESSION_READY, SESSION_CLOSE_TIMEOUT_MS);

                // Gather all failed requests.
                ArrayList<CaptureFailure> failures =
                        captureCallback.getCaptureFailures(NUM_REPROCESS_CAPTURES - 1);
                ArrayList<CaptureRequest> failedRequests = new ArrayList<>();
                for (CaptureFailure failure : failures) {
                    failedRequests.add(failure.getRequest());
                }

                // For each request that didn't fail must have a valid result.
                for (int i = 1; i < reprocessRequests.size(); i++) {
                    CaptureRequest request = reprocessRequests.get(i);
                    if (!failedRequests.contains(request)) {
                        captureCallback.getTotalCaptureResultForRequest(request,
                                CAPTURE_TIMEOUT_FRAMES);
                    }
                }

                // Drain the image reader listeners.
                mFirstImageReaderListener.drain();
                if (!mShareOneImageReader) {
                    mSecondImageReaderListener.drain();
                }

                // Make sure all input surfaces are released.
                for (int i = 0; i < NUM_REPROCESS_CAPTURES; i++) {
                    mImageWriterListener.waitForImageReleased(CAPTURE_TIMEOUT_MS);
                }
            }
        } finally {
            closeReprossibleSession();
            closeImageReaders();
        }
    }

    /**
     * Test timestamps for reprocess requests. Reprocess request's shutter timestamp, result's
     * sensor timestamp, and output image's timestamp should match the reprocess input's timestamp.
     */
    private void testReprocessTimestamps(String cameraId, Size inputSize, int inputFormat,
            Size reprocessOutputSize, int reprocessOutputFormat) throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""testReprocessTimestamps: cameraId: "" + cameraId + "" inputSize: "" +
                    inputSize + "" inputFormat: "" + inputFormat + "" reprocessOutputSize: "" +
                    reprocessOutputSize + "" reprocessOutputFormat: "" + reprocessOutputFormat);
        }

        try {
            setupImageReaders(inputSize, inputFormat, reprocessOutputSize, reprocessOutputFormat,
                    NUM_REPROCESS_CAPTURES);
            setupReprocessableSession(/*previewSurface*/null, NUM_REPROCESS_CAPTURES);

            // Prepare reprocess capture requests.
            ArrayList<CaptureRequest> reprocessRequests = new ArrayList<>(NUM_REPROCESS_CAPTURES);
            ArrayList<Long> expectedTimestamps = new ArrayList<>(NUM_REPROCESS_CAPTURES);

            for (int i = 0; i < NUM_REPROCESS_CAPTURES; i++) {
                TotalCaptureResult result = submitCaptureRequest(mFirstImageReader.getSurface(),
                        /*inputResult*/null);

                mImageWriter.queueInputImage(
                        mFirstImageReaderListener.getImage(CAPTURE_TIMEOUT_MS));
                CaptureRequest.Builder builder = mCamera.createReprocessCaptureRequest(result);
                builder.addTarget(getReprocessOutputImageReader().getSurface());
                reprocessRequests.add(builder.build());
                // Reprocess result's timestamp should match input image's timestamp.
                expectedTimestamps.add(result.get(CaptureResult.SENSOR_TIMESTAMP));
            }

            // Submit reprocess requests.
            SimpleCaptureCallback captureCallback = new SimpleCaptureCallback();
            mSession.captureBurst(reprocessRequests, captureCallback, mHandler);

            // Verify we get the expected timestamps.
            for (int i = 0; i < reprocessRequests.size(); i++) {
                captureCallback.waitForCaptureStart(reprocessRequests.get(i),
                        expectedTimestamps.get(i), CAPTURE_TIMEOUT_FRAMES);
            }

            TotalCaptureResult[] reprocessResults =
                    captureCallback.getTotalCaptureResultsForRequests(reprocessRequests,
                    CAPTURE_TIMEOUT_FRAMES);

            for (int i = 0; i < expectedTimestamps.size(); i++) {
                // Verify the result timestamps match the input image's timestamps.
                long expected = expectedTimestamps.get(i);
                long timestamp = reprocessResults[i].get(CaptureResult.SENSOR_TIMESTAMP);
                assertEquals(""Reprocess result timestamp ("" + timestamp + "") doesn't match input "" +
                        ""image's timestamp ("" + expected + "")"", expected, timestamp);

                // Verify the reprocess output image timestamps match the input image's timestamps.
                Image image = getReprocessOutputImageReaderListener().getImage(CAPTURE_TIMEOUT_MS);
                timestamp = image.getTimestamp();
                image.close();

                assertEquals(""Reprocess output timestamp ("" + timestamp + "") doesn't match input "" +
                        ""image's timestamp ("" + expected + "")"", expected, timestamp);
            }

            // Make sure all input surfaces are released.
            for (int i = 0; i < NUM_REPROCESS_CAPTURES; i++) {
                mImageWriterListener.waitForImageReleased(CAPTURE_TIMEOUT_MS);
            }
        } finally {
            closeReprossibleSession();
            closeImageReaders();
        }
    }

    /**
     * Test JPEG tags for reprocess requests. Reprocess result's JPEG tags and JPEG image's tags
     * match reprocess request's JPEG tags.
     */
    private void testReprocessJpegExif(String cameraId, Size inputSize, int inputFormat,
            Size reprocessOutputSize) throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""testReprocessJpegExif: cameraId: "" + cameraId + "" inputSize: "" +
                    inputSize + "" inputFormat: "" + inputFormat + "" reprocessOutputSize: "" +
                    reprocessOutputSize);
        }

        Size[] thumbnailSizes = mStaticInfo.getAvailableThumbnailSizesChecked();
        Size[] testThumbnailSizes = new Size[EXIF_TEST_DATA.length];
        Arrays.fill(testThumbnailSizes, thumbnailSizes[thumbnailSizes.length - 1]);
        // Make sure thumbnail size (0, 0) is covered.
        testThumbnailSizes[0] = new Size(0, 0);

        try {
            setupImageReaders(inputSize, inputFormat, reprocessOutputSize, ImageFormat.JPEG,
                    EXIF_TEST_DATA.length);
            setupReprocessableSession(/*previewSurface*/null, EXIF_TEST_DATA.length);

            // Prepare reprocess capture requests.
            ArrayList<CaptureRequest> reprocessRequests = new ArrayList<>(EXIF_TEST_DATA.length);

            for (int i = 0; i < EXIF_TEST_DATA.length; i++) {
                TotalCaptureResult result = submitCaptureRequest(mFirstImageReader.getSurface(),
                        /*inputResult*/null);
                mImageWriter.queueInputImage(
                        mFirstImageReaderListener.getImage(CAPTURE_TIMEOUT_MS));

                CaptureRequest.Builder builder = mCamera.createReprocessCaptureRequest(result);
                builder.addTarget(getReprocessOutputImageReader().getSurface());

                // set jpeg keys
                setJpegKeys(builder, EXIF_TEST_DATA[i], testThumbnailSizes[i], mCollector);
                reprocessRequests.add(builder.build());
            }

            // Submit reprocess requests.
            SimpleCaptureCallback captureCallback = new SimpleCaptureCallback();
            mSession.captureBurst(reprocessRequests, captureCallback, mHandler);

            TotalCaptureResult[] reprocessResults =
                    captureCallback.getTotalCaptureResultsForRequests(reprocessRequests,
                    CAPTURE_TIMEOUT_FRAMES);

            for (int i = 0; i < EXIF_TEST_DATA.length; i++) {
                // Verify output image's and result's JPEG EXIF data.
                Image image = getReprocessOutputImageReaderListener().getImage(CAPTURE_TIMEOUT_MS);
                verifyJpegKeys(image, reprocessResults[i], reprocessOutputSize,
                        testThumbnailSizes[i], EXIF_TEST_DATA[i], mStaticInfo, mCollector,
                        mDebugFileNameBase, ImageFormat.JPEG);
                image.close();

            }
        } finally {
            closeReprossibleSession();
            closeImageReaders();
        }
    }



    /**
     * Test the following keys in reprocess results match the keys in reprocess requests:
     *   1. EDGE_MODE
     *   2. NOISE_REDUCTION_MODE
     *   3. REPROCESS_EFFECTIVE_EXPOSURE_FACTOR (only for YUV reprocess)
     */
    private void testReprocessRequestKeys(String cameraId, Size inputSize, int inputFormat,
            Size reprocessOutputSize, int reprocessOutputFormat) throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""testReprocessRequestKeys: cameraId: "" + cameraId + "" inputSize: "" +
                    inputSize + "" inputFormat: "" + inputFormat + "" reprocessOutputSize: "" +
                    reprocessOutputSize + "" reprocessOutputFormat: "" + reprocessOutputFormat);
        }

        final Integer[] EDGE_MODES = {CaptureRequest.EDGE_MODE_FAST,
                CaptureRequest.EDGE_MODE_HIGH_QUALITY, CaptureRequest.EDGE_MODE_OFF,
                CaptureRequest.EDGE_MODE_ZERO_SHUTTER_LAG};
        final Integer[] NR_MODES = {CaptureRequest.NOISE_REDUCTION_MODE_HIGH_QUALITY,
                CaptureRequest.NOISE_REDUCTION_MODE_OFF,
                CaptureRequest.NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG,
                CaptureRequest.NOISE_REDUCTION_MODE_FAST};
        final Float[] EFFECTIVE_EXP_FACTORS = {null, 1.0f, 2.5f, 4.0f};
        int numFrames = EDGE_MODES.length;

        try {
            setupImageReaders(inputSize, inputFormat, reprocessOutputSize, reprocessOutputFormat,
                    numFrames);
            setupReprocessableSession(/*previewSurface*/null, numFrames);

            // Prepare reprocess capture requests.
            ArrayList<CaptureRequest> reprocessRequests = new ArrayList<>(numFrames);

            for (int i = 0; i < numFrames; i++) {
                TotalCaptureResult result = submitCaptureRequest(mFirstImageReader.getSurface(),
                        /*inputResult*/null);
                mImageWriter.queueInputImage(
                        mFirstImageReaderListener.getImage(CAPTURE_TIMEOUT_MS));

                CaptureRequest.Builder builder = mCamera.createReprocessCaptureRequest(result);
                builder.addTarget(getReprocessOutputImageReader().getSurface());

                // Set reprocess request keys
                builder.set(CaptureRequest.EDGE_MODE, EDGE_MODES[i]);
                builder.set(CaptureRequest.NOISE_REDUCTION_MODE, NR_MODES[i]);
                if (inputFormat == ImageFormat.YUV_420_888) {
                    builder.set(CaptureRequest.REPROCESS_EFFECTIVE_EXPOSURE_FACTOR,
                            EFFECTIVE_EXP_FACTORS[i]);
                }
                reprocessRequests.add(builder.build());
            }

            // Submit reprocess requests.
            SimpleCaptureCallback captureCallback = new SimpleCaptureCallback();
            mSession.captureBurst(reprocessRequests, captureCallback, mHandler);

            TotalCaptureResult[] reprocessResults =
                    captureCallback.getTotalCaptureResultsForRequests(reprocessRequests,
                    CAPTURE_TIMEOUT_FRAMES);

            for (int i = 0; i < numFrames; i++) {
                // Verify result's keys
                Integer resultEdgeMode = reprocessResults[i].get(CaptureResult.EDGE_MODE);
                Integer resultNoiseReductionMode =
                        reprocessResults[i].get(CaptureResult.NOISE_REDUCTION_MODE);

                assertEquals(""Reprocess result edge mode ("" + resultEdgeMode +
                        "") doesn't match requested edge mode ("" + EDGE_MODES[i] + "")"",
                        resultEdgeMode, EDGE_MODES[i]);
                assertEquals(""Reprocess result noise reduction mode ("" + resultNoiseReductionMode +
                        "") doesn't match requested noise reduction mode ("" +
                        NR_MODES[i] + "")"", resultNoiseReductionMode,
                        NR_MODES[i]);

                if (inputFormat == ImageFormat.YUV_420_888) {
                    Float resultEffectiveExposureFactor = reprocessResults[i].get(
                            CaptureResult.REPROCESS_EFFECTIVE_EXPOSURE_FACTOR);
                    assertEquals(""Reprocess effective exposure factor ("" +
                            resultEffectiveExposureFactor + "") doesn't match requested "" +
                            ""effective exposure factor ("" + EFFECTIVE_EXP_FACTORS[i] + "")"",
                            resultEffectiveExposureFactor, EFFECTIVE_EXP_FACTORS[i]);
                }
            }
        } finally {
            closeReprossibleSession();
            closeImageReaders();
        }
    }

    /**
     * Set up two image readers: one for regular capture (used for reprocess input) and one for
     * reprocess capture.
     */
    private void setupImageReaders(Size inputSize, int inputFormat, Size reprocessOutputSize,
            int reprocessOutputFormat, int maxImages) {

        mShareOneImageReader = false;
        // If the regular output and reprocess output have the same size and format,
        // they can share one image reader.
        if (inputFormat == reprocessOutputFormat &&
                inputSize.equals(reprocessOutputSize)) {
            maxImages *= 2;
            mShareOneImageReader = true;
        }
        // create an ImageReader for the regular capture
        mFirstImageReaderListener = new SimpleImageReaderListener();
        mFirstImageReader = makeImageReader(inputSize, inputFormat, maxImages,
                mFirstImageReaderListener, mHandler);

        if (!mShareOneImageReader) {
            // create an ImageReader for the reprocess capture
            mSecondImageReaderListener = new SimpleImageReaderListener();
            mSecondImageReader = makeImageReader(reprocessOutputSize, reprocessOutputFormat,
                    maxImages, mSecondImageReaderListener, mHandler);
        }
    }

    /**
     * Close two image readers.
     */
    private void closeImageReaders() {
        CameraTestUtils.closeImageReader(mFirstImageReader);
        mFirstImageReader = null;
        CameraTestUtils.closeImageReader(mSecondImageReader);
        mSecondImageReader = null;
    }

    /**
     * Get the ImageReader for reprocess output.
     */
    private ImageReader getReprocessOutputImageReader() {
        if (mShareOneImageReader) {
            return mFirstImageReader;
        } else {
            return mSecondImageReader;
        }
    }

    private SimpleImageReaderListener getReprocessOutputImageReaderListener() {
        if (mShareOneImageReader) {
            return mFirstImageReaderListener;
        } else {
            return mSecondImageReaderListener;
        }
    }

    /**
     * Set up a reprocessable session and create an ImageWriter with the sessoin's input surface.
     */
    private void setupReprocessableSession(Surface previewSurface, int numImageWriterImages)
            throws Exception {
        // create a reprocessable capture session
        List<Surface> outSurfaces = new ArrayList<Surface>();
        outSurfaces.add(mFirstImageReader.getSurface());
        if (!mShareOneImageReader) {
            outSurfaces.add(mSecondImageReader.getSurface());
        }
        if (previewSurface != null) {
            outSurfaces.add(previewSurface);
        }

        InputConfiguration inputConfig = new InputConfiguration(mFirstImageReader.getWidth(),
                mFirstImageReader.getHeight(), mFirstImageReader.getImageFormat());
        String inputConfigString = inputConfig.toString();
        if (VERBOSE) {
            Log.v(TAG, ""InputConfiguration: "" + inputConfigString);
        }
        assertTrue(String.format(""inputConfig is wrong: %dx%d format %d. Expect %dx%d format %d"",
                inputConfig.getWidth(), inputConfig.getHeight(), inputConfig.getFormat(),
                mFirstImageReader.getWidth(), mFirstImageReader.getHeight(),
                mFirstImageReader.getImageFormat()),
                inputConfig.getWidth() == mFirstImageReader.getWidth() &&
                inputConfig.getHeight() == mFirstImageReader.getHeight() &&
                inputConfig.getFormat() == mFirstImageReader.getImageFormat());

        mSessionListener = new BlockingSessionCallback();
        mSession = configureReprocessableCameraSession(mCamera, inputConfig, outSurfaces,
                mSessionListener, mHandler);

        // create an ImageWriter
        mInputSurface = mSession.getInputSurface();
        mImageWriter = ImageWriter.newInstance(mInputSurface,
                numImageWriterImages);

        mImageWriterListener = new SimpleImageWriterListener(mImageWriter);
        mImageWriter.setOnImageReleasedListener(mImageWriterListener, mHandler);
    }

    /**
     * Close the reprocessable session and ImageWriter.
     */
    private void closeReprossibleSession() {
        mInputSurface = null;

        if (mSession != null) {
            mSession.close();
            mSession = null;
        }

        if (mImageWriter != null) {
            mImageWriter.close();
            mImageWriter = null;
        }
    }

    /**
     * Do one reprocess capture.
     */
    private ImageResultHolder doReprocessCapture() throws Exception {
        return doReprocessBurstCapture(/*numBurst*/1)[0];
    }

    /**
     * Do a burst of reprocess captures.
     */
    private ImageResultHolder[] doReprocessBurstCapture(int numBurst) throws Exception {
        boolean[] isReprocessCaptures = new boolean[numBurst];
        for (int i = 0; i < numBurst; i++) {
            isReprocessCaptures[i] = true;
        }

        return doMixedReprocessBurstCapture(isReprocessCaptures);
    }

    /**
     * Do a burst of captures that are mixed with regular and reprocess captures.
     *
     * @param isReprocessCaptures An array whose elements indicate whether it's a reprocess capture
     *                            request. If the element is true, it represents a reprocess capture
     *                            request. If the element is false, it represents a regular capture
     *                            request. The size of the array is the number of capture requests
     *                            in the burst.
     */
    private ImageResultHolder[] doMixedReprocessBurstCapture(boolean[] isReprocessCaptures)
            throws Exception {
        if (isReprocessCaptures == null || isReprocessCaptures.length <= 0) {
            throw new IllegalArgumentException(""isReprocessCaptures must have at least 1 capture."");
        }

        boolean hasReprocessRequest = false;
        boolean hasRegularRequest = false;

        TotalCaptureResult[] results = new TotalCaptureResult[isReprocessCaptures.length];
        for (int i = 0; i < isReprocessCaptures.length; i++) {
            // submit a capture and get the result if this entry is a reprocess capture.
            if (isReprocessCaptures[i]) {
                results[i] = submitCaptureRequest(mFirstImageReader.getSurface(),
                        /*inputResult*/null);
                mImageWriter.queueInputImage(
                        mFirstImageReaderListener.getImage(CAPTURE_TIMEOUT_MS));
                hasReprocessRequest = true;
            } else {
                hasRegularRequest = true;
            }
        }

        Surface[] outputSurfaces = new Surface[isReprocessCaptures.length];
        for (int i = 0; i < isReprocessCaptures.length; i++) {
            outputSurfaces[i] = getReprocessOutputImageReader().getSurface();
        }

        TotalCaptureResult[] finalResults = submitMixedCaptureBurstRequest(outputSurfaces, results);

        ImageResultHolder[] holders = new ImageResultHolder[isReprocessCaptures.length];
        for (int i = 0; i < isReprocessCaptures.length; i++) {
            Image image = getReprocessOutputImageReaderListener().getImage(CAPTURE_TIMEOUT_MS);
            if (hasReprocessRequest && hasRegularRequest) {
                // If there are mixed requests, images and results may not be in the same order.
                for (int j = 0; j < finalResults.length; j++) {
                    if (finalResults[j] != null &&
                            finalResults[j].get(CaptureResult.SENSOR_TIMESTAMP) ==
                            image.getTimestamp()) {
                        holders[i] = new ImageResultHolder(image, finalResults[j]);
                        finalResults[j] = null;
                        break;
                    }
                }

                assertNotNull(""Cannot find a result matching output image's timestamp: "" +
                        image.getTimestamp(), holders[i]);
            } else {
                // If no mixed requests, images and results should be in the same order.
                holders[i] = new ImageResultHolder(image, finalResults[i]);
            }
        }

        return holders;
    }

    /**
     * Start preview without a listener.
     */
    private void startPreview(Surface previewSurface) throws Exception {
        CaptureRequest.Builder builder = mCamera.createCaptureRequest(ZSL_TEMPLATE);
        builder.addTarget(previewSurface);
        mSession.setRepeatingRequest(builder.build(), null, mHandler);
    }

    /**
     * Issue a capture request and return the result. If inputResult is null, it's a regular
     * request. Otherwise, it's a reprocess request.
     */
    private TotalCaptureResult submitCaptureRequest(Surface output,
            TotalCaptureResult inputResult) throws Exception {
        Surface[] outputs = new Surface[1];
        outputs[0] = output;
        TotalCaptureResult[] inputResults = new TotalCaptureResult[1];
        inputResults[0] = inputResult;

        return submitMixedCaptureBurstRequest(outputs, inputResults)[0];
    }

    /**
     * Submit a burst request mixed with regular and reprocess requests.
     *
     * @param outputs An array of output surfaces. One output surface will be used in one request
     *                so the length of the array is the number of requests in a burst request.
     * @param inputResults An array of input results. If it's null, all requests are regular
     *                     requests. If an element is null, that element represents a regular
     *                     request. If an element if not null, that element represents a reprocess
     *                     request.
     *
     */
    private TotalCaptureResult[] submitMixedCaptureBurstRequest(Surface[] outputs,
            TotalCaptureResult[] inputResults) throws Exception {
        if (outputs == null || outputs.length <= 0) {
            throw new IllegalArgumentException(""outputs must have at least 1 surface"");
        } else if (inputResults != null && inputResults.length != outputs.length) {
            throw new IllegalArgumentException(""The lengths of outputs and inputResults "" +
                    ""don't match"");
        }

        int numReprocessCaptures = 0;
        SimpleCaptureCallback captureCallback = new SimpleCaptureCallback();
        ArrayList<CaptureRequest> captureRequests = new ArrayList<>(outputs.length);

        // Prepare a list of capture requests. Whether it's a regular or reprocess capture request
        // is based on inputResults array.
        for (int i = 0; i < outputs.length; i++) {
            CaptureRequest.Builder builder;
            boolean isReprocess = (inputResults != null && inputResults[i] != null);
            if (isReprocess) {
                builder = mCamera.createReprocessCaptureRequest(inputResults[i]);
                numReprocessCaptures++;
            } else {
                builder = mCamera.createCaptureRequest(CAPTURE_TEMPLATE);
            }
            builder.addTarget(outputs[i]);
            CaptureRequest request = builder.build();
            assertTrue(""Capture request reprocess type "" + request.isReprocess() + "" is wrong."",
                request.isReprocess() == isReprocess);

            captureRequests.add(request);
        }

        if (captureRequests.size() == 1) {
            mSession.capture(captureRequests.get(0), captureCallback, mHandler);
        } else {
            mSession.captureBurst(captureRequests, captureCallback, mHandler);
        }

        TotalCaptureResult[] results;
        if (numReprocessCaptures == 0 || numReprocessCaptures == outputs.length) {
            results = new TotalCaptureResult[outputs.length];
            // If the requests are not mixed, they should come in order.
            for (int i = 0; i < results.length; i++){
                results[i] = captureCallback.getTotalCaptureResultForRequest(
                        captureRequests.get(i), CAPTURE_TIMEOUT_FRAMES);
            }
        } else {
            // If the requests are mixed, they may not come in order.
            results = captureCallback.getTotalCaptureResultsForRequests(
                    captureRequests, CAPTURE_TIMEOUT_FRAMES * captureRequests.size());
        }

        // make sure all input surfaces are released.
        for (int i = 0; i < numReprocessCaptures; i++) {
            mImageWriterListener.waitForImageReleased(CAPTURE_TIMEOUT_MS);
        }

        return results;
    }

    private Size getMaxSize(int format, StaticMetadata.StreamDirection direction) {
        Size[] sizes = mStaticInfo.getAvailableSizesForFormatChecked(format, direction);
        return getAscendingOrderSizes(Arrays.asList(sizes), /*ascending*/false).get(0);
    }

    private boolean isYuvReprocessSupported(String cameraId) throws Exception {
        return isReprocessSupported(cameraId, ImageFormat.YUV_420_888);
    }

    private boolean isOpaqueReprocessSupported(String cameraId) throws Exception {
        return isReprocessSupported(cameraId, ImageFormat.PRIVATE);
    }

    private void dumpImage(Image image, String name) {
        String filename = mDebugFileNameBase + name;
        switch(image.getFormat()) {
            case ImageFormat.JPEG:
                filename += "".jpg"";
                break;
            case ImageFormat.HEIC:
                filename += "".heic"";
                break;
            case ImageFormat.NV16:
            case ImageFormat.NV21:
            case ImageFormat.YUV_420_888:
                filename += "".yuv"";
                break;
            default:
                filename += ""."" + image.getFormat();
                break;
        }

        Log.d(TAG, ""dumping an image to "" + filename);
        dumpFile(filename , getDataFromImage(image));
    }

    /**
     * A class that holds an Image and a TotalCaptureResult.
     */
    private static class ImageResultHolder {
        private final Image mImage;
        private final TotalCaptureResult mResult;

        public ImageResultHolder(Image image, TotalCaptureResult result) {
            mImage = image;
            mResult = result;
        }

        public Image getImage() {
            return mImage;
        }

        public TotalCaptureResult getTotalCaptureResult() {
            return mResult;
        }
    }
}"	""	""	"JPEG 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testAvailableStreamConfigs"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"public void testAvailableStreamConfigs() throws Exception {
        boolean firstBackFacingCamera = true;
        for (int i = 0; i < mAllCameraIds.length; i++) {
            CameraCharacteristics c = mCharacteristics.get(i);
            StreamConfigurationMap config =
                    c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            assertNotNull(String.format(""No stream configuration map found for: ID %s"",
                    mAllCameraIds[i]), config);
            int[] outputFormats = config.getOutputFormats();

            int[] actualCapabilities = c.get(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
            assertNotNull(""android.request.availableCapabilities must never be null"",
                    actualCapabilities);

            // Check required formats exist (JPEG, and YUV_420_888).
            if (!arrayContains(actualCapabilities, BC)) {
                Log.i(TAG, ""Camera "" + mAllCameraIds[i] +
                    "": BACKWARD_COMPATIBLE capability not supported, skipping test"");
                continue;
            }

            boolean isMonochromeWithY8 = arrayContains(actualCapabilities, MONOCHROME)
                    && arrayContains(outputFormats, ImageFormat.Y8);
            boolean isHiddenPhysicalCamera = !arrayContains(mCameraIdsUnderTest, mAllCameraIds[i]);
            boolean supportHeic = arrayContains(outputFormats, ImageFormat.HEIC);

            assertArrayContains(
                    String.format(""No valid YUV_420_888 preview formats found for: ID %s"",
                            mAllCameraIds[i]), outputFormats, ImageFormat.YUV_420_888);
            if (isMonochromeWithY8) {
                assertArrayContains(
                        String.format(""No valid Y8 preview formats found for: ID %s"",
                                mAllCameraIds[i]), outputFormats, ImageFormat.Y8);
            }
            assertArrayContains(String.format(""No JPEG image format for: ID %s"",
                    mAllCameraIds[i]), outputFormats, ImageFormat.JPEG);

            Size[] yuvSizes = config.getOutputSizes(ImageFormat.YUV_420_888);
            Size[] y8Sizes = config.getOutputSizes(ImageFormat.Y8);
            Size[] jpegSizes = config.getOutputSizes(ImageFormat.JPEG);
            Size[] heicSizes = config.getOutputSizes(ImageFormat.HEIC);
            Size[] privateSizes = config.getOutputSizes(ImageFormat.PRIVATE);

            CameraTestUtils.assertArrayNotEmpty(yuvSizes,
                    String.format(""No sizes for preview format %x for: ID %s"",
                            ImageFormat.YUV_420_888, mAllCameraIds[i]));
            if (isMonochromeWithY8) {
                CameraTestUtils.assertArrayNotEmpty(y8Sizes,
                    String.format(""No sizes for preview format %x for: ID %s"",
                            ImageFormat.Y8, mAllCameraIds[i]));
            }

            Rect activeRect = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE);
            Size pixelArraySize = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.SENSOR_INFO_PIXEL_ARRAY_SIZE);

            int activeArrayHeight = activeRect.height();
            int activeArrayWidth = activeRect.width();
            long sensorResolution = pixelArraySize.getHeight() * pixelArraySize.getWidth() ;
            Integer lensFacing = c.get(CameraCharacteristics.LENS_FACING);
            assertNotNull(""Can't get lens facing info for camera id: "" + mAllCameraIds[i],
                    lensFacing);

            // Check that the sensor sizes are atleast what the CDD specifies
            switch(lensFacing) {
                case CameraCharacteristics.LENS_FACING_FRONT:
                    assertTrue(""Front Sensor resolution should be at least "" +
                            MIN_FRONT_SENSOR_RESOLUTION + "" pixels, is ""+ sensorResolution,
                            sensorResolution >= MIN_FRONT_SENSOR_RESOLUTION);
                    break;
                case CameraCharacteristics.LENS_FACING_BACK:
                    if (firstBackFacingCamera) {
                        assertTrue(""Back Sensor resolution should be at least ""
                                + MIN_BACK_SENSOR_RESOLUTION +
                                "" pixels, is ""+ sensorResolution,
                                sensorResolution >= MIN_BACK_SENSOR_RESOLUTION);
                        firstBackFacingCamera = false;
                    }
                    break;
                default:
                    break;
            }

            Integer hwLevel = c.get(CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL);

            if (activeArrayWidth >= FULLHD.getWidth() &&
                    activeArrayHeight >= FULLHD.getHeight()) {
                assertArrayContainsAnyOf(String.format(
                        ""Required FULLHD size not found for format %x for: ID %s"",
                        ImageFormat.JPEG, mAllCameraIds[i]), jpegSizes,
                        new Size[] {FULLHD, FULLHD_ALT});
                if (supportHeic) {
                    assertArrayContainsAnyOf(String.format(
                            ""Required FULLHD size not found for format %x for: ID %s"",
                            ImageFormat.HEIC, mAllCameraIds[i]), heicSizes,
                            new Size[] {FULLHD, FULLHD_ALT});
                }
            }

            if (activeArrayWidth >= HD.getWidth() &&
                    activeArrayHeight >= HD.getHeight()) {
                assertArrayContains(String.format(
                        ""Required HD size not found for format %x for: ID %s"",
                        ImageFormat.JPEG, mAllCameraIds[i]), jpegSizes, HD);
                if (supportHeic) {
                    assertArrayContains(String.format(
                            ""Required HD size not found for format %x for: ID %s"",
                            ImageFormat.HEIC, mAllCameraIds[i]), heicSizes, HD);
                }
            }

            if (activeArrayWidth >= VGA.getWidth() &&
                    activeArrayHeight >= VGA.getHeight()) {
                assertArrayContains(String.format(
                        ""Required VGA size not found for format %x for: ID %s"",
                        ImageFormat.JPEG, mAllCameraIds[i]), jpegSizes, VGA);
                if (supportHeic) {
                    assertArrayContains(String.format(
                            ""Required VGA size not found for format %x for: ID %s"",
                            ImageFormat.HEIC, mAllCameraIds[i]), heicSizes, VGA);
                }
            }

            if (activeArrayWidth >= QVGA.getWidth() &&
                    activeArrayHeight >= QVGA.getHeight()) {
                assertArrayContains(String.format(
                        ""Required QVGA size not found for format %x for: ID %s"",
                        ImageFormat.JPEG, mAllCameraIds[i]), jpegSizes, QVGA);
                if (supportHeic) {
                    assertArrayContains(String.format(
                            ""Required QVGA size not found for format %x for: ID %s"",
                            ImageFormat.HEIC, mAllCameraIds[i]), heicSizes, QVGA);
                }

            }

            ArrayList<Size> jpegSizesList = new ArrayList<>(Arrays.asList(jpegSizes));
            ArrayList<Size> yuvSizesList = new ArrayList<>(Arrays.asList(yuvSizes));
            ArrayList<Size> privateSizesList = new ArrayList<>(Arrays.asList(privateSizes));
            boolean isExternalCamera = (hwLevel ==
                    CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL);
            Size maxVideoSize = null;
            if (isExternalCamera || isHiddenPhysicalCamera) {
                // TODO: for now, use FULLHD 30 as largest possible video size for external camera.
                // For hidden physical camera, since we don't require CamcorderProfile to be
                // available, use FULLHD 30 as maximum video size as well.
                List<Size> videoSizes = CameraTestUtils.getSupportedVideoSizes(
                        mAllCameraIds[i], mCameraManager, FULLHD);
                for (Size sz : videoSizes) {
                    long minFrameDuration = config.getOutputMinFrameDuration(
                            android.media.MediaRecorder.class, sz);
                    // Give some margin for rounding error
                    if (minFrameDuration < (1e9 / 29.9)) {
                        maxVideoSize = sz;
                        break;
                    }
                }
            } else {
                int cameraId = Integer.valueOf(mAllCameraIds[i]);
                CamcorderProfile maxVideoProfile = CamcorderProfile.get(
                        cameraId, CamcorderProfile.QUALITY_HIGH);
                maxVideoSize = new Size(
                        maxVideoProfile.videoFrameWidth, maxVideoProfile.videoFrameHeight);
            }
            if (maxVideoSize == null) {
                fail(""Camera "" + mAllCameraIds[i] + "" does not support any 30fps video output"");
            }

            // Handle FullHD special case first
            if (jpegSizesList.contains(FULLHD)) {
                if (compareHardwareLevel(hwLevel, LEVEL_3) >= 0 || hwLevel == FULL ||
                        (hwLevel == LIMITED &&
                        maxVideoSize.getWidth() >= FULLHD.getWidth() &&
                        maxVideoSize.getHeight() >= FULLHD.getHeight())) {
                    boolean yuvSupportFullHD = yuvSizesList.contains(FULLHD) ||
                            yuvSizesList.contains(FULLHD_ALT);
                    boolean privateSupportFullHD = privateSizesList.contains(FULLHD) ||
                            privateSizesList.contains(FULLHD_ALT);
                    assertTrue(""Full device FullHD YUV size not found"", yuvSupportFullHD);
                    assertTrue(""Full device FullHD PRIVATE size not found"", privateSupportFullHD);

                    if (isMonochromeWithY8) {
                        ArrayList<Size> y8SizesList = new ArrayList<>(Arrays.asList(y8Sizes));
                        boolean y8SupportFullHD = y8SizesList.contains(FULLHD) ||
                                y8SizesList.contains(FULLHD_ALT);
                        assertTrue(""Full device FullHD Y8 size not found"", y8SupportFullHD);
                    }
                }
                // remove all FullHD or FullHD_Alt sizes for the remaining of the test
                jpegSizesList.remove(FULLHD);
                jpegSizesList.remove(FULLHD_ALT);
            }

            // Check all sizes other than FullHD
            if (hwLevel == LIMITED) {
                // Remove all jpeg sizes larger than max video size
                ArrayList<Size> toBeRemoved = new ArrayList<>();
                for (Size size : jpegSizesList) {
                    if (size.getWidth() >= maxVideoSize.getWidth() &&
                            size.getHeight() >= maxVideoSize.getHeight()) {
                        toBeRemoved.add(size);
                    }
                }
                jpegSizesList.removeAll(toBeRemoved);
            }

            if (compareHardwareLevel(hwLevel, LEVEL_3) >= 0 || hwLevel == FULL ||
                    hwLevel == LIMITED) {
                if (!yuvSizesList.containsAll(jpegSizesList)) {
                    for (Size s : jpegSizesList) {
                        if (!yuvSizesList.contains(s)) {
                            fail(""Size "" + s + "" not found in YUV format"");
                        }
                    }
                }

                if (isMonochromeWithY8) {
                    ArrayList<Size> y8SizesList = new ArrayList<>(Arrays.asList(y8Sizes));
                    if (!y8SizesList.containsAll(jpegSizesList)) {
                        for (Size s : jpegSizesList) {
                            if (!y8SizesList.contains(s)) {
                                fail(""Size "" + s + "" not found in Y8 format"");
                            }
                        }
                    }
                }
            }

            if (!privateSizesList.containsAll(yuvSizesList)) {
                for (Size s : yuvSizesList) {
                    if (!privateSizesList.contains(s)) {
                        fail(""Size "" + s + "" not found in PRIVATE format"");
                    }
                }
            }
        }
    }

    private void verifyCommonRecommendedConfiguration(String id, CameraCharacteristics c,
            RecommendedStreamConfigurationMap config, boolean checkNoInput,
            boolean checkNoHighRes, boolean checkNoHighSpeed, boolean checkNoPrivate,
            boolean checkNoDepth) {
        StreamConfigurationMap fullConfig = c.get(
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        assertNotNull(String.format(""No stream configuration map found for ID: %s!"", id),
                fullConfig);

        Set<Integer> recommendedOutputFormats = config.getOutputFormats();

        if (checkNoInput) {
            Set<Integer> inputFormats = config.getInputFormats();
            assertTrue(String.format(""Recommended configuration must not include any input "" +
                    ""streams for ID: %s"", id),
                    ((inputFormats == null) || (inputFormats.size() == 0)));
        }

        if (checkNoHighRes) {
            for (int format : recommendedOutputFormats) {
                Set<Size> highResSizes = config.getHighResolutionOutputSizes(format);
                assertTrue(String.format(""Recommended configuration should not include any "" +
                        ""high resolution sizes, which cannot operate at full "" +
                        ""BURST_CAPTURE rate for ID: %s"", id),
                        ((highResSizes == null) || (highResSizes.size() == 0)));
            }
        }

        if (checkNoHighSpeed) {
            Set<Size> highSpeedSizes = config.getHighSpeedVideoSizes();
            assertTrue(String.format(""Recommended configuration must not include any high "" +
                    ""speed configurations for ID: %s"", id),
                    ((highSpeedSizes == null) || (highSpeedSizes.size() == 0)));
        }

        int[] exhaustiveOutputFormats = fullConfig.getOutputFormats();
        for (Integer formatInteger : recommendedOutputFormats) {
            int format = formatInteger.intValue();
            assertArrayContains(String.format(""Unsupported recommended output format: %d for "" +
                    ""ID: %s "", format, id), exhaustiveOutputFormats, format);
            Set<Size> recommendedSizes = config.getOutputSizes(format);

            switch (format) {
                case ImageFormat.PRIVATE:
                    if (checkNoPrivate) {
                        fail(String.format(""Recommended configuration must not include "" +
                                ""PRIVATE format entries for ID: %s"", id));
                    }

                    Set<Size> classOutputSizes = config.getOutputSizes(ImageReader.class);
                    assertCollectionContainsAnyOf(String.format(""Recommended output sizes for "" +
                            ""ImageReader class don't match the output sizes for the "" +
                            ""corresponding format for ID: %s"", id), classOutputSizes,
                            recommendedSizes);
                    break;
                case ImageFormat.DEPTH16:
                case ImageFormat.DEPTH_POINT_CLOUD:
                    if (checkNoDepth) {
                        fail(String.format(""Recommended configuration must not include any DEPTH "" +
                                ""formats for ID: %s"", id));
                    }
                    break;
                default:
            }
            Size [] exhaustiveSizes = fullConfig.getOutputSizes(format);
            for (Size sz : recommendedSizes) {
                assertArrayContains(String.format(""Unsupported recommended size %s for "" +
                        ""format: %d for ID: %s"", sz.toString(), format, id),
                        exhaustiveSizes, sz);

                long recommendedMinDuration = config.getOutputMinFrameDuration(format, sz);
                long availableMinDuration = fullConfig.getOutputMinFrameDuration(format, sz);
                assertTrue(String.format(""Recommended minimum frame duration %d for size "" +
                        ""%s format: %d doesn't match with currently available minimum"" +
                        "" frame duration of %d for ID: %s"", recommendedMinDuration,
                        sz.toString(), format, availableMinDuration, id),
                        (recommendedMinDuration == availableMinDuration));
                long recommendedStallDuration = config.getOutputStallDuration(format, sz);
                long availableStallDuration = fullConfig.getOutputStallDuration(format, sz);
                assertTrue(String.format(""Recommended stall duration %d for size %s"" +
                        "" format: %d doesn't match with currently available stall "" +
                        ""duration of %d for ID: %s"", recommendedStallDuration,
                        sz.toString(), format, availableStallDuration, id),
                        (recommendedStallDuration == availableStallDuration));

                ImageReader reader = ImageReader.newInstance(sz.getWidth(), sz.getHeight(), format,
                        /*maxImages*/1);
                Surface readerSurface = reader.getSurface();
                assertTrue(String.format(""ImageReader surface using format %d and size %s is not"" +
                        "" supported for ID: %s"", format, sz.toString(), id),
                        config.isOutputSupportedFor(readerSurface));
                if (format == ImageFormat.PRIVATE) {
                    long classMinDuration = config.getOutputMinFrameDuration(ImageReader.class, sz);
                    assertTrue(String.format(""Recommended minimum frame duration %d for size "" +
                            ""%s format: %d doesn't match with the duration %d for "" +
                            ""ImageReader class of the same size"", recommendedMinDuration,
                            sz.toString(), format, classMinDuration),
                            classMinDuration == recommendedMinDuration);
                    long classStallDuration = config.getOutputStallDuration(ImageReader.class, sz);
                    assertTrue(String.format(""Recommended stall duration %d for size "" +
                            ""%s format: %d doesn't match with the stall duration %d for "" +
                            ""ImageReader class of the same size"", recommendedStallDuration,
                            sz.toString(), format, classStallDuration),
                            classStallDuration == recommendedStallDuration);
                }
            }
        }
    }

    private void verifyRecommendedPreviewConfiguration(String cameraId, CameraCharacteristics c,
            RecommendedStreamConfigurationMap previewConfig) {
        verifyCommonRecommendedConfiguration(cameraId, c, previewConfig, /*checkNoInput*/ true,
                /*checkNoHighRes*/ true, /*checkNoHighSpeed*/ true, /*checkNoPrivate*/ false,
                /*checkNoDepth*/ true);

        Set<Integer> outputFormats = previewConfig.getOutputFormats();
        assertTrue(String.format(""No valid YUV_420_888 and PRIVATE preview "" +
                ""formats found in recommended preview configuration for ID: %s"", cameraId),
                outputFormats.containsAll(Arrays.asList(new Integer(ImageFormat.YUV_420_888),
                        new Integer(ImageFormat.PRIVATE))));
    }

    private void verifyRecommendedVideoConfiguration(String cameraId, CameraCharacteristics c,
            RecommendedStreamConfigurationMap videoConfig) {
        verifyCommonRecommendedConfiguration(cameraId, c, videoConfig, /*checkNoInput*/ true,
                /*checkNoHighRes*/ true, /*checkNoHighSpeed*/ false, /*checkNoPrivate*/false,
                /*checkNoDepth*/ true);

        Set<Size> highSpeedSizes = videoConfig.getHighSpeedVideoSizes();
        StreamConfigurationMap fullConfig = c.get(
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        assertNotNull(""No stream configuration map found!"", fullConfig);
        Size [] availableHighSpeedSizes = fullConfig.getHighSpeedVideoSizes();
        if ((highSpeedSizes != null) && (highSpeedSizes.size() > 0)) {
            for (Size sz : highSpeedSizes) {
                assertArrayContains(String.format(""Recommended video configuration includes "" +
                        ""unsupported high speed configuration with size %s for ID: %s"",
                        sz.toString(), cameraId), availableHighSpeedSizes, sz);
                Set<Range<Integer>>  highSpeedFpsRanges =
                    videoConfig.getHighSpeedVideoFpsRangesFor(sz);
                Range<Integer> [] availableHighSpeedFpsRanges =
                    fullConfig.getHighSpeedVideoFpsRangesFor(sz);
                for (Range<Integer> fpsRange : highSpeedFpsRanges) {
                    assertArrayContains(String.format(""Recommended video configuration includes "" +
                            ""unsupported high speed fps range [%d %d] for ID: %s"",
                            fpsRange.getLower().intValue(), fpsRange.getUpper().intValue(),
                            cameraId), availableHighSpeedFpsRanges, fpsRange);
                }
            }
        }

        final int[] profileList = {
            CamcorderProfile.QUALITY_2160P,
            CamcorderProfile.QUALITY_1080P,
            CamcorderProfile.QUALITY_480P,
            CamcorderProfile.QUALITY_720P,
            CamcorderProfile.QUALITY_CIF,
            CamcorderProfile.QUALITY_HIGH,
            CamcorderProfile.QUALITY_LOW,
            CamcorderProfile.QUALITY_QCIF,
            CamcorderProfile.QUALITY_QVGA,
        };
        Set<Size> privateSizeSet = videoConfig.getOutputSizes(ImageFormat.PRIVATE);
        for (int profile : profileList) {
            int idx = Integer.valueOf(cameraId);
            if (CamcorderProfile.hasProfile(idx, profile)) {
                CamcorderProfile videoProfile = CamcorderProfile.get(idx, profile);
                Size profileSize  = new Size(videoProfile.videoFrameWidth,
                        videoProfile.videoFrameHeight);
                assertCollectionContainsAnyOf(String.format(""Recommended video configuration "" +
                        ""doesn't include supported video profile size %s with Private format "" +
                        ""for ID: %s"", profileSize.toString(), cameraId), privateSizeSet,
                        Arrays.asList(profileSize));
            }
        }
    }

    private Pair<Boolean, Size> isSizeWithinSensorMargin(Size sz, Size sensorSize) {
        final float SIZE_ERROR_MARGIN = 0.03f;
        float croppedWidth = (float)sensorSize.getWidth();
        float croppedHeight = (float)sensorSize.getHeight();
        float sensorAspectRatio = (float)sensorSize.getWidth() / (float)sensorSize.getHeight();
        float maxAspectRatio = (float)sz.getWidth() / (float)sz.getHeight();
        if (sensorAspectRatio < maxAspectRatio) {
            croppedHeight = (float)sensorSize.getWidth() / maxAspectRatio;
        } else if (sensorAspectRatio > maxAspectRatio) {
            croppedWidth = (float)sensorSize.getHeight() * maxAspectRatio;
        }
        Size croppedSensorSize = new Size((int)croppedWidth, (int)croppedHeight);

        Boolean match = new Boolean(
            (sz.getWidth() <= croppedSensorSize.getWidth() * (1.0 + SIZE_ERROR_MARGIN) &&
             sz.getWidth() >= croppedSensorSize.getWidth() * (1.0 - SIZE_ERROR_MARGIN) &&
             sz.getHeight() <= croppedSensorSize.getHeight() * (1.0 + SIZE_ERROR_MARGIN) &&
             sz.getHeight() >= croppedSensorSize.getHeight() * (1.0 - SIZE_ERROR_MARGIN)));

        return Pair.create(match, croppedSensorSize);
    }

    private void verifyRecommendedSnapshotConfiguration(String cameraId, CameraCharacteristics c,
            RecommendedStreamConfigurationMap snapshotConfig) {
        verifyCommonRecommendedConfiguration(cameraId, c, snapshotConfig, /*checkNoInput*/ true,
                /*checkNoHighRes*/ false, /*checkNoHighSpeed*/ true, /*checkNoPrivate*/false,
                /*checkNoDepth*/ false);
        Rect activeRect = CameraTestUtils.getValueNotNull(
                c, CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE);
        Size arraySize = new Size(activeRect.width(), activeRect.height());


        ArraySet<Size> snapshotSizeSet = new ArraySet<>(snapshotConfig.getOutputSizes(
                    ImageFormat.JPEG));
        Set<Size> highResSnapshotSizeSet = snapshotConfig.getHighResolutionOutputSizes(
                ImageFormat.JPEG);
        if (highResSnapshotSizeSet != null) {
            snapshotSizeSet.addAll(highResSnapshotSizeSet);
        }
        Size[] snapshotSizes = new Size[snapshotSizeSet.size()];
        snapshotSizes = snapshotSizeSet.toArray(snapshotSizes);
        Size maxJpegSize = CameraTestUtils.getMaxSize(snapshotSizes);
        assertTrue(String.format(""Maximum recommended Jpeg size %s should be within 3 percent "" +
                ""of the area of the advertised array size %s for ID: %s"",
                maxJpegSize.toString(), arraySize.toString(), cameraId),
                isSizeWithinSensorMargin(maxJpegSize, arraySize).first.booleanValue());
    }

    private void verifyRecommendedVideoSnapshotConfiguration(String cameraId,
            CameraCharacteristics c,
            RecommendedStreamConfigurationMap videoSnapshotConfig,
            RecommendedStreamConfigurationMap videoConfig) {
        verifyCommonRecommendedConfiguration(cameraId, c, videoSnapshotConfig,
                /*checkNoInput*/ true, /*checkNoHighRes*/ false, /*checkNoHighSpeed*/ true,
                /*checkNoPrivate*/ true, /*checkNoDepth*/ true);

        Set<Integer> outputFormats = videoSnapshotConfig.getOutputFormats();
        assertCollectionContainsAnyOf(String.format(""No valid JPEG format found "" +
                ""in recommended video snapshot configuration for ID: %s"", cameraId),
                outputFormats, Arrays.asList(new Integer(ImageFormat.JPEG)));
        assertTrue(String.format(""Recommended video snapshot configuration must only advertise "" +
                ""JPEG format for ID: %s"", cameraId), outputFormats.size() == 1);

        Set<Size> privateVideoSizeSet = videoConfig.getOutputSizes(ImageFormat.PRIVATE);
        Size[] privateVideoSizes = new Size[privateVideoSizeSet.size()];
        privateVideoSizes = privateVideoSizeSet.toArray(privateVideoSizes);
        Size maxVideoSize = CameraTestUtils.getMaxSize(privateVideoSizes);
        Set<Size> outputSizes = videoSnapshotConfig.getOutputSizes(ImageFormat.JPEG);
        assertCollectionContainsAnyOf(String.format(""The maximum recommended video size %s "" +
                ""should be present in the recommended video snapshot configurations for ID: %s"",
                maxVideoSize.toString(), cameraId), outputSizes, Arrays.asList(maxVideoSize));
    }

    private void verifyRecommendedRawConfiguration(String cameraId,
            CameraCharacteristics c, RecommendedStreamConfigurationMap rawConfig) {
        verifyCommonRecommendedConfiguration(cameraId, c, rawConfig, /*checkNoInput*/ true,
                /*checkNoHighRes*/ false, /*checkNoHighSpeed*/ true, /*checkNoPrivate*/ true,
                /*checkNoDepth*/ true);

        Set<Integer> outputFormats = rawConfig.getOutputFormats();
        for (Integer outputFormatInteger : outputFormats) {
            int outputFormat = outputFormatInteger.intValue();
            switch (outputFormat) {
                case ImageFormat.RAW10:
                case ImageFormat.RAW12:
                case ImageFormat.RAW_PRIVATE:
                case ImageFormat.RAW_SENSOR:
                    break;
                default:
                    fail(String.format(""Recommended raw configuration map must not contain "" +
                            "" non-RAW formats like: %d for ID: %s"", outputFormat, cameraId));

            }
        }
    }

    private void verifyRecommendedZSLConfiguration(String cameraId, CameraCharacteristics c,
            RecommendedStreamConfigurationMap zslConfig) {
        verifyCommonRecommendedConfiguration(cameraId, c, zslConfig, /*checkNoInput*/ false,
                /*checkNoHighRes*/ false, /*checkNoHighSpeed*/ true, /*checkNoPrivate*/ false,
                /*checkNoDepth*/ false);

        StreamConfigurationMap fullConfig =
            c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        assertNotNull(String.format(""No stream configuration map found for ID: %s!"", cameraId),
                fullConfig);
        Set<Integer> inputFormats = zslConfig.getInputFormats();
        int [] availableInputFormats = fullConfig.getInputFormats();
        for (Integer inputFormatInteger : inputFormats) {
            int inputFormat = inputFormatInteger.intValue();
            assertArrayContains(String.format(""Recommended ZSL configuration includes "" +
                    ""unsupported input format %d for ID: %s"", inputFormat, cameraId),
                    availableInputFormats, inputFormat);

            Set<Size> inputSizes = zslConfig.getInputSizes(inputFormat);
            Size [] availableInputSizes = fullConfig.getInputSizes(inputFormat);
            assertTrue(String.format(""Recommended ZSL configuration input format %d includes "" +
                    ""invalid input sizes for ID: %s"", inputFormat, cameraId),
                    ((inputSizes != null) && (inputSizes.size() > 0)));
            for (Size inputSize : inputSizes) {
                assertArrayContains(String.format(""Recommended ZSL configuration includes "" +
                        ""unsupported input format %d with size %s ID: %s"", inputFormat,
                        inputSize.toString(), cameraId), availableInputSizes, inputSize);
            }
            Set<Integer> validOutputFormats = zslConfig.getValidOutputFormatsForInput(inputFormat);
            int [] availableValidOutputFormats = fullConfig.getValidOutputFormatsForInput(
                    inputFormat);
            for (Integer outputFormatInteger : validOutputFormats) {
                int outputFormat = outputFormatInteger.intValue();
                assertArrayContains(String.format(""Recommended ZSL configuration includes "" +
                        ""unsupported output format %d for input %s ID: %s"", outputFormat,
                        inputFormat, cameraId), availableValidOutputFormats, outputFormat);
            }
        }
    }

    private void checkFormatLatency(int format, long latencyThresholdMs,
            RecommendedStreamConfigurationMap configMap) throws Exception {
        Set<Size> availableSizes = configMap.getOutputSizes(format);
        assertNotNull(String.format(""No available sizes for output format: %d"", format),
                availableSizes);

        ImageReader previewReader = null;
        long threshold = (long) (latencyThresholdMs * LATENCY_TOLERANCE_FACTOR);
        // for each resolution, check that the end-to-end latency doesn't exceed the given threshold
        for (Size sz : availableSizes) {
            try {
                // Create ImageReaders, capture session and requests
                final ImageReader.OnImageAvailableListener mockListener = mock(
                        ImageReader.OnImageAvailableListener.class);
                createDefaultImageReader(sz, format, MAX_NUM_IMAGES, mockListener);
                Size previewSize = mOrderedPreviewSizes.get(0);
                previewReader = createImageReader(previewSize, ImageFormat.YUV_420_888,
                        MAX_NUM_IMAGES, new CameraTestUtils.ImageDropperListener());
                Surface previewSurface = previewReader.getSurface();
                List<Surface> surfaces = new ArrayList<Surface>();
                surfaces.add(previewSurface);
                surfaces.add(mReaderSurface);
                createSession(surfaces);
                CaptureRequest.Builder captureBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
                captureBuilder.addTarget(previewSurface);
                CaptureRequest request = captureBuilder.build();

                // Let preview run for a while
                startCapture(request, /*repeating*/ true, new SimpleCaptureCallback(), mHandler);
                Thread.sleep(PREVIEW_RUN_MS);

                // Start capture.
                captureBuilder = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
                captureBuilder.addTarget(mReaderSurface);
                request = captureBuilder.build();

                for (int i = 0; i < MAX_NUM_IMAGES; i++) {
                    startCapture(request, /*repeating*/ false, new SimpleCaptureCallback(),
                            mHandler);
                    verify(mockListener, timeout(threshold).times(1)).onImageAvailable(
                            any(ImageReader.class));
                    reset(mockListener);
                }

                // stop capture.
                stopCapture(/*fast*/ false);
            } finally {
                closeDefaultImageReader();

                if (previewReader != null) {
                    previewReader.close();
                    previewReader = null;
                }
            }

        }
    }

    private void verifyRecommendedLowLatencyConfiguration(String cameraId, CameraCharacteristics c,
            RecommendedStreamConfigurationMap lowLatencyConfig) throws Exception {
        verifyCommonRecommendedConfiguration(cameraId, c, lowLatencyConfig, /*checkNoInput*/ true,
                /*checkNoHighRes*/ false, /*checkNoHighSpeed*/ true, /*checkNoPrivate*/ false,
                /*checkNoDepth*/ true);

        try {
            openDevice(cameraId);

            Set<Integer> formats = lowLatencyConfig.getOutputFormats();
            for (Integer format : formats) {
                checkFormatLatency(format.intValue(), LOW_LATENCY_THRESHOLD_MS, lowLatencyConfig);
            }
        } finally {
            closeDevice(cameraId);
        }

    }"	""	""	"JPEG 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testKeys"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"public void testKeys() {
        for (int i = 0; i < mAllCameraIds.length; i++) {
            CameraCharacteristics c = mCharacteristics.get(i);
            mCollector.setCameraId(mAllCameraIds[i]);

            if (VERBOSE) {
                Log.v(TAG, ""testKeys - testing characteristics for camera "" + mAllCameraIds[i]);
            }

            List<CameraCharacteristics.Key<?>> allKeys = c.getKeys();
            assertNotNull(""Camera characteristics keys must not be null"", allKeys);
            assertFalse(""Camera characteristics keys must have at least 1 key"",
                    allKeys.isEmpty());

            for (CameraCharacteristics.Key<?> key : allKeys) {
                assertKeyPrefixValid(key.getName());

                // All characteristics keys listed must never be null
                mCollector.expectKeyValueNotNull(c, key);

                // TODO: add a check that key must not be @hide
            }

            /*
             * List of keys that must be present in camera characteristics (not null).
             *
             * Keys for LIMITED, FULL devices might be available despite lacking either
             * the hardware level or the capability. This is *OK*. This only lists the
             * *minimal* requirements for a key to be listed.
             *
             * LEGACY devices are a bit special since they map to api1 devices, so we know
             * for a fact most keys are going to be illegal there so they should never be
             * available.
             *
             * For LIMITED-level keys, if the level is >= LIMITED, then the capabilities are used to
             * do the actual checking.
             */
            {
                //                                           (Key Name)                                     (HW Level)  (Capabilities <Var-Arg>)
                expectKeyAvailable(c, CameraCharacteristics.COLOR_CORRECTION_AVAILABLE_ABERRATION_MODES     , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AVAILABLE_MODES                         , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AE_AVAILABLE_ANTIBANDING_MODES          , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AE_AVAILABLE_MODES                      , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES          , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AE_COMPENSATION_RANGE                   , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AE_COMPENSATION_STEP                    , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AE_LOCK_AVAILABLE                       , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AF_AVAILABLE_MODES                      , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AVAILABLE_EFFECTS                       , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AVAILABLE_SCENE_MODES                   , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES     , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AWB_AVAILABLE_MODES                     , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_AWB_LOCK_AVAILABLE                      , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_MAX_REGIONS_AE                          , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_MAX_REGIONS_AF                          , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.CONTROL_MAX_REGIONS_AWB                         , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.EDGE_AVAILABLE_EDGE_MODES                       , FULL     ,   NONE                 );
                expectKeyAvailable(c, CameraCharacteristics.FLASH_INFO_AVAILABLE                            , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES             , OPT      ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL                   , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.INFO_VERSION                                    , OPT      ,   NONE                 );
                expectKeyAvailable(c, CameraCharacteristics.JPEG_AVAILABLE_THUMBNAIL_SIZES                  , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.LENS_FACING                                     , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.LENS_INFO_AVAILABLE_APERTURES                   , FULL     ,   MANUAL_SENSOR        );
                expectKeyAvailable(c, CameraCharacteristics.LENS_INFO_AVAILABLE_FILTER_DENSITIES            , FULL     ,   MANUAL_SENSOR        );
                expectKeyAvailable(c, CameraCharacteristics.LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION       , LIMITED  ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.LENS_INFO_FOCUS_DISTANCE_CALIBRATION            , LIMITED  ,   MANUAL_SENSOR        );
                expectKeyAvailable(c, CameraCharacteristics.LENS_INFO_HYPERFOCAL_DISTANCE                   , LIMITED  ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.LENS_INFO_MINIMUM_FOCUS_DISTANCE                , LIMITED  ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES                  , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.REQUEST_MAX_NUM_INPUT_STREAMS                   , OPT      ,   YUV_REPROCESS, OPAQUE_REPROCESS);
                expectKeyAvailable(c, CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP                 , OPT      ,   CONSTRAINED_HIGH_SPEED);
                expectKeyAvailable(c, CameraCharacteristics.REQUEST_MAX_NUM_OUTPUT_PROC                     , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.REQUEST_MAX_NUM_OUTPUT_PROC_STALLING            , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.REQUEST_MAX_NUM_OUTPUT_RAW                      , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.REQUEST_PARTIAL_RESULT_COUNT                    , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.REQUEST_PIPELINE_MAX_DEPTH                      , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SCALER_AVAILABLE_MAX_DIGITAL_ZOOM               , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP                 , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SCALER_CROPPING_TYPE                            , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_BLACK_LEVEL_PATTERN                      , FULL     ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE                   , OPT      ,   BC, RAW              );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_COLOR_FILTER_ARRANGEMENT            , FULL     ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_EXPOSURE_TIME_RANGE                 , FULL     ,   MANUAL_SENSOR        );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_MAX_FRAME_DURATION                  , FULL     ,   MANUAL_SENSOR        );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_PIXEL_ARRAY_SIZE                    , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_SENSITIVITY_RANGE                   , FULL     ,   MANUAL_SENSOR        );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_WHITE_LEVEL                         , OPT      ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_TIMESTAMP_SOURCE                    , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_MAX_ANALOG_SENSITIVITY                   , FULL     ,   MANUAL_SENSOR        );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_ORIENTATION                              , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SHADING_AVAILABLE_MODES                         , LIMITED  ,   MANUAL_POSTPROC, RAW );
                expectKeyAvailable(c, CameraCharacteristics.STATISTICS_INFO_AVAILABLE_FACE_DETECT_MODES     , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.STATISTICS_INFO_AVAILABLE_HOT_PIXEL_MAP_MODES   , OPT      ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.STATISTICS_INFO_AVAILABLE_LENS_SHADING_MAP_MODES, LIMITED  ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.STATISTICS_INFO_MAX_FACE_COUNT                  , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SYNC_MAX_LATENCY                                , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.TONEMAP_AVAILABLE_TONE_MAP_MODES                , FULL     ,   MANUAL_POSTPROC      );
                expectKeyAvailable(c, CameraCharacteristics.TONEMAP_MAX_CURVE_POINTS                        , FULL     ,   MANUAL_POSTPROC      );

                // Future: Use column editors for modifying above, ignore line length to keep 1 key per line

                // TODO: check that no other 'android' keys are listed in #getKeys if they aren't in the above list
            }

            int[] actualCapabilities = c.get(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
            assertNotNull(""android.request.availableCapabilities must never be null"",
                    actualCapabilities);
            boolean isMonochrome = arrayContains(actualCapabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MONOCHROME);
            if (!isMonochrome) {
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_CALIBRATION_TRANSFORM1                   , OPT      ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_COLOR_TRANSFORM1                         , OPT      ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_FORWARD_MATRIX1                          , OPT      ,   RAW                  );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_REFERENCE_ILLUMINANT1                    , OPT      ,   RAW                  );


                // Only check for these if the second reference illuminant is included
                if (allKeys.contains(CameraCharacteristics.SENSOR_REFERENCE_ILLUMINANT2)) {
                    expectKeyAvailable(c, CameraCharacteristics.SENSOR_REFERENCE_ILLUMINANT2                    , OPT      ,   RAW                  );
                    expectKeyAvailable(c, CameraCharacteristics.SENSOR_COLOR_TRANSFORM2                         , OPT      ,   RAW                  );
                    expectKeyAvailable(c, CameraCharacteristics.SENSOR_CALIBRATION_TRANSFORM2                   , OPT      ,   RAW                  );
                    expectKeyAvailable(c, CameraCharacteristics.SENSOR_FORWARD_MATRIX2                          , OPT      ,   RAW                  );
                }
            }

            // Required key if any of RAW format output is supported
            StreamConfigurationMap config =
                    c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            assertNotNull(String.format(""No stream configuration map found for: ID %s"",
                    mAllCameraIds[i]), config);
            if (config.isOutputSupportedFor(ImageFormat.RAW_SENSOR) ||
                    config.isOutputSupportedFor(ImageFormat.RAW10)  ||
                    config.isOutputSupportedFor(ImageFormat.RAW12)  ||
                    config.isOutputSupportedFor(ImageFormat.RAW_PRIVATE)) {
                expectKeyAvailable(c,
                        CameraCharacteristics.CONTROL_POST_RAW_SENSITIVITY_BOOST_RANGE, OPT, BC);
            }

            // External Camera exceptional keys
            Integer hwLevel = c.get(CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL);
            boolean isExternalCamera = (hwLevel ==
                    CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL);
            if (!isExternalCamera) {
                expectKeyAvailable(c, CameraCharacteristics.LENS_INFO_AVAILABLE_FOCAL_LENGTHS               , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_AVAILABLE_TEST_PATTERN_MODES             , OPT      ,   BC                   );
                expectKeyAvailable(c, CameraCharacteristics.SENSOR_INFO_PHYSICAL_SIZE                       , OPT      ,   BC                   );
            }


            // Verify version is a short text string.
            if (allKeys.contains(CameraCharacteristics.INFO_VERSION)) {
                final String TEXT_REGEX = ""[\\p{Alnum}\\p{Punct}\\p{Space}]*"";
                final int MAX_VERSION_LENGTH = 256;

                String version = c.get(CameraCharacteristics.INFO_VERSION);
                mCollector.expectTrue(""Version contains non-text characters: "" + version,
                        version.matches(TEXT_REGEX));
                mCollector.expectLessOrEqual(""Version too long: "" + version, MAX_VERSION_LENGTH,
                        version.length());
            }
        }
    }

    /**
     * Test values for static metadata used by the RAW capability.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testStaticBurstCharacteristics"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"public void testStaticBurstCharacteristics() throws Exception {
        for (int i = 0; i < mAllCameraIds.length; i++) {
            CameraCharacteristics c = mCharacteristics.get(i);
            int[] actualCapabilities = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);

            // Check if the burst capability is defined
            boolean haveBurstCapability = arrayContains(actualCapabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_BURST_CAPTURE);
            boolean haveBC = arrayContains(actualCapabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE);

            if(haveBurstCapability && !haveBC) {
                fail(""Must have BACKWARD_COMPATIBLE capability if BURST_CAPTURE capability is defined"");
            }

            if (!haveBC) continue;

            StreamConfigurationMap config =
                    c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            assertNotNull(String.format(""No stream configuration map found for: ID %s"",
                    mAllCameraIds[i]), config);
            Rect activeRect = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE);
            Size sensorSize = new Size(activeRect.width(), activeRect.height());

            // Ensure that max YUV size matches max JPEG size
            Size maxYuvSize = CameraTestUtils.getMaxSize(
                    config.getOutputSizes(ImageFormat.YUV_420_888));
            Size maxFastYuvSize = maxYuvSize;

            Size[] slowYuvSizes = config.getHighResolutionOutputSizes(ImageFormat.YUV_420_888);
            Size maxSlowYuvSizeLessThan24M = null;
            if (haveBurstCapability && slowYuvSizes != null && slowYuvSizes.length > 0) {
                Size maxSlowYuvSize = CameraTestUtils.getMaxSize(slowYuvSizes);
                final int SIZE_24MP_BOUND = 24000000;
                maxSlowYuvSizeLessThan24M =
                        CameraTestUtils.getMaxSizeWithBound(slowYuvSizes, SIZE_24MP_BOUND);
                maxYuvSize = CameraTestUtils.getMaxSize(new Size[]{maxYuvSize, maxSlowYuvSize});
            }

            Size maxJpegSize = CameraTestUtils.getMaxSize(CameraTestUtils.getSupportedSizeForFormat(
                    ImageFormat.JPEG, mAllCameraIds[i], mCameraManager));

            boolean haveMaxYuv = maxYuvSize != null ?
                (maxJpegSize.getWidth() <= maxYuvSize.getWidth() &&
                        maxJpegSize.getHeight() <= maxYuvSize.getHeight()) : false;

            Pair<Boolean, Size> maxYuvMatchSensorPair = isSizeWithinSensorMargin(maxYuvSize,
                    sensorSize);

            // No need to do null check since framework will generate the key if HAL don't supply
            boolean haveAeLock = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.CONTROL_AE_LOCK_AVAILABLE);
            boolean haveAwbLock = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.CONTROL_AWB_LOCK_AVAILABLE);

            // Ensure that some >=8MP YUV output is fast enough - needs to be at least 20 fps

            long maxFastYuvRate =
                    config.getOutputMinFrameDuration(ImageFormat.YUV_420_888, maxFastYuvSize);
            final long MIN_8MP_DURATION_BOUND_NS = 50000000; // 50 ms, 20 fps
            boolean haveFastYuvRate = maxFastYuvRate <= MIN_8MP_DURATION_BOUND_NS;

            final int SIZE_8MP_BOUND = 8000000;
            boolean havefast8MPYuv = (maxFastYuvSize.getWidth() * maxFastYuvSize.getHeight()) >
                    SIZE_8MP_BOUND;

            // Ensure that max YUV output smaller than 24MP is fast enough
            // - needs to be at least 10 fps
            final long MIN_MAXSIZE_DURATION_BOUND_NS = 100000000; // 100 ms, 10 fps
            long maxYuvRate = maxFastYuvRate;
            if (maxSlowYuvSizeLessThan24M != null) {
                maxYuvRate = config.getOutputMinFrameDuration(
                        ImageFormat.YUV_420_888, maxSlowYuvSizeLessThan24M);
            }
            boolean haveMaxYuvRate = maxYuvRate <= MIN_MAXSIZE_DURATION_BOUND_NS;

            // Ensure that there's an FPS range that's fast enough to capture at above
            // minFrameDuration, for full-auto bursts at the fast resolutions
            Range[] fpsRanges = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES);
            float minYuvFps = 1.f / maxFastYuvRate;

            boolean haveFastAeTargetFps = false;
            for (Range<Integer> r : fpsRanges) {
                if (r.getLower() >= minYuvFps) {
                    haveFastAeTargetFps = true;
                    break;
                }
            }

            // Ensure that maximum sync latency is small enough for fast setting changes, even if
            // it's not quite per-frame

            Integer maxSyncLatencyValue = c.get(CameraCharacteristics.SYNC_MAX_LATENCY);
            assertNotNull(String.format(""No sync latency declared for ID %s"", mAllCameraIds[i]),
                    maxSyncLatencyValue);

            int maxSyncLatency = maxSyncLatencyValue;
            final long MAX_LATENCY_BOUND = 4;
            boolean haveFastSyncLatency =
                (maxSyncLatency <= MAX_LATENCY_BOUND) && (maxSyncLatency >= 0);

            if (haveBurstCapability) {
                assertTrue(""Must have slow YUV size array when BURST_CAPTURE capability is defined!"",
                        slowYuvSizes != null);
                assertTrue(
                        String.format(""BURST-capable camera device %s does not have maximum YUV "" +
                                ""size that is at least max JPEG size"",
                                mAllCameraIds[i]),
                        haveMaxYuv);
                assertTrue(
                        String.format(""BURST-capable camera device %s max-resolution "" +
                                ""YUV frame rate is too slow"" +
                                ""(%d ns min frame duration reported, less than %d ns expected)"",
                                mAllCameraIds[i], maxYuvRate, MIN_MAXSIZE_DURATION_BOUND_NS),
                        haveMaxYuvRate);
                assertTrue(
                        String.format(""BURST-capable camera device %s >= 8MP YUV output "" +
                                ""frame rate is too slow"" +
                                ""(%d ns min frame duration reported, less than %d ns expected)"",
                                mAllCameraIds[i], maxYuvRate, MIN_8MP_DURATION_BOUND_NS),
                        haveFastYuvRate);
                assertTrue(
                        String.format(""BURST-capable camera device %s does not list an AE target "" +
                                "" FPS range with min FPS >= %f, for full-AUTO bursts"",
                                mAllCameraIds[i], minYuvFps),
                        haveFastAeTargetFps);
                assertTrue(
                        String.format(""BURST-capable camera device %s YUV sync latency is too long"" +
                                ""(%d frames reported, [0, %d] frames expected)"",
                                mAllCameraIds[i], maxSyncLatency, MAX_LATENCY_BOUND),
                        haveFastSyncLatency);
                assertTrue(
                        String.format(""BURST-capable camera device %s max YUV size %s should be"" +
                                ""close to active array size %s or cropped active array size %s"",
                                mAllCameraIds[i], maxYuvSize.toString(), sensorSize.toString(),
                                maxYuvMatchSensorPair.second.toString()),
                        maxYuvMatchSensorPair.first.booleanValue());
                assertTrue(
                        String.format(""BURST-capable camera device %s does not support AE lock"",
                                mAllCameraIds[i]),
                        haveAeLock);
                assertTrue(
                        String.format(""BURST-capable camera device %s does not support AWB lock"",
                                mAllCameraIds[i]),
                        haveAwbLock);
            } else {
                assertTrue(""Must have null slow YUV size array when no BURST_CAPTURE capability!"",
                        slowYuvSizes == null);
                assertTrue(
                        String.format(""Camera device %s has all the requirements for BURST"" +
                                "" capability but does not report it!"", mAllCameraIds[i]),
                        !(haveMaxYuv && haveMaxYuvRate && haveFastYuvRate && haveFastAeTargetFps &&
                                haveFastSyncLatency && maxYuvMatchSensorPair.first.booleanValue() &&
                                haveAeLock && haveAwbLock));
            }
        }
    }

    /**
     * Check reprocessing capabilities.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testReprocessingCharacteristics"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"public void testReprocessingCharacteristics() {
        for (int i = 0; i < mAllCameraIds.length; i++) {
            Log.i(TAG, ""testReprocessingCharacteristics: Testing camera ID "" + mAllCameraIds[i]);

            CameraCharacteristics c = mCharacteristics.get(i);
            int[] capabilities = c.get(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
            assertNotNull(""android.request.availableCapabilities must never be null"",
                    capabilities);
            boolean supportYUV = arrayContains(capabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING);
            boolean supportOpaque = arrayContains(capabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING);
            StreamConfigurationMap configs =
                    c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            Integer maxNumInputStreams =
                    c.get(CameraCharacteristics.REQUEST_MAX_NUM_INPUT_STREAMS);
            int[] availableEdgeModes = c.get(CameraCharacteristics.EDGE_AVAILABLE_EDGE_MODES);
            int[] availableNoiseReductionModes = c.get(
                    CameraCharacteristics.NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES);

            int[] inputFormats = configs.getInputFormats();
            int[] outputFormats = configs.getOutputFormats();
            boolean isMonochromeWithY8 = arrayContains(capabilities, MONOCHROME)
                    && arrayContains(outputFormats, ImageFormat.Y8);

            boolean supportZslEdgeMode = false;
            boolean supportZslNoiseReductionMode = false;
            boolean supportHiQNoiseReductionMode = false;
            boolean supportHiQEdgeMode = false;

            if (availableEdgeModes != null) {
                supportZslEdgeMode = Arrays.asList(CameraTestUtils.toObject(availableEdgeModes)).
                        contains(CaptureRequest.EDGE_MODE_ZERO_SHUTTER_LAG);
                supportHiQEdgeMode = Arrays.asList(CameraTestUtils.toObject(availableEdgeModes)).
                        contains(CaptureRequest.EDGE_MODE_HIGH_QUALITY);
            }

            if (availableNoiseReductionModes != null) {
                supportZslNoiseReductionMode = Arrays.asList(
                        CameraTestUtils.toObject(availableNoiseReductionModes)).contains(
                        CaptureRequest.NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG);
                supportHiQNoiseReductionMode = Arrays.asList(
                        CameraTestUtils.toObject(availableNoiseReductionModes)).contains(
                        CaptureRequest.NOISE_REDUCTION_MODE_HIGH_QUALITY);
            }

            if (supportYUV || supportOpaque) {
                mCollector.expectTrue(""Support reprocessing but max number of input stream is "" +
                        maxNumInputStreams, maxNumInputStreams != null && maxNumInputStreams > 0);
                mCollector.expectTrue(""Support reprocessing but EDGE_MODE_ZERO_SHUTTER_LAG is "" +
                        ""not supported"", supportZslEdgeMode);
                mCollector.expectTrue(""Support reprocessing but "" +
                        ""NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG is not supported"",
                        supportZslNoiseReductionMode);

                // For reprocessing, if we only require OFF and ZSL mode, it will be just like jpeg
                // encoding. We implicitly require FAST to make reprocessing meaningful, which means
                // that we also require HIGH_QUALITY.
                mCollector.expectTrue(""Support reprocessing but EDGE_MODE_HIGH_QUALITY is "" +
                        ""not supported"", supportHiQEdgeMode);
                mCollector.expectTrue(""Support reprocessing but "" +
                        ""NOISE_REDUCTION_MODE_HIGH_QUALITY is not supported"",
                        supportHiQNoiseReductionMode);

                // Verify mandatory input formats are supported
                mCollector.expectTrue(""YUV_420_888 input must be supported for YUV reprocessing"",
                        !supportYUV || arrayContains(inputFormats, ImageFormat.YUV_420_888));
                mCollector.expectTrue(""Y8 input must be supported for YUV reprocessing on "" +
                        ""MONOCHROME devices with Y8 support"", !supportYUV || !isMonochromeWithY8
                        || arrayContains(inputFormats, ImageFormat.Y8));
                mCollector.expectTrue(""PRIVATE input must be supported for OPAQUE reprocessing"",
                        !supportOpaque || arrayContains(inputFormats, ImageFormat.PRIVATE));

                // max capture stall must be reported if one of the reprocessing is supported.
                final int MAX_ALLOWED_STALL_FRAMES = 4;
                Integer maxCaptureStall = c.get(CameraCharacteristics.REPROCESS_MAX_CAPTURE_STALL);
                mCollector.expectTrue(""max capture stall must be non-null and no larger than ""
                        + MAX_ALLOWED_STALL_FRAMES,
                        maxCaptureStall != null && maxCaptureStall <= MAX_ALLOWED_STALL_FRAMES);

                for (int input : inputFormats) {
                    // Verify mandatory output formats are supported
                    int[] outputFormatsForInput = configs.getValidOutputFormatsForInput(input);
                    mCollector.expectTrue(
                        ""YUV_420_888 output must be supported for reprocessing"",
                        input == ImageFormat.Y8
                        || arrayContains(outputFormatsForInput, ImageFormat.YUV_420_888));
                    mCollector.expectTrue(
                        ""Y8 output must be supported for reprocessing on MONOCHROME devices with""
                        + "" Y8 support"", !isMonochromeWithY8 || input == ImageFormat.YUV_420_888
                        || arrayContains(outputFormatsForInput, ImageFormat.Y8));
                    mCollector.expectTrue(""JPEG output must be supported for reprocessing"",
                            arrayContains(outputFormatsForInput, ImageFormat.JPEG));

                    // Verify camera can output the reprocess input formats and sizes.
                    Size[] inputSizes = configs.getInputSizes(input);
                    Size[] outputSizes = configs.getOutputSizes(input);
                    Size[] highResOutputSizes = configs.getHighResolutionOutputSizes(input);
                    mCollector.expectTrue(""no input size supported for format "" + input,
                            inputSizes.length > 0);
                    mCollector.expectTrue(""no output size supported for format "" + input,
                            outputSizes.length > 0);

                    for (Size inputSize : inputSizes) {
                        mCollector.expectTrue(""Camera must be able to output the supported "" +
                                ""reprocessing input size"",
                                arrayContains(outputSizes, inputSize) ||
                                arrayContains(highResOutputSizes, inputSize));
                    }
                }
            } else {
                mCollector.expectTrue(""Doesn't support reprocessing but report input format: "" +
                        Arrays.toString(inputFormats), inputFormats.length == 0);
                mCollector.expectTrue(""Doesn't support reprocessing but max number of input "" +
                        ""stream is "" + maxNumInputStreams,
                        maxNumInputStreams == null || maxNumInputStreams == 0);
                mCollector.expectTrue(""Doesn't support reprocessing but "" +
                        ""EDGE_MODE_ZERO_SHUTTER_LAG is supported"", !supportZslEdgeMode);
                mCollector.expectTrue(""Doesn't support reprocessing but "" +
                        ""NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG is supported"",
                        !supportZslNoiseReductionMode);
            }
        }
    }

    /**
     * Check ultra high resolution sensor characteristics.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testUltraHighResolutionSensorCharacteristics"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"public void testUltraHighResolutionSensorCharacteristics() {
        for (int i = 0; i < mAllCameraIds.length; i++) {
            CameraCharacteristics c = mCharacteristics.get(i);
            String cameraId = mAllCameraIds[i];
            int[] capabilities = c.get(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
            assertNotNull(""android.request.availableCapabilities must never be null"",
                    capabilities);
            boolean isUltraHighResolutionSensor = arrayContains(capabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR);

            boolean supportsRemosaic = arrayContains(capabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_REMOSAIC_REPROCESSING);

            if (!isUltraHighResolutionSensor) {
                Log.i(TAG, ""Camera id "" + cameraId + "" not ultra high resolution. Skipping "" +
                        ""testUltraHighResolutionSensorCharacteristics"");
                continue;
            }
            assertArrayContains(
                    String.format(""Ultra high resolution sensor, camera id %s"" +
                    "" must also have the RAW capability"", cameraId), capabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW);
            StreamConfigurationMap configs =
                    c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP_MAXIMUM_RESOLUTION);
            assertNotNull(""Maximum resolution stream configuration map must not be null for ultra"" +
                    "" high resolution sensor camera "" + cameraId, configs);
            Size uhrPixelArraySize = CameraTestUtils.getValueNotNull(
                c, CameraCharacteristics.SENSOR_INFO_PIXEL_ARRAY_SIZE_MAXIMUM_RESOLUTION);
            long uhrSensorSize = uhrPixelArraySize.getHeight() * uhrPixelArraySize.getWidth();

            assertTrue(""ULTRA_HIGH_RESOLUTION_SENSOR pixel array size should be at least "" +
                    MIN_UHR_SENSOR_RESOLUTION + "" pixels, is "" + uhrSensorSize + "", for camera id ""
                    + cameraId, uhrSensorSize >= MIN_UHR_SENSOR_RESOLUTION);

            int[] outputFormats = configs.getOutputFormats();
            assertArrayContains(String.format(""No max res JPEG image format for ultra high"" +
                  "" resolution sensor: ID %s"", cameraId), outputFormats, ImageFormat.JPEG);
            assertArrayContains(String.format(""No max res YUV_420_88 image format for ultra high"" +
                  "" resolution sensor: ID %s"", cameraId), outputFormats, ImageFormat.YUV_420_888);
            assertArrayContains(String.format(""No max res RAW_SENSOR image format for ultra high"" +
                  "" resolution sensor: ID %s"", cameraId), outputFormats, ImageFormat.RAW_SENSOR);

            if (supportsRemosaic) {
                testRemosaicReprocessingCharacteristics(cameraId, c);
            }
      }

    }
    /**
     * Check remosaic reprocessing capabilities. Check that ImageFormat.RAW_SENSOR is supported as
     * input and output.
     */
    private void testRemosaicReprocessingCharacteristics(String cameraId, CameraCharacteristics c) {
        StreamConfigurationMap configs =
                c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP_MAXIMUM_RESOLUTION);
        Integer maxNumInputStreams =
                c.get(CameraCharacteristics.REQUEST_MAX_NUM_INPUT_STREAMS);
        int[] inputFormats = configs.getInputFormats();
        int[] outputFormats = configs.getOutputFormats();

        mCollector.expectTrue(""Support reprocessing but max number of input stream is "" +
                maxNumInputStreams, maxNumInputStreams != null && maxNumInputStreams > 0);

        // Verify mandatory input formats are supported
        mCollector.expectTrue(""RAW_SENSOR input support needed for REMOSAIC reprocessing"",
                arrayContains(inputFormats, ImageFormat.RAW_SENSOR));
        // max capture stall must be reported if one of the reprocessing is supported.
        final int MAX_ALLOWED_STALL_FRAMES = 4;
        Integer maxCaptureStall = c.get(CameraCharacteristics.REPROCESS_MAX_CAPTURE_STALL);
        mCollector.expectTrue(""max capture stall must be non-null and no larger than ""
                + MAX_ALLOWED_STALL_FRAMES,
                maxCaptureStall != null && maxCaptureStall <= MAX_ALLOWED_STALL_FRAMES);

        for (int input : inputFormats) {
            // Verify mandatory output formats are supported
            int[] outputFormatsForInput = configs.getValidOutputFormatsForInput(input);

            // Verify camera can output the reprocess input formats and sizes.
            Size[] inputSizes = configs.getInputSizes(input);
            Size[] outputSizes = configs.getOutputSizes(input);
            Size[] highResOutputSizes = configs.getHighResolutionOutputSizes(input);
            mCollector.expectTrue(""no input size supported for format "" + input,
                    inputSizes.length > 0);
            mCollector.expectTrue(""no output size supported for format "" + input,
                    outputSizes.length > 0);

            for (Size inputSize : inputSizes) {
                mCollector.expectTrue(""Camera must be able to output the supported "" +
                        ""reprocessing input size"",
                        arrayContains(outputSizes, inputSize) ||
                        arrayContains(highResOutputSizes, inputSize));
            }
        }
    }


    /**
     * Check depth output capability
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testDepthOutputCharacteristics"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"public void testDepthOutputCharacteristics() {
        for (int i = 0; i < mAllCameraIds.length; i++) {
            Log.i(TAG, ""testDepthOutputCharacteristics: Testing camera ID "" + mAllCameraIds[i]);

            CameraCharacteristics c = mCharacteristics.get(i);
            int[] capabilities = c.get(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
            assertNotNull(""android.request.availableCapabilities must never be null"",
                    capabilities);
            boolean supportDepth = arrayContains(capabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_DEPTH_OUTPUT);
            StreamConfigurationMap configs =
                    c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);

            int[] outputFormats = configs.getOutputFormats();
            boolean hasDepth16 = arrayContains(outputFormats, ImageFormat.DEPTH16);

            Boolean depthIsExclusive = c.get(CameraCharacteristics.DEPTH_DEPTH_IS_EXCLUSIVE);

            float[] poseRotation = c.get(CameraCharacteristics.LENS_POSE_ROTATION);
            float[] poseTranslation = c.get(CameraCharacteristics.LENS_POSE_TRANSLATION);
            Integer poseReference = c.get(CameraCharacteristics.LENS_POSE_REFERENCE);
            float[] cameraIntrinsics = c.get(CameraCharacteristics.LENS_INTRINSIC_CALIBRATION);
            float[] distortion = getLensDistortion(c);
            Size pixelArraySize = c.get(CameraCharacteristics.SENSOR_INFO_PIXEL_ARRAY_SIZE);
            Rect precorrectionArray = c.get(
                CameraCharacteristics.SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE);
            Rect activeArray = c.get(
                CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE);
            Integer facing = c.get(CameraCharacteristics.LENS_FACING);
            float jpegAspectRatioThreshold = .01f;
            boolean jpegSizeMatch = false;

            // Verify pre-correction array encloses active array
            mCollector.expectTrue(""preCorrectionArray ["" + precorrectionArray.left + "", "" +
                    precorrectionArray.top + "", "" + precorrectionArray.right + "", "" +
                    precorrectionArray.bottom + ""] does not enclose activeArray["" +
                    activeArray.left + "", "" + activeArray.top + "", "" + activeArray.right +
                    "", "" + activeArray.bottom,
                    precorrectionArray.contains(activeArray.left, activeArray.top) &&
                    precorrectionArray.contains(activeArray.right-1, activeArray.bottom-1));

            // Verify pixel array encloses pre-correction array
            mCollector.expectTrue(""preCorrectionArray ["" + precorrectionArray.left + "", "" +
                    precorrectionArray.top + "", "" + precorrectionArray.right + "", "" +
                    precorrectionArray.bottom + ""] isn't enclosed by pixelArray["" +
                    pixelArraySize.getWidth() + "", "" + pixelArraySize.getHeight() + ""]"",
                    precorrectionArray.left >= 0 &&
                    precorrectionArray.left < pixelArraySize.getWidth() &&
                    precorrectionArray.right > 0 &&
                    precorrectionArray.right <= pixelArraySize.getWidth() &&
                    precorrectionArray.top >= 0 &&
                    precorrectionArray.top < pixelArraySize.getHeight() &&
                    precorrectionArray.bottom > 0 &&
                    precorrectionArray.bottom <= pixelArraySize.getHeight());

            if (supportDepth) {
                mCollector.expectTrue(""Supports DEPTH_OUTPUT but does not support DEPTH16"",
                        hasDepth16);
                if (hasDepth16) {
                    Size[] depthSizes = configs.getOutputSizes(ImageFormat.DEPTH16);
                    Size[] jpegSizes = configs.getOutputSizes(ImageFormat.JPEG);
                    mCollector.expectTrue(""Supports DEPTH_OUTPUT but no sizes for DEPTH16 supported!"",
                            depthSizes != null && depthSizes.length > 0);
                    if (depthSizes != null) {
                        for (Size depthSize : depthSizes) {
                            mCollector.expectTrue(""All depth16 sizes must be positive"",
                                    depthSize.getWidth() > 0 && depthSize.getHeight() > 0);
                            long minFrameDuration = configs.getOutputMinFrameDuration(
                                    ImageFormat.DEPTH16, depthSize);
                            mCollector.expectTrue(""Non-negative min frame duration for depth size ""
                                    + depthSize + "" expected, got "" + minFrameDuration,
                                    minFrameDuration >= 0);
                            long stallDuration = configs.getOutputStallDuration(
                                    ImageFormat.DEPTH16, depthSize);
                            mCollector.expectTrue(""Non-negative stall duration for depth size ""
                                    + depthSize + "" expected, got "" + stallDuration,
                                    stallDuration >= 0);
                            if ((jpegSizes != null) && (!jpegSizeMatch)) {
                                for (Size jpegSize : jpegSizes) {
                                    if (jpegSize.equals(depthSize)) {
                                        jpegSizeMatch = true;
                                        break;
                                    } else {
                                        float depthAR = (float) depthSize.getWidth() /
                                                (float) depthSize.getHeight();
                                        float jpegAR = (float) jpegSize.getWidth() /
                                                (float) jpegSize.getHeight();
                                        if (Math.abs(depthAR - jpegAR) <=
                                                jpegAspectRatioThreshold) {
                                            jpegSizeMatch = true;
                                            break;
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
                if (arrayContains(outputFormats, ImageFormat.DEPTH_POINT_CLOUD)) {
                    Size[] depthCloudSizes = configs.getOutputSizes(ImageFormat.DEPTH_POINT_CLOUD);
                    mCollector.expectTrue(""Supports DEPTH_POINT_CLOUD "" +
                            ""but no sizes for DEPTH_POINT_CLOUD supported!"",
                            depthCloudSizes != null && depthCloudSizes.length > 0);
                    if (depthCloudSizes != null) {
                        for (Size depthCloudSize : depthCloudSizes) {
                            mCollector.expectTrue(""All depth point cloud sizes must be nonzero"",
                                    depthCloudSize.getWidth() > 0);
                            mCollector.expectTrue(""All depth point cloud sizes must be N x 1"",
                                    depthCloudSize.getHeight() == 1);
                            long minFrameDuration = configs.getOutputMinFrameDuration(
                                    ImageFormat.DEPTH_POINT_CLOUD, depthCloudSize);
                            mCollector.expectTrue(""Non-negative min frame duration for depth size ""
                                    + depthCloudSize + "" expected, got "" + minFrameDuration,
                                    minFrameDuration >= 0);
                            long stallDuration = configs.getOutputStallDuration(
                                    ImageFormat.DEPTH_POINT_CLOUD, depthCloudSize);
                            mCollector.expectTrue(""Non-negative stall duration for depth size ""
                                    + depthCloudSize + "" expected, got "" + stallDuration,
                                    stallDuration >= 0);
                        }
                    }
                }
                if (arrayContains(outputFormats, ImageFormat.DEPTH_JPEG)) {
                    mCollector.expectTrue(""Supports DEPTH_JPEG but has no DEPTH16 support!"",
                            hasDepth16);
                    mCollector.expectTrue(""Supports DEPTH_JPEG but DEPTH_IS_EXCLUSIVE is not "" +
                            ""defined"", depthIsExclusive != null);
                    mCollector.expectTrue(""Supports DEPTH_JPEG but DEPTH_IS_EXCLUSIVE is true"",
                            !depthIsExclusive.booleanValue());
                    Size[] depthJpegSizes = configs.getOutputSizes(ImageFormat.DEPTH_JPEG);
                    mCollector.expectTrue(""Supports DEPTH_JPEG "" +
                            ""but no sizes for DEPTH_JPEG supported!"",
                            depthJpegSizes != null && depthJpegSizes.length > 0);
                    mCollector.expectTrue(""Supports DEPTH_JPEG but there are no JPEG sizes with"" +
                            "" matching DEPTH16 aspect ratio"", jpegSizeMatch);
                    if (depthJpegSizes != null) {
                        for (Size depthJpegSize : depthJpegSizes) {
                            mCollector.expectTrue(""All depth jpeg sizes must be nonzero"",
                                    depthJpegSize.getWidth() > 0 && depthJpegSize.getHeight() > 0);
                            long minFrameDuration = configs.getOutputMinFrameDuration(
                                    ImageFormat.DEPTH_JPEG, depthJpegSize);
                            mCollector.expectTrue(""Non-negative min frame duration for depth jpeg"" +
                                   "" size "" + depthJpegSize + "" expected, got "" + minFrameDuration,
                                    minFrameDuration >= 0);
                            long stallDuration = configs.getOutputStallDuration(
                                    ImageFormat.DEPTH_JPEG, depthJpegSize);
                            mCollector.expectTrue(""Non-negative stall duration for depth jpeg size ""
                                    + depthJpegSize + "" expected, got "" + stallDuration,
                                    stallDuration >= 0);
                        }
                    }
                } else {
                    boolean canSupportDynamicDepth = jpegSizeMatch && !depthIsExclusive;
                    mCollector.expectTrue(""Device must support DEPTH_JPEG, please check whether "" +
                            ""library libdepthphoto.so is part of the device PRODUCT_PACKAGES"",
                            !canSupportDynamicDepth);
                }


                mCollector.expectTrue(""Supports DEPTH_OUTPUT but DEPTH_IS_EXCLUSIVE is not defined"",
                        depthIsExclusive != null);

                verifyLensCalibration(poseRotation, poseTranslation, poseReference,
                        cameraIntrinsics, distortion, precorrectionArray, facing);

            } else {
                boolean hasFields =
                    hasDepth16 && (poseTranslation != null) &&
                    (poseRotation != null) && (cameraIntrinsics != null) &&
                    (distortion != null) && (depthIsExclusive != null);

                mCollector.expectTrue(
                        ""All necessary depth fields defined, but DEPTH_OUTPUT capability is not listed"",
                        !hasFields);

                boolean reportCalibration = poseTranslation != null ||
                        poseRotation != null || cameraIntrinsics !=null;
                // Verify calibration keys are co-existing
                if (reportCalibration) {
                    mCollector.expectTrue(
                            ""Calibration keys must be co-existing"",
                            poseTranslation != null && poseRotation != null &&
                            cameraIntrinsics !=null);
                }

                boolean reportDistortion = distortion != null;
                if (reportDistortion) {
                    mCollector.expectTrue(
                            ""Calibration keys must present where distortion is reported"",
                            reportCalibration);
                }
            }
        }
    }

    private void verifyLensCalibration(float[] poseRotation, float[] poseTranslation,
            Integer poseReference, float[] cameraIntrinsics, float[] distortion,
            Rect precorrectionArray, Integer facing) {

        mCollector.expectTrue(
            ""LENS_POSE_ROTATION not right size"",
            poseRotation != null && poseRotation.length == 4);
        mCollector.expectTrue(
            ""LENS_POSE_TRANSLATION not right size"",
            poseTranslation != null && poseTranslation.length == 3);
        mCollector.expectTrue(
            ""LENS_POSE_REFERENCE is not defined"",
            poseReference != null);
        mCollector.expectTrue(
            ""LENS_INTRINSIC_CALIBRATION not right size"",
            cameraIntrinsics != null && cameraIntrinsics.length == 5);
        mCollector.expectTrue(
            ""LENS_DISTORTION not right size"",
            distortion != null && distortion.length == 6);

        if (poseRotation != null && poseRotation.length == 4) {
            float normSq =
                    poseRotation[0] * poseRotation[0] +
                    poseRotation[1] * poseRotation[1] +
                    poseRotation[2] * poseRotation[2] +
                    poseRotation[3] * poseRotation[3];
            mCollector.expectTrue(
                ""LENS_POSE_ROTATION quarternion must be unit-length"",
                0.9999f < normSq && normSq < 1.0001f);

            if (facing.intValue() == CameraMetadata.LENS_FACING_FRONT ||
                    facing.intValue() == CameraMetadata.LENS_FACING_BACK) {
                // Use the screen's natural facing to test pose rotation
                int[] facingSensor = new int[]{0, 0, 1};
                float[][] r = new float[][] {
                        { 1.0f - 2 * poseRotation[1] * poseRotation[1]
                              - 2 * poseRotation[2] * poseRotation[2],
                          2 * poseRotation[0] * poseRotation[1]
                              - 2 * poseRotation[2] * poseRotation[3],
                          2 * poseRotation[0] * poseRotation[2]
                              + 2 * poseRotation[1] * poseRotation[3] },
                        { 2 * poseRotation[0] * poseRotation[1]
                              + 2 * poseRotation[2] * poseRotation[3],
                          1.0f - 2 * poseRotation[0] * poseRotation[0]
                              - 2 * poseRotation[2] * poseRotation[2],
                          2 * poseRotation[1] * poseRotation[2]
                              - 2 * poseRotation[0] * poseRotation[3] },
                        { 2 * poseRotation[0] * poseRotation[2]
                              - 2 * poseRotation[1] * poseRotation[3],
                          2 * poseRotation[1] * poseRotation[2]
                              + 2 * poseRotation[0] * poseRotation[3],
                          1.0f - 2 * poseRotation[0] * poseRotation[0]
                              - 2 * poseRotation[1] * poseRotation[1] }
                      };
                // The screen natural facing in camera's coordinate system
                float facingCameraX = r[0][0] * facingSensor[0] + r[0][1] * facingSensor[1] +
                        r[0][2] * facingSensor[2];
                float facingCameraY = r[1][0] * facingSensor[0] + r[1][1] * facingSensor[1] +
                        r[1][2] * facingSensor[2];
                float facingCameraZ = r[2][0] * facingSensor[0] + r[2][1] * facingSensor[1] +
                        r[2][2] * facingSensor[2];

                mCollector.expectTrue(""LENS_POSE_ROTATION must be consistent with lens facing"",
                        (facingCameraZ > 0) ^
                        (facing.intValue() == CameraMetadata.LENS_FACING_BACK));

                if (poseReference == CameraCharacteristics.LENS_POSE_REFERENCE_UNDEFINED) {
                    mCollector.expectTrue(
                            ""LENS_POSE_ROTATION quarternion must be consistent with camera's "" +
                            ""default facing"",
                            Math.abs(facingCameraX) < 0.00001f &&
                            Math.abs(facingCameraY) < 0.00001f &&
                            Math.abs(facingCameraZ) > 0.99999f &&
                            Math.abs(facingCameraZ) < 1.00001f);
                }
            }

            // TODO: Cross-validate orientation and poseRotation
        }

        if (poseTranslation != null && poseTranslation.length == 3) {
            float normSq =
                    poseTranslation[0] * poseTranslation[0] +
                    poseTranslation[1] * poseTranslation[1] +
                    poseTranslation[2] * poseTranslation[2];
            mCollector.expectTrue(""Pose translation is larger than 1 m"",
                    normSq < 1.f);

            // Pose translation should be all 0s for UNDEFINED pose reference.
            if (poseReference != null && poseReference ==
                    CameraCharacteristics.LENS_POSE_REFERENCE_UNDEFINED) {
                mCollector.expectTrue(""Pose translation aren't all 0s "",
                        normSq < 0.00001f);
            }
        }

        if (poseReference != null) {
            int ref = poseReference;
            boolean validReference = false;
            switch (ref) {
                case CameraCharacteristics.LENS_POSE_REFERENCE_PRIMARY_CAMERA:
                case CameraCharacteristics.LENS_POSE_REFERENCE_GYROSCOPE:
                case CameraCharacteristics.LENS_POSE_REFERENCE_UNDEFINED:
                    // Allowed values
                    validReference = true;
                    break;
                default:
            }
            mCollector.expectTrue(""POSE_REFERENCE has unknown value"", validReference);
        }

        mCollector.expectTrue(""Does not have precorrection active array defined"",
                precorrectionArray != null);

        if (cameraIntrinsics != null && precorrectionArray != null) {
            float fx = cameraIntrinsics[0];
            float fy = cameraIntrinsics[1];
            float cx = cameraIntrinsics[2];
            float cy = cameraIntrinsics[3];
            float s = cameraIntrinsics[4];
            mCollector.expectTrue(""Optical center expected to be within precorrection array"",
                    0 <= cx && cx < precorrectionArray.width() &&
                    0 <= cy && cy < precorrectionArray.height());

            // TODO: Verify focal lengths and skew are reasonable
        }

        if (distortion != null) {
            // TODO: Verify radial distortion
        }

    }

    /**
     * Cross-check StreamConfigurationMap output
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testStreamConfigurationMap"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"public void testStreamConfigurationMap() throws Exception {
        for (int i = 0; i < mAllCameraIds.length; i++) {
            Log.i(TAG, ""testStreamConfigurationMap: Testing camera ID "" + mAllCameraIds[i]);
            CameraCharacteristics c = mCharacteristics.get(i);
            StreamConfigurationMap config =
                    c.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            assertNotNull(String.format(""No stream configuration map found for: ID %s"",
                            mAllCameraIds[i]), config);

            int[] actualCapabilities = c.get(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
            assertNotNull(""android.request.availableCapabilities must never be null"",
                    actualCapabilities);

            if (arrayContains(actualCapabilities, BC)) {
                assertTrue(""ImageReader must be supported"",
                    config.isOutputSupportedFor(android.media.ImageReader.class));
                assertTrue(""MediaRecorder must be supported"",
                    config.isOutputSupportedFor(android.media.MediaRecorder.class));
                assertTrue(""MediaCodec must be supported"",
                    config.isOutputSupportedFor(android.media.MediaCodec.class));
                assertTrue(""Allocation must be supported"",
                    config.isOutputSupportedFor(android.renderscript.Allocation.class));
                assertTrue(""SurfaceHolder must be supported"",
                    config.isOutputSupportedFor(android.view.SurfaceHolder.class));
                assertTrue(""SurfaceTexture must be supported"",
                    config.isOutputSupportedFor(android.graphics.SurfaceTexture.class));

                assertTrue(""YUV_420_888 must be supported"",
                    config.isOutputSupportedFor(ImageFormat.YUV_420_888));
                assertTrue(""JPEG must be supported"",
                    config.isOutputSupportedFor(ImageFormat.JPEG));
            } else {
                assertTrue(""YUV_420_88 may not be supported if BACKWARD_COMPATIBLE capability is not listed"",
                    !config.isOutputSupportedFor(ImageFormat.YUV_420_888));
                assertTrue(""JPEG may not be supported if BACKWARD_COMPATIBLE capability is not listed"",
                    !config.isOutputSupportedFor(ImageFormat.JPEG));
            }

            // Check RAW

            if (arrayContains(actualCapabilities,
                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW)) {
                assertTrue(""RAW_SENSOR must be supported if RAW capability is advertised"",
                    config.isOutputSupportedFor(ImageFormat.RAW_SENSOR));
            }

            // Cross check public formats and sizes

            int[] supportedFormats = config.getOutputFormats();
            for (int format : supportedFormats) {
                assertTrue(""Format "" + format + "" fails cross check"",
                        config.isOutputSupportedFor(format));
                List<Size> supportedSizes = CameraTestUtils.getAscendingOrderSizes(
                        Arrays.asList(config.getOutputSizes(format)), /*ascending*/true);
                if (arrayContains(actualCapabilities,
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_BURST_CAPTURE)) {
                    supportedSizes.addAll(
                        Arrays.asList(config.getHighResolutionOutputSizes(format)));
                    supportedSizes = CameraTestUtils.getAscendingOrderSizes(
                        supportedSizes, /*ascending*/true);
                }
                assertTrue(""Supported format "" + format + "" has no sizes listed"",
                        supportedSizes.size() > 0);
                for (int j = 0; j < supportedSizes.size(); j++) {
                    Size size = supportedSizes.get(j);
                    if (VERBOSE) {
                        Log.v(TAG,
                                String.format(""Testing camera %s, format %d, size %s"",
                                        mAllCameraIds[i], format, size.toString()));
                    }

                    long stallDuration = config.getOutputStallDuration(format, size);
                    switch(format) {
                        case ImageFormat.YUV_420_888:
                            assertTrue(""YUV_420_888 may not have a non-zero stall duration"",
                                    stallDuration == 0);
                            break;
                        case ImageFormat.JPEG:
                        case ImageFormat.RAW_SENSOR:
                            final float TOLERANCE_FACTOR = 2.0f;
                            long prevDuration = 0;
                            if (j > 0) {
                                prevDuration = config.getOutputStallDuration(
                                        format, supportedSizes.get(j - 1));
                            }
                            long nextDuration = Long.MAX_VALUE;
                            if (j < (supportedSizes.size() - 1)) {
                                nextDuration = config.getOutputStallDuration(
                                        format, supportedSizes.get(j + 1));
                            }
                            long curStallDuration = config.getOutputStallDuration(format, size);
                            // Stall duration should be in a reasonable range: larger size should
                            // normally have larger stall duration.
                            mCollector.expectInRange(""Stall duration (format "" + format +
                                    "" and size "" + size + "") is not in the right range"",
                                    curStallDuration,
                                    (long) (prevDuration / TOLERANCE_FACTOR),
                                    (long) (nextDuration * TOLERANCE_FACTOR));
                            break;
                        default:
                            assertTrue(""Negative stall duration for format "" + format,
                                    stallDuration >= 0);
                            break;
                    }
                    long minDuration = config.getOutputMinFrameDuration(format, size);
                    if (arrayContains(actualCapabilities,
                            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
                        assertTrue(""MANUAL_SENSOR capability, need positive min frame duration for""
                                + ""format "" + format + "" for size "" + size + "" minDuration "" +
                                minDuration,
                                minDuration > 0);
                    } else {
                        assertTrue(""Need non-negative min frame duration for format "" + format,
                                minDuration >= 0);
                    }

                    // todo: test opaque image reader when it's supported.
                    if (format != ImageFormat.PRIVATE) {
                        ImageReader testReader = ImageReader.newInstance(
                            size.getWidth(),
                            size.getHeight(),
                            format,
                            1);
                        Surface testSurface = testReader.getSurface();

                        assertTrue(
                            String.format(""isOutputSupportedFor fails for config %s, format %d"",
                                    size.toString(), format),
                            config.isOutputSupportedFor(testSurface));

                        testReader.close();
                    }
                } // sizes

                // Try an invalid size in this format, should round
                Size invalidSize = findInvalidSize(supportedSizes);
                int MAX_ROUNDING_WIDTH = 1920;
                // todo: test opaque image reader when it's supported.
                if (format != ImageFormat.PRIVATE &&
                        invalidSize.getWidth() <= MAX_ROUNDING_WIDTH) {
                    ImageReader testReader = ImageReader.newInstance(
                                                                     invalidSize.getWidth(),
                                                                     invalidSize.getHeight(),
                                                                     format,
                                                                     1);
                    Surface testSurface = testReader.getSurface();

                    assertTrue(
                               String.format(""isOutputSupportedFor fails for config %s, %d"",
                                       invalidSize.toString(), format),
                               config.isOutputSupportedFor(testSurface));

                    testReader.close();
                }
            } // formats

            // Cross-check opaque format and sizes
            if (arrayContains(actualCapabilities, BC)) {
                SurfaceTexture st = new SurfaceTexture(1);
                Surface surf = new Surface(st);

                Size[] opaqueSizes = CameraTestUtils.getSupportedSizeForClass(SurfaceTexture.class,
                        mAllCameraIds[i], mCameraManager);
                assertTrue(""Opaque format has no sizes listed"",
                        opaqueSizes.length > 0);
                for (Size size : opaqueSizes) {
                    long stallDuration = config.getOutputStallDuration(SurfaceTexture.class, size);
                    assertTrue(""Opaque output may not have a non-zero stall duration"",
                            stallDuration == 0);

                    long minDuration = config.getOutputMinFrameDuration(SurfaceTexture.class, size);
                    if (arrayContains(actualCapabilities,
                                    CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
                        assertTrue(""MANUAL_SENSOR capability, need positive min frame duration for""
                                + ""opaque format"",
                                minDuration > 0);
                    } else {
                        assertTrue(""Need non-negative min frame duration for opaque format "",
                                minDuration >= 0);
                    }
                    st.setDefaultBufferSize(size.getWidth(), size.getHeight());

                    assertTrue(
                            String.format(""isOutputSupportedFor fails for SurfaceTexture config %s"",
                                    size.toString()),
                            config.isOutputSupportedFor(surf));

                } // opaque sizes

                // Try invalid opaque size, should get rounded
                Size invalidSize = findInvalidSize(opaqueSizes);
                st.setDefaultBufferSize(invalidSize.getWidth(), invalidSize.getHeight());
                assertTrue(
                        String.format(""isOutputSupportedFor fails for SurfaceTexture config %s"",
                                invalidSize.toString()),
                        config.isOutputSupportedFor(surf));

            }
        } // mCharacteristics
    }

    /**
     * Test high speed capability and cross-check the high speed sizes and fps ranges from
     * the StreamConfigurationMap.
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"getCharacteristics"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"/*
 *.
 */

package android.hardware.camera2.cts;

import android.content.Context;
import android.content.pm.PackageManager;
import android.graphics.ImageFormat;
import android.graphics.Rect;
import android.graphics.SurfaceTexture;
import android.hardware.Camera;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraCharacteristics.Key;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CameraMetadata;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.cts.helpers.CameraErrorCollector;
import android.hardware.camera2.cts.helpers.StaticMetadata;
import android.hardware.camera2.cts.testcases.Camera2AndroidTestCase;
import android.hardware.camera2.params.BlackLevelPattern;
import android.hardware.camera2.params.ColorSpaceTransform;
import android.hardware.camera2.params.RecommendedStreamConfigurationMap;
import android.hardware.camera2.params.StreamConfigurationMap;
import android.media.CamcorderProfile;
import android.media.ImageReader;
import android.os.Build;
import android.util.ArraySet;
import android.util.DisplayMetrics;
import android.util.Log;
import android.util.Rational;
import android.util.Range;
import android.util.Size;
import android.util.Pair;
import android.util.Patterns;
import android.view.Display;
import android.view.Surface;
import android.view.WindowManager;

import com.android.compatibility.common.util.CddTest;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Objects;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.Set;

import org.junit.runners.Parameterized;
import org.junit.runner.RunWith;
import org.junit.Test;

import static android.hardware.camera2.cts.helpers.AssertHelpers.*;
import static android.hardware.camera2.cts.CameraTestUtils.SimpleCaptureCallback;
import static android.hardware.cts.helpers.CameraUtils.matchParametersToCharacteristics;

import static junit.framework.Assert.*;

import static org.mockito.Mockito.*;

/**
 * Extended tests for static camera characteristics.
 */
@RunWith(Parameterized.class)
public class ExtendedCameraCharacteristicsTest extends Camera2AndroidTestCase {
    private static final String TAG = ""ExChrsTest""; // must be short so next line doesn't throw
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);

    private static final String PREFIX_ANDROID = ""android"";

    /*
     * Constants for static RAW metadata.
     */
    private static final int MIN_ALLOWABLE_WHITELEVEL = 32; // must have sensor bit depth > 5

    private List<CameraCharacteristics> mCharacteristics;

    private static final Size FULLHD = new Size(1920, 1080);
    private static final Size FULLHD_ALT = new Size(1920, 1088);
    private static final Size HD = new Size(1280, 720);
    private static final Size VGA = new Size(640, 480);
    private static final Size QVGA = new Size(320, 240);
    private static final Size UHD = new Size(3840, 2160);
    private static final Size DC4K = new Size(4096, 2160);

    private static final long MIN_BACK_SENSOR_RESOLUTION = 2000000;
    private static final long MIN_FRONT_SENSOR_RESOLUTION = VGA.getHeight() * VGA.getWidth();
    private static final long LOW_LATENCY_THRESHOLD_MS = 200;
    private static final float LATENCY_TOLERANCE_FACTOR = 1.1f; // 10% tolerance
    private static final int MAX_NUM_IMAGES = 5;
    private static final long PREVIEW_RUN_MS = 500;
    private static final long FRAME_DURATION_30FPS_NSEC = (long) 1e9 / 30;

    private static final long MIN_BACK_SENSOR_PERF_CLASS_RESOLUTION = 12000000;
    private static final long MIN_FRONT_SENSOR_S_PERF_CLASS_RESOLUTION = 5000000;
    private static final long MIN_FRONT_SENSOR_R_PERF_CLASS_RESOLUTION = 4000000;

    private static final long MIN_UHR_SENSOR_RESOLUTION = 24000000;
    /*
     * HW Levels short hand
     */
    private static final int LEGACY = CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY;
    private static final int LIMITED = CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED;
    private static final int FULL = CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_FULL;
    private static final int LEVEL_3 = CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_3;
    private static final int EXTERNAL = CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL;
    private static final int OPT = Integer.MAX_VALUE;  // For keys that are optional on all hardware levels.

    /*
     * Capabilities short hand
     */
    private static final int NONE = -1;
    private static final int BC =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE;
    private static final int MANUAL_SENSOR =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR;
    private static final int MANUAL_POSTPROC =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_POST_PROCESSING;
    private static final int RAW =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_RAW;
    private static final int YUV_REPROCESS =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING;
    private static final int OPAQUE_REPROCESS =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING;
    private static final int CONSTRAINED_HIGH_SPEED =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_CONSTRAINED_HIGH_SPEED_VIDEO;
    private static final int MONOCHROME =
            CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MONOCHROME;
    private static final int HIGH_SPEED_FPS_LOWER_MIN = 30;
    private static final int HIGH_SPEED_FPS_UPPER_MIN = 120;

    @Override
    public void setUp() throws Exception {
        super.setUp();
        mCharacteristics = new ArrayList<>();
        for (int i = 0; i < mAllCameraIds.length; i++) {
            mCharacteristics.add(mAllStaticInfo.get(mAllCameraIds[i]).getCharacteristics());
        }
    }

    @Override
    public void tearDown() throws Exception {
        super.tearDown();
        mCharacteristics = null;
    }

    /**
     * Test that the available stream configurations contain a few required formats and sizes.
     */
    @CddTest(requirement=""7.5.1/C-1-2"")"	""	""	"1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.ExtendedCameraCharacteristicsTest"	"testCameraPerfClassCharacteristics"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/ExtendedCameraCharacteristicsTest.java"	""	"@CddTest(requirement=""7.5"")
    public void testCameraPerfClassCharacteristics() throws Exception {
        if (mAdoptShellPerm) {
            // Skip test for system camera. Performance class is only applicable for public camera
            // ids.
            return;
        }
        boolean isRPerfClass = CameraTestUtils.isRPerfClass();
        boolean isSPerfClass = CameraTestUtils.isSPerfClass();
        if (!isRPerfClass && !isSPerfClass) {
            return;
        }

        boolean hasPrimaryRear = false;
        boolean hasPrimaryFront = false;
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            String cameraId = mCameraIdsUnderTest[i];
            boolean isPrimaryRear = CameraTestUtils.isPrimaryRearFacingCamera(
                    mCameraManager, cameraId);
            boolean isPrimaryFront = CameraTestUtils.isPrimaryFrontFacingCamera(
                    mCameraManager, cameraId);
            if (!isPrimaryRear && !isPrimaryFront) {
                continue;
            }

            CameraCharacteristics c = mCharacteristics.get(i);
            StaticMetadata staticInfo = mAllStaticInfo.get(cameraId);

            // H-1-1, H-1-2
            Size pixelArraySize = CameraTestUtils.getValueNotNull(
                    c, CameraCharacteristics.SENSOR_INFO_PIXEL_ARRAY_SIZE);
            long sensorResolution = pixelArraySize.getHeight() * pixelArraySize.getWidth();
            StreamConfigurationMap config = staticInfo.getValueFromKeyNonNull(
                    CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
            assertNotNull(""No stream configuration map found for ID "" + cameraId, config);
            List<Size> videoSizes = CameraTestUtils.getSupportedVideoSizes(cameraId,
                    mCameraManager, null /*bound*/);

            if (isPrimaryRear) {
                hasPrimaryRear = true;
                mCollector.expectTrue(""Primary rear camera resolution should be at least "" +
                        MIN_BACK_SENSOR_PERF_CLASS_RESOLUTION + "" pixels, is ""+
                        sensorResolution,
                        sensorResolution >= MIN_BACK_SENSOR_PERF_CLASS_RESOLUTION);

                // 4K @ 30fps
                boolean supportUHD = videoSizes.contains(UHD);
                boolean supportDC4K = videoSizes.contains(DC4K);
                mCollector.expectTrue(""Primary rear camera should support 4k video recording"",
                        supportUHD || supportDC4K);
                if (supportUHD || supportDC4K) {
                    long minFrameDuration = config.getOutputMinFrameDuration(
                            android.media.MediaRecorder.class, supportDC4K ? DC4K : UHD);
                    mCollector.expectTrue(""Primary rear camera should support 4k video @ 30fps"",
                            minFrameDuration < (1e9 / 29.9));
                }
            } else {
                hasPrimaryFront = true;
                if (isSPerfClass) {
                    mCollector.expectTrue(""Primary front camera resolution should be at least "" +
                            MIN_FRONT_SENSOR_S_PERF_CLASS_RESOLUTION + "" pixels, is ""+
                            sensorResolution,
                            sensorResolution >= MIN_FRONT_SENSOR_S_PERF_CLASS_RESOLUTION);
                } else {
                    mCollector.expectTrue(""Primary front camera resolution should be at least "" +
                            MIN_FRONT_SENSOR_R_PERF_CLASS_RESOLUTION + "" pixels, is ""+
                            sensorResolution,
                            sensorResolution >= MIN_FRONT_SENSOR_R_PERF_CLASS_RESOLUTION);
                }
                // 1080P @ 30fps
                boolean supportFULLHD = videoSizes.contains(FULLHD);
                mCollector.expectTrue(""Primary front camera should support 1080P video recording"",
                        supportFULLHD);
                if (supportFULLHD) {
                    long minFrameDuration = config.getOutputMinFrameDuration(
                            android.media.MediaRecorder.class, FULLHD);
                    mCollector.expectTrue(""Primary front camera should support 1080P video @ 30fps"",
                            minFrameDuration < (1e9 / 29.9));
                }
            }

            String facingString = hasPrimaryRear ? ""rear"" : ""front"";
            // H-1-3
            if (isSPerfClass || (isRPerfClass && isPrimaryRear)) {
                mCollector.expectTrue(""Primary "" + facingString +
                        "" camera should be at least FULL, but is "" +
                        toStringHardwareLevel(staticInfo.getHardwareLevelChecked()),
                        staticInfo.isHardwareLevelAtLeastFull());
            } else {
                mCollector.expectTrue(""Primary "" + facingString +
                        "" camera should be at least LIMITED, but is "" +
                        toStringHardwareLevel(staticInfo.getHardwareLevelChecked()),
                        staticInfo.isHardwareLevelAtLeastLimited());
            }

            // H-1-4
            Integer timestampSource = c.get(CameraCharacteristics.SENSOR_INFO_TIMESTAMP_SOURCE);
            mCollector.expectTrue(
                    ""Primary "" + facingString + "" camera should support real-time timestamp source"",
                    timestampSource != null &&
                    timestampSource.equals(CameraMetadata.SENSOR_INFO_TIMESTAMP_SOURCE_REALTIME));

            // H-1-8
            if (isSPerfClass && isPrimaryRear) {
                mCollector.expectTrue(""Primary rear camera should support RAW capability"",
                        staticInfo.isCapabilitySupported(RAW));
            }
        }
        mCollector.expectTrue(""There must be a primary rear camera for performance class."",
                hasPrimaryRear);
        mCollector.expectTrue(""There must be a primary front camera for performance class."",
                hasPrimaryFront);
    }

    /**
     * Get lens distortion coefficients, as a list of 6 floats; returns null if no valid
     * distortion field is available
     */
    private float[] getLensDistortion(CameraCharacteristics c) {
        float[] distortion = null;
        float[] newDistortion = c.get(CameraCharacteristics.LENS_DISTORTION);
        if (Build.VERSION.DEVICE_INITIAL_SDK_INT > Build.VERSION_CODES.O_MR1 || newDistortion != null) {
            // New devices need to use fixed radial distortion definition; old devices can
            // opt-in to it
            if (newDistortion != null && newDistortion.length == 5) {
                distortion = new float[6];
                distortion[0] = 1.0f;
                for (int i = 1; i < 6; i++) {
                    distortion[i] = newDistortion[i-1];
                }
            }
        } else {
            // Select old field only if on older first SDK and new definition not available
            distortion = c.get(CameraCharacteristics.LENS_RADIAL_DISTORTION);
        }
        return distortion;
    }

    /**
     * Create an invalid size that's close to one of the good sizes in the list, but not one of them
     */
    private Size findInvalidSize(Size[] goodSizes) {
        return findInvalidSize(Arrays.asList(goodSizes));
    }

    /**
     * Create an invalid size that's close to one of the good sizes in the list, but not one of them
     */
    private Size findInvalidSize(List<Size> goodSizes) {
        Size invalidSize = new Size(goodSizes.get(0).getWidth() + 1, goodSizes.get(0).getHeight());
        while(goodSizes.contains(invalidSize)) {
            invalidSize = new Size(invalidSize.getWidth() + 1, invalidSize.getHeight());
        }
        return invalidSize;
    }

    /**
     * Check key is present in characteristics if the hardware level is at least {@code hwLevel};
     * check that the key is present if the actual capabilities are one of {@code capabilities}.
     *
     * @return value of the {@code key} from {@code c}
     */
    private <T> T expectKeyAvailable(CameraCharacteristics c, CameraCharacteristics.Key<T> key,
            int hwLevel, int... capabilities) {

        Integer actualHwLevel = c.get(CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL);
        assertNotNull(""android.info.supportedHardwareLevel must never be null"", actualHwLevel);

        int[] actualCapabilities = c.get(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
        assertNotNull(""android.request.availableCapabilities must never be null"",
                actualCapabilities);

        List<Key<?>> allKeys = c.getKeys();

        T value = c.get(key);

        // For LIMITED-level targeted keys, rely on capability check, not level
        if ((compareHardwareLevel(actualHwLevel, hwLevel) >= 0) && (hwLevel != LIMITED)) {
            mCollector.expectTrue(
                    String.format(""Key (%s) must be in characteristics for this hardware level "" +
                            ""(required minimal HW level %s, actual HW level %s)"",
                            key.getName(), toStringHardwareLevel(hwLevel),
                            toStringHardwareLevel(actualHwLevel)),
                    value != null);
            mCollector.expectTrue(
                    String.format(""Key (%s) must be in characteristics list of keys for this "" +
                            ""hardware level (required minimal HW level %s, actual HW level %s)"",
                            key.getName(), toStringHardwareLevel(hwLevel),
                            toStringHardwareLevel(actualHwLevel)),
                    allKeys.contains(key));
        } else if (arrayContainsAnyOf(actualCapabilities, capabilities)) {
            if (!(hwLevel == LIMITED && compareHardwareLevel(actualHwLevel, hwLevel) < 0)) {
                // Don't enforce LIMITED-starting keys on LEGACY level, even if cap is defined
                mCollector.expectTrue(
                    String.format(""Key (%s) must be in characteristics for these capabilities "" +
                            ""(required capabilities %s, actual capabilities %s)"",
                            key.getName(), Arrays.toString(capabilities),
                            Arrays.toString(actualCapabilities)),
                    value != null);
                mCollector.expectTrue(
                    String.format(""Key (%s) must be in characteristics list of keys for "" +
                            ""these capabilities (required capabilities %s, actual capabilities %s)"",
                            key.getName(), Arrays.toString(capabilities),
                            Arrays.toString(actualCapabilities)),
                    allKeys.contains(key));
            }
        } else {
            if (actualHwLevel == LEGACY && hwLevel != OPT) {
                if (value != null || allKeys.contains(key)) {
                    Log.w(TAG, String.format(
                            ""Key (%s) is not required for LEGACY devices but still appears"",
                            key.getName()));
                }
            }
            // OK: Key may or may not be present.
        }
        return value;
    }

    private static boolean arrayContains(int[] arr, int needle) {
        if (arr == null) {
            return false;
        }

        for (int elem : arr) {
            if (elem == needle) {
                return true;
            }
        }

        return false;
    }

    private static <T> boolean arrayContains(T[] arr, T needle) {
        if (arr == null) {
            return false;
        }

        for (T elem : arr) {
            if (elem.equals(needle)) {
                return true;
            }
        }

        return false;
    }

    private static boolean arrayContainsAnyOf(int[] arr, int[] needles) {
        for (int needle : needles) {
            if (arrayContains(arr, needle)) {
                return true;
            }
        }
        return false;
    }

    /**
     * The key name has a prefix of either ""android."" or a valid TLD; other prefixes are not valid.
     */
    private static void assertKeyPrefixValid(String keyName) {
        assertStartsWithAndroidOrTLD(
                ""All metadata keys must start with 'android.' (built-in keys) "" +
                ""or valid TLD (vendor-extended keys)"", keyName);
    }

    private static void assertTrueForKey(String msg, CameraCharacteristics.Key<?> key,
            boolean actual) {
        assertTrue(msg + "" (key = '"" + key.getName() + ""')"", actual);
    }

    private static <T> void assertOneOf(String msg, T[] expected, T actual) {
        for (int i = 0; i < expected.length; ++i) {
            if (Objects.equals(expected[i], actual)) {
                return;
            }
        }

        fail(String.format(""%s: (expected one of %s, actual %s)"",
                msg, Arrays.toString(expected), actual));
    }

    private static <T> void assertStartsWithAndroidOrTLD(String msg, String keyName) {
        String delimiter = ""."";
        if (keyName.startsWith(PREFIX_ANDROID + delimiter)) {
            return;
        }
        Pattern tldPattern = Pattern.compile(Patterns.TOP_LEVEL_DOMAIN_STR);
        Matcher match = tldPattern.matcher(keyName);
        if (match.find(0) && (0 == match.start()) && (!match.hitEnd())) {
            if (keyName.regionMatches(match.end(), delimiter, 0, delimiter.length())) {
                return;
            }
        }

        fail(String.format(""%s: (expected to start with %s or valid TLD, but value was %s)"",
                msg, PREFIX_ANDROID + delimiter, keyName));
    }

    /** Return a positive int if left > right, 0 if left==right, negative int if left < right */
    private static int compareHardwareLevel(int left, int right) {
        return remapHardwareLevel(left) - remapHardwareLevel(right);
    }

    /** Remap HW levels worst<->best, 0 = LEGACY, 1 = LIMITED, 2 = FULL, ..., N = LEVEL_N */
    private static int remapHardwareLevel(int level) {
        switch (level) {
            case OPT:
                return Integer.MAX_VALUE;
            case LEGACY:
                return 0; // lowest
            case EXTERNAL:
                return 1; // second lowest
            case LIMITED:
                return 2;
            case FULL:
                return 3; // good
            case LEVEL_3:
                return 4;
            default:
                fail(""Unknown HW level: "" + level);
        }
        return -1;
    }

    private static String toStringHardwareLevel(int level) {
        switch (level) {
            case LEGACY:
                return ""LEGACY"";
            case LIMITED:
                return ""LIMITED"";
            case FULL:
                return ""FULL"";
            case EXTERNAL:
                return ""EXTERNAL"";
            default:
                if (level >= LEVEL_3) {
                    return String.format(""LEVEL_%d"", level);
                }
        }

        // unknown
        Log.w(TAG, ""Unknown hardware level "" + level);
        return Integer.toString(level);
    }
}"	""	""	"1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.Camera_SizeTest"	"testConstructor"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/Camera_SizeTest.java"	""	"public void testConstructor() {
        if (Camera.getNumberOfCameras() < 1) {
            return;
        }

        Camera camera = Camera.open(0);
        Parameters parameters = camera.getParameters();

        checkSize(parameters, WIDTH1, HEIGHT1);
        checkSize(parameters, WIDTH2, HEIGHT2);
        checkSize(parameters, WIDTH3, HEIGHT3);

        camera.release();
    }

    /**
     * Check that the largest available preview and jpeg outputs have the same aspect ratio.  This
     * aspect ratio must be the same as the physical camera sensor, and the FOV for these outputs
     * must not be cropped.
     *
     * This is only required for backward compatibility of the Camera2 API when running in LEGACY
     * mode.
     *
     * @see {@link android.hardware.camera2.CameraCharacteristics#INFO_SUPPORTED_HARDWARE_LEVEL}
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.cts.Camera_SizeTest"	"testMaxAspectRatios"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/cts/Camera_SizeTest.java"	""	"public void testMaxAspectRatios() throws Exception {
        int[] cameraIds = CameraUtils.deriveCameraIdsUnderTest();
        for (int id : cameraIds) {
            if (CameraUtils.isLegacyHAL(getContext(), id)) {

                Camera camera = Camera.open(id);
                Parameters parameters = camera.getParameters();

                List<Camera.Size> supportedJpegDimens = parameters.getSupportedPictureSizes();
                List<Camera.Size> supportedPreviewDimens = parameters.getSupportedPreviewSizes();

                Collections.sort(supportedJpegDimens, new CameraUtils.LegacySizeComparator());
                Collections.sort(supportedPreviewDimens, new CameraUtils.LegacySizeComparator());

                Camera.Size largestJpegDimen =
                        supportedJpegDimens.get(supportedJpegDimens.size() - 1);
                Camera.Size largestPreviewDimen =
                        supportedPreviewDimens.get(supportedPreviewDimens.size() - 1);

                float jpegAspect = largestJpegDimen.width / (float) largestJpegDimen.height;
                float previewAspect =
                        largestPreviewDimen.width / (float) largestPreviewDimen.height;

                if (Math.abs(jpegAspect - previewAspect) >= ASPECT_RATIO_TOLERANCE) {
                    Log.w(TAG,
                            ""Largest preview dimension (w="" + largestPreviewDimen.width + "", h="" +
                            largestPreviewDimen.height + "") should have the same aspect ratio "" +
                            ""as the largest Jpeg dimension (w="" + largestJpegDimen.width +
                            "", h="" + largestJpegDimen.height + "")"");
                }


                camera.release();
            }
        }
    }

    private void checkSize(Parameters parameters, int width, int height) {
        parameters.setPictureSize(width, height);
        assertEquals(width, parameters.getPictureSize().width);
        assertEquals(height, parameters.getPictureSize().height);
    }

    private static void addTestToSuite(TestSuite testSuite, String testName) {
        Camera_SizeTest test = new Camera_SizeTest();
        test.setName(testName);
        testSuite.addTest(test);
    }
}"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.CameraTestUtils"	"ImageDropperListener"	""	"/home/gpoor/cts-12-source/cts/tests/camera/utils/src/android/hardware/camera2/cts/CameraTestUtils.java"	""	"public void test/*
 *.
 */

package android.hardware.camera2.cts;

import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.graphics.ImageFormat;
import android.graphics.PointF;
import android.graphics.Rect;
import android.graphics.SurfaceTexture;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraConstrainedHighSpeedCaptureSession;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CameraMetadata;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CaptureFailure;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.MultiResolutionImageReader;
import android.hardware.camera2.cts.helpers.CameraErrorCollector;
import android.hardware.camera2.cts.helpers.StaticMetadata;
import android.hardware.camera2.params.InputConfiguration;
import android.hardware.camera2.TotalCaptureResult;
import android.hardware.cts.helpers.CameraUtils;
import android.hardware.camera2.params.MeteringRectangle;
import android.hardware.camera2.params.MandatoryStreamCombination;
import android.hardware.camera2.params.MandatoryStreamCombination.MandatoryStreamInformation;
import android.hardware.camera2.params.MultiResolutionStreamConfigurationMap;
import android.hardware.camera2.params.MultiResolutionStreamInfo;
import android.hardware.camera2.params.OutputConfiguration;
import android.hardware.camera2.params.SessionConfiguration;
import android.hardware.camera2.params.StreamConfigurationMap;
import android.location.Location;
import android.location.LocationManager;
import android.media.ExifInterface;
import android.media.Image;
import android.media.ImageReader;
import android.media.ImageWriter;
import android.media.Image.Plane;
import android.os.Build;
import android.os.ConditionVariable;
import android.os.Handler;
import android.util.Log;
import android.util.Pair;
import android.util.Size;
import android.util.Range;
import android.view.Display;
import android.view.Surface;
import android.view.WindowManager;

import com.android.ex.camera2.blocking.BlockingCameraManager;
import com.android.ex.camera2.blocking.BlockingCameraManager.BlockingOpenException;
import com.android.ex.camera2.blocking.BlockingSessionCallback;
import com.android.ex.camera2.blocking.BlockingStateCallback;
import com.android.ex.camera2.exceptions.TimeoutRuntimeException;

import junit.framework.Assert;

import org.mockito.Mockito;

import java.io.FileOutputStream;
import java.io.IOException;
import java.lang.reflect.Array;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.Comparator;
import java.util.Date;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.Executor;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.Semaphore;
import java.util.concurrent.TimeUnit;
import java.text.ParseException;
import java.text.SimpleDateFormat;

/**
 * A package private utility class for wrapping up the camera2 cts test common utility functions
 */
public class CameraTestUtils extends Assert {
    private static final String TAG = ""CameraTestUtils"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);
    private static final boolean DEBUG = Log.isLoggable(TAG, Log.DEBUG);
    public static final Size SIZE_BOUND_720P = new Size(1280, 720);
    public static final Size SIZE_BOUND_1080P = new Size(1920, 1088);
    public static final Size SIZE_BOUND_2K = new Size(2048, 1088);
    public static final Size SIZE_BOUND_QHD = new Size(2560, 1440);
    public static final Size SIZE_BOUND_2160P = new Size(3840, 2160);
    // Only test the preview size that is no larger than 1080p.
    public static final Size PREVIEW_SIZE_BOUND = SIZE_BOUND_1080P;
    // Default timeouts for reaching various states
    public static final int CAMERA_OPEN_TIMEOUT_MS = 3000;
    public static final int CAMERA_CLOSE_TIMEOUT_MS = 3000;
    public static final int CAMERA_IDLE_TIMEOUT_MS = 3000;
    public static final int CAMERA_ACTIVE_TIMEOUT_MS = 1000;
    public static final int CAMERA_BUSY_TIMEOUT_MS = 1000;
    public static final int CAMERA_UNCONFIGURED_TIMEOUT_MS = 1000;
    public static final int CAMERA_CONFIGURE_TIMEOUT_MS = 3000;
    public static final int CAPTURE_RESULT_TIMEOUT_MS = 3000;
    public static final int CAPTURE_IMAGE_TIMEOUT_MS = 3000;

    public static final int SESSION_CONFIGURE_TIMEOUT_MS = 3000;
    public static final int SESSION_CLOSE_TIMEOUT_MS = 3000;
    public static final int SESSION_READY_TIMEOUT_MS = 5000;
    public static final int SESSION_ACTIVE_TIMEOUT_MS = 1000;

    public static final int MAX_READER_IMAGES = 5;

    // Compensate for the loss of ""sensitivity"" and ""sensitivityBoost""
    public static final int MAX_ISO_MISMATCH = 3;

    public static final String OFFLINE_CAMERA_ID = ""offline_camera_id"";
    public static final String REPORT_LOG_NAME = ""CtsCameraTestCases"";

    private static final int EXIF_DATETIME_LENGTH = 19;
    private static final int EXIF_DATETIME_ERROR_MARGIN_SEC = 60;
    private static final float EXIF_FOCAL_LENGTH_ERROR_MARGIN = 0.001f;
    private static final float EXIF_EXPOSURE_TIME_ERROR_MARGIN_RATIO = 0.05f;
    private static final float EXIF_EXPOSURE_TIME_MIN_ERROR_MARGIN_SEC = 0.002f;
    private static final float EXIF_APERTURE_ERROR_MARGIN = 0.001f;

    private static final float ZOOM_RATIO_THRESHOLD = 0.01f;

    private static final Location sTestLocation0 = new Location(LocationManager.GPS_PROVIDER);
    private static final Location sTestLocation1 = new Location(LocationManager.GPS_PROVIDER);
    private static final Location sTestLocation2 = new Location(LocationManager.NETWORK_PROVIDER);

    static {
        sTestLocation0.setTime(1199145600000L);
        sTestLocation0.setLatitude(37.736071);
        sTestLocation0.setLongitude(-122.441983);
        sTestLocation0.setAltitude(21.0);

        sTestLocation1.setTime(1199145601000L);
        sTestLocation1.setLatitude(0.736071);
        sTestLocation1.setLongitude(0.441983);
        sTestLocation1.setAltitude(1.0);

        sTestLocation2.setTime(1199145602000L);
        sTestLocation2.setLatitude(-89.736071);
        sTestLocation2.setLongitude(-179.441983);
        sTestLocation2.setAltitude(100000.0);
    }

    // Exif test data vectors.
    public static final ExifTestData[] EXIF_TEST_DATA = {
            new ExifTestData(
                    /*gpsLocation*/ sTestLocation0,
                    /* orientation */90,
                    /* jpgQuality */(byte) 80,
                    /* thumbQuality */(byte) 75),
            new ExifTestData(
                    /*gpsLocation*/ sTestLocation1,
                    /* orientation */180,
                    /* jpgQuality */(byte) 90,
                    /* thumbQuality */(byte) 85),
            new ExifTestData(
                    /*gpsLocation*/ sTestLocation2,
                    /* orientation */270,
                    /* jpgQuality */(byte) 100,
                    /* thumbQuality */(byte) 100)
    };

    /**
     * Create an {@link android.media.ImageReader} object and get the surface.
     *
     * @param size The size of this ImageReader to be created.
     * @param format The format of this ImageReader to be created
     * @param maxNumImages The max number of images that can be acquired simultaneously.
     * @param listener The listener used by this ImageReader to notify callbacks.
     * @param handler The handler to use for any listener callbacks.
     */
    public static ImageReader makeImageReader(Size size, int format, int maxNumImages,
            ImageReader.OnImageAvailableListener listener, Handler handler) {
        ImageReader reader;
        reader = ImageReader.newInstance(size.getWidth(), size.getHeight(), format,
                maxNumImages);
        reader.setOnImageAvailableListener(listener, handler);
        if (VERBOSE) Log.v(TAG, ""Created ImageReader size "" + size);
        return reader;
    }

    /**
     * Create an ImageWriter and hook up the ImageListener.
     *
     * @param inputSurface The input surface of the ImageWriter.
     * @param maxImages The max number of Images that can be dequeued simultaneously.
     * @param listener The listener used by this ImageWriter to notify callbacks
     * @param handler The handler to post listener callbacks.
     * @return ImageWriter object created.
     */
    public static ImageWriter makeImageWriter(
            Surface inputSurface, int maxImages,
            ImageWriter.OnImageReleasedListener listener, Handler handler) {
        ImageWriter writer = ImageWriter.newInstance(inputSurface, maxImages);
        writer.setOnImageReleasedListener(listener, handler);
        return writer;
    }

    /**
     * Utility class to store the targets for mandatory stream combination test.
     */
    public static class StreamCombinationTargets {
        public List<SurfaceTexture> mPrivTargets = new ArrayList<>();
        public List<ImageReader> mJpegTargets = new ArrayList<>();
        public List<ImageReader> mYuvTargets = new ArrayList<>();
        public List<ImageReader> mY8Targets = new ArrayList<>();
        public List<ImageReader> mRawTargets = new ArrayList<>();
        public List<ImageReader> mHeicTargets = new ArrayList<>();
        public List<ImageReader> mDepth16Targets = new ArrayList<>();

        public List<MultiResolutionImageReader> mPrivMultiResTargets = new ArrayList<>();
        public List<MultiResolutionImageReader> mJpegMultiResTargets = new ArrayList<>();
        public List<MultiResolutionImageReader> mYuvMultiResTargets = new ArrayList<>();
        public List<MultiResolutionImageReader> mRawMultiResTargets = new ArrayList<>();

        public void close() {
            for (SurfaceTexture target : mPrivTargets) {
                target.release();
            }
            for (ImageReader target : mJpegTargets) {
                target.close();
            }
            for (ImageReader target : mYuvTargets) {
                target.close();
            }
            for (ImageReader target : mY8Targets) {
                target.close();
            }
            for (ImageReader target : mRawTargets) {
                target.close();
            }
            for (ImageReader target : mHeicTargets) {
                target.close();
            }
            for (ImageReader target : mDepth16Targets) {
                target.close();
            }

            for (MultiResolutionImageReader target : mPrivMultiResTargets) {
                target.close();
            }
            for (MultiResolutionImageReader target : mJpegMultiResTargets) {
                target.close();
            }
            for (MultiResolutionImageReader target : mYuvMultiResTargets) {
                target.close();
            }
            for (MultiResolutionImageReader target : mRawMultiResTargets) {
                target.close();
            }
        }
    }

    private static void configureTarget(StreamCombinationTargets targets,
            List<OutputConfiguration> outputConfigs, List<Surface> outputSurfaces,
            int format, Size targetSize, int numBuffers, String overridePhysicalCameraId,
            MultiResolutionStreamConfigurationMap multiResStreamConfig,
            boolean createMultiResiStreamConfig, ImageDropperListener listener, Handler handler) {
        if (createMultiResiStreamConfig) {
            Collection<MultiResolutionStreamInfo> multiResolutionStreams =
                    multiResStreamConfig.getOutputInfo(format);
            MultiResolutionImageReader multiResReader = new MultiResolutionImageReader(
                    multiResolutionStreams, format, numBuffers);
            multiResReader.setOnImageAvailableListener(listener, new HandlerExecutor(handler));
            Collection<OutputConfiguration> configs =
                    OutputConfiguration.createInstancesForMultiResolutionOutput(multiResReader);
            outputConfigs.addAll(configs);
            outputSurfaces.add(multiResReader.getSurface());
            switch (format) {
                case ImageFormat.PRIVATE:
                    targets.mPrivMultiResTargets.add(multiResReader);
                    break;
                case ImageFormat.JPEG:
                    targets.mJpegMultiResTargets.add(multiResReader);
                    break;
                case ImageFormat.YUV_420_888:
                    targets.mYuvMultiResTargets.add(multiResReader);
                    break;
                case ImageFormat.RAW_SENSOR:
                    targets.mRawMultiResTargets.add(multiResReader);
                    break;
                default:
                    fail(""Unknown/Unsupported output format "" + format);
            }
        } else {
            if (format == ImageFormat.PRIVATE) {
                SurfaceTexture target = new SurfaceTexture(/*random int*/1);
                target.setDefaultBufferSize(targetSize.getWidth(), targetSize.getHeight());
                OutputConfiguration config = new OutputConfiguration(new Surface(target));
                if (overridePhysicalCameraId != null) {
                    config.setPhysicalCameraId(overridePhysicalCameraId);
                }
                outputConfigs.add(config);
                outputSurfaces.add(config.getSurface());
                targets.mPrivTargets.add(target);
            } else {
                ImageReader target = ImageReader.newInstance(targetSize.getWidth(),
                        targetSize.getHeight(), format, numBuffers);
                target.setOnImageAvailableListener(listener, handler);
                OutputConfiguration config = new OutputConfiguration(target.getSurface());
                if (overridePhysicalCameraId != null) {
                    config.setPhysicalCameraId(overridePhysicalCameraId);
                }
                outputConfigs.add(config);
                outputSurfaces.add(config.getSurface());

                switch (format) {
                    case ImageFormat.JPEG:
                      targets.mJpegTargets.add(target);
                      break;
                    case ImageFormat.YUV_420_888:
                      targets.mYuvTargets.add(target);
                      break;
                    case ImageFormat.Y8:
                      targets.mY8Targets.add(target);
                      break;
                    case ImageFormat.RAW_SENSOR:
                      targets.mRawTargets.add(target);
                      break;
                    case ImageFormat.HEIC:
                      targets.mHeicTargets.add(target);
                      break;
                    case ImageFormat.DEPTH16:
                      targets.mDepth16Targets.add(target);
                      break;
                    default:
                      fail(""Unknown/Unsupported output format "" + format);
                }
            }
        }
    }

    public static void setupConfigurationTargets(List<MandatoryStreamInformation> streamsInfo,
            StreamCombinationTargets targets,
            List<OutputConfiguration> outputConfigs,
            List<Surface> outputSurfaces, int numBuffers,
            boolean substituteY8, boolean substituteHeic, String overridenPhysicalCameraId,
            MultiResolutionStreamConfigurationMap multiResStreamConfig, Handler handler) {
            List<Surface> uhSurfaces = new ArrayList<Surface>();
        setupConfigurationTargets(streamsInfo, targets, outputConfigs, outputSurfaces, uhSurfaces,
            numBuffers, substituteY8, substituteHeic, overridenPhysicalCameraId,
            multiResStreamConfig, handler);
    }

    public static void setupConfigurationTargets(List<MandatoryStreamInformation> streamsInfo,
            StreamCombinationTargets targets,
            List<OutputConfiguration> outputConfigs,
            List<Surface> outputSurfaces, List<Surface> uhSurfaces, int numBuffers,
            boolean substituteY8, boolean substituteHeic, String overridePhysicalCameraId,
            MultiResolutionStreamConfigurationMap multiResStreamConfig, Handler handler) {

        ImageDropperListener imageDropperListener = new ImageDropperListener();
        List<Surface> chosenSurfaces;
        for (MandatoryStreamInformation streamInfo : streamsInfo) {
            if (streamInfo.isInput()) {
                continue;
            }
            chosenSurfaces = outputSurfaces;
            if (streamInfo.isUltraHighResolution()) {
                chosenSurfaces = uhSurfaces;
            }
            int format = streamInfo.getFormat();
            if (substituteY8 && (format == ImageFormat.YUV_420_888)) {
                format = ImageFormat.Y8;
            } else if (substituteHeic && (format == ImageFormat.JPEG)) {
                format = ImageFormat.HEIC;
            }
            Size[] availableSizes = new Size[streamInfo.getAvailableSizes().size()];
            availableSizes = streamInfo.getAvailableSizes().toArray(availableSizes);
            Size targetSize = CameraTestUtils.getMaxSize(availableSizes);
            boolean createMultiResReader =
                    (multiResStreamConfig != null &&
                     !multiResStreamConfig.getOutputInfo(format).isEmpty() &&
                     streamInfo.isMaximumSize());
            switch (format) {
                case ImageFormat.PRIVATE:
                case ImageFormat.JPEG:
                case ImageFormat.YUV_420_888:
                case ImageFormat.Y8:
                case ImageFormat.HEIC:
                case ImageFormat.DEPTH16:
                {
                    configureTarget(targets, outputConfigs, chosenSurfaces, format,
                            targetSize, numBuffers, overridePhysicalCameraId, multiResStreamConfig,
                            createMultiResReader, imageDropperListener, handler);
                    break;
                }
                case ImageFormat.RAW_SENSOR: {
                    // targetSize could be null in the logical camera case where only
                    // physical camera supports RAW stream.
                    if (targetSize != null) {
                        configureTarget(targets, outputConfigs, chosenSurfaces, format,
                                targetSize, numBuffers, overridePhysicalCameraId,
                                multiResStreamConfig, createMultiResReader, imageDropperListener,
                                handler);
                    }
                    break;
                }
                default:
                    fail(""Unknown output format "" + format);
            }
        }
    }

    /**
     * Close pending images and clean up an {@link android.media.ImageReader} object.
     * @param reader an {@link android.media.ImageReader} to close.
     */
    public static void closeImageReader(ImageReader reader) {
        if (reader != null) {
            reader.close();
        }
    }

    /**
     * Close the pending images then close current active {@link ImageReader} objects.
     */
    public static void closeImageReaders(ImageReader[] readers) {
        if ((readers != null) && (readers.length > 0)) {
            for (ImageReader reader : readers) {
                CameraTestUtils.closeImageReader(reader);
            }
        }
    }

    /**
     * Close pending images and clean up an {@link android.media.ImageWriter} object.
     * @param writer an {@link android.media.ImageWriter} to close.
     */
    public static void closeImageWriter(ImageWriter writer) {
        if (writer != null) {
            writer.close();
        }
    }

    /**
     * Dummy listener that release the image immediately once it is available.
     *
     * <p>
     * It can be used for the case where we don't care the image data at all.
     * </p>
     */
    public static class ImageDropperListener implements ImageReader.OnImageAvailableListener {
        @Override
        public synchronized void onImageAvailable(ImageReader reader) {
            Image image = null;
            try {
                image = reader.acquireNextImage();
            } finally {
                if (image != null) {
                    image.close();
                    mImagesDropped++;
                }
            }
        }

        public synchronized int getImageCount() {
            return mImagesDropped;
        }

        public synchronized void resetImageCount() {
            mImagesDropped = 0;
        }

        private int mImagesDropped = 0;
    }

    /**
     * Image listener that release the image immediately after validating the image
     */
    public static class ImageVerifierListener implements ImageReader.OnImageAvailableListener {
        private Size mSize;
        private int mFormat;
        // Whether the parent ImageReader is valid or not. If the parent ImageReader
        // is destroyed, the acquired Image may become invalid.
        private boolean mReaderIsValid;

        public ImageVerifierListener(Size sz, int format) {
            mSize = sz;
            mFormat = format;
            mReaderIsValid = true;
        }

        public synchronized void onReaderDestroyed() {
            mReaderIsValid = false;
        }

        @Override
        public synchronized void onImageAvailable(ImageReader reader) {
            Image image = null;
            try {
                image = reader.acquireNextImage();
            } finally {
                if (image != null) {
                    // Should only do some quick validity checks in callback, as the ImageReader
                    // could be closed asynchronously, which will close all images acquired from
                    // this ImageReader.
                    checkImage(image, mSize.getWidth(), mSize.getHeight(), mFormat);
                    // checkAndroidImageFormat calls into underlying Image object, which could
                    // become invalid if the ImageReader is destroyed.
                    if (mReaderIsValid) {
                        checkAndroidImageFormat(image);
                    }
                    image.close();
                }
            }
        }
    }

    public static class SimpleImageReaderListener
            implements ImageReader.OnImageAvailableListener {
        private final LinkedBlockingQueue<Image> mQueue =
                new LinkedBlockingQueue<Image>();
        // Indicate whether this listener will drop images or not,
        // when the queued images reaches the reader maxImages
        private final boolean mAsyncMode;
        // maxImages held by the queue in async mode.
        private final int mMaxImages;

        /**
         * Create a synchronous SimpleImageReaderListener that queues the images
         * automatically when they are available, no image will be dropped. If
         * the caller doesn't call getImage(), the producer will eventually run
         * into buffer starvation.
         */
        public SimpleImageReaderListener() {
            mAsyncMode = false;
            mMaxImages = 0;
        }

        /**
         * Create a synchronous/asynchronous SimpleImageReaderListener that
         * queues the images automatically when they are available. For
         * asynchronous listener, image will be dropped if the queued images
         * reach to maxImages queued. If the caller doesn't call getImage(), the
         * producer will not be blocked. For synchronous listener, no image will
         * be dropped. If the caller doesn't call getImage(), the producer will
         * eventually run into buffer starvation.
         *
         * @param asyncMode If the listener is operating at asynchronous mode.
         * @param maxImages The max number of images held by this listener.
         */
        /**
         *
         * @param asyncMode
         */
        public SimpleImageReaderListener(boolean asyncMode, int maxImages) {
            mAsyncMode = asyncMode;
            mMaxImages = maxImages;
        }

        @Override
        public void onImageAvailable(ImageReader reader) {
            try {
                Image imge = reader.acquireNextImage();
                if (imge == null) {
                    return;
                }
                mQueue.put(imge);
                if (mAsyncMode && mQueue.size() >= mMaxImages) {
                    Image img = mQueue.poll();
                    img.close();
                }
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onImageAvailable"");
            }
        }

        /**
         * Get an image from the image reader.
         *
         * @param timeout Timeout value for the wait.
         * @return The image from the image reader.
         */
        public Image getImage(long timeout) throws InterruptedException {
            Image image = mQueue.poll(timeout, TimeUnit.MILLISECONDS);
            assertNotNull(""Wait for an image timed out in "" + timeout + ""ms"", image);
            return image;
        }

        /**
         * Drain the pending images held by this listener currently.
         *
         */
        public void drain() {
            while (!mQueue.isEmpty()) {
                Image image = mQueue.poll();
                assertNotNull(""Unable to get an image"", image);
                image.close();
            }
        }
    }

    public static class SimpleImageWriterListener implements ImageWriter.OnImageReleasedListener {
        private final Semaphore mImageReleasedSema = new Semaphore(0);
        private final ImageWriter mWriter;
        @Override
        public void onImageReleased(ImageWriter writer) {
            if (writer != mWriter) {
                return;
            }

            if (VERBOSE) {
                Log.v(TAG, ""Input image is released"");
            }
            mImageReleasedSema.release();
        }

        public SimpleImageWriterListener(ImageWriter writer) {
            if (writer == null) {
                throw new IllegalArgumentException(""writer cannot be null"");
            }
            mWriter = writer;
        }

        public void waitForImageReleased(long timeoutMs) throws InterruptedException {
            if (!mImageReleasedSema.tryAcquire(timeoutMs, TimeUnit.MILLISECONDS)) {
                fail(""wait for image available timed out after "" + timeoutMs + ""ms"");
            }
        }
    }

    public static class ImageAndMultiResStreamInfo {
        public final Image image;
        public final MultiResolutionStreamInfo streamInfo;

        public ImageAndMultiResStreamInfo(Image image, MultiResolutionStreamInfo streamInfo) {
            this.image = image;
            this.streamInfo = streamInfo;
        }
    }

    public static class SimpleMultiResolutionImageReaderListener
            implements ImageReader.OnImageAvailableListener {
        public SimpleMultiResolutionImageReaderListener(MultiResolutionImageReader owner,
                int maxBuffers, boolean acquireLatest) {
            mOwner = owner;
            mMaxBuffers = maxBuffers;
            mAcquireLatest = acquireLatest;
        }

        @Override
        public void onImageAvailable(ImageReader reader) {
            if (VERBOSE) Log.v(TAG, ""new image available"");

            if (mAcquireLatest) {
                mLastReader = reader;
                mImageAvailable.open();
            } else {
                if (mQueue.size() < mMaxBuffers) {
                    Image image = reader.acquireNextImage();
                    MultiResolutionStreamInfo multiResStreamInfo =
                            mOwner.getStreamInfoForImageReader(reader);
                    mQueue.offer(new ImageAndMultiResStreamInfo(image, multiResStreamInfo));
                }
            }
        }

        public ImageAndMultiResStreamInfo getAnyImageAndInfoAvailable(long timeoutMs)
                throws Exception {
            if (mAcquireLatest) {
                Image image = null;
                if (mImageAvailable.block(timeoutMs)) {
                    if (mLastReader != null) {
                        image = mLastReader.acquireLatestImage();
                        if (VERBOSE) Log.v(TAG, ""acquireLatestImage"");
                    } else {
                        fail(""invalid image reader"");
                    }
                    mImageAvailable.close();
                } else {
                    fail(""wait for image available time out after "" + timeoutMs + ""ms"");
                }
                return new ImageAndMultiResStreamInfo(image,
                        mOwner.getStreamInfoForImageReader(mLastReader));
            } else {
                ImageAndMultiResStreamInfo imageAndInfo = mQueue.poll(timeoutMs,
                        java.util.concurrent.TimeUnit.MILLISECONDS);
                if (imageAndInfo == null) {
                    fail(""wait for image available timed out after "" + timeoutMs + ""ms"");
                }
                return imageAndInfo;
            }
        }

        public void reset() {
            while (!mQueue.isEmpty()) {
                ImageAndMultiResStreamInfo imageAndInfo = mQueue.poll();
                assertNotNull(""Acquired image is not valid"", imageAndInfo.image);
                imageAndInfo.image.close();
            }
            mImageAvailable.close();
            mLastReader = null;
        }

        private LinkedBlockingQueue<ImageAndMultiResStreamInfo> mQueue =
                new LinkedBlockingQueue<ImageAndMultiResStreamInfo>();
        private final MultiResolutionImageReader mOwner;
        private final int mMaxBuffers;
        private final boolean mAcquireLatest;
        private ConditionVariable mImageAvailable = new ConditionVariable();
        private ImageReader mLastReader = null;
    }

    public static class SimpleCaptureCallback extends CameraCaptureSession.CaptureCallback {
        private final LinkedBlockingQueue<TotalCaptureResult> mQueue =
                new LinkedBlockingQueue<TotalCaptureResult>();
        private final LinkedBlockingQueue<CaptureFailure> mFailureQueue =
                new LinkedBlockingQueue<>();
        // (Surface, framenumber) pair for lost buffers
        private final LinkedBlockingQueue<Pair<Surface, Long>> mBufferLostQueue =
                new LinkedBlockingQueue<>();
        private final LinkedBlockingQueue<Integer> mAbortQueue =
                new LinkedBlockingQueue<>();
        // Pair<CaptureRequest, Long> is a pair of capture request and timestamp.
        private final LinkedBlockingQueue<Pair<CaptureRequest, Long>> mCaptureStartQueue =
                new LinkedBlockingQueue<>();
        // Pair<Int, Long> is a pair of sequence id and frame number
        private final LinkedBlockingQueue<Pair<Integer, Long>> mCaptureSequenceCompletedQueue =
                new LinkedBlockingQueue<>();

        private AtomicLong mNumFramesArrived = new AtomicLong(0);

        @Override
        public void onCaptureStarted(CameraCaptureSession session, CaptureRequest request,
                long timestamp, long frameNumber) {
            try {
                mCaptureStartQueue.put(new Pair(request, timestamp));
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureStarted"");
            }
        }

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                TotalCaptureResult result) {
            try {
                mNumFramesArrived.incrementAndGet();
                mQueue.put(result);
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureCompleted"");
            }
        }

        @Override
        public void onCaptureFailed(CameraCaptureSession session, CaptureRequest request,
                CaptureFailure failure) {
            try {
                mFailureQueue.put(failure);
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureFailed"");
            }
        }

        @Override
        public void onCaptureSequenceAborted(CameraCaptureSession session, int sequenceId) {
            try {
                mAbortQueue.put(sequenceId);
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureAborted"");
            }
        }

        @Override
        public void onCaptureSequenceCompleted(CameraCaptureSession session, int sequenceId,
                long frameNumber) {
            try {
                mCaptureSequenceCompletedQueue.put(new Pair(sequenceId, frameNumber));
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureSequenceCompleted"");
            }
        }

        @Override
        public void onCaptureBufferLost(CameraCaptureSession session,
                CaptureRequest request, Surface target, long frameNumber) {
            try {
                mBufferLostQueue.put(new Pair<>(target, frameNumber));
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureBufferLost"");
            }
        }

        public long getTotalNumFrames() {
            return mNumFramesArrived.get();
        }

        public CaptureResult getCaptureResult(long timeout) {
            return getTotalCaptureResult(timeout);
        }

        public TotalCaptureResult getCaptureResult(long timeout, long timestamp) {
            try {
                long currentTs = -1L;
                TotalCaptureResult result;
                while (true) {
                    result = mQueue.poll(timeout, TimeUnit.MILLISECONDS);
                    if (result == null) {
                        throw new RuntimeException(
                                ""Wait for a capture result timed out in "" + timeout + ""ms"");
                    }
                    currentTs = result.get(CaptureResult.SENSOR_TIMESTAMP);
                    if (currentTs == timestamp) {
                        return result;
                    }
                }

            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }
        }

        public TotalCaptureResult getTotalCaptureResult(long timeout) {
            try {
                TotalCaptureResult result = mQueue.poll(timeout, TimeUnit.MILLISECONDS);
                assertNotNull(""Wait for a capture result timed out in "" + timeout + ""ms"", result);
                return result;
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }
        }

        /**
         * Get the {@link #CaptureResult capture result} for a given
         * {@link #CaptureRequest capture request}.
         *
         * @param myRequest The {@link #CaptureRequest capture request} whose
         *            corresponding {@link #CaptureResult capture result} was
         *            being waited for
         * @param numResultsWait Number of frames to wait for the capture result
         *            before timeout.
         * @throws TimeoutRuntimeException If more than numResultsWait results are
         *            seen before the result matching myRequest arrives, or each
         *            individual wait for result times out after
         *            {@value #CAPTURE_RESULT_TIMEOUT_MS}ms.
         */
        public CaptureResult getCaptureResultForRequest(CaptureRequest myRequest,
                int numResultsWait) {
            return getTotalCaptureResultForRequest(myRequest, numResultsWait);
        }

        /**
         * Get the {@link #TotalCaptureResult total capture result} for a given
         * {@link #CaptureRequest capture request}.
         *
         * @param myRequest The {@link #CaptureRequest capture request} whose
         *            corresponding {@link #TotalCaptureResult capture result} was
         *            being waited for
         * @param numResultsWait Number of frames to wait for the capture result
         *            before timeout.
         * @throws TimeoutRuntimeException If more than numResultsWait results are
         *            seen before the result matching myRequest arrives, or each
         *            individual wait for result times out after
         *            {@value #CAPTURE_RESULT_TIMEOUT_MS}ms.
         */
        public TotalCaptureResult getTotalCaptureResultForRequest(CaptureRequest myRequest,
                int numResultsWait) {
            ArrayList<CaptureRequest> captureRequests = new ArrayList<>(1);
            captureRequests.add(myRequest);
            return getTotalCaptureResultsForRequests(captureRequests, numResultsWait)[0];
        }

        /**
         * Get an array of {@link #TotalCaptureResult total capture results} for a given list of
         * {@link #CaptureRequest capture requests}. This can be used when the order of results
         * may not the same as the order of requests.
         *
         * @param captureRequests The list of {@link #CaptureRequest capture requests} whose
         *            corresponding {@link #TotalCaptureResult capture results} are
         *            being waited for.
         * @param numResultsWait Number of frames to wait for the capture results
         *            before timeout.
         * @throws TimeoutRuntimeException If more than numResultsWait results are
         *            seen before all the results matching captureRequests arrives.
         */
        public TotalCaptureResult[] getTotalCaptureResultsForRequests(
                List<CaptureRequest> captureRequests, int numResultsWait) {
            if (numResultsWait < 0) {
                throw new IllegalArgumentException(""numResultsWait must be no less than 0"");
            }
            if (captureRequests == null || captureRequests.size() == 0) {
                throw new IllegalArgumentException(""captureRequests must have at least 1 request."");
            }

            // Create a request -> a list of result indices map that it will wait for.
            HashMap<CaptureRequest, ArrayList<Integer>> remainingResultIndicesMap = new HashMap<>();
            for (int i = 0; i < captureRequests.size(); i++) {
                CaptureRequest request = captureRequests.get(i);
                ArrayList<Integer> indices = remainingResultIndicesMap.get(request);
                if (indices == null) {
                    indices = new ArrayList<>();
                    remainingResultIndicesMap.put(request, indices);
                }
                indices.add(i);
            }

            TotalCaptureResult[] results = new TotalCaptureResult[captureRequests.size()];
            int i = 0;
            do {
                TotalCaptureResult result = getTotalCaptureResult(CAPTURE_RESULT_TIMEOUT_MS);
                CaptureRequest request = result.getRequest();
                ArrayList<Integer> indices = remainingResultIndicesMap.get(request);
                if (indices != null) {
                    results[indices.get(0)] = result;
                    indices.remove(0);

                    // Remove the entry if all results for this request has been fulfilled.
                    if (indices.isEmpty()) {
                        remainingResultIndicesMap.remove(request);
                    }
                }

                if (remainingResultIndicesMap.isEmpty()) {
                    return results;
                }
            } while (i++ < numResultsWait);

            throw new TimeoutRuntimeException(""Unable to get the expected capture result after ""
                    + ""waiting for "" + numResultsWait + "" results"");
        }

        /**
         * Get an array list of {@link #CaptureFailure capture failure} with maxNumFailures entries
         * at most. If it times out before maxNumFailures failures are received, return the failures
         * received so far.
         *
         * @param maxNumFailures The maximal number of failures to return. If it times out before
         *                       the maximal number of failures are received, return the received
         *                       failures so far.
         * @throws UnsupportedOperationException If an error happens while waiting on the failure.
         */
        public ArrayList<CaptureFailure> getCaptureFailures(long maxNumFailures) {
            ArrayList<CaptureFailure> failures = new ArrayList<>();
            try {
                for (int i = 0; i < maxNumFailures; i++) {
                    CaptureFailure failure = mFailureQueue.poll(CAPTURE_RESULT_TIMEOUT_MS,
                            TimeUnit.MILLISECONDS);
                    if (failure == null) {
                        // If waiting on a failure times out, return the failures so far.
                        break;
                    }
                    failures.add(failure);
                }
            }  catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }

            return failures;
        }

        /**
         * Get an array list of lost buffers with maxNumLost entries at most.
         * If it times out before maxNumLost buffer lost callbacks are received, return the
         * lost callbacks received so far.
         *
         * @param maxNumLost The maximal number of buffer lost failures to return. If it times out
         *                   before the maximal number of failures are received, return the received
         *                   buffer lost failures so far.
         * @throws UnsupportedOperationException If an error happens while waiting on the failure.
         */
        public ArrayList<Pair<Surface, Long>> getLostBuffers(long maxNumLost) {
            ArrayList<Pair<Surface, Long>> failures = new ArrayList<>();
            try {
                for (int i = 0; i < maxNumLost; i++) {
                    Pair<Surface, Long> failure = mBufferLostQueue.poll(CAPTURE_RESULT_TIMEOUT_MS,
                            TimeUnit.MILLISECONDS);
                    if (failure == null) {
                        // If waiting on a failure times out, return the failures so far.
                        break;
                    }
                    failures.add(failure);
                }
            }  catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }

            return failures;
        }

        /**
         * Get an array list of aborted capture sequence ids with maxNumAborts entries
         * at most. If it times out before maxNumAborts are received, return the aborted sequences
         * received so far.
         *
         * @param maxNumAborts The maximal number of aborted sequences to return. If it times out
         *                     before the maximal number of aborts are received, return the received
         *                     failed sequences so far.
         * @throws UnsupportedOperationException If an error happens while waiting on the failed
         *                                       sequences.
         */
        public ArrayList<Integer> geAbortedSequences(long maxNumAborts) {
            ArrayList<Integer> abortList = new ArrayList<>();
            try {
                for (int i = 0; i < maxNumAborts; i++) {
                    Integer abortSequence = mAbortQueue.poll(CAPTURE_RESULT_TIMEOUT_MS,
                            TimeUnit.MILLISECONDS);
                    if (abortSequence == null) {
                        break;
                    }
                    abortList.add(abortSequence);
                }
            }  catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }

            return abortList;
        }

        /**
         * Wait until the capture start of a request and expected timestamp arrives or it times
         * out after a number of capture starts.
         *
         * @param request The request for the capture start to wait for.
         * @param timestamp The timestamp for the capture start to wait for.
         * @param numCaptureStartsWait The number of capture start events to wait for before timing
         *                             out.
         */
        public void waitForCaptureStart(CaptureRequest request, Long timestamp,
                int numCaptureStartsWait) throws Exception {
            Pair<CaptureRequest, Long> expectedShutter = new Pair<>(request, timestamp);

            int i = 0;
            do {
                Pair<CaptureRequest, Long> shutter = mCaptureStartQueue.poll(
                        CAPTURE_RESULT_TIMEOUT_MS, TimeUnit.MILLISECONDS);

                if (shutter == null) {
                    throw new TimeoutRuntimeException(""Unable to get any more capture start "" +
                            ""event after waiting for "" + CAPTURE_RESULT_TIMEOUT_MS + "" ms."");
                } else if (expectedShutter.equals(shutter)) {
                    return;
                }

            } while (i++ < numCaptureStartsWait);

            throw new TimeoutRuntimeException(""Unable to get the expected capture start "" +
                    ""event after waiting for "" + numCaptureStartsWait + "" capture starts"");
        }

        /**
         * Wait until it receives capture sequence completed callback for a given squence ID.
         *
         * @param sequenceId The sequence ID of the capture sequence completed callback to wait for.
         * @param timeoutMs Time to wait for each capture sequence complete callback before
         *                  timing out.
         */
        public long getCaptureSequenceLastFrameNumber(int sequenceId, long timeoutMs) {
            try {
                while (true) {
                    Pair<Integer, Long> completedSequence =
                            mCaptureSequenceCompletedQueue.poll(timeoutMs, TimeUnit.MILLISECONDS);
                    assertNotNull(""Wait for a capture sequence completed timed out in "" +
                            timeoutMs + ""ms"", completedSequence);

                    if (completedSequence.first.equals(sequenceId)) {
                        return completedSequence.second.longValue();
                    }
                }
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }
        }

        public boolean hasMoreResults()
        {
            return !mQueue.isEmpty();
        }

        public boolean hasMoreFailures()
        {
            return !mFailureQueue.isEmpty();
        }

        public int getNumLostBuffers()
        {
            return mBufferLostQueue.size();
        }

        public boolean hasMoreAbortedSequences()
        {
            return !mAbortQueue.isEmpty();
        }

        public void drain() {
            mQueue.clear();
            mNumFramesArrived.getAndSet(0);
            mFailureQueue.clear();
            mBufferLostQueue.clear();
            mCaptureStartQueue.clear();
            mAbortQueue.clear();
        }
    }

    public static boolean hasCapability(CameraCharacteristics characteristics, int capability) {
        int [] capabilities =
                characteristics.get(CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES);
        for (int c : capabilities) {
            if (c == capability) {
                return true;
            }
        }
        return false;
    }

    public static boolean isSystemCamera(CameraManager manager, String cameraId)
            throws CameraAccessException {
        CameraCharacteristics characteristics = manager.getCameraCharacteristics(cameraId);
        return hasCapability(characteristics,
                CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_SYSTEM_CAMERA);
    }

    public static String[] getCameraIdListForTesting(CameraManager manager,
            boolean getSystemCameras)
            throws CameraAccessException {
        String [] ids = manager.getCameraIdListNoLazy();
        List<String> idsForTesting = new ArrayList<String>();
        for (String id : ids) {
            boolean isSystemCamera = isSystemCamera(manager, id);
            if (getSystemCameras == isSystemCamera) {
                idsForTesting.add(id);
            }
        }
        return idsForTesting.toArray(new String[idsForTesting.size()]);
    }

    public static Set<Set<String>> getConcurrentCameraIds(CameraManager manager,
            boolean getSystemCameras)
            throws CameraAccessException {
        Set<String> cameraIds = new HashSet<String>(Arrays.asList(getCameraIdListForTesting(manager, getSystemCameras)));
        Set<Set<String>> combinations =  manager.getConcurrentCameraIds();
        Set<Set<String>> correctComb = new HashSet<Set<String>>();
        for (Set<String> comb : combinations) {
            Set<String> filteredIds = new HashSet<String>();
            for (String id : comb) {
                if (cameraIds.contains(id)) {
                    filteredIds.add(id);
                }
            }
            if (filteredIds.isEmpty()) {
                continue;
            }
            correctComb.add(filteredIds);
        }
        return correctComb;
    }

    /**
     * Block until the camera is opened.
     *
     * <p>Don't use this to test #onDisconnected/#onError since this will throw
     * an AssertionError if it fails to open the camera device.</p>
     *
     * @return CameraDevice opened camera device
     *
     * @throws IllegalArgumentException
     *            If the handler is null, or if the handler's looper is current.
     * @throws CameraAccessException
     *            If open fails immediately.
     * @throws BlockingOpenException
     *            If open fails after blocking for some amount of time.
     * @throws TimeoutRuntimeException
     *            If opening times out. Typically unrecoverable.
     */
    public static CameraDevice openCamera(CameraManager manager, String cameraId,
            CameraDevice.StateCallback listener, Handler handler) throws CameraAccessException,
            BlockingOpenException {

        /**
         * Although camera2 API allows 'null' Handler (it will just use the current
         * thread's Looper), this is not what we want for CTS.
         *
         * In CTS the default looper is used only to process events in between test runs,
         * so anything sent there would not be executed inside a test and the test would fail.
         *
         * In this case, BlockingCameraManager#openCamera performs the check for us.
         */
        return (new BlockingCameraManager(manager)).openCamera(cameraId, listener, handler);
    }


    /**
     * Block until the camera is opened.
     *
     * <p>Don't use this to test #onDisconnected/#onError since this will throw
     * an AssertionError if it fails to open the camera device.</p>
     *
     * @throws IllegalArgumentException
     *            If the handler is null, or if the handler's looper is current.
     * @throws CameraAccessException
     *            If open fails immediately.
     * @throws BlockingOpenException
     *            If open fails after blocking for some amount of time.
     * @throws TimeoutRuntimeException
     *            If opening times out. Typically unrecoverable.
     */
    public static CameraDevice openCamera(CameraManager manager, String cameraId, Handler handler)
            throws CameraAccessException,
            BlockingOpenException {
        return openCamera(manager, cameraId, /*listener*/null, handler);
    }

    /**
     * Configure a new camera session with output surfaces and type.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputSurfaces The surface list that used for camera output.
     * @param listener The callback CameraDevice will notify when capture results are available.
     */
    public static CameraCaptureSession configureCameraSession(CameraDevice camera,
            List<Surface> outputSurfaces, boolean isHighSpeed,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);
        if (isHighSpeed) {
            camera.createConstrainedHighSpeedCaptureSession(outputSurfaces,
                    sessionListener, handler);
        } else {
            camera.createCaptureSession(outputSurfaces, sessionListener, handler);
        }
        CameraCaptureSession session =
                sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertFalse(""Camera session should not be a reprocessable session"",
                session.isReprocessable());
        String sessionType = isHighSpeed ? ""High Speed"" : ""Normal"";
        assertTrue(""Capture session type must be "" + sessionType,
                isHighSpeed ==
                CameraConstrainedHighSpeedCaptureSession.class.isAssignableFrom(session.getClass()));

        return session;
    }

    /**
     * Build a new constrained camera session with output surfaces, type and recording session
     * parameters.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputSurfaces The surface list that used for camera output.
     * @param listener The callback CameraDevice will notify when capture results are available.
     * @param initialRequest Initial request settings to use as session parameters.
     */
    public static CameraCaptureSession buildConstrainedCameraSession(CameraDevice camera,
            List<Surface> outputSurfaces, CameraCaptureSession.StateCallback listener,
            Handler handler, CaptureRequest initialRequest) throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);

        List<OutputConfiguration> outConfigurations = new ArrayList<>(outputSurfaces.size());
        for (Surface surface : outputSurfaces) {
            outConfigurations.add(new OutputConfiguration(surface));
        }
        SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_HIGH_SPEED, outConfigurations,
                new HandlerExecutor(handler), sessionListener);
        sessionConfig.setSessionParameters(initialRequest);
        camera.createCaptureSession(sessionConfig);

        CameraCaptureSession session =
                sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertFalse(""Camera session should not be a reprocessable session"",
                session.isReprocessable());
        assertTrue(""Capture session type must be High Speed"",
                CameraConstrainedHighSpeedCaptureSession.class.isAssignableFrom(
                        session.getClass()));

        return session;
    }

    /**
     * Configure a new camera session with output configurations.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputs The OutputConfiguration list that is used for camera output.
     * @param listener The callback CameraDevice will notify when capture results are available.
     */
    public static CameraCaptureSession configureCameraSessionWithConfig(CameraDevice camera,
            List<OutputConfiguration> outputs,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);
        camera.createCaptureSessionByOutputConfigurations(outputs, sessionListener, handler);
        CameraCaptureSession session =
                sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertFalse(""Camera session should not be a reprocessable session"",
                session.isReprocessable());
        return session;
    }

    /**
     * Try configure a new camera session with output configurations.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputs The OutputConfiguration list that is used for camera output.
     * @param initialRequest The session parameters passed in during stream configuration
     * @param listener The callback CameraDevice will notify when capture results are available.
     */
    public static CameraCaptureSession tryConfigureCameraSessionWithConfig(CameraDevice camera,
            List<OutputConfiguration> outputs, CaptureRequest initialRequest,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);
        SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_REGULAR, outputs, new HandlerExecutor(handler),
                sessionListener);
        sessionConfig.setSessionParameters(initialRequest);
        camera.createCaptureSession(sessionConfig);

        Integer[] sessionStates = {BlockingSessionCallback.SESSION_READY,
                                   BlockingSessionCallback.SESSION_CONFIGURE_FAILED};
        int state = sessionListener.getStateWaiter().waitForAnyOfStates(
                Arrays.asList(sessionStates), SESSION_CONFIGURE_TIMEOUT_MS);

        CameraCaptureSession session = null;
        if (state == BlockingSessionCallback.SESSION_READY) {
            session = sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
            assertFalse(""Camera session should not be a reprocessable session"",
                    session.isReprocessable());
        }
        return session;
    }

    /**
     * Configure a new camera session with output surfaces and initial session parameters.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputSurfaces The surface list that used for camera output.
     * @param listener The callback CameraDevice will notify when session is available.
     * @param handler The handler used to notify callbacks.
     * @param initialRequest Initial request settings to use as session parameters.
     */
    public static CameraCaptureSession configureCameraSessionWithParameters(CameraDevice camera,
            List<Surface> outputSurfaces, BlockingSessionCallback listener,
            Handler handler, CaptureRequest initialRequest) throws CameraAccessException {
        List<OutputConfiguration> outConfigurations = new ArrayList<>(outputSurfaces.size());
        for (Surface surface : outputSurfaces) {
            outConfigurations.add(new OutputConfiguration(surface));
        }
        SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_REGULAR, outConfigurations,
                new HandlerExecutor(handler), listener);
        sessionConfig.setSessionParameters(initialRequest);
        camera.createCaptureSession(sessionConfig);

        CameraCaptureSession session = listener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertFalse(""Camera session should not be a reprocessable session"",
                session.isReprocessable());
        assertFalse(""Capture session type must be regular"",
                CameraConstrainedHighSpeedCaptureSession.class.isAssignableFrom(
                        session.getClass()));

        return session;
    }

    /**
     * Configure a new camera session with output surfaces.
     *
     * @param camera The CameraDevice to be configured.
     * @param outputSurfaces The surface list that used for camera output.
     * @param listener The callback CameraDevice will notify when capture results are available.
     */
    public static CameraCaptureSession configureCameraSession(CameraDevice camera,
            List<Surface> outputSurfaces,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {

        return configureCameraSession(camera, outputSurfaces, /*isHighSpeed*/false,
                listener, handler);
    }

    public static CameraCaptureSession configureReprocessableCameraSession(CameraDevice camera,
            InputConfiguration inputConfiguration, List<Surface> outputSurfaces,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {
        List<OutputConfiguration> outputConfigs = new ArrayList<OutputConfiguration>();
        for (Surface surface : outputSurfaces) {
            outputConfigs.add(new OutputConfiguration(surface));
        }
        CameraCaptureSession session = configureReprocessableCameraSessionWithConfigurations(
                camera, inputConfiguration, outputConfigs, listener, handler);

        return session;
    }

    public static CameraCaptureSession configureReprocessableCameraSessionWithConfigurations(
            CameraDevice camera, InputConfiguration inputConfiguration,
            List<OutputConfiguration> outputConfigs, CameraCaptureSession.StateCallback listener,
            Handler handler) throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);
        SessionConfiguration sessionConfig = new SessionConfiguration(
                SessionConfiguration.SESSION_REGULAR, outputConfigs, new HandlerExecutor(handler),
                sessionListener);
        sessionConfig.setInputConfiguration(inputConfiguration);
        camera.createCaptureSession(sessionConfig);

        Integer[] sessionStates = {BlockingSessionCallback.SESSION_READY,
                                   BlockingSessionCallback.SESSION_CONFIGURE_FAILED};
        int state = sessionListener.getStateWaiter().waitForAnyOfStates(
                Arrays.asList(sessionStates), SESSION_CONFIGURE_TIMEOUT_MS);

        assertTrue(""Creating a reprocessable session failed."",
                state == BlockingSessionCallback.SESSION_READY);
        CameraCaptureSession session =
                sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertTrue(""Camera session should be a reprocessable session"", session.isReprocessable());

        return session;
    }

    /**
     * Create a reprocessable camera session with input and output configurations.
     *
     * @param camera The CameraDevice to be configured.
     * @param inputConfiguration The input configuration used to create this session.
     * @param outputs The output configurations used to create this session.
     * @param listener The callback CameraDevice will notify when capture results are available.
     * @param handler The handler used to notify callbacks.
     * @return The session ready to use.
     * @throws CameraAccessException
     */
    public static CameraCaptureSession configureReprocCameraSessionWithConfig(CameraDevice camera,
            InputConfiguration inputConfiguration, List<OutputConfiguration> outputs,
            CameraCaptureSession.StateCallback listener, Handler handler)
            throws CameraAccessException {
        BlockingSessionCallback sessionListener = new BlockingSessionCallback(listener);
        camera.createReprocessableCaptureSessionByConfigurations(inputConfiguration, outputs,
                sessionListener, handler);

        Integer[] sessionStates = {BlockingSessionCallback.SESSION_READY,
                                   BlockingSessionCallback.SESSION_CONFIGURE_FAILED};
        int state = sessionListener.getStateWaiter().waitForAnyOfStates(
                Arrays.asList(sessionStates), SESSION_CONFIGURE_TIMEOUT_MS);

        assertTrue(""Creating a reprocessable session failed."",
                state == BlockingSessionCallback.SESSION_READY);

        CameraCaptureSession session =
                sessionListener.waitAndGetSession(SESSION_CONFIGURE_TIMEOUT_MS);
        assertTrue(""Camera session should be a reprocessable session"", session.isReprocessable());

        return session;
    }

    public static <T> void assertArrayNotEmpty(T arr, String message) {
        assertTrue(message, arr != null && Array.getLength(arr) > 0);
    }

    /**
     * Check if the format is a legal YUV format camera supported.
     */
    public static void checkYuvFormat(int format) {
        if ((format != ImageFormat.YUV_420_888) &&
                (format != ImageFormat.NV21) &&
                (format != ImageFormat.YV12)) {
            fail(""Wrong formats: "" + format);
        }
    }

    /**
     * Check if image size and format match given size and format.
     */
    public static void checkImage(Image image, int width, int height, int format) {
        // Image reader will wrap YV12/NV21 image by YUV_420_888
        if (format == ImageFormat.NV21 || format == ImageFormat.YV12) {
            format = ImageFormat.YUV_420_888;
        }
        assertNotNull(""Input image is invalid"", image);
        assertEquals(""Format doesn't match"", format, image.getFormat());
        assertEquals(""Width doesn't match"", width, image.getWidth());
        assertEquals(""Height doesn't match"", height, image.getHeight());
    }

    /**
     * <p>Read data from all planes of an Image into a contiguous unpadded, unpacked
     * 1-D linear byte array, such that it can be write into disk, or accessed by
     * software conveniently. It supports YUV_420_888/NV21/YV12 and JPEG input
     * Image format.</p>
     *
     * <p>For YUV_420_888/NV21/YV12/Y8/Y16, it returns a byte array that contains
     * the Y plane data first, followed by U(Cb), V(Cr) planes if there is any
     * (xstride = width, ystride = height for chroma and luma components).</p>
     *
     * <p>For JPEG, it returns a 1-D byte array contains a complete JPEG image.</p>
     *
     * <p>For YUV P010, it returns a byte array that contains Y plane first, followed
     * by the interleaved U(Cb)/V(Cr) plane.</p>
     */
    public static byte[] getDataFromImage(Image image) {
        assertNotNull(""Invalid image:"", image);
        int format = image.getFormat();
        int width = image.getWidth();
        int height = image.getHeight();
        int rowStride, pixelStride;
        byte[] data = null;

        // Read image data
        Plane[] planes = image.getPlanes();
        assertTrue(""Fail to get image planes"", planes != null && planes.length > 0);

        // Check image validity
        checkAndroidImageFormat(image);

        ByteBuffer buffer = null;
        // JPEG doesn't have pixelstride and rowstride, treat it as 1D buffer.
        // Same goes for DEPTH_POINT_CLOUD, RAW_PRIVATE, DEPTH_JPEG, and HEIC
        if (format == ImageFormat.JPEG || format == ImageFormat.DEPTH_POINT_CLOUD ||
                format == ImageFormat.RAW_PRIVATE || format == ImageFormat.DEPTH_JPEG ||
                format == ImageFormat.HEIC) {
            buffer = planes[0].getBuffer();
            assertNotNull(""Fail to get jpeg/depth/heic ByteBuffer"", buffer);
            data = new byte[buffer.remaining()];
            buffer.get(data);
            buffer.rewind();
            return data;
        } else if (format == ImageFormat.YCBCR_P010) {
            // P010 samples are stored within 16 bit values
            int offset = 0;
            int bytesPerPixelRounded = (ImageFormat.getBitsPerPixel(format) + 7) / 8;
            data = new byte[width * height * bytesPerPixelRounded];
            assertTrue(""Unexpected number of planes, expected "" + 3 + "" actual "" + planes.length,
                    planes.length == 3);
            for (int i = 0; i < 2; i++) {
                buffer = planes[i].getBuffer();
                assertNotNull(""Fail to get bytebuffer from plane"", buffer);
                buffer.rewind();
                rowStride = planes[i].getRowStride();
                if (VERBOSE) {
                    Log.v(TAG, ""rowStride "" + rowStride);
                    Log.v(TAG, ""width "" + width);
                    Log.v(TAG, ""height "" + height);
                }
                int h = (i == 0) ? height : height / 2;
                for (int row = 0; row < h; row++) {
                    int length = rowStride;
                    buffer.get(data, offset, length);
                    offset += length;
                }
                if (VERBOSE) Log.v(TAG, ""Finished reading data from plane "" + i);
                buffer.rewind();
            }
            return data;
        }

        int offset = 0;
        data = new byte[width * height * ImageFormat.getBitsPerPixel(format) / 8];
        int maxRowSize = planes[0].getRowStride();
        for (int i = 0; i < planes.length; i++) {
            if (maxRowSize < planes[i].getRowStride()) {
                maxRowSize = planes[i].getRowStride();
            }
        }
        byte[] rowData = new byte[maxRowSize];
        if(VERBOSE) Log.v(TAG, ""get data from "" + planes.length + "" planes"");
        for (int i = 0; i < planes.length; i++) {
            buffer = planes[i].getBuffer();
            assertNotNull(""Fail to get bytebuffer from plane"", buffer);
            buffer.rewind();
            rowStride = planes[i].getRowStride();
            pixelStride = planes[i].getPixelStride();
            assertTrue(""pixel stride "" + pixelStride + "" is invalid"", pixelStride > 0);
            if (VERBOSE) {
                Log.v(TAG, ""pixelStride "" + pixelStride);
                Log.v(TAG, ""rowStride "" + rowStride);
                Log.v(TAG, ""width "" + width);
                Log.v(TAG, ""height "" + height);
            }
            // For multi-planar yuv images, assuming yuv420 with 2x2 chroma subsampling.
            int w = (i == 0) ? width : width / 2;
            int h = (i == 0) ? height : height / 2;
            assertTrue(""rowStride "" + rowStride + "" should be >= width "" + w , rowStride >= w);
            for (int row = 0; row < h; row++) {
                int bytesPerPixel = ImageFormat.getBitsPerPixel(format) / 8;
                int length;
                if (pixelStride == bytesPerPixel) {
                    // Special case: optimized read of the entire row
                    length = w * bytesPerPixel;
                    buffer.get(data, offset, length);
                    offset += length;
                } else {
                    // Generic case: should work for any pixelStride but slower.
                    // Use intermediate buffer to avoid read byte-by-byte from
                    // DirectByteBuffer, which is very bad for performance
                    length = (w - 1) * pixelStride + bytesPerPixel;
                    buffer.get(rowData, 0, length);
                    for (int col = 0; col < w; col++) {
                        data[offset++] = rowData[col * pixelStride];
                    }
                }
                // Advance buffer the remainder of the row stride
                if (row < h - 1) {
                    buffer.position(buffer.position() + rowStride - length);
                }
            }
            if (VERBOSE) Log.v(TAG, ""Finished reading data from plane "" + i);
            buffer.rewind();
        }
        return data;
    }

    /**
     * <p>Check android image format validity for an image, only support below formats:</p>
     *
     * <p>YUV_420_888/NV21/YV12, can add more for future</p>
     */
    public static void checkAndroidImageFormat(Image image) {
        int format = image.getFormat();
        Plane[] planes = image.getPlanes();
        switch (format) {
            case ImageFormat.YUV_420_888:
            case ImageFormat.NV21:
            case ImageFormat.YV12:
            case ImageFormat.YCBCR_P010:
                assertEquals(""YUV420 format Images should have 3 planes"", 3, planes.length);
                break;
            case ImageFormat.JPEG:
            case ImageFormat.RAW_SENSOR:
            case ImageFormat.RAW_PRIVATE:
            case ImageFormat.DEPTH16:
            case ImageFormat.DEPTH_POINT_CLOUD:
            case ImageFormat.DEPTH_JPEG:
            case ImageFormat.Y8:
            case ImageFormat.HEIC:
                assertEquals(""JPEG/RAW/depth/Y8 Images should have one plane"", 1, planes.length);
                break;
            default:
                fail(""Unsupported Image Format: "" + format);
        }
    }

    public static void dumpFile(String fileName, Bitmap data) {
        FileOutputStream outStream;
        try {
            Log.v(TAG, ""output will be saved as "" + fileName);
            outStream = new FileOutputStream(fileName);
        } catch (IOException ioe) {
            throw new RuntimeException(""Unable to create debug output file "" + fileName, ioe);
        }

        try {
            data.compress(Bitmap.CompressFormat.JPEG, /*quality*/90, outStream);
            outStream.close();
        } catch (IOException ioe) {
            throw new RuntimeException(""failed writing data to file "" + fileName, ioe);
        }
    }

    public static void dumpFile(String fileName, byte[] data) {
        FileOutputStream outStream;
        try {
            Log.v(TAG, ""output will be saved as "" + fileName);
            outStream = new FileOutputStream(fileName);
        } catch (IOException ioe) {
            throw new RuntimeException(""Unable to create debug output file "" + fileName, ioe);
        }

        try {
            outStream.write(data);
            outStream.close();
        } catch (IOException ioe) {
            throw new RuntimeException(""failed writing data to file "" + fileName, ioe);
        }
    }

    /**
     * Get the available output sizes for the user-defined {@code format}.
     *
     * <p>Note that implementation-defined/hidden formats are not supported.</p>
     */
    public static Size[] getSupportedSizeForFormat(int format, String cameraId,
            CameraManager cameraManager) throws CameraAccessException {
        CameraCharacteristics properties = cameraManager.getCameraCharacteristics(cameraId);
        assertNotNull(""Can't get camera characteristics!"", properties);
        if (VERBOSE) {
            Log.v(TAG, ""get camera characteristics for camera: "" + cameraId);
        }
        StreamConfigurationMap configMap =
                properties.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        Size[] availableSizes = configMap.getOutputSizes(format);
        assertArrayNotEmpty(availableSizes, ""availableSizes should not be empty for format: ""
                + format);
        Size[] highResAvailableSizes = configMap.getHighResolutionOutputSizes(format);
        if (highResAvailableSizes != null && highResAvailableSizes.length > 0) {
            Size[] allSizes = new Size[availableSizes.length + highResAvailableSizes.length];
            System.arraycopy(availableSizes, 0, allSizes, 0,
                    availableSizes.length);
            System.arraycopy(highResAvailableSizes, 0, allSizes, availableSizes.length,
                    highResAvailableSizes.length);
            availableSizes = allSizes;
        }
        if (VERBOSE) Log.v(TAG, ""Supported sizes are: "" + Arrays.deepToString(availableSizes));
        return availableSizes;
    }

    /**
     * Get the available output sizes for the given class.
     *
     */
    public static Size[] getSupportedSizeForClass(Class klass, String cameraId,
            CameraManager cameraManager) throws CameraAccessException {
        CameraCharacteristics properties = cameraManager.getCameraCharacteristics(cameraId);
        assertNotNull(""Can't get camera characteristics!"", properties);
        if (VERBOSE) {
            Log.v(TAG, ""get camera characteristics for camera: "" + cameraId);
        }
        StreamConfigurationMap configMap =
                properties.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        Size[] availableSizes = configMap.getOutputSizes(klass);
        assertArrayNotEmpty(availableSizes, ""availableSizes should not be empty for class: ""
                + klass);
        Size[] highResAvailableSizes = configMap.getHighResolutionOutputSizes(ImageFormat.PRIVATE);
        if (highResAvailableSizes != null && highResAvailableSizes.length > 0) {
            Size[] allSizes = new Size[availableSizes.length + highResAvailableSizes.length];
            System.arraycopy(availableSizes, 0, allSizes, 0,
                    availableSizes.length);
            System.arraycopy(highResAvailableSizes, 0, allSizes, availableSizes.length,
                    highResAvailableSizes.length);
            availableSizes = allSizes;
        }
        if (VERBOSE) Log.v(TAG, ""Supported sizes are: "" + Arrays.deepToString(availableSizes));
        return availableSizes;
    }

    /**
     * Size comparator that compares the number of pixels it covers.
     *
     * <p>If two the areas of two sizes are same, compare the widths.</p>
     */
    public static class SizeComparator implements Comparator<Size> {
        @Override
        public int compare(Size lhs, Size rhs) {
            return CameraUtils
                    .compareSizes(lhs.getWidth(), lhs.getHeight(), rhs.getWidth(), rhs.getHeight());
        }
    }

    /**
     * Get sorted size list in descending order. Remove the sizes larger than
     * the bound. If the bound is null, don't do the size bound filtering.
     */
    static public List<Size> getSupportedPreviewSizes(String cameraId,
            CameraManager cameraManager, Size bound) throws CameraAccessException {

        Size[] rawSizes = getSupportedSizeForClass(android.view.SurfaceHolder.class, cameraId,
                cameraManager);
        assertArrayNotEmpty(rawSizes,
                ""Available sizes for SurfaceHolder class should not be empty"");
        if (VERBOSE) {
            Log.v(TAG, ""Supported sizes are: "" + Arrays.deepToString(rawSizes));
        }

        if (bound == null) {
            return getAscendingOrderSizes(Arrays.asList(rawSizes), /*ascending*/false);
        }

        List<Size> sizes = new ArrayList<Size>();
        for (Size sz: rawSizes) {
            if (sz.getWidth() <= bound.getWidth() && sz.getHeight() <= bound.getHeight()) {
                sizes.add(sz);
            }
        }
        return getAscendingOrderSizes(sizes, /*ascending*/false);
    }

    /**
     * Get a sorted list of sizes from a given size list.
     *
     * <p>
     * The size is compare by area it covers, if the areas are same, then
     * compare the widths.
     * </p>
     *
     * @param sizeList The input size list to be sorted
     * @param ascending True if the order is ascending, otherwise descending order
     * @return The ordered list of sizes
     */
    static public List<Size> getAscendingOrderSizes(final List<Size> sizeList, boolean ascending) {
        if (sizeList == null) {
            throw new IllegalArgumentException(""sizeList shouldn't be null"");
        }

        Comparator<Size> comparator = new SizeComparator();
        List<Size> sortedSizes = new ArrayList<Size>();
        sortedSizes.addAll(sizeList);
        Collections.sort(sortedSizes, comparator);
        if (!ascending) {
            Collections.reverse(sortedSizes);
        }

        return sortedSizes;
    }

    /**
     * Get sorted (descending order) size list for given format. Remove the sizes larger than
     * the bound. If the bound is null, don't do the size bound filtering.
     */
    static public List<Size> getSortedSizesForFormat(String cameraId,
            CameraManager cameraManager, int format, Size bound) throws CameraAccessException {
        Comparator<Size> comparator = new SizeComparator();
        Size[] sizes = getSupportedSizeForFormat(format, cameraId, cameraManager);
        List<Size> sortedSizes = null;
        if (bound != null) {
            sortedSizes = new ArrayList<Size>(/*capacity*/1);
            for (Size sz : sizes) {
                if (comparator.compare(sz, bound) <= 0) {
                    sortedSizes.add(sz);
                }
            }
        } else {
            sortedSizes = Arrays.asList(sizes);
        }
        assertTrue(""Supported size list should have at least one element"",
                sortedSizes.size() > 0);

        Collections.sort(sortedSizes, comparator);
        // Make it in descending order.
        Collections.reverse(sortedSizes);
        return sortedSizes;
    }

    /**
     * Get supported video size list for a given camera device.
     *
     * <p>
     * Filter out the sizes that are larger than the bound. If the bound is
     * null, don't do the size bound filtering.
     * </p>
     */
    static public List<Size> getSupportedVideoSizes(String cameraId,
            CameraManager cameraManager, Size bound) throws CameraAccessException {

        Size[] rawSizes = getSupportedSizeForClass(android.media.MediaRecorder.class,
                cameraId, cameraManager);
        assertArrayNotEmpty(rawSizes,
                ""Available sizes for MediaRecorder class should not be empty"");
        if (VERBOSE) {
            Log.v(TAG, ""Supported sizes are: "" + Arrays.deepToString(rawSizes));
        }

        if (bound == null) {
            return getAscendingOrderSizes(Arrays.asList(rawSizes), /*ascending*/false);
        }

        List<Size> sizes = new ArrayList<Size>();
        for (Size sz: rawSizes) {
            if (sz.getWidth() <= bound.getWidth() && sz.getHeight() <= bound.getHeight()) {
                sizes.add(sz);
            }
        }
        return getAscendingOrderSizes(sizes, /*ascending*/false);
    }

    /**
     * Get supported video size list (descending order) for a given camera device.
     *
     * <p>
     * Filter out the sizes that are larger than the bound. If the bound is
     * null, don't do the size bound filtering.
     * </p>
     */
    static public List<Size> getSupportedStillSizes(String cameraId,
            CameraManager cameraManager, Size bound) throws CameraAccessException {
        return getSortedSizesForFormat(cameraId, cameraManager, ImageFormat.JPEG, bound);
    }

    static public List<Size> getSupportedHeicSizes(String cameraId,
            CameraManager cameraManager, Size bound) throws CameraAccessException {
        return getSortedSizesForFormat(cameraId, cameraManager, ImageFormat.HEIC, bound);
    }

    static public Size getMinPreviewSize(String cameraId, CameraManager cameraManager)
            throws CameraAccessException {
        List<Size> sizes = getSupportedPreviewSizes(cameraId, cameraManager, null);
        return sizes.get(sizes.size() - 1);
    }

    /**
     * Get max supported preview size for a camera device.
     */
    static public Size getMaxPreviewSize(String cameraId, CameraManager cameraManager)
            throws CameraAccessException {
        return getMaxPreviewSize(cameraId, cameraManager, /*bound*/null);
    }

    /**
     * Get max preview size for a camera device in the supported sizes that are no larger
     * than the bound.
     */
    static public Size getMaxPreviewSize(String cameraId, CameraManager cameraManager, Size bound)
            throws CameraAccessException {
        List<Size> sizes = getSupportedPreviewSizes(cameraId, cameraManager, bound);
        return sizes.get(0);
    }

    /**
     * Get max depth size for a camera device.
     */
    static public Size getMaxDepthSize(String cameraId, CameraManager cameraManager)
            throws CameraAccessException {
        List<Size> sizes = getSortedSizesForFormat(cameraId, cameraManager, ImageFormat.DEPTH16,
                /*bound*/ null);
        return sizes.get(0);
    }

    /**
     * Get the largest size by area.
     *
     * @param sizes an array of sizes, must have at least 1 element
     *
     * @return Largest Size
     *
     * @throws IllegalArgumentException if sizes was null or had 0 elements
     */
    public static Size getMaxSize(Size... sizes) {
        if (sizes == null || sizes.length == 0) {
            throw new IllegalArgumentException(""sizes was empty"");
        }

        Size sz = sizes[0];
        for (Size size : sizes) {
            if (size.getWidth() * size.getHeight() > sz.getWidth() * sz.getHeight()) {
                sz = size;
            }
        }

        return sz;
    }

    /**
     * Get the largest size by area within (less than) bound
     *
     * @param sizes an array of sizes, must have at least 1 element
     *
     * @return Largest Size. Null if no such size exists within bound.
     *
     * @throws IllegalArgumentException if sizes was null or had 0 elements, or bound is invalid.
     */
    public static Size getMaxSizeWithBound(Size[] sizes, int bound) {
        if (sizes == null || sizes.length == 0) {
            throw new IllegalArgumentException(""sizes was empty"");
        }
        if (bound <= 0) {
            throw new IllegalArgumentException(""bound is invalid"");
        }

        Size sz = null;
        for (Size size : sizes) {
            if (size.getWidth() * size.getHeight() >= bound) {
                continue;
            }

            if (sz == null ||
                    size.getWidth() * size.getHeight() > sz.getWidth() * sz.getHeight()) {
                sz = size;
            }
        }

        return sz;
    }

    /**
     * Returns true if the given {@code array} contains the given element.
     *
     * @param array {@code array} to check for {@code elem}
     * @param elem {@code elem} to test for
     * @return {@code true} if the given element is contained
     */
    public static boolean contains(int[] array, int elem) {
        if (array == null) return false;
        for (int i = 0; i < array.length; i++) {
            if (elem == array[i]) return true;
        }
        return false;
    }

    /**
     * Get object array from byte array.
     *
     * @param array Input byte array to be converted
     * @return Byte object array converted from input byte array
     */
    public static Byte[] toObject(byte[] array) {
        return convertPrimitiveArrayToObjectArray(array, Byte.class);
    }

    /**
     * Get object array from int array.
     *
     * @param array Input int array to be converted
     * @return Integer object array converted from input int array
     */
    public static Integer[] toObject(int[] array) {
        return convertPrimitiveArrayToObjectArray(array, Integer.class);
    }

    /**
     * Get object array from float array.
     *
     * @param array Input float array to be converted
     * @return Float object array converted from input float array
     */
    public static Float[] toObject(float[] array) {
        return convertPrimitiveArrayToObjectArray(array, Float.class);
    }

    /**
     * Get object array from double array.
     *
     * @param array Input double array to be converted
     * @return Double object array converted from input double array
     */
    public static Double[] toObject(double[] array) {
        return convertPrimitiveArrayToObjectArray(array, Double.class);
    }

    /**
     * Convert a primitive input array into its object array version (e.g. from int[] to Integer[]).
     *
     * @param array Input array object
     * @param wrapperClass The boxed class it converts to
     * @return Boxed version of primitive array
     */
    private static <T> T[] convertPrimitiveArrayToObjectArray(final Object array,
            final Class<T> wrapperClass) {
        // getLength does the null check and isArray check already.
        int arrayLength = Array.getLength(array);
        if (arrayLength == 0) {
            throw new IllegalArgumentException(""Input array shouldn't be empty"");
        }

        @SuppressWarnings(""unchecked"")
        final T[] result = (T[]) Array.newInstance(wrapperClass, arrayLength);
        for (int i = 0; i < arrayLength; i++) {
            Array.set(result, i, Array.get(array, i));
        }
        return result;
    }

    /**
     * Validate image based on format and size.
     *
     * @param image The image to be validated.
     * @param width The image width.
     * @param height The image height.
     * @param format The image format.
     * @param filePath The debug dump file path, null if don't want to dump to
     *            file.
     * @throws UnsupportedOperationException if calling with an unknown format
     */
    public static void validateImage(Image image, int width, int height, int format,
            String filePath) {
        checkImage(image, width, height, format);

        /**
         * TODO: validate timestamp:
         * 1. capture result timestamp against the image timestamp (need
         * consider frame drops)
         * 2. timestamps should be monotonically increasing for different requests
         */
        if(VERBOSE) Log.v(TAG, ""validating Image"");
        byte[] data = getDataFromImage(image);
        assertTrue(""Invalid image data"", data != null && data.length > 0);

        switch (format) {
            // Clients must be able to process and handle depth jpeg images like any other
            // regular jpeg.
            case ImageFormat.DEPTH_JPEG:
            case ImageFormat.JPEG:
                validateJpegData(data, width, height, filePath);
                break;
            case ImageFormat.YCBCR_P010:
                validateP010Data(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.YUV_420_888:
            case ImageFormat.YV12:
                validateYuvData(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.RAW_SENSOR:
                validateRaw16Data(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.DEPTH16:
                validateDepth16Data(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.DEPTH_POINT_CLOUD:
                validateDepthPointCloudData(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.RAW_PRIVATE:
                validateRawPrivateData(data, width, height, image.getTimestamp(), filePath);
                break;
            case ImageFormat.Y8:
                validateY8Data(data, width, height, format, image.getTimestamp(), filePath);
                break;
            case ImageFormat.HEIC:
                validateHeicData(data, width, height, filePath);
                break;
            default:
                throw new UnsupportedOperationException(""Unsupported format for validation: ""
                        + format);
        }
    }

    public static class HandlerExecutor implements Executor {
        private final Handler mHandler;

        public HandlerExecutor(Handler handler) {
            assertNotNull(""handler must be valid"", handler);
            mHandler = handler;
        }

        @Override
        public void execute(Runnable runCmd) {
            mHandler.post(runCmd);
        }
    }

    /**
     * Provide a mock for {@link CameraDevice.StateCallback}.
     *
     * <p>Only useful because mockito can't mock {@link CameraDevice.StateCallback} which is an
     * abstract class.</p>
     *
     * <p>
     * Use this instead of other classes when needing to verify interactions, since
     * trying to spy on {@link BlockingStateCallback} (or others) will cause unnecessary extra
     * interactions which will cause false test failures.
     * </p>
     *
     */
    public static class MockStateCallback extends CameraDevice.StateCallback {

        @Override
        public void onOpened(CameraDevice camera) {
        }

        @Override
        public void onDisconnected(CameraDevice camera) {
        }

        @Override
        public void onError(CameraDevice camera, int error) {
        }

        private MockStateCallback() {}

        /**
         * Create a Mockito-ready mocked StateCallback.
         */
        public static MockStateCallback mock() {
            return Mockito.spy(new MockStateCallback());
        }
    }

    public static void validateJpegData(byte[] jpegData, int width, int height, String filePath) {
        BitmapFactory.Options bmpOptions = new BitmapFactory.Options();
        // DecodeBound mode: only parse the frame header to get width/height.
        // it doesn't decode the pixel.
        bmpOptions.inJustDecodeBounds = true;
        BitmapFactory.decodeByteArray(jpegData, 0, jpegData.length, bmpOptions);
        assertEquals(width, bmpOptions.outWidth);
        assertEquals(height, bmpOptions.outHeight);

        // Pixel decoding mode: decode whole image. check if the image data
        // is decodable here.
        assertNotNull(""Decoding jpeg failed"",
                BitmapFactory.decodeByteArray(jpegData, 0, jpegData.length));
        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + "".jpeg"";
            dumpFile(fileName, jpegData);
        }
    }

    private static void validateYuvData(byte[] yuvData, int width, int height, int format,
            long ts, String filePath) {
        checkYuvFormat(format);
        if (VERBOSE) Log.v(TAG, ""Validating YUV data"");
        int expectedSize = width * height * ImageFormat.getBitsPerPixel(format) / 8;
        assertEquals(""Yuv data doesn't match"", expectedSize, yuvData.length);

        // TODO: Can add data validation for test pattern.

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".yuv"";
            dumpFile(fileName, yuvData);
        }
    }

    private static void validateP010Data(byte[] p010Data, int width, int height, int format,
            long ts, String filePath) {
        if (VERBOSE) Log.v(TAG, ""Validating P010 data"");
        // The P010 10 bit samples are stored in two bytes so the size needs to be adjusted
        // accordingly.
        int bytesPerPixelRounded = (ImageFormat.getBitsPerPixel(format) + 7) / 8;
        int expectedSize = width * height * bytesPerPixelRounded;
        assertEquals(""P010 data doesn't match"", expectedSize, p010Data.length);

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".p010"";
            dumpFile(fileName, p010Data);
        }
    }
    private static void validateRaw16Data(byte[] rawData, int width, int height, int format,
            long ts, String filePath) {
        if (VERBOSE) Log.v(TAG, ""Validating raw data"");
        int expectedSize = width * height * ImageFormat.getBitsPerPixel(format) / 8;
        assertEquals(""Raw data doesn't match"", expectedSize, rawData.length);

        // TODO: Can add data validation for test pattern.

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".raw16"";
            dumpFile(fileName, rawData);
        }

        return;
    }

    private static void validateY8Data(byte[] rawData, int width, int height, int format,
            long ts, String filePath) {
        if (VERBOSE) Log.v(TAG, ""Validating Y8 data"");
        int expectedSize = width * height * ImageFormat.getBitsPerPixel(format) / 8;
        assertEquals(""Y8 data doesn't match"", expectedSize, rawData.length);

        // TODO: Can add data validation for test pattern.

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".y8"";
            dumpFile(fileName, rawData);
        }

        return;
    }

    private static void validateRawPrivateData(byte[] rawData, int width, int height,
            long ts, String filePath) {
        if (VERBOSE) Log.v(TAG, ""Validating private raw data"");
        // Expect each RAW pixel should occupy at least one byte and no more than 30 bytes
        int expectedSizeMin = width * height;
        int expectedSizeMax = width * height * 30;

        assertTrue(""Opaque RAW size "" + rawData.length + ""out of normal bound ["" +
                expectedSizeMin + "","" + expectedSizeMax + ""]"",
                expectedSizeMin <= rawData.length && rawData.length <= expectedSizeMax);

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".rawPriv"";
            dumpFile(fileName, rawData);
        }

        return;
    }

    private static void validateDepth16Data(byte[] depthData, int width, int height, int format,
            long ts, String filePath) {

        if (VERBOSE) Log.v(TAG, ""Validating depth16 data"");
        int expectedSize = width * height * ImageFormat.getBitsPerPixel(format) / 8;
        assertEquals(""Depth data doesn't match"", expectedSize, depthData.length);


        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".depth16"";
            dumpFile(fileName, depthData);
        }

        return;

    }

    private static void validateDepthPointCloudData(byte[] depthData, int width, int height, int format,
            long ts, String filePath) {

        if (VERBOSE) Log.v(TAG, ""Validating depth point cloud data"");

        // Can't validate size since it is variable

        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + ""_"" + ts / 1e6 + "".depth_point_cloud"";
            dumpFile(fileName, depthData);
        }

        return;

    }

    private static void validateHeicData(byte[] heicData, int width, int height, String filePath) {
        BitmapFactory.Options bmpOptions = new BitmapFactory.Options();
        // DecodeBound mode: only parse the frame header to get width/height.
        // it doesn't decode the pixel.
        bmpOptions.inJustDecodeBounds = true;
        BitmapFactory.decodeByteArray(heicData, 0, heicData.length, bmpOptions);
        assertEquals(width, bmpOptions.outWidth);
        assertEquals(height, bmpOptions.outHeight);

        // Pixel decoding mode: decode whole image. check if the image data
        // is decodable here.
        assertNotNull(""Decoding heic failed"",
                BitmapFactory.decodeByteArray(heicData, 0, heicData.length));
        if (DEBUG && filePath != null) {
            String fileName =
                    filePath + ""/"" + width + ""x"" + height + "".heic"";
            dumpFile(fileName, heicData);
        }
    }

    public static <T> T getValueNotNull(CaptureResult result, CaptureResult.Key<T> key) {
        if (result == null) {
            throw new IllegalArgumentException(""Result must not be null"");
        }

        T value = result.get(key);
        assertNotNull(""Value of Key "" + key.getName() + ""shouldn't be null"", value);
        return value;
    }

    public static <T> T getValueNotNull(CameraCharacteristics characteristics,
            CameraCharacteristics.Key<T> key) {
        if (characteristics == null) {
            throw new IllegalArgumentException(""Camera characteristics must not be null"");
        }

        T value = characteristics.get(key);
        assertNotNull(""Value of Key "" + key.getName() + ""shouldn't be null"", value);
        return value;
    }

    /**
     * Get a crop region for a given zoom factor and center position.
     * <p>
     * The center position is normalized position in range of [0, 1.0], where
     * (0, 0) represents top left corner, (1.0. 1.0) represents bottom right
     * corner. The center position could limit the effective minimal zoom
     * factor, for example, if the center position is (0.75, 0.75), the
     * effective minimal zoom position becomes 2.0. If the requested zoom factor
     * is smaller than 2.0, a crop region with 2.0 zoom factor will be returned.
     * </p>
     * <p>
     * The aspect ratio of the crop region is maintained the same as the aspect
     * ratio of active array.
     * </p>
     *
     * @param zoomFactor The zoom factor to generate the crop region, it must be
     *            >= 1.0
     * @param center The normalized zoom center point that is in the range of [0, 1].
     * @param maxZoom The max zoom factor supported by this device.
     * @param activeArray The active array size of this device.
     * @return crop region for the given normalized center and zoom factor.
     */
    public static Rect getCropRegionForZoom(float zoomFactor, final PointF center,
            final float maxZoom, final Rect activeArray) {
        if (zoomFactor < 1.0) {
            throw new IllegalArgumentException(""zoom factor "" + zoomFactor + "" should be >= 1.0"");
        }
        if (center.x > 1.0 || center.x < 0) {
            throw new IllegalArgumentException(""center.x "" + center.x
                    + "" should be in range of [0, 1.0]"");
        }
        if (center.y > 1.0 || center.y < 0) {
            throw new IllegalArgumentException(""center.y "" + center.y
                    + "" should be in range of [0, 1.0]"");
        }
        if (maxZoom < 1.0) {
            throw new IllegalArgumentException(""max zoom factor "" + maxZoom + "" should be >= 1.0"");
        }
        if (activeArray == null) {
            throw new IllegalArgumentException(""activeArray must not be null"");
        }

        float minCenterLength = Math.min(Math.min(center.x, 1.0f - center.x),
                Math.min(center.y, 1.0f - center.y));
        float minEffectiveZoom =  0.5f / minCenterLength;
        if (minEffectiveZoom > maxZoom) {
            throw new IllegalArgumentException(""Requested center "" + center.toString() +
                    "" has minimal zoomable factor "" + minEffectiveZoom + "", which exceeds max""
                            + "" zoom factor "" + maxZoom);
        }

        if (zoomFactor < minEffectiveZoom) {
            Log.w(TAG, ""Requested zoomFactor "" + zoomFactor + "" < minimal zoomable factor ""
                    + minEffectiveZoom + "". It will be overwritten by "" + minEffectiveZoom);
            zoomFactor = minEffectiveZoom;
        }

        int cropCenterX = (int)(activeArray.width() * center.x);
        int cropCenterY = (int)(activeArray.height() * center.y);
        int cropWidth = (int) (activeArray.width() / zoomFactor);
        int cropHeight = (int) (activeArray.height() / zoomFactor);

        return new Rect(
                /*left*/cropCenterX - cropWidth / 2,
                /*top*/cropCenterY - cropHeight / 2,
                /*right*/ cropCenterX + cropWidth / 2,
                /*bottom*/cropCenterY + cropHeight / 2);
    }

    /**
     * Get AeAvailableTargetFpsRanges and sort them in descending order by max fps
     *
     * @param staticInfo camera static metadata
     * @return AeAvailableTargetFpsRanges in descending order by max fps
     */
    public static Range<Integer>[] getDescendingTargetFpsRanges(StaticMetadata staticInfo) {
        Range<Integer>[] fpsRanges = staticInfo.getAeAvailableTargetFpsRangesChecked();
        Arrays.sort(fpsRanges, new Comparator<Range<Integer>>() {
            public int compare(Range<Integer> r1, Range<Integer> r2) {
                return r2.getUpper() - r1.getUpper();
            }
        });
        return fpsRanges;
    }

    /**
     * Get AeAvailableTargetFpsRanges with max fps not exceeding 30
     *
     * @param staticInfo camera static metadata
     * @return AeAvailableTargetFpsRanges with max fps not exceeding 30
     */
    public static List<Range<Integer>> getTargetFpsRangesUpTo30(StaticMetadata staticInfo) {
        Range<Integer>[] fpsRanges = staticInfo.getAeAvailableTargetFpsRangesChecked();
        ArrayList<Range<Integer>> fpsRangesUpTo30 = new ArrayList<Range<Integer>>();
        for (Range<Integer> fpsRange : fpsRanges) {
            if (fpsRange.getUpper() <= 30) {
                fpsRangesUpTo30.add(fpsRange);
            }
        }
        return fpsRangesUpTo30;
    }

    /**
     * Get AeAvailableTargetFpsRanges with max fps greater than 30
     *
     * @param staticInfo camera static metadata
     * @return AeAvailableTargetFpsRanges with max fps greater than 30
     */
    public static List<Range<Integer>> getTargetFpsRangesGreaterThan30(StaticMetadata staticInfo) {
        Range<Integer>[] fpsRanges = staticInfo.getAeAvailableTargetFpsRangesChecked();
        ArrayList<Range<Integer>> fpsRangesGreaterThan30 = new ArrayList<Range<Integer>>();
        for (Range<Integer> fpsRange : fpsRanges) {
            if (fpsRange.getUpper() > 30) {
                fpsRangesGreaterThan30.add(fpsRange);
            }
        }
        return fpsRangesGreaterThan30;
    }

    /**
     * Calculate output 3A region from the intersection of input 3A region and cropped region.
     *
     * @param requestRegions The input 3A regions
     * @param cropRect The cropped region
     * @return expected 3A regions output in capture result
     */
    public static MeteringRectangle[] getExpectedOutputRegion(
            MeteringRectangle[] requestRegions, Rect cropRect){
        MeteringRectangle[] resultRegions = new MeteringRectangle[requestRegions.length];
        for (int i = 0; i < requestRegions.length; i++) {
            Rect requestRect = requestRegions[i].getRect();
            Rect resultRect = new Rect();
            boolean intersect = resultRect.setIntersect(requestRect, cropRect);
            resultRegions[i] = new MeteringRectangle(
                    resultRect,
                    intersect ? requestRegions[i].getMeteringWeight() : 0);
        }
        return resultRegions;
    }

    /**
     * Copy source image data to destination image.
     *
     * @param src The source image to be copied from.
     * @param dst The destination image to be copied to.
     * @throws IllegalArgumentException If the source and destination images have
     *             different format, size, or one of the images is not copyable.
     */
    public static void imageCopy(Image src, Image dst) {
        if (src == null || dst == null) {
            throw new IllegalArgumentException(""Images should be non-null"");
        }
        if (src.getFormat() != dst.getFormat()) {
            throw new IllegalArgumentException(""Src and dst images should have the same format"");
        }
        if (src.getFormat() == ImageFormat.PRIVATE ||
                dst.getFormat() == ImageFormat.PRIVATE) {
            throw new IllegalArgumentException(""PRIVATE format images are not copyable"");
        }

        Size srcSize = new Size(src.getWidth(), src.getHeight());
        Size dstSize = new Size(dst.getWidth(), dst.getHeight());
        if (!srcSize.equals(dstSize)) {
            throw new IllegalArgumentException(""source image size "" + srcSize + "" is different""
                    + "" with "" + ""destination image size "" + dstSize);
        }

        // TODO: check the owner of the dst image, it must be from ImageWriter, other source may
        // not be writable. Maybe we should add an isWritable() method in image class.

        Plane[] srcPlanes = src.getPlanes();
        Plane[] dstPlanes = dst.getPlanes();
        ByteBuffer srcBuffer = null;
        ByteBuffer dstBuffer = null;
        for (int i = 0; i < srcPlanes.length; i++) {
            srcBuffer = srcPlanes[i].getBuffer();
            dstBuffer = dstPlanes[i].getBuffer();
            int srcPos = srcBuffer.position();
            srcBuffer.rewind();
            dstBuffer.rewind();
            int srcRowStride = srcPlanes[i].getRowStride();
            int dstRowStride = dstPlanes[i].getRowStride();
            int srcPixStride = srcPlanes[i].getPixelStride();
            int dstPixStride = dstPlanes[i].getPixelStride();

            if (srcPixStride > 2 || dstPixStride > 2) {
                throw new IllegalArgumentException(""source pixel stride "" + srcPixStride +
                        "" with destination pixel stride "" + dstPixStride +
                        "" is not supported"");
            }

            if (srcRowStride == dstRowStride && srcPixStride == dstPixStride &&
                    srcPixStride == 1) {
                // Fast path, just copy the content in the byteBuffer all together.
                dstBuffer.put(srcBuffer);
            } else {
                Size effectivePlaneSize = getEffectivePlaneSizeForImage(src, i);
                int srcRowByteCount = srcRowStride;
                int dstRowByteCount = dstRowStride;
                byte[] srcDataRow = new byte[Math.max(srcRowStride, dstRowStride)];

                if (srcPixStride == dstPixStride && srcPixStride == 1) {
                    // Row by row copy case
                    for (int row = 0; row < effectivePlaneSize.getHeight(); row++) {
                        if (row == effectivePlaneSize.getHeight() - 1) {
                            // Special case for interleaved planes: need handle the last row
                            // carefully to avoid memory corruption. Check if we have enough bytes
                            // to copy.
                            srcRowByteCount = Math.min(srcRowByteCount, srcBuffer.remaining());
                            dstRowByteCount = Math.min(dstRowByteCount, dstBuffer.remaining());
                        }
                        srcBuffer.get(srcDataRow, /*offset*/0, srcRowByteCount);
                        dstBuffer.put(srcDataRow, /*offset*/0, dstRowByteCount);
                    }
                } else {
                    // Row by row per pixel copy case
                    byte[] dstDataRow = new byte[dstRowByteCount];
                    for (int row = 0; row < effectivePlaneSize.getHeight(); row++) {
                        if (row == effectivePlaneSize.getHeight() - 1) {
                            // Special case for interleaved planes: need handle the last row
                            // carefully to avoid memory corruption. Check if we have enough bytes
                            // to copy.
                            int remainingBytes = srcBuffer.remaining();
                            if (srcRowByteCount > remainingBytes) {
                                srcRowByteCount = remainingBytes;
                            }
                            remainingBytes = dstBuffer.remaining();
                            if (dstRowByteCount > remainingBytes) {
                                dstRowByteCount = remainingBytes;
                            }
                        }
                        srcBuffer.get(srcDataRow, /*offset*/0, srcRowByteCount);
                        int pos = dstBuffer.position();
                        dstBuffer.get(dstDataRow, /*offset*/0, dstRowByteCount);
                        dstBuffer.position(pos);
                        for (int x = 0; x < effectivePlaneSize.getWidth(); x++) {
                            dstDataRow[x * dstPixStride] = srcDataRow[x * srcPixStride];
                        }
                        dstBuffer.put(dstDataRow, /*offset*/0, dstRowByteCount);
                    }
                }
            }
            srcBuffer.position(srcPos);
            dstBuffer.rewind();
        }
    }

    private static Size getEffectivePlaneSizeForImage(Image image, int planeIdx) {
        switch (image.getFormat()) {
            case ImageFormat.YUV_420_888:
                if (planeIdx == 0) {
                    return new Size(image.getWidth(), image.getHeight());
                } else {
                    return new Size(image.getWidth() / 2, image.getHeight() / 2);
                }
            case ImageFormat.JPEG:
            case ImageFormat.RAW_SENSOR:
            case ImageFormat.RAW10:
            case ImageFormat.RAW12:
            case ImageFormat.DEPTH16:
                return new Size(image.getWidth(), image.getHeight());
            case ImageFormat.PRIVATE:
                return new Size(0, 0);
            default:
                throw new UnsupportedOperationException(
                        String.format(""Invalid image format %d"", image.getFormat()));
        }
    }

    /**
     * <p>
     * Checks whether the two images are strongly equal.
     * </p>
     * <p>
     * Two images are strongly equal if and only if the data, formats, sizes,
     * and timestamps are same. For {@link ImageFormat#PRIVATE PRIVATE} format
     * images, the image data is not not accessible thus the data comparison is
     * effectively skipped as the number of planes is zero.
     * </p>
     * <p>
     * Note that this method compares the pixel data even outside of the crop
     * region, which may not be necessary for general use case.
     * </p>
     *
     * @param lhsImg First image to be compared with.
     * @param rhsImg Second image to be compared with.
     * @return true if the two images are equal, false otherwise.
     * @throws IllegalArgumentException If either of image is null.
     */
    public static boolean isImageStronglyEqual(Image lhsImg, Image rhsImg) {
        if (lhsImg == null || rhsImg == null) {
            throw new IllegalArgumentException(""Images should be non-null"");
        }

        if (lhsImg.getFormat() != rhsImg.getFormat()) {
            Log.i(TAG, ""lhsImg format "" + lhsImg.getFormat() + "" is different with rhsImg format ""
                    + rhsImg.getFormat());
            return false;
        }

        if (lhsImg.getWidth() != rhsImg.getWidth()) {
            Log.i(TAG, ""lhsImg width "" + lhsImg.getWidth() + "" is different with rhsImg width ""
                    + rhsImg.getWidth());
            return false;
        }

        if (lhsImg.getHeight() != rhsImg.getHeight()) {
            Log.i(TAG, ""lhsImg height "" + lhsImg.getHeight() + "" is different with rhsImg height ""
                    + rhsImg.getHeight());
            return false;
        }

        if (lhsImg.getTimestamp() != rhsImg.getTimestamp()) {
            Log.i(TAG, ""lhsImg timestamp "" + lhsImg.getTimestamp()
                    + "" is different with rhsImg timestamp "" + rhsImg.getTimestamp());
            return false;
        }

        if (!lhsImg.getCropRect().equals(rhsImg.getCropRect())) {
            Log.i(TAG, ""lhsImg crop rect "" + lhsImg.getCropRect()
                    + "" is different with rhsImg crop rect "" + rhsImg.getCropRect());
            return false;
        }

        // Compare data inside of the image.
        Plane[] lhsPlanes = lhsImg.getPlanes();
        Plane[] rhsPlanes = rhsImg.getPlanes();
        ByteBuffer lhsBuffer = null;
        ByteBuffer rhsBuffer = null;
        for (int i = 0; i < lhsPlanes.length; i++) {
            lhsBuffer = lhsPlanes[i].getBuffer();
            rhsBuffer = rhsPlanes[i].getBuffer();
            lhsBuffer.rewind();
            rhsBuffer.rewind();
            // Special case for YUV420_888 buffer with different layout or
            // potentially differently interleaved U/V planes.
            if (lhsImg.getFormat() == ImageFormat.YUV_420_888 &&
                    (lhsPlanes[i].getPixelStride() != rhsPlanes[i].getPixelStride() ||
                     lhsPlanes[i].getRowStride() != rhsPlanes[i].getRowStride() ||
                     (lhsPlanes[i].getPixelStride() != 1))) {
                int width = getEffectivePlaneSizeForImage(lhsImg, i).getWidth();
                int height = getEffectivePlaneSizeForImage(lhsImg, i).getHeight();
                int rowSizeL = lhsPlanes[i].getRowStride();
                int rowSizeR = rhsPlanes[i].getRowStride();
                byte[] lhsRow = new byte[rowSizeL];
                byte[] rhsRow = new byte[rowSizeR];
                int pixStrideL = lhsPlanes[i].getPixelStride();
                int pixStrideR = rhsPlanes[i].getPixelStride();
                for (int r = 0; r < height; r++) {
                    if (r == height -1) {
                        rowSizeL = lhsBuffer.remaining();
                        rowSizeR = rhsBuffer.remaining();
                    }
                    lhsBuffer.get(lhsRow, /*offset*/0, rowSizeL);
                    rhsBuffer.get(rhsRow, /*offset*/0, rowSizeR);
                    for (int c = 0; c < width; c++) {
                        if (lhsRow[c * pixStrideL] != rhsRow[c * pixStrideR]) {
                            Log.i(TAG, String.format(
                                    ""byte buffers for plane %d row %d col %d don't match."",
                                    i, r, c));
                            return false;
                        }
                    }
                }
            } else {
                // Compare entire buffer directly
                if (!lhsBuffer.equals(rhsBuffer)) {
                    Log.i(TAG, ""byte buffers for plane "" +  i + "" don't match."");
                    return false;
                }
            }
        }

        return true;
    }

    /**
     * Set jpeg related keys in a capture request builder.
     *
     * @param builder The capture request builder to set the keys inl
     * @param exifData The exif data to set.
     * @param thumbnailSize The thumbnail size to set.
     * @param collector The camera error collector to collect errors.
     */
    public static void setJpegKeys(CaptureRequest.Builder builder, ExifTestData exifData,
            Size thumbnailSize, CameraErrorCollector collector) {
        builder.set(CaptureRequest.JPEG_THUMBNAIL_SIZE, thumbnailSize);
        builder.set(CaptureRequest.JPEG_GPS_LOCATION, exifData.gpsLocation);
        builder.set(CaptureRequest.JPEG_ORIENTATION, exifData.jpegOrientation);
        builder.set(CaptureRequest.JPEG_QUALITY, exifData.jpegQuality);
        builder.set(CaptureRequest.JPEG_THUMBNAIL_QUALITY,
                exifData.thumbnailQuality);

        // Validate request set and get.
        collector.expectEquals(""JPEG thumbnail size request set and get should match"",
                thumbnailSize, builder.get(CaptureRequest.JPEG_THUMBNAIL_SIZE));
        collector.expectTrue(""GPS locations request set and get should match."",
                areGpsFieldsEqual(exifData.gpsLocation,
                builder.get(CaptureRequest.JPEG_GPS_LOCATION)));
        collector.expectEquals(""JPEG orientation request set and get should match"",
                exifData.jpegOrientation,
                builder.get(CaptureRequest.JPEG_ORIENTATION));
        collector.expectEquals(""JPEG quality request set and get should match"",
                exifData.jpegQuality, builder.get(CaptureRequest.JPEG_QUALITY));
        collector.expectEquals(""JPEG thumbnail quality request set and get should match"",
                exifData.thumbnailQuality,
                builder.get(CaptureRequest.JPEG_THUMBNAIL_QUALITY));
    }

    /**
     * Simple validation of JPEG"	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.testcases.Camera2SurfaceViewTestCase"	"isInstantApp"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/testcases/Camera2SurfaceViewTestCase.java"	""	"public void test/*
 *.
 */

package android.hardware.camera2.cts.testcases;

import static android.hardware.camera2.cts.CameraTestUtils.*;

import static com.android.ex.camera2.blocking.BlockingStateCallback.STATE_CLOSED;
import androidx.test.InstrumentationRegistry;
import android.app.UiAutomation;

import android.content.Context;
import android.graphics.ImageFormat;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraCaptureSession.CaptureCallback;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CameraMetadata;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.cts.Camera2SurfaceViewCtsActivity;
import android.hardware.camera2.cts.Camera2ParameterizedTestCase;
import android.hardware.camera2.cts.CameraTestUtils;
import android.hardware.camera2.cts.CameraTestUtils.SimpleCaptureCallback;
import android.hardware.camera2.cts.helpers.CameraErrorCollector;
import android.hardware.camera2.cts.helpers.StaticMetadata;
import android.hardware.camera2.cts.helpers.StaticMetadata.CheckLevel;
import android.media.ImageReader;
import android.os.Handler;
import android.os.HandlerThread;
import android.os.Looper;
import android.util.Log;
import android.util.Range;
import android.util.Size;
import android.view.Surface;
import android.view.SurfaceHolder;
import android.view.View;
import android.view.WindowManager;

import androidx.test.rule.ActivityTestRule;

import com.android.ex.camera2.blocking.BlockingSessionCallback;
import com.android.ex.camera2.blocking.BlockingStateCallback;
import com.android.ex.camera2.exceptions.TimeoutRuntimeException;

import org.junit.After;
import org.junit.Before;
import org.junit.Ignore;
import org.junit.Rule;

import java.io.File;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.junit.runners.Parameterized;
import org.junit.runners.Parameterized.Parameter;
import org.junit.runners.Parameterized.Parameters;


/**
 * Camera2 Preview test case base class by using SurfaceView as rendering target.
 *
 * <p>This class encapsulates the SurfaceView based preview common functionalities.
 * The setup and teardown of CameraManager, test HandlerThread, Activity, Camera IDs
 * and CameraStateCallback are handled in this class. Some basic preview related utility
 * functions are provided to facilitate the derived preview-based test classes.
 * </p>
 */

public class Camera2SurfaceViewTestCase extends Camera2ParameterizedTestCase {
    private static final String TAG = ""SurfaceViewTestCase"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);
    private static final int WAIT_FOR_SURFACE_CHANGE_TIMEOUT_MS = 1000;

    protected static final int WAIT_FOR_RESULT_TIMEOUT_MS = 3000;
    protected static final float FRAME_DURATION_ERROR_MARGIN = 0.01f; // 1 percent error margin.
    protected static final int NUM_RESULTS_WAIT_TIMEOUT = 100;
    protected static final int NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY = 8;
    protected static final int MIN_FRAME_DURATION_ERROR_MARGIN = 100; // ns

    protected HandlerThread mHandlerThread;
    protected Handler mHandler;
    protected BlockingStateCallback mCameraListener;
    protected BlockingSessionCallback mSessionListener;
    protected CameraErrorCollector mCollector;
    protected HashMap<String, StaticMetadata> mAllStaticInfo;
    // Per device fields:
    protected StaticMetadata mStaticInfo;
    protected CameraDevice mCamera;
    protected CameraCaptureSession mSession;
    protected ImageReader mReader;
    protected Surface mReaderSurface;
    protected Surface mPreviewSurface;
    protected SurfaceHolder mPreviewHolder;
    protected Size mPreviewSize;
    protected List<Size> mOrderedPreviewSizes; // In descending order.
    protected List<Size> m1080pBoundedOrderedPreviewSizes; // In descending order.
    protected List<Size> mOrderedVideoSizes; // In descending order.
    protected List<Size> mOrderedStillSizes; // In descending order.
    protected HashMap<Size, Long> mMinPreviewFrameDurationMap;
    protected String mDebugFileNameBase;

    protected WindowManager mWindowManager;

    @Rule
    public ActivityTestRule<Camera2SurfaceViewCtsActivity> mActivityRule =
            new ActivityTestRule<>(Camera2SurfaceViewCtsActivity.class);

    @Before
    public void setUp() throws Exception {
        super.setUp();
        mHandlerThread = new HandlerThread(TAG);
        mHandlerThread.start();
        mHandler = new Handler(mHandlerThread.getLooper());
        mCameraListener = new BlockingStateCallback();
        mCollector = new CameraErrorCollector();

        File filesDir = mContext.getPackageManager().isInstantApp()
                ? mContext.getFilesDir()
                : mContext.getExternalFilesDir(null);

        mDebugFileNameBase = filesDir.getPath();

        mAllStaticInfo = new HashMap<String, StaticMetadata>();
        List<String> hiddenPhysicalIds = new ArrayList<>();
        for (String cameraId : mCameraIdsUnderTest) {
            CameraCharacteristics props = mCameraManager.getCameraCharacteristics(cameraId);
            StaticMetadata staticMetadata = new StaticMetadata(props,
                    CheckLevel.ASSERT, /*collector*/null);
            mAllStaticInfo.put(cameraId, staticMetadata);

            for (String physicalId : props.getPhysicalCameraIds()) {
                if (!Arrays.asList(mCameraIdsUnderTest).contains(physicalId) &&
                        !hiddenPhysicalIds.contains(physicalId)) {
                    hiddenPhysicalIds.add(physicalId);
                    props = mCameraManager.getCameraCharacteristics(physicalId);
                    staticMetadata = new StaticMetadata(
                            mCameraManager.getCameraCharacteristics(physicalId),
                            CheckLevel.ASSERT, /*collector*/null);
                    mAllStaticInfo.put(physicalId, staticMetadata);
                }
            }
        }

        mWindowManager = (WindowManager) mContext.getSystemService(Context.WINDOW_SERVICE);
    }

    @After
    public void tearDown() throws Exception {
        mHandlerThread.quitSafely();
        mHandler = null;
        mCameraListener = null;

        try {
            mCollector.verify();
        } catch (Throwable e) {
            // When new Exception(e) is used, exception info will be printed twice.
            throw new Exception(e.getMessage());
        }
        super.tearDown();
    }

    /**
     * Start camera preview by using the given request, preview size and capture
     * listener.
     * <p>
     * If preview is already started, calling this function will stop the
     * current preview stream and start a new preview stream with given
     * parameters. No need to call stopPreview between two startPreview calls.
     * </p>
     *
     * @param request The request builder used to start the preview.
     * @param previewSz The size of the camera device output preview stream.
     * @param listener The callbacks the camera device will notify when preview
     *            capture is available.
     */
    protected void startPreview(CaptureRequest.Builder request, Size previewSz,
            CaptureCallback listener) throws Exception {
        // Update preview size.
        updatePreviewSurface(previewSz);
        if (VERBOSE) {
            Log.v(TAG, ""start preview with size "" + mPreviewSize.toString());
        }

        configurePreviewOutput(request);

        mSession.setRepeatingRequest(request.build(), listener, mHandler);
    }

    /**
     * Configure the preview output stream.
     *
     * @param request The request to be configured with preview surface
     */
    protected void configurePreviewOutput(CaptureRequest.Builder request)
            throws CameraAccessException {
        List<Surface> outputSurfaces = new ArrayList<Surface>(/*capacity*/1);
        outputSurfaces.add(mPreviewSurface);
        mSessionListener = new BlockingSessionCallback();
        mSession = configureCameraSession(mCamera, outputSurfaces, mSessionListener, mHandler);

        request.addTarget(mPreviewSurface);
    }

    /**
     * Create a {@link CaptureRequest#Builder} and add the default preview surface.
     *
     * @return The {@link CaptureRequest#Builder} to be created
     * @throws CameraAccessException When create capture request from camera fails
     */
    protected CaptureRequest.Builder createRequestForPreview() throws CameraAccessException {
        if (mPreviewSurface == null) {
            throw new IllegalStateException(
                    ""Preview surface is not set yet, call updatePreviewSurface or startPreview""
                    + ""first to set the preview surface properly."");
        }
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        requestBuilder.addTarget(mPreviewSurface);
        return requestBuilder;
    }

    /**
     * Stop preview for current camera device by closing the session.
     * Does _not_ wait for the device to go idle
     */
    protected void stopPreview() throws Exception {
        // Stop repeat, wait for captures to complete, and disconnect from surfaces
        if (mSession != null) {
            if (VERBOSE) Log.v(TAG, ""Stopping preview"");
            mSession.close();
        }
    }

    /**
     * Stop preview for current camera device by closing the session and waiting for it to close,
     * resulting in an idle device.
     */
    protected void stopPreviewAndDrain() throws Exception {
        // Stop repeat, wait for captures to complete, and disconnect from surfaces
        if (mSession != null) {
            if (VERBOSE) Log.v(TAG, ""Stopping preview and waiting for idle"");
            mSession.close();
            mSessionListener.getStateWaiter().waitForState(BlockingSessionCallback.SESSION_CLOSED,
                    /*timeoutMs*/WAIT_FOR_RESULT_TIMEOUT_MS);
        }
    }

    /**
     * Setup still (JPEG) capture configuration and start preview.
     * <p>
     * The default max number of image is set to image reader.
     * </p>
     *
     * @param previewRequest The capture request to be used for preview
     * @param stillRequest The capture request to be used for still capture
     * @param previewSz Preview size
     * @param stillSz The still capture size
     * @param resultListener Capture result listener
     * @param imageListener The still capture image listener
     */
    protected void prepareStillCaptureAndStartPreview(CaptureRequest.Builder previewRequest,
            CaptureRequest.Builder stillRequest, Size previewSz, Size stillSz,
            CaptureCallback resultListener,
            ImageReader.OnImageAvailableListener imageListener, boolean isHeic) throws Exception {
        prepareCaptureAndStartPreview(previewRequest, stillRequest, previewSz, stillSz,
                isHeic ? ImageFormat.HEIC : ImageFormat.JPEG, resultListener, MAX_READER_IMAGES,
                imageListener);
    }

    /**
     * Setup still (JPEG) capture configuration and start preview.
     *
     * @param previewRequest The capture request to be used for preview
     * @param stillRequest The capture request to be used for still capture
     * @param previewSz Preview size
     * @param stillSz The still capture size
     * @param resultListener Capture result listener
     * @param maxNumImages The max number of images set to the image reader
     * @param imageListener The still capture image listener
     */
    protected void prepareStillCaptureAndStartPreview(CaptureRequest.Builder previewRequest,
            CaptureRequest.Builder stillRequest, Size previewSz, Size stillSz,
            CaptureCallback resultListener, int maxNumImages,
            ImageReader.OnImageAvailableListener imageListener, boolean isHeic) throws Exception {
        prepareCaptureAndStartPreview(previewRequest, stillRequest, previewSz, stillSz,
                isHeic ? ImageFormat.HEIC : ImageFormat.JPEG, resultListener, maxNumImages, imageListener);
    }

    /**
     * Setup raw capture configuration and start preview.
     *
     * <p>
     * The default max number of image is set to image reader.
     * </p>
     *
     * @param previewRequest The capture request to be used for preview
     * @param rawRequest The capture request to be used for raw capture
     * @param previewSz Preview size
     * @param rawSz The raw capture size
     * @param resultListener Capture result listener
     * @param imageListener The raw capture image listener
     */
    protected void prepareRawCaptureAndStartPreview(CaptureRequest.Builder previewRequest,
            CaptureRequest.Builder rawRequest, Size previewSz, Size rawSz,
            CaptureCallback resultListener,
            ImageReader.OnImageAvailableListener imageListener) throws Exception {
        prepareCaptureAndStartPreview(previewRequest, rawRequest, previewSz, rawSz,
                ImageFormat.RAW_SENSOR, resultListener, MAX_READER_IMAGES, imageListener);
    }

    /**
     * Wait for expected result key value available in a certain number of results.
     *
     * <p>
     * Check the result immediately if numFramesWait is 0.
     * </p>
     *
     * @param listener The capture listener to get capture result
     * @param resultKey The capture result key associated with the result value
     * @param expectedValue The result value need to be waited for
     * @param numResultsWait Number of frame to wait before times out
     * @throws TimeoutRuntimeException If more than numResultsWait results are
     * seen before the result matching myRequest arrives, or each individual wait
     * for result times out after {@value #WAIT_FOR_RESULT_TIMEOUT_MS}ms.
     */
    protected static <T> void waitForResultValue(SimpleCaptureCallback listener,
            CaptureResult.Key<T> resultKey,
            T expectedValue, int numResultsWait) {
        CameraTestUtils.waitForResultValue(listener, resultKey, expectedValue,
                numResultsWait, WAIT_FOR_RESULT_TIMEOUT_MS);
    }

    /**
     * Wait for any expected result key values available in a certain number of results.
     *
     * <p>
     * Check the result immediately if numFramesWait is 0.
     * </p>
     *
     * @param listener The capture listener to get capture result.
     * @param resultKey The capture result key associated with the result value.
     * @param expectedValues The list of result value need to be waited for,
     * return immediately if the list is empty.
     * @param numResultsWait Number of frame to wait before times out.
     * @throws TimeoutRuntimeException If more than numResultsWait results are.
     * seen before the result matching myRequest arrives, or each individual wait
     * for result times out after {@value #WAIT_FOR_RESULT_TIMEOUT_MS}ms.
     */
    protected static <T> void waitForAnyResultValue(SimpleCaptureCallback listener,
            CaptureResult.Key<T> resultKey,
            List<T> expectedValues, int numResultsWait) {
        CameraTestUtils.waitForAnyResultValue(listener, resultKey, expectedValues, numResultsWait,
                WAIT_FOR_RESULT_TIMEOUT_MS);
    }

    /**
     * Submit a burst of the same capture request, then submit additional captures in order to
     * ensure that the camera will be synchronized.
     *
     * <p>
     * The additional capture count is determined by android.sync.maxLatency (or
     * a fixed {@value #NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY}) captures if maxLatency is unknown).
     * </p>
     *
     * <p>Returns the number of captures that were submitted (at least 1), which is useful
     * with {@link #waitForNumResults}.</p>
     *
     * @param request capture request to forward to {@link CameraDevice#capture}
     * @param listener request listener to forward to {@link CameraDevice#capture}
     * @param handler handler to forward to {@link CameraDevice#capture}
     *
     * @return the number of captures that were submitted
     *
     * @throws CameraAccessException if capturing failed
     */
    protected int captureRequestsSynchronizedBurst(
            CaptureRequest request, int count, CaptureCallback listener, Handler handler)
                    throws CameraAccessException {
        return captureRequestsSynchronizedImpl(request, count, listener, handler, true);
    }
    /**
     * Submit a capture once, then submit additional captures in order to ensure that
     * the camera will be synchronized.
     *
     * <p>
     * The additional capture count is determined by android.sync.maxLatency (or
     * a fixed {@value #NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY}) captures if maxLatency is unknown).
     * </p>
     *
     * <p>Returns the number of captures that were submitted (at least 1), which is useful
     * with {@link #waitForNumResults}.</p>
     *
     * @param request capture request to forward to {@link CameraDevice#capture}
     * @param listener request listener to forward to {@link CameraDevice#capture}
     * @param handler handler to forward to {@link CameraDevice#capture}
     *
     * @return the number of captures that were submitted
     *
     * @throws CameraAccessException if capturing failed
     */
    protected int captureRequestsSynchronized(
            CaptureRequest request, CaptureCallback listener, Handler handler)
                    throws CameraAccessException {
        return captureRequestsSynchronizedImpl(request, /*count*/1, listener, handler, false);
    }

    /**
     * Submit a capture {@code count} times, then submit additional captures in order to ensure that
     * the camera will be synchronized.
     *
     * <p>
     * The additional capture count is determined by android.sync.maxLatency (or
     * a fixed {@value #NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY}) captures if maxLatency is unknown).
     * </p>
     *
     * <p>Returns the number of captures that were submitted (at least 1), which is useful
     * with {@link #waitForNumResults}.</p>
     *
     * @param request capture request to forward to {@link CameraDevice#capture}
     * @param count the number of times to submit the request (minimally), must be at least 1
     * @param listener request listener to forward to {@link CameraDevice#capture}
     * @param handler handler to forward to {@link CameraDevice#capture}
     *
     * @return the number of captures that were submitted
     *
     * @throws IllegalArgumentException if {@code count} was not at least 1
     * @throws CameraAccessException if capturing failed
     */
    protected int captureRequestsSynchronized(
            CaptureRequest request, int count, CaptureCallback listener, Handler handler)
                    throws CameraAccessException {
        return captureRequestsSynchronizedImpl(request, count, listener, handler, false);
    }

    /**
     * Wait for numResultWait frames
     *
     * @param resultListener The capture listener to get capture result back.
     * @param numResultsWait Number of frame to wait
     *
     * @return the last result, or {@code null} if there was none
     */
    protected static CaptureResult waitForNumResults(SimpleCaptureCallback resultListener,
            int numResultsWait) {
        return CameraTestUtils.waitForNumResults(resultListener, numResultsWait,
                WAIT_FOR_RESULT_TIMEOUT_MS);
    }

    /**
     * Wait for enough results for settings to be applied
     *
     * @param resultListener The capture listener to get capture result back.
     * @param numResultWaitForUnknownLatency Number of frame to wait if camera device latency is
     *                                       unknown.
     */
    protected void waitForSettingsApplied(SimpleCaptureCallback resultListener,
            int numResultWaitForUnknownLatency) {
        int maxLatency = mStaticInfo.getSyncMaxLatency();
        if (maxLatency == CameraMetadata.SYNC_MAX_LATENCY_UNKNOWN) {
            maxLatency = numResultWaitForUnknownLatency;
        }
        // Wait for settings to take effect
        waitForNumResults(resultListener, maxLatency);
    }

    /**
     * Wait for AE to be stabilized before capture: CONVERGED or FLASH_REQUIRED.
     *
     * <p>Waits for {@code android.sync.maxLatency} number of results first, to make sure
     * that the result is synchronized (or {@code numResultWaitForUnknownLatency} if the latency
     * is unknown.</p>
     *
     * <p>This is a no-op for {@code LEGACY} devices since they don't report
     * the {@code aeState} result.</p>
     *
     * @param resultListener The capture listener to get capture result back.
     * @param numResultWaitForUnknownLatency Number of frame to wait if camera device latency is
     *                                       unknown.
     */
    protected void waitForAeStable(SimpleCaptureCallback resultListener,
            int numResultWaitForUnknownLatency) {
        CameraTestUtils.waitForAeStable(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY,
                mStaticInfo, WAIT_FOR_RESULT_TIMEOUT_MS, NUM_RESULTS_WAIT_TIMEOUT);
    }

    /**
     * Wait for AE to be: LOCKED
     *
     * <p>Waits for {@code android.sync.maxLatency} number of results first, to make sure
     * that the result is synchronized (or {@code numResultWaitForUnknownLatency} if the latency
     * is unknown.</p>
     *
     * <p>This is a no-op for {@code LEGACY} devices since they don't report
     * the {@code aeState} result.</p>
     *
     * @param resultListener The capture listener to get capture result back.
     * @param numResultWaitForUnknownLatency Number of frame to wait if camera device latency is
     *                                       unknown.
     */
    protected void waitForAeLocked(SimpleCaptureCallback resultListener,
            int numResultWaitForUnknownLatency) {

        waitForSettingsApplied(resultListener, numResultWaitForUnknownLatency);

        if (!mStaticInfo.isHardwareLevelAtLeastLimited()) {
            // No-op for legacy devices
            return;
        }

        List<Integer> expectedAeStates = new ArrayList<Integer>();
        expectedAeStates.add(new Integer(CaptureResult.CONTROL_AE_STATE_LOCKED));
        CameraTestUtils.waitForAnyResultValue(resultListener, CaptureResult.CONTROL_AE_STATE,
                expectedAeStates, NUM_RESULTS_WAIT_TIMEOUT, WAIT_FOR_RESULT_TIMEOUT_MS);
    }

    /**
     * Create an {@link ImageReader} object and get the surface.
     *
     * @param size The size of this ImageReader to be created.
     * @param format The format of this ImageReader to be created
     * @param maxNumImages The max number of images that can be acquired simultaneously.
     * @param listener The listener used by this ImageReader to notify callbacks.
     */
    protected void createImageReader(Size size, int format, int maxNumImages,
            ImageReader.OnImageAvailableListener listener) throws Exception {
        closeImageReader();

        ImageReader r = makeImageReader(size, format, maxNumImages, listener,
                mHandler);
        mReader = r;
        mReaderSurface = r.getSurface();
    }

    /**
     * Close the pending images then close current active {@link ImageReader} object.
     */
    protected void closeImageReader() {
        CameraTestUtils.closeImageReader(mReader);
        mReader = null;
        mReaderSurface = null;
    }

    /**
     * Close the pending images then close current active {@link ImageReader} objects.
     */
    protected void closeImageReaders(ImageReader[] readers) {
        CameraTestUtils.closeImageReaders(readers);
    }

    /**
     * Setup still capture configuration and start preview.
     *
     * @param previewRequest The capture request to be used for preview
     * @param stillRequest The capture request to be used for still capture
     * @param previewSz Preview size
     * @param captureSizes Still capture sizes
     * @param formats The single capture image formats
     * @param resultListener Capture result listener
     * @param maxNumImages The max number of images set to the image reader
     * @param imageListeners The single capture capture image listeners
     * @param isHeic HEIC still capture if true, JPEG still capture if false
     */
    protected ImageReader[] prepareStillCaptureAndStartPreview(
            CaptureRequest.Builder previewRequest, CaptureRequest.Builder stillRequest,
            Size previewSz, Size[] captureSizes, int[] formats, CaptureCallback resultListener,
            int maxNumImages, ImageReader.OnImageAvailableListener[] imageListeners,
            boolean isHeic)
            throws Exception {

        if ((captureSizes == null) || (formats == null) || (imageListeners == null) &&
                (captureSizes.length != formats.length) ||
                (formats.length != imageListeners.length)) {
            throw new IllegalArgumentException(""Invalid capture sizes/formats or image listeners!"");
        }

        if (VERBOSE) {
            Log.v(TAG, String.format(""Prepare still capture and preview (%s)"",
                    previewSz.toString()));
        }

        // Update preview size.
        updatePreviewSurface(previewSz);

        ImageReader[] readers = new ImageReader[captureSizes.length];
        List<Surface> outputSurfaces = new ArrayList<Surface>();
        outputSurfaces.add(mPreviewSurface);
        for (int i = 0; i < captureSizes.length; i++) {
            readers[i] = makeImageReader(captureSizes[i], formats[i], maxNumImages,
                    imageListeners[i], mHandler);
            outputSurfaces.add(readers[i].getSurface());
        }

        mSessionListener = new BlockingSessionCallback();
        mSession = configureCameraSession(mCamera, outputSurfaces, mSessionListener, mHandler);

        // Configure the requests.
        previewRequest.addTarget(mPreviewSurface);
        stillRequest.addTarget(mPreviewSurface);
        for (int i = 0; i < readers.length; i++) {
            stillRequest.addTarget(readers[i].getSurface());
        }

        // Start preview.
        mSession.setRepeatingRequest(previewRequest.build(), resultListener, mHandler);

        return readers;
    }

    /**
     * Open a camera device and get the StaticMetadata for a given camera id.
     *
     * @param cameraId The id of the camera device to be opened.
     */
    protected void openDevice(String cameraId) throws Exception {
        mCamera = CameraTestUtils.openCamera(
                mCameraManager, cameraId, mCameraListener, mHandler);
        mCollector.setCameraId(cameraId);
        mStaticInfo = new StaticMetadata(mCameraManager.getCameraCharacteristics(cameraId),
                CheckLevel.ASSERT, /*collector*/null);
        if (mStaticInfo.isColorOutputSupported()) {
            mOrderedPreviewSizes = getSupportedPreviewSizes(cameraId, mCameraManager,
                    getPreviewSizeBound(mWindowManager, PREVIEW_SIZE_BOUND));
            m1080pBoundedOrderedPreviewSizes = getSupportedPreviewSizes(cameraId, mCameraManager,
                    PREVIEW_SIZE_BOUND);
            mOrderedVideoSizes = getSupportedVideoSizes(cameraId, mCameraManager, PREVIEW_SIZE_BOUND);
            mOrderedStillSizes = getSupportedStillSizes(cameraId, mCameraManager, null);
            // Use ImageFormat.YUV_420_888 for now. TODO: need figure out what's format for preview
            // in public API side.
            mMinPreviewFrameDurationMap =
                mStaticInfo.getAvailableMinFrameDurationsForFormatChecked(ImageFormat.YUV_420_888);
        }
    }

    /**
     * Close the current actively used camera device.
     */
    protected void closeDevice() {
        if (mCamera != null) {
            mCamera.close();
            mCameraListener.waitForState(STATE_CLOSED, CAMERA_CLOSE_TIMEOUT_MS);
            mCamera = null;
            mSession = null;
            mSessionListener = null;
            mStaticInfo = null;
            mOrderedPreviewSizes = null;
            mOrderedVideoSizes = null;
            mOrderedStillSizes = null;
        }
    }

    /**
     * Update the preview surface size.
     *
     * @param size The preview size to be updated.
     */
    protected void updatePreviewSurface(Size size) {
        if (size.equals(mPreviewSize) && mPreviewSurface != null) {
            Log.w(TAG, ""Skipping update preview surface size..."");
            return;
        }

        mPreviewSize = size;
        Camera2SurfaceViewCtsActivity ctsActivity = mActivityRule.getActivity();
        final SurfaceHolder holder = ctsActivity.getSurfaceView().getHolder();
        Handler handler = new Handler(Looper.getMainLooper());
        handler.post(new Runnable() {
            @Override
            public void run() {
                holder.setFixedSize(mPreviewSize.getWidth(), mPreviewSize.getHeight());
            }
        });

        boolean res = ctsActivity.waitForSurfaceSizeChanged(
                WAIT_FOR_SURFACE_CHANGE_TIMEOUT_MS, mPreviewSize.getWidth(),
                mPreviewSize.getHeight());
        assertTrue(""wait for surface change to "" + mPreviewSize.toString() + "" timed out"", res);
        mPreviewHolder = holder;
        mPreviewSurface = holder.getSurface();
        assertNotNull(""Preview surface is null"", mPreviewSurface);
        assertTrue(""Preview surface is invalid"", mPreviewSurface.isValid());
    }

    /**
     * Recreate the SurfaceView's Surface
     *
     * Hide and unhide the activity's preview SurfaceView, so that its backing Surface is
     * recreated
     */
    protected void recreatePreviewSurface() {
        Camera2SurfaceViewCtsActivity ctsActivity = mActivityRule.getActivity();
        setPreviewVisibility(View.GONE);
        boolean res = ctsActivity.waitForSurfaceState(
            WAIT_FOR_SURFACE_CHANGE_TIMEOUT_MS, /*valid*/ false);
        assertTrue(""wait for surface destroyed timed out"", res);
        setPreviewVisibility(View.VISIBLE);
        res = ctsActivity.waitForSurfaceState(
            WAIT_FOR_SURFACE_CHANGE_TIMEOUT_MS, /*valid*/ true);
        assertTrue(""wait for surface created timed out"", res);
    }

    /**
     * Show/hide the preview SurfaceView.
     *
     * If set to View.GONE, the surfaceDestroyed callback will fire
     * @param visibility the new new visibility to set, one of View.VISIBLE / INVISIBLE / GONE
     */
    protected void setPreviewVisibility(int visibility) {
        final Camera2SurfaceViewCtsActivity ctsActivity = mActivityRule.getActivity();
        Handler handler = new Handler(Looper.getMainLooper());
        handler.post(new Runnable() {
            @Override
            public void run() {
                ctsActivity.getSurfaceView().setVisibility(visibility);
            }
        });
    }

    /**
     * Setup single capture configuration and start preview.
     *
     * @param previewRequest The capture request to be used for preview
     * @param stillRequest The capture request to be used for still capture
     * @param previewSz Preview size
     * @param captureSz Still capture size
     * @param format The single capture image format
     * @param resultListener Capture result listener
     * @param maxNumImages The max number of images set to the image reader
     * @param imageListener The single capture capture image listener
     */
    protected void prepareCaptureAndStartPreview(CaptureRequest.Builder previewRequest,
            CaptureRequest.Builder stillRequest, Size previewSz, Size captureSz, int format,
            CaptureCallback resultListener, int maxNumImages,
            ImageReader.OnImageAvailableListener imageListener) throws Exception {
        prepareCaptureAndStartPreview(previewRequest, stillRequest, previewSz, captureSz,
            format, resultListener, null, maxNumImages, imageListener);
    }

    /**
     * Setup single capture configuration and start preview.
     *
     * @param previewRequest The capture request to be used for preview
     * @param stillRequest The capture request to be used for still capture
     * @param previewSz Preview size
     * @param captureSz Still capture size
     * @param format The single capture image format
     * @param resultListener Capture result listener
     * @param sessionListener Session listener
     * @param maxNumImages The max number of images set to the image reader
     * @param imageListener The single capture capture image listener
     */
    protected void prepareCaptureAndStartPreview(CaptureRequest.Builder previewRequest,
            CaptureRequest.Builder stillRequest, Size previewSz, Size captureSz, int format,
            CaptureCallback resultListener, CameraCaptureSession.StateCallback sessionListener,
            int maxNumImages, ImageReader.OnImageAvailableListener imageListener) throws Exception {
        if (VERBOSE) {
            Log.v(TAG, String.format(""Prepare single capture (%s) and preview (%s)"",
                    captureSz.toString(), previewSz.toString()));
        }

        // Update preview size.
        updatePreviewSurface(previewSz);

        // Create ImageReader.
        createImageReader(captureSz, format, maxNumImages, imageListener);

        // Configure output streams with preview and jpeg streams.
        List<Surface> outputSurfaces = new ArrayList<Surface>();
        outputSurfaces.add(mPreviewSurface);
        outputSurfaces.add(mReaderSurface);
        if (sessionListener == null) {
            mSessionListener = new BlockingSessionCallback();
        } else {
            mSessionListener = new BlockingSessionCallback(sessionListener);
        }
        mSession = configureCameraSession(mCamera, outputSurfaces, mSessionListener, mHandler);

        // Configure the requests.
        previewRequest.addTarget(mPreviewSurface);
        stillRequest.addTarget(mPreviewSurface);
        stillRequest.addTarget(mReaderSurface);

        // Start preview.
        mSession.setRepeatingRequest(previewRequest.build(), resultListener, mHandler);
    }

    /**
     * Get the max preview size that supports the given fpsRange.
     *
     * @param fpsRange The fps range the returned size must support.
     * @return max size that support the given fps range.
     */
    protected Size getMaxPreviewSizeForFpsRange(Range<Integer> fpsRange) {
        if (fpsRange == null || fpsRange.getLower() <= 0 || fpsRange.getUpper() <= 0) {
            throw new IllegalArgumentException(""Invalid fps range argument"");
        }
        if (mOrderedPreviewSizes == null || mMinPreviewFrameDurationMap == null) {
            throw new IllegalStateException(""mOrderedPreviewSizes and mMinPreviewFrameDurationMap""
                    + "" must be initialized"");
        }

        long[] frameDurationRange =
                new long[]{(long) (1e9 / fpsRange.getUpper()), (long) (1e9 / fpsRange.getLower())};
        for (Size size : mOrderedPreviewSizes) {
            Long minDuration = mMinPreviewFrameDurationMap.get(size);
            if (minDuration == null ||
                    minDuration == 0) {
                if (mStaticInfo.isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
                    throw new IllegalArgumentException(
                            ""No min frame duration available for the size "" + size);
                }
                continue;
            }
            if (minDuration <= (frameDurationRange[0] + MIN_FRAME_DURATION_ERROR_MARGIN)) {
                return size;
            }
        }

        // Search again for sizes not bounded by display size
        for (Size size : m1080pBoundedOrderedPreviewSizes) {
            Long minDuration = mMinPreviewFrameDurationMap.get(size);
            if (minDuration == null ||
                    minDuration == 0) {
                if (mStaticInfo.isCapabilitySupported(
                        CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
                    throw new IllegalArgumentException(
                            ""No min frame duration available for the size "" + size);
                }
                continue;
            }
            if (minDuration <= (frameDurationRange[0] + MIN_FRAME_DURATION_ERROR_MARGIN)) {
                return size;
            }
        }
        return null;
    }

    protected boolean isReprocessSupported(String cameraId, int format)
            throws CameraAccessException {
        if (format != ImageFormat.YUV_420_888 && format != ImageFormat.PRIVATE) {
            throw new IllegalArgumentException(
                    ""format "" + format + "" is not supported for reprocessing"");
        }

        StaticMetadata info =
                new StaticMetadata(mCameraManager.getCameraCharacteristics(cameraId),
                                   CheckLevel.ASSERT, /*collector*/ null);
        int cap = CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING;
        if (format == ImageFormat.PRIVATE) {
            cap = CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING;
        }
        return info.isCapabilitySupported(cap);
    }

    protected Range<Integer> getSuitableFpsRangeForDuration(String cameraId, long frameDuration) {
        return CameraTestUtils.getSuitableFpsRangeForDuration(cameraId, frameDuration, mStaticInfo);
    }

    private int captureRequestsSynchronizedImpl(
            CaptureRequest request, int count, CaptureCallback listener, Handler handler,
            boolean isBurst) throws CameraAccessException {
        if (count < 1) {
            throw new IllegalArgumentException(""count must be positive"");
        }

        int maxLatency = mStaticInfo.getSyncMaxLatency();
        if (maxLatency == CameraMetadata.SYNC_MAX_LATENCY_UNKNOWN) {
            maxLatency = NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY;
        }

        assertTrue(""maxLatency is non-negative"", maxLatency >= 0);

        int numCaptures = maxLatency + count;
        ArrayList<CaptureRequest> burstCaptureRequests = new ArrayList<>();
        for (int i = 0; i < numCaptures; ++i) {
            if (isBurst) {
                burstCaptureRequests.add(request);
            } else {
                mSession.capture(request, listener, handler);
            }
        }
        if (isBurst) {
            mSession.captureBurst(burstCaptureRequests, listener, handler);
        }

        return numCaptures;
    }
}"	""	""	"JPEG 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.orientation.CameraOrientationActivity"	"setPassFailButtonClickListeners"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/orientation/CameraOrientationActivity.java"	""	"public void test/*
 * Copyright (C) 2012 The Android Open Source Project
 *
 * Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except
 * in compliance with the License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License
 * is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
 * or implied. See the License for the specific language governing permissions and limitations under
 * the License.
 */
package com.android.cts.verifier.camera.orientation;

import android.content.Intent;
import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.graphics.ImageFormat;
import android.graphics.Matrix;
import android.hardware.Camera;
import android.os.Bundle;
import android.os.Handler;
import android.util.Log;
import android.view.SurfaceHolder;
import android.view.SurfaceView;
import android.view.View;
import android.view.View.OnClickListener;
import android.widget.Button;
import android.widget.ImageButton;
import android.widget.ImageView;
import android.widget.LinearLayout.LayoutParams;
import android.widget.TextView;

import com.android.cts.verifier.PassFailButtons;
import com.android.cts.verifier.R;
import com.android.cts.verifier.TestResult;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.List;
import java.util.TreeSet;

/**
 * Tests for manual verification of the CDD-required camera output formats
 * for preview callbacks
 */
public class CameraOrientationActivity extends PassFailButtons.Activity
implements OnClickListener, SurfaceHolder.Callback {

    private static final String TAG = ""CameraOrientation"";
    private static final int STATE_OFF = 0;
    private static final int STATE_PREVIEW = 1;
    private static final int STATE_CAPTURE = 2;
    private static final int NUM_ORIENTATIONS = 4;
    private static final String STAGE_INDEX_EXTRA = ""stageIndex"";
    private static final int VGA_WIDTH = 640;
    private static final int VGA_HEIGHT = 480;

    private ImageButton mPassButton;
    private ImageButton mFailButton;
    private Button mTakePictureButton;

    private SurfaceView mCameraView;
    private ImageView mFormatView;
    private SurfaceHolder mSurfaceHolder;
    private Camera mCamera;
    private List<Camera.Size> mPreviewSizes;
    private List<Camera.Size> mPictureSizes;
    private Camera.Size mOptimalPreviewSize;
    private Camera.Size mOptimalPictureSize;
    private List<Integer> mPreviewOrientations;
    private int mNextPreviewOrientation;
    private int mNumCameras;
    private int mCurrentCameraId = -1;
    private int mState = STATE_OFF;
    private boolean mSizeAdjusted;

    private StringBuilder mReportBuilder = new StringBuilder();
    private final TreeSet<String> mTestedCombinations = new TreeSet<String>();
    private final TreeSet<String> mUntestedCombinations = new TreeSet<String>();

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        setContentView(R.layout.co_main);
        setPassFailButtonClickListeners();
        setInfoResources(R.string.camera_orientation, R.string.co_info, -1);
        mNumCameras = Camera.getNumberOfCameras();

        mPassButton         = (ImageButton) findViewById(R.id.pass_button);
        mFailButton         = (ImageButton) findViewById(R.id.fail_button);
        mTakePictureButton  = (Button) findViewById(R.id.take_picture_button);
        mFormatView         = (ImageView) findViewById(R.id.format_view);
        mCameraView         = (SurfaceView) findViewById(R.id.camera_view);

        mFormatView.setOnClickListener(this);
        mCameraView.setOnClickListener(this);
        mTakePictureButton.setOnClickListener(this);

        mSurfaceHolder = mCameraView.getHolder();
        mSurfaceHolder.addCallback(this);

        mPreviewOrientations = new ArrayList<Integer>();
        mPreviewOrientations.add(0);
        mPreviewOrientations.add(90);
        mPreviewOrientations.add(180);
        mPreviewOrientations.add(270);

        // This activity is reused multiple times
        // to test each camera/orientation combination
        final int stageIndex = getIntent().getIntExtra(STAGE_INDEX_EXTRA, 0);
        Settings settings = getSettings(stageIndex);

        // Hitting the pass button goes to the next test activity.
        // Only the last one uses the PassFailButtons click callback function,
        // which gracefully terminates the activity.
        if (stageIndex + 1 < mNumCameras * NUM_ORIENTATIONS) {
            setPassButtonGoesToNextStage(stageIndex);
        }

        String[] availableOrientations = new String[NUM_ORIENTATIONS];
        for (int i=0; i<availableOrientations.length; i++) {
            // append degree symbol
            availableOrientations[i] = Integer.toString(i * 90) + ""\u00b0"";
        }

        resetButtons();

        // Set initial values
        mSizeAdjusted = false;
        mCurrentCameraId = settings.mCameraId;
        TextView cameraLabel = (TextView) findViewById(R.id.camera_text);
        cameraLabel.setText(
                getString(R.string.co_camera_label)
                + "" "" + (mCurrentCameraId+1) + "" of "" + mNumCameras);

        mNextPreviewOrientation = settings.mOrientation;
        TextView orientationLabel =
                (TextView) findViewById(R.id.orientation_text);
        orientationLabel.setText(
                getString(R.string.co_orientation_label)
                + "" ""
                + Integer.toString(mNextPreviewOrientation+1)
                + "" of ""
                + Integer.toString(NUM_ORIENTATIONS)
                + "": ""
                + mPreviewOrientations.get(mNextPreviewOrientation) + ""\u00b0""
                + "" ""
                + getString(R.string.co_orientation_direction_label)
                );

        TextView instructionLabel =
                (TextView) findViewById(R.id.instruction_text);
        instructionLabel.setText(R.string.co_instruction_text_photo_label);

        mTakePictureButton.setEnabled(false);
        setUpCamera(mCurrentCameraId);
    }

    @Override
    public void onResume() {
        super.onResume();
        setUpCamera(mCurrentCameraId);
    }

    @Override
    public void onPause() {
        super.onPause();
        shutdownCamera();
    }

    @Override
    public String getTestDetails() {
        return mReportBuilder.toString();
    }

    private void setUpCamera(int id) {
        shutdownCamera();

        Log.v(TAG, ""Setting up Camera "" + id);
        mCurrentCameraId = id;

        try {
            mCamera = Camera.open(id);
        } catch (Exception e) {
            Log.e(TAG, ""Error opening camera"");
        }

        Camera.Parameters p = mCamera.getParameters();

        class SizeCompare implements Comparator<Camera.Size> {
            @Override
            public int compare(Camera.Size lhs, Camera.Size rhs) {
                if (lhs.width < rhs.width) return -1;
                if (lhs.width > rhs.width) return 1;
                if (lhs.height < rhs.height) return -1;
                if (lhs.height > rhs.height) return 1;
                return 0;
            }
        }
        SizeCompare s = new SizeCompare();
        TreeSet<Camera.Size> sortedResolutions = new TreeSet<Camera.Size>(s);

        // Get preview resolutions
        List<Camera.Size> unsortedSizes = p.getSupportedPreviewSizes();
        sortedResolutions.addAll(unsortedSizes);
        mPreviewSizes = new ArrayList<Camera.Size>(sortedResolutions);

        // Get picture resolutions
        unsortedSizes = p.getSupportedPictureSizes();
        sortedResolutions.clear();
        sortedResolutions.addAll(unsortedSizes);
        mPictureSizes = new ArrayList<Camera.Size>(sortedResolutions);

        startPreview();
    }

    private void shutdownCamera() {
        if (mCamera != null) {
            mCamera.setPreviewCallback(null);
            mCamera.stopPreview();
            mCamera.release();
            mCamera = null;
            mState = STATE_OFF;
        }
    }

    private void startPreview() {
        if (mState != STATE_OFF) {
            // Stop for a while to drain callbacks
            mCamera.setPreviewCallback(null);
            mCamera.stopPreview();
            mState = STATE_OFF;
            Handler h = new Handler();
            Runnable mDelayedPreview = new Runnable() {
                @Override
                public void run() {
                    startPreview();
                }
            };
            h.postDelayed(mDelayedPreview, 300);
            return;
        }

        mCamera.setPreviewCallback(mPreviewCallback);

        try {
            mCamera.setPreviewDisplay(mCameraView.getHolder());
        } catch (IOException ioe) {
            Log.e(TAG, ""Unable to connect camera to display"");
        }

        Camera.Parameters p = mCamera.getParameters();
        Log.v(TAG, ""Initializing picture format"");
        p.setPictureFormat(ImageFormat.JPEG);
        mOptimalPictureSize = getOptimalSize(mPictureSizes, VGA_WIDTH, VGA_HEIGHT);
        Log.v(TAG, ""Initializing picture size to ""
                + mOptimalPictureSize.width + ""x"" + mOptimalPictureSize.height);
        p.setPictureSize(mOptimalPictureSize.width, mOptimalPictureSize.height);
        mOptimalPreviewSize = getOptimalSize(mPreviewSizes, VGA_WIDTH, VGA_HEIGHT);
        Log.v(TAG, ""Initializing preview size to ""
                + mOptimalPreviewSize.width + ""x"" + mOptimalPreviewSize.height);
        p.setPreviewSize(mOptimalPreviewSize.width, mOptimalPreviewSize.height);

        Log.v(TAG, ""Setting camera parameters"");
        mCamera.setParameters(p);
        Log.v(TAG, ""Setting color filter"");
        mFormatView.setColorFilter(null);
        Log.v(TAG, ""Starting preview"");
        try {
            mCamera.startPreview();
        } catch (Exception e) {
            Log.d(TAG, ""Cannot start preview"", e);
        }

        // set preview orientation
        int degrees = mPreviewOrientations.get(mNextPreviewOrientation);
        mCamera.setDisplayOrientation(degrees);

        android.hardware.Camera.CameraInfo info =
                new android.hardware.Camera.CameraInfo();
        android.hardware.Camera.getCameraInfo(mCurrentCameraId, info);
        if (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {
            TextView cameraExtraLabel =
                    (TextView) findViewById(R.id.instruction_extra_text);
            cameraExtraLabel.setText(
                    getString(R.string.co_instruction_text_extra_label));
        }

        mState = STATE_PREVIEW;
    }

    @Override
    public void onClick(View view) {
        Log.v(TAG, ""Click detected"");

        if (view == mFormatView || view == mTakePictureButton) {
            if(mState == STATE_PREVIEW) {
                mTakePictureButton.setEnabled(false);
                Log.v(TAG, ""Taking picture"");
                mCamera.takePicture(null, null, null, mCameraCallback);
                mState = STATE_CAPTURE;
            }
        }

        if(view == mPassButton || view == mFailButton) {
            final int stageIndex =
                    getIntent().getIntExtra(STAGE_INDEX_EXTRA, 0);
            String[] cameraNames = new String[mNumCameras];
            int counter = 0;
            for (int i = 0; i < mNumCameras; i++) {
                cameraNames[i] = ""Camera "" + i;

                for(int j = 0; j < mPreviewOrientations.size(); j++) {
                    String combination = cameraNames[i] + "", ""
                            + mPreviewOrientations.get(j)
                            + ""\u00b0""
                            + ""\n"";

                    if(counter < stageIndex) {
                        // test already passed, or else wouldn't have made
                        // it to current stageIndex
                        mTestedCombinations.add(combination);
                    }

                    if(counter == stageIndex) {
                        // current test configuration
                        if(view == mPassButton) {
                            mTestedCombinations.add(combination);
                        }
                        else if(view == mFailButton) {
                            mUntestedCombinations.add(combination);
                        }
                    }

                    if(counter > stageIndex) {
                        // test not passed yet, since haven't made it to
                        // stageIndex
                        mUntestedCombinations.add(combination);
                    }

                    counter++;
                }
            }

            mReportBuilder = new StringBuilder();
            mReportBuilder.append(""Passed combinations:\n"");
            for (String combination : mTestedCombinations) {
                mReportBuilder.append(combination);
            }
            mReportBuilder.append(""Failed/untested combinations:\n"");
            for (String combination : mUntestedCombinations) {
                mReportBuilder.append(combination);
            }

            if(view == mPassButton) {
                TestResult.setPassedResult(this, ""CameraOrientationActivity"",
                        getTestDetails());
            }
            if(view == mFailButton) {
                TestResult.setFailedResult(this, ""CameraOrientationActivity"",
                        getTestDetails());
            }

            // restart activity to test next orientation
            Intent intent = new Intent(CameraOrientationActivity.this,
                    CameraOrientationActivity.class);
            intent.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP
                    | Intent.FLAG_ACTIVITY_FORWARD_RESULT);
            intent.putExtra(STAGE_INDEX_EXTRA, stageIndex + 1);
            startActivity(intent);
        }
    }

    private void resetButtons() {
        enablePassFailButtons(false);
    }

    private void enablePassFailButtons(boolean enable) {
        mPassButton.setEnabled(enable);
        mFailButton.setEnabled(enable);
    }

    // find a supported size with ratio less than tolerance threshold, and
    // which is closest to height and width of given dimensions without
    // being larger than either of given dimensions
    private Camera.Size getOptimalSize(List<Camera.Size> sizes, int w,
            int h) {
        final double ASPECT_TOLERANCE = 0.1;
        double targetRatio = (double) w / (double) h;
        if (sizes == null) return null;

        Camera.Size optimalSize = null;
        int minDiff = Integer.MAX_VALUE;
        int curDiff;

        int targetHeight = h;
        int targetWidth = w;

        boolean aspectRatio = true;
        boolean maintainCeiling = true;
        while(true) {
            for (Camera.Size size : sizes) {
                if(aspectRatio) {
                    double ratio = (double) size.width / size.height;
                    if (Math.abs(ratio - targetRatio) > ASPECT_TOLERANCE) {
                        continue;
                    }
                }
                curDiff = Math.abs(size.height - targetHeight) +
                        Math.abs(size.width - targetWidth);
                if (maintainCeiling && curDiff < minDiff
                        && size.height <= targetHeight
                        && size.width <= targetWidth) {
                    optimalSize = size;
                    minDiff = curDiff;
                } else if (maintainCeiling == false
                               && curDiff < minDiff) {
                    //try to get as close as possible
                    optimalSize = size;
                    minDiff = curDiff;
                }
            }
            if (optimalSize == null && aspectRatio == true) {
                // Cannot find a match, so repeat search and
                // ignore aspect ratio requirement
                aspectRatio = false;
            } else if (maintainCeiling == true) {
                //Camera resolutions are greater than ceiling provided
                //lets try to get as close as we can
                maintainCeiling = false;
            } else {
                break;
            }
        }

        return optimalSize;
    }

    @Override
    public void surfaceChanged(SurfaceHolder holder, int format, int width,
            int height) {
        startPreview();
    }

    private void setTestedConfiguration(int cameraId, int orientation) {
        String combination = ""Camera "" + cameraId + "", ""
                + orientation
                + ""\u00b0""
                + ""\n"";
        if (!mTestedCombinations.contains(combination)) {
            mTestedCombinations.add(combination);
            mUntestedCombinations.remove(combination);
        }
    }

    @Override
    public void surfaceCreated(SurfaceHolder holder) {
        // Auto-generated method stub
    }

    @Override
    public void surfaceDestroyed(SurfaceHolder holder) {
        // Auto-generated method stub
    }

    private final Camera.PreviewCallback mPreviewCallback =
            new Camera.PreviewCallback() {
        @Override
        public void onPreviewFrame(byte[] data, Camera camera) {
            // adjust camera preview to match output image's aspect ratio
            if(!mSizeAdjusted && mState == STATE_PREVIEW) {
                int viewWidth = mFormatView.getWidth();
                int viewHeight = mFormatView.getHeight();
                int newWidth, newHeight;

                if (viewWidth == 0 || viewHeight == 0){
                    return;
                }

                if (mPreviewOrientations.get(mNextPreviewOrientation) == 0
                    || mPreviewOrientations.get(mNextPreviewOrientation) == 180) {
                    // make preview width same as output image width,
                    // then calculate height using output image's height/width ratio
                    newWidth = viewWidth;
                    newHeight = (int) (viewWidth * ((double) mOptimalPreviewSize.height /
                            (double) mOptimalPreviewSize.width));
                }
                else {
                    newHeight = viewHeight;
                    newWidth = (int) (viewHeight * ((double) mOptimalPreviewSize.height /
                            (double) mOptimalPreviewSize.width));
                }

                LayoutParams layoutParams = new LayoutParams(newWidth, newHeight);
                mCameraView.setLayoutParams(layoutParams);
                mSizeAdjusted = true;
                mTakePictureButton.setEnabled(true);
            }
        }
    };

    private final Camera.PictureCallback mCameraCallback =
            new Camera.PictureCallback() {
        @Override
        public void onPictureTaken(byte[] data, Camera mCamera) {
            if (data != null) {
                Bitmap inputImage;
                inputImage = BitmapFactory.decodeByteArray(data, 0, data.length);

                int degrees = mPreviewOrientations.get(mNextPreviewOrientation);
                android.hardware.Camera.CameraInfo info =
                        new android.hardware.Camera.CameraInfo();
                android.hardware.Camera.getCameraInfo(mCurrentCameraId, info);
                float mirrorX[];
                if (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {
                    // mirror the image along vertical axis
                    mirrorX = new float[] {-1, 0, 0, 0, 1, 1, 0, 0, 1};
                    degrees = (360 - degrees) % 360; // compensate the mirror
                } else {
                    // leave image the same via identity matrix
                    mirrorX = new float[] {1, 0, 0, 0, 1, 0, 0, 0, 1};
                }

                // use matrix to transform the image
                Matrix matrixMirrorX = new Matrix();
                matrixMirrorX.setValues(mirrorX);
                Matrix mat = new Matrix();
                mat.postRotate(degrees);
                mat.postConcat(matrixMirrorX);

                Bitmap inputImageAdjusted = Bitmap.createBitmap(inputImage,
                        0,
                        0,
                        inputImage.getWidth(),
                        inputImage.getHeight(),
                        mat,
                        true);
                mFormatView.setImageBitmap(inputImageAdjusted);

                Log.v(TAG, ""Output image set"");
                enablePassFailButtons(true);

                TextView instructionLabel =
                        (TextView) findViewById(R.id.instruction_text);
                instructionLabel.setText(
                        R.string.co_instruction_text_passfail_label);
            }

            startPreview();
        }
    };

    private void setPassButtonGoesToNextStage(final int stageIndex) {
        findViewById(R.id.pass_button).setOnClickListener(this);
    }

    private Settings getSettings(int stageIndex) {
        int curCameraId = stageIndex / NUM_ORIENTATIONS;
        int curOrientation = stageIndex % NUM_ORIENTATIONS;
        return new Settings(stageIndex, curCameraId, curOrientation);
    }

    // Bundle of settings for testing a particular
    // camera/orientation combination
    class Settings {
        int mCameraId;
        int mOrientation;

        Settings(int stageIndex, int cameraId, int orientation) {
            mCameraId = cameraId;
            mOrientation = orientation;
        }
    }
}"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RecordingTest"	"testMediaCodecRecording"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RecordingTest.java"	""	"public void testMediaCodecRecording() throws Exception {
        // TODO. Need implement.
    }

    /**
     * <p>
     * Test video snapshot for each camera.
     * </p>
     * <p>
     * This test covers video snapshot typical use case. The MediaRecorder is used to record the
     * video for each available video size. The largest still capture size is selected to
     * capture the JPEG image. The still capture images are validated according to the capture
     * configuration. The timestamp of capture result before and after video snapshot is also
     * checked to make sure no frame drop caused by video snapshot.
     * </p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RecordingTest"	"testVideoSnapshot"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RecordingTest.java"	""	"(timeout=60*60*1000) // timeout = 60 mins for long running tests
    public void testVideoSnapshot() throws Exception {
        videoSnapshotHelper(/*burstTest*/false);
    }

    /**
     * <p>
     * Test burst video snapshot for each camera.
     * </p>
     * <p>
     * This test covers burst video snapshot capture. The MediaRecorder is used to record the
     * video for each available video size. The largest still capture size is selected to
     * capture the JPEG image. {@value #BURST_VIDEO_SNAPSHOT_NUM} video snapshot requests will be
     * sent during the test. The still capture images are validated according to the capture
     * configuration.
     * </p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RecordingTest"	"testRecordingWithDifferentPreviewSizes"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RecordingTest.java"	""	"public void testRecordingWithDifferentPreviewSizes() throws Exception {
        if (!MediaUtils.checkCodecForDomain(true /* encoder */, ""video"")) {
            return; // skipped
        }
        mPersistentSurface = MediaCodec.createPersistentInputSurface();
        assertNotNull(""Failed to create persistent input surface!"", mPersistentSurface);

        try {
            doRecordingWithDifferentPreviewSizes();
        } finally {
            mPersistentSurface.release();
            mPersistentSurface = null;
        }
    }

    public void doRecordingWithDifferentPreviewSizes() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing recording with different preview sizes for camera "" +
                        mCameraIdsUnderTest[i]);
                StaticMetadata staticInfo = mAllStaticInfo.get(mCameraIdsUnderTest[i]);
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                if (staticInfo.isExternalCamera()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support CamcorderProfile, skipping"");
                    continue;
                }
                // Re-use the MediaRecorder object for the same camera device.
                mMediaRecorder = new MediaRecorder();
                openDevice(mCameraIdsUnderTest[i]);

                initSupportedVideoSize(mCameraIdsUnderTest[i]);

                Size maxPreviewSize = mOrderedPreviewSizes.get(0);
                List<Range<Integer> > fpsRanges = Arrays.asList(
                        mStaticInfo.getAeAvailableTargetFpsRangesChecked());
                int cameraId = Integer.valueOf(mCamera.getId());
                int maxVideoFrameRate = -1;
                for (int profileId : mCamcorderProfileList) {
                    if (!CamcorderProfile.hasProfile(cameraId, profileId)) {
                        continue;
                    }
                    CamcorderProfile profile = CamcorderProfile.get(cameraId, profileId);

                    Size videoSz = new Size(profile.videoFrameWidth, profile.videoFrameHeight);
                    Range<Integer> fpsRange = new Range(
                            profile.videoFrameRate, profile.videoFrameRate);
                    if (maxVideoFrameRate < profile.videoFrameRate) {
                        maxVideoFrameRate = profile.videoFrameRate;
                    }

                    if (allowedUnsupported(cameraId, profileId)) {
                        continue;
                    }

                    if (mStaticInfo.isHardwareLevelLegacy() &&
                            (videoSz.getWidth() > maxPreviewSize.getWidth() ||
                             videoSz.getHeight() > maxPreviewSize.getHeight())) {
                        // Skip. Legacy mode can only do recording up to max preview size
                        continue;
                    }
                    assertTrue(""Video size "" + videoSz.toString() + "" for profile ID "" + profileId +
                                    "" must be one of the camera device supported video size!"",
                                    mSupportedVideoSizes.contains(videoSz));
                    assertTrue(""Frame rate range "" + fpsRange + "" (for profile ID "" + profileId +
                            "") must be one of the camera device available FPS range!"",
                            fpsRanges.contains(fpsRange));

                    // Configure preview and recording surfaces.
                    mOutMediaFileName = mDebugFileNameBase + ""/test_video_surface_reconfig.mp4"";

                    // prepare preview surface by using video size.
                    List<Size> previewSizes = getPreviewSizesForVideo(videoSz,
                            profile.videoFrameRate);
                    if (previewSizes.size() <= 1) {
                        continue;
                    }

                    // 1. Do video recording using largest compatbile preview sizes
                    prepareRecordingWithProfile(profile);
                    updatePreviewSurface(previewSizes.get(0));
                    SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
                    startRecording(
                            /* useMediaRecorder */true, resultListener,
                            /*useVideoStab*/false, fpsRange, false);
                    SystemClock.sleep(RECORDING_DURATION_MS);
                    stopRecording(/* useMediaRecorder */true, /* useIntermediateSurface */false,
                            /* stopStreaming */false);

                    // 2. Reconfigure with the same recording surface, but switch to a smaller
                    // preview size.
                    prepareRecordingWithProfile(profile);
                    updatePreviewSurface(previewSizes.get(1));
                    SimpleCaptureCallback resultListener2 = new SimpleCaptureCallback();
                    startRecording(
                            /* useMediaRecorder */true, resultListener2,
                            /*useVideoStab*/false, fpsRange, false);
                    SystemClock.sleep(RECORDING_DURATION_MS);
                    stopRecording(/* useMediaRecorder */true);
                    break;
                }
            } finally {
                closeDevice();
                releaseRecorder();
            }
        }
    }

    /**
     * Test camera preview and video surface sharing for maximum supported size.
     */
    private void videoPreviewSurfaceSharingTestByCamera() throws Exception {
        for (Size sz : mOrderedPreviewSizes) {
            if (!isSupported(sz, VIDEO_FRAME_RATE, VIDEO_FRAME_RATE)) {
                continue;
            }

            if (VERBOSE) {
                Log.v(TAG, ""Testing camera recording with video size "" + sz.toString());
            }

            // Configure preview and recording surfaces.
            mOutMediaFileName = mDebugFileNameBase + ""/test_video_share.mp4"";
            if (DEBUG_DUMP) {
                mOutMediaFileName = mDebugFileNameBase + ""/test_video_share_"" + mCamera.getId() +
                    ""_"" + sz.toString() + "".mp4"";
            }

            // Allow external camera to use variable fps range
            Range<Integer> fpsRange = null;
            if (mStaticInfo.isExternalCamera()) {
                Range<Integer>[] availableFpsRange =
                        mStaticInfo.getAeAvailableTargetFpsRangesChecked();

                boolean foundRange = false;
                int minFps = 0;
                for (int i = 0; i < availableFpsRange.length; i += 1) {
                    if (minFps < availableFpsRange[i].getLower()
                            && VIDEO_FRAME_RATE == availableFpsRange[i].getUpper()) {
                        minFps = availableFpsRange[i].getLower();
                        foundRange = true;
                    }
                }
                assertTrue(""Cannot find FPS range for maxFps "" + VIDEO_FRAME_RATE, foundRange);
                fpsRange = Range.create(minFps, VIDEO_FRAME_RATE);
            }

            // Use AVC and AAC a/v compression format.
            prepareRecording(sz, VIDEO_FRAME_RATE, VIDEO_FRAME_RATE);

            // prepare preview surface by using video size.
            updatePreviewSurfaceWithVideo(sz, VIDEO_FRAME_RATE);

            // Start recording
            SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
            if (!startSharedRecording(/* useMediaRecorder */true, resultListener,
                    /*useVideoStab*/false, fpsRange)) {
                mMediaRecorder.reset();
                continue;
            }

            // Record certain duration.
            SystemClock.sleep(RECORDING_DURATION_MS);

            // Stop recording and preview
            stopRecording(/* useMediaRecorder */true);
            // Convert number of frames camera produced into the duration in unit of ms.
            float frameDurationMinMs = 1000.0f / VIDEO_FRAME_RATE;
            float durationMinMs = resultListener.getTotalNumFrames() * frameDurationMinMs;
            float durationMaxMs = durationMinMs;
            float frameDurationMaxMs = 0.f;
            if (fpsRange != null) {
                frameDurationMaxMs = 1000.0f / fpsRange.getLower();
                durationMaxMs = resultListener.getTotalNumFrames() * frameDurationMaxMs;
            }

            // Validation.
            validateRecording(sz, durationMinMs, durationMaxMs,
                    frameDurationMinMs, frameDurationMaxMs,
                    FRMDRP_RATE_TOLERANCE);

            break;
        }
    }

    /**
     * Test slow motion recording where capture rate (camera output) is different with
     * video (playback) frame rate for each camera if high speed recording is supported
     * by both camera and encoder.
     *
     * <p>
     * Normal recording use cases make the capture rate (camera output frame
     * rate) the same as the video (playback) frame rate. This guarantees that
     * the motions in the scene play at the normal speed. If the capture rate is
     * faster than video frame rate, for a given time duration, more number of
     * frames are captured than it can be played in the same time duration. This
     * generates ""slow motion"" effect during playback.
     * </p>
     */
    private void slowMotionRecording() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.i(TAG, ""Testing slow motion recording for camera "" + id);
                StaticMetadata staticInfo = mAllStaticInfo.get(id);
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id +
                            "" does not support color outputs, skipping"");
                    continue;
                }
                if (!staticInfo.isHighSpeedVideoSupported()) {
                    continue;
                }

                // Re-use the MediaRecorder object for the same camera device.
                mMediaRecorder = new MediaRecorder();
                openDevice(id);

                StreamConfigurationMap config =
                        mStaticInfo.getValueFromKeyNonNull(
                                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
                Size[] highSpeedVideoSizes = config.getHighSpeedVideoSizes();
                for (Size size : highSpeedVideoSizes) {
                    Range<Integer> fpsRange = getHighestHighSpeedFixedFpsRangeForSize(config, size);
                    mCollector.expectNotNull(""Unable to find the fixed frame rate fps range for "" +
                            ""size "" + size, fpsRange);
                    if (fpsRange == null) {
                        continue;
                    }

                    int captureRate = fpsRange.getLower();
                    int videoFramerate = captureRate / SLOWMO_SLOW_FACTOR;
                    // Skip the test if the highest recording FPS supported by CamcorderProfile
                    if (fpsRange.getUpper() > getFpsFromHighSpeedProfileForSize(size)) {
                        Log.w(TAG, ""high speed recording "" + size + ""@"" + captureRate + ""fps""
                                + "" is not supported by CamcorderProfile"");
                        continue;
                    }

                    mOutMediaFileName = mDebugFileNameBase + ""/test_slowMo_video.mp4"";
                    if (DEBUG_DUMP) {
                        mOutMediaFileName = mDebugFileNameBase + ""/test_slowMo_video_"" + id + ""_""
                                + size.toString() + "".mp4"";
                    }

                    prepareRecording(size, videoFramerate, captureRate);

                    // prepare preview surface by using video size.
                    updatePreviewSurfaceWithVideo(size, captureRate);

                    // Start recording
                    SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
                    startSlowMotionRecording(/*useMediaRecorder*/true, videoFramerate, captureRate,
                            fpsRange, resultListener, /*useHighSpeedSession*/false);

                    // Record certain duration.
                    SystemClock.sleep(RECORDING_DURATION_MS);

                    // Stop recording and preview
                    stopRecording(/*useMediaRecorder*/true);
                    // Convert number of frames camera produced into the duration in unit of ms.
                    float frameDurationMs = 1000.0f / videoFramerate;
                    float durationMs = resultListener.getTotalNumFrames() * frameDurationMs;

                    // Validation.
                    validateRecording(size, durationMs, frameDurationMs, FRMDRP_RATE_TOLERANCE);
                }

            } finally {
                closeDevice();
                releaseRecorder();
            }
        }
    }

    private void constrainedHighSpeedRecording() throws Exception {
        for (String id : mCameraIdsUnderTest) {
            try {
                Log.i(TAG, ""Testing constrained high speed recording for camera "" + id);

                if (!mAllStaticInfo.get(id).isConstrainedHighSpeedVideoSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" doesn't support high speed recording, skipping."");
                    continue;
                }

                // Re-use the MediaRecorder object for the same camera device.
                mMediaRecorder = new MediaRecorder();
                openDevice(id);

                StreamConfigurationMap config =
                        mStaticInfo.getValueFromKeyNonNull(
                                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
                Size[] highSpeedVideoSizes = config.getHighSpeedVideoSizes();
                for (Size size : highSpeedVideoSizes) {
                    List<Range<Integer>> fixedFpsRanges =
                            getHighSpeedFixedFpsRangeForSize(config, size);
                    mCollector.expectTrue(""Unable to find the fixed frame rate fps range for "" +
                            ""size "" + size, fixedFpsRanges.size() > 0);
                    // Test recording for each FPS range
                    for (Range<Integer> fpsRange : fixedFpsRanges) {
                        int captureRate = fpsRange.getLower();
                        final int VIDEO_FRAME_RATE = 30;
                        // Skip the test if the highest recording FPS supported by CamcorderProfile
                        if (fpsRange.getUpper() > getFpsFromHighSpeedProfileForSize(size)) {
                            Log.w(TAG, ""high speed recording "" + size + ""@"" + captureRate + ""fps""
                                    + "" is not supported by CamcorderProfile"");
                            continue;
                        }

                        SimpleCaptureCallback previewResultListener = new SimpleCaptureCallback();

                        // prepare preview surface by using video size.
                        updatePreviewSurfaceWithVideo(size, captureRate);

                        startConstrainedPreview(fpsRange, previewResultListener);

                        mOutMediaFileName = mDebugFileNameBase + ""/test_cslowMo_video_"" +
                            captureRate + ""fps_"" + id + ""_"" + size.toString() + "".mp4"";

                        prepareRecording(size, VIDEO_FRAME_RATE, captureRate);

                        SystemClock.sleep(PREVIEW_DURATION_MS);

                        stopCameraStreaming();

                        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
                        // Start recording
                        startSlowMotionRecording(/*useMediaRecorder*/true, VIDEO_FRAME_RATE,
                                captureRate, fpsRange, resultListener,
                                /*useHighSpeedSession*/true);

                        // Record certain duration.
                        SystemClock.sleep(RECORDING_DURATION_MS);

                        // Stop recording and preview
                        stopRecording(/*useMediaRecorder*/true);

                        startConstrainedPreview(fpsRange, previewResultListener);

                        // Convert number of frames camera produced into the duration in unit of ms.
                        float frameDurationMs = 1000.0f / VIDEO_FRAME_RATE;
                        float durationMs = resultListener.getTotalNumFrames() * frameDurationMs;

                        // Validation.
                        validateRecording(size, durationMs, frameDurationMs, FRMDRP_RATE_TOLERANCE);

                        SystemClock.sleep(PREVIEW_DURATION_MS);

                        stopCameraStreaming();
                    }
                }

            } finally {
                closeDevice();
                releaseRecorder();
            }
        }
    }

    /**
     * Get high speed FPS from CamcorderProfiles for a given size.
     *
     * @param size The size used to search the CamcorderProfiles for the FPS.
     * @return high speed video FPS, 0 if the given size is not supported by the CamcorderProfiles.
     */
    private int getFpsFromHighSpeedProfileForSize(Size size) {
        for (int quality = CamcorderProfile.QUALITY_HIGH_SPEED_480P;
                quality <= CamcorderProfile.QUALITY_HIGH_SPEED_2160P; quality++) {
            if (CamcorderProfile.hasProfile(quality)) {
                CamcorderProfile profile = CamcorderProfile.get(quality);
                if (size.equals(new Size(profile.videoFrameWidth, profile.videoFrameHeight))){
                    return profile.videoFrameRate;
                }
            }
        }

        return 0;
    }

    private Range<Integer> getHighestHighSpeedFixedFpsRangeForSize(StreamConfigurationMap config,
            Size size) {
        Range<Integer>[] availableFpsRanges = config.getHighSpeedVideoFpsRangesFor(size);
        Range<Integer> maxRange = availableFpsRanges[0];
        boolean foundRange = false;
        for (Range<Integer> range : availableFpsRanges) {
            if (range.getLower().equals(range.getUpper()) && range.getLower() >= maxRange.getLower()) {
                foundRange = true;
                maxRange = range;
            }
        }

        if (!foundRange) {
            return null;
        }
        return maxRange;
    }

    private List<Range<Integer>> getHighSpeedFixedFpsRangeForSize(StreamConfigurationMap config,
            Size size) {
        Range<Integer>[] availableFpsRanges = config.getHighSpeedVideoFpsRangesFor(size);
        List<Range<Integer>> fixedRanges = new ArrayList<Range<Integer>>();
        for (Range<Integer> range : availableFpsRanges) {
            if (range.getLower().equals(range.getUpper())) {
                fixedRanges.add(range);
            }
        }
        return fixedRanges;
    }

    private void startConstrainedPreview(Range<Integer> fpsRange,
            CameraCaptureSession.CaptureCallback listener) throws Exception {
        List<Surface> outputSurfaces = new ArrayList<Surface>(1);
        assertTrue(""Preview surface should be valid"", mPreviewSurface.isValid());
        outputSurfaces.add(mPreviewSurface);
        mSessionListener = new BlockingSessionCallback();

        List<CaptureRequest> slowMoRequests = null;
        CaptureRequest.Builder requestBuilder =
            mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
        requestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
        requestBuilder.addTarget(mPreviewSurface);
        CaptureRequest initialRequest = requestBuilder.build();
        CameraTestUtils.checkSessionConfigurationWithSurfaces(mCamera, mHandler,
                outputSurfaces, /*inputConfig*/ null, SessionConfiguration.SESSION_HIGH_SPEED,
                /*defaultSupport*/ true, ""Constrained session configuration query failed"");
        mSession = buildConstrainedCameraSession(mCamera, outputSurfaces, mSessionListener,
                mHandler, initialRequest);
        slowMoRequests = ((CameraConstrainedHighSpeedCaptureSession) mSession).
            createHighSpeedRequestList(initialRequest);

        mSession.setRepeatingBurst(slowMoRequests, listener, mHandler);
    }

    private void startSlowMotionRecording(boolean useMediaRecorder, int videoFrameRate,
            int captureRate, Range<Integer> fpsRange,
            CameraCaptureSession.CaptureCallback listener, boolean useHighSpeedSession)
            throws Exception {
        List<Surface> outputSurfaces = new ArrayList<Surface>(2);
        assertTrue(""Both preview and recording surfaces should be valid"",
                mPreviewSurface.isValid() && mRecordingSurface.isValid());
        outputSurfaces.add(mPreviewSurface);
        outputSurfaces.add(mRecordingSurface);
        // Video snapshot surface
        if (mReaderSurface != null) {
            outputSurfaces.add(mReaderSurface);
        }
        mSessionListener = new BlockingSessionCallback();

        // Create slow motion request list
        List<CaptureRequest> slowMoRequests = null;
        if (useHighSpeedSession) {
            CaptureRequest.Builder requestBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
            requestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
            requestBuilder.addTarget(mPreviewSurface);
            requestBuilder.addTarget(mRecordingSurface);
            CaptureRequest initialRequest = requestBuilder.build();
            mSession = buildConstrainedCameraSession(mCamera, outputSurfaces, mSessionListener,
                    mHandler, initialRequest);
            slowMoRequests = ((CameraConstrainedHighSpeedCaptureSession) mSession).
                    createHighSpeedRequestList(initialRequest);
        } else {
            CaptureRequest.Builder recordingRequestBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
            recordingRequestBuilder.set(CaptureRequest.CONTROL_MODE,
                    CaptureRequest.CONTROL_MODE_USE_SCENE_MODE);
            recordingRequestBuilder.set(CaptureRequest.CONTROL_SCENE_MODE,
                    CaptureRequest.CONTROL_SCENE_MODE_HIGH_SPEED_VIDEO);

            CaptureRequest.Builder recordingOnlyBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
            recordingOnlyBuilder.set(CaptureRequest.CONTROL_MODE,
                    CaptureRequest.CONTROL_MODE_USE_SCENE_MODE);
            recordingOnlyBuilder.set(CaptureRequest.CONTROL_SCENE_MODE,
                    CaptureRequest.CONTROL_SCENE_MODE_HIGH_SPEED_VIDEO);
            int slowMotionFactor = captureRate / videoFrameRate;

            // Make sure camera output frame rate is set to correct value.
            recordingRequestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
            recordingRequestBuilder.addTarget(mRecordingSurface);
            recordingRequestBuilder.addTarget(mPreviewSurface);
            recordingOnlyBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
            recordingOnlyBuilder.addTarget(mRecordingSurface);

            CaptureRequest initialRequest = recordingRequestBuilder.build();
            mSession = configureCameraSessionWithParameters(mCamera, outputSurfaces,
                    mSessionListener, mHandler, initialRequest);

            slowMoRequests = new ArrayList<CaptureRequest>();
            slowMoRequests.add(initialRequest);// Preview + recording.

            for (int i = 0; i < slowMotionFactor - 1; i++) {
                slowMoRequests.add(recordingOnlyBuilder.build()); // Recording only.
            }
        }

        mSession.setRepeatingBurst(slowMoRequests, listener, mHandler);

        if (useMediaRecorder) {
            mMediaRecorder.start();
        } else {
            // TODO: need implement MediaCodec path.
        }

    }

    private void basicRecordingTestByCamera(int[] camcorderProfileList, boolean useVideoStab)
            throws Exception {
        basicRecordingTestByCamera(camcorderProfileList, useVideoStab, false);
    }

    private void basicRecordingTestByCamera(int[] camcorderProfileList, boolean useVideoStab,
            boolean useIntermediateSurface) throws Exception {
        basicRecordingTestByCamera(camcorderProfileList, useVideoStab,
                useIntermediateSurface, false);
    }

    /**
     * Test camera recording by using each available CamcorderProfile for a
     * given camera. preview size is set to the video size.
     */
    private void basicRecordingTestByCamera(int[] camcorderProfileList, boolean useVideoStab,
            boolean useIntermediateSurface, boolean useEncoderProfiles) throws Exception {
        Size maxPreviewSize = mOrderedPreviewSizes.get(0);
        List<Range<Integer> > fpsRanges = Arrays.asList(
                mStaticInfo.getAeAvailableTargetFpsRangesChecked());
        int cameraId = Integer.valueOf(mCamera.getId());
        int maxVideoFrameRate = -1;

        // only validate recording for non-perf measurement runs
        boolean validateRecording = !isPerfMeasure();
        for (int profileId : camcorderProfileList) {
            if (!CamcorderProfile.hasProfile(cameraId, profileId)) {
                continue;
            }

            CamcorderProfile profile = CamcorderProfile.get(cameraId, profileId);
            Size videoSz = new Size(profile.videoFrameWidth, profile.videoFrameHeight);

            Range<Integer> fpsRange = new Range(profile.videoFrameRate, profile.videoFrameRate);
            if (maxVideoFrameRate < profile.videoFrameRate) {
                maxVideoFrameRate = profile.videoFrameRate;
            }

            if (allowedUnsupported(cameraId, profileId)) {
                continue;
            }

            if (mStaticInfo.isHardwareLevelLegacy() &&
                    (videoSz.getWidth() > maxPreviewSize.getWidth() ||
                     videoSz.getHeight() > maxPreviewSize.getHeight())) {
                // Skip. Legacy mode can only do recording up to max preview size
                continue;
            }
            assertTrue(""Video size "" + videoSz.toString() + "" for profile ID "" + profileId +
                            "" must be one of the camera device supported video size!"",
                            mSupportedVideoSizes.contains(videoSz));
            assertTrue(""Frame rate range "" + fpsRange + "" (for profile ID "" + profileId +
                    "") must be one of the camera device available FPS range!"",
                    fpsRanges.contains(fpsRange));


            if (useEncoderProfiles) {
                // Iterate through all video-audio codec combination
                EncoderProfiles profiles = CamcorderProfile.getAll(mCamera.getId(), profileId);
                for (EncoderProfiles.VideoProfile videoProfile : profiles.getVideoProfiles()) {
                    boolean hasAudioProfile = false;
                    for (EncoderProfiles.AudioProfile audioProfile : profiles.getAudioProfiles()) {
                        hasAudioProfile = true;
                        doBasicRecordingByProfile(profiles, videoProfile, audioProfile,
                                useVideoStab, useIntermediateSurface, validateRecording);
                        // Only measure the default video profile of the largest video
                        // recording size when measuring perf
                        if (isPerfMeasure()) {
                            break;
                        }
                    }
                    // Timelapse profiles do not have audio track
                    if (!hasAudioProfile) {
                        doBasicRecordingByProfile(profiles, videoProfile, /* audioProfile */null,
                                useVideoStab, useIntermediateSurface, validateRecording);
                    }
                }
            } else {
                doBasicRecordingByProfile(
                        profile, useVideoStab, useIntermediateSurface, validateRecording);
            }

            if (isPerfMeasure()) {
                // Only measure the largest video recording size when measuring perf
                break;
            }
        }
        if (maxVideoFrameRate != -1) {
            // At least one CamcorderProfile is present, check FPS
            assertTrue(""At least one CamcorderProfile must support >= 24 FPS"",
                    maxVideoFrameRate >= 24);
        }
    }

    private void doBasicRecordingByProfile(
            CamcorderProfile profile, boolean userVideoStab,
            boolean useIntermediateSurface, boolean validate) throws Exception {
        Size videoSz = new Size(profile.videoFrameWidth, profile.videoFrameHeight);
        int frameRate = profile.videoFrameRate;

        if (VERBOSE) {
            Log.v(TAG, ""Testing camera recording with video size "" + videoSz.toString());
        }

        // Configure preview and recording surfaces.
        mOutMediaFileName = mDebugFileNameBase + ""/test_video.mp4"";
        if (DEBUG_DUMP) {
            mOutMediaFileName = mDebugFileNameBase + ""/test_video_"" + mCamera.getId() + ""_""
                    + videoSz.toString() + "".mp4"";
        }

        setupMediaRecorder(profile);
        completeBasicRecording(videoSz, frameRate, userVideoStab, useIntermediateSurface, validate);
    }

    private void doBasicRecordingByProfile(
            EncoderProfiles profiles,
            EncoderProfiles.VideoProfile videoProfile, EncoderProfiles.AudioProfile audioProfile,
            boolean userVideoStab, boolean useIntermediateSurface, boolean validate)
                    throws Exception {
        Size videoSz = new Size(videoProfile.getWidth(), videoProfile.getHeight());
        int frameRate = videoProfile.getFrameRate();

        if (VERBOSE) {
            Log.v(TAG, ""Testing camera recording with video size "" + videoSz.toString() +
                  "", video codec "" + videoProfile.getMediaType() + "", and audio codec "" +
                  (audioProfile == null ? ""(null)"" : audioProfile.getMediaType()));
        }

        // Configure preview and recording surfaces.
        mOutMediaFileName = mDebugFileNameBase + ""/test_video.mp4"";
        if (DEBUG_DUMP) {
            mOutMediaFileName = mDebugFileNameBase + ""/test_video_"" + mCamera.getId() + ""_""
                    + videoSz.toString() + ""_"" + videoProfile.getCodec();
            if (audioProfile != null) {
                mOutMediaFileName += ""_"" + audioProfile.getCodec();
            }
            mOutMediaFileName += "".mp4"";
        }

        setupMediaRecorder(profiles, videoProfile, audioProfile);
        completeBasicRecording(videoSz, frameRate, userVideoStab, useIntermediateSurface, validate);
    }

    private void completeBasicRecording(
            Size videoSz, int frameRate, boolean useVideoStab,
            boolean useIntermediateSurface, boolean validate) throws Exception {
        prepareRecording(useIntermediateSurface);

        // prepare preview surface by using video size.
        updatePreviewSurfaceWithVideo(videoSz, frameRate);

        // Start recording
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
        startRecording(/* useMediaRecorder */true, resultListener, useVideoStab,
                useIntermediateSurface);

        // Record certain duration.
        SystemClock.sleep(RECORDING_DURATION_MS);

        // Stop recording and preview
        stopRecording(/* useMediaRecorder */true, useIntermediateSurface,
                /* stopCameraStreaming */true);
        // Convert number of frames camera produced into the duration in unit of ms.
        float frameDurationMs = 1000.0f / frameRate;
        float durationMs = 0.f;
        if (useIntermediateSurface) {
            durationMs = mQueuer.getQueuedCount() * frameDurationMs;
        } else {
            durationMs = resultListener.getTotalNumFrames() * frameDurationMs;
        }

        if (VERBOSE) {
            Log.v(TAG, ""video frame rate: "" + frameRate +
                            "", num of frames produced: "" + resultListener.getTotalNumFrames());
        }

        if (validate) {
            validateRecording(videoSz, durationMs, frameDurationMs, FRMDRP_RATE_TOLERANCE);
        }
    }

    /**
     * Test camera recording for each supported video size by camera, preview
     * size is set to the video size.
     */
    private void recordingSizeTestByCamera() throws Exception {
        for (Size sz : mSupportedVideoSizes) {
            if (!isSupported(sz, VIDEO_FRAME_RATE, VIDEO_FRAME_RATE)) {
                continue;
            }

            if (VERBOSE) {
                Log.v(TAG, ""Testing camera recording with video size "" + sz.toString());
            }

            // Configure preview and recording surfaces.
            mOutMediaFileName = mDebugFileNameBase + ""/test_video.mp4"";
            if (DEBUG_DUMP) {
                mOutMediaFileName = mDebugFileNameBase + ""/test_video_"" + mCamera.getId() + ""_""
                        + sz.toString() + "".mp4"";
            }

            // Allow external camera to use variable fps range
            Range<Integer> fpsRange = null;
            if (mStaticInfo.isExternalCamera()) {
                Range<Integer>[] availableFpsRange =
                        mStaticInfo.getAeAvailableTargetFpsRangesChecked();

                boolean foundRange = false;
                int minFps = 0;
                for (int i = 0; i < availableFpsRange.length; i += 1) {
                    if (minFps < availableFpsRange[i].getLower()
                            && VIDEO_FRAME_RATE == availableFpsRange[i].getUpper()) {
                        minFps = availableFpsRange[i].getLower();
                        foundRange = true;
                    }
                }
                assertTrue(""Cannot find FPS range for maxFps "" + VIDEO_FRAME_RATE, foundRange);
                fpsRange = Range.create(minFps, VIDEO_FRAME_RATE);
            }

            // Use AVC and AAC a/v compression format.
            prepareRecording(sz, VIDEO_FRAME_RATE, VIDEO_FRAME_RATE);

            // prepare preview surface by using video size.
            updatePreviewSurfaceWithVideo(sz, VIDEO_FRAME_RATE);

            // Start recording
            SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
            startRecording(
                    /* useMediaRecorder */true, resultListener,
                    /*useVideoStab*/false, fpsRange, false);

            // Record certain duration.
            SystemClock.sleep(RECORDING_DURATION_MS);

            // Stop recording and preview
            stopRecording(/* useMediaRecorder */true);
            // Convert number of frames camera produced into the duration in unit of ms.
            float frameDurationMinMs = 1000.0f / VIDEO_FRAME_RATE;
            float durationMinMs = resultListener.getTotalNumFrames() * frameDurationMinMs;
            float durationMaxMs = durationMinMs;
            float frameDurationMaxMs = 0.f;
            if (fpsRange != null) {
                frameDurationMaxMs = 1000.0f / fpsRange.getLower();
                durationMaxMs = resultListener.getTotalNumFrames() * frameDurationMaxMs;
            }

            // Validation.
            validateRecording(sz, durationMinMs, durationMaxMs,
                    frameDurationMinMs, frameDurationMaxMs,
                    FRMDRP_RATE_TOLERANCE);
        }
    }

    /**
     * Initialize the supported video sizes.
     */
    private void initSupportedVideoSize(String cameraId)  throws Exception {
        int id = Integer.valueOf(cameraId);
        Size maxVideoSize = SIZE_BOUND_720P;
        if (CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_2160P)) {
            maxVideoSize = SIZE_BOUND_2160P;
        } else if (CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_QHD)) {
            maxVideoSize = SIZE_BOUND_QHD;
        } else if (CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_2K)) {
            maxVideoSize = SIZE_BOUND_2K;
        } else if (CamcorderProfile.hasProfile(id, CamcorderProfile.QUALITY_1080P)) {
            maxVideoSize = SIZE_BOUND_1080P;
        }

        mSupportedVideoSizes =
                getSupportedVideoSizes(cameraId, mCameraManager, maxVideoSize);
    }

    /**
     * Simple wrapper to wrap normal/burst video snapshot tests
     */
    private void videoSnapshotHelper(boolean burstTest) throws Exception {
            for (String id : mCameraIdsUnderTest) {
                try {
                    Log.i(TAG, ""Testing video snapshot for camera "" + id);

                    StaticMetadata staticInfo = mAllStaticInfo.get(id);
                    if (!staticInfo.isColorOutputSupported()) {
                        Log.i(TAG, ""Camera "" + id +
                                "" does not support color outputs, skipping"");
                        continue;
                    }

                    if (staticInfo.isExternalCamera()) {
                        Log.i(TAG, ""Camera "" + id +
                                "" does not support CamcorderProfile, skipping"");
                        continue;
                    }

                    // Re-use the MediaRecorder object for the same camera device.
                    mMediaRecorder = new MediaRecorder();

                    openDevice(id);

                    initSupportedVideoSize(id);

                    videoSnapshotTestByCamera(burstTest);
                } finally {
                    closeDevice();
                    releaseRecorder();
                }
            }
    }

    /**
     * Returns {@code true} if the {@link CamcorderProfile} ID is allowed to be unsupported.
     *
     * <p>This only allows unsupported profiles when using the LEGACY mode of the Camera API.</p>
     *
     * @param profileId a {@link CamcorderProfile} ID to check.
     * @return {@code true} if supported.
     */
    private boolean allowedUnsupported(int cameraId, int profileId) {
        if (!mStaticInfo.isHardwareLevelLegacy()) {
            return false;
        }

        switch(profileId) {
            case CamcorderProfile.QUALITY_2160P:
            case CamcorderProfile.QUALITY_1080P:
            case CamcorderProfile.QUALITY_HIGH:
                return !CamcorderProfile.hasProfile(cameraId, profileId) ||
                        CamcorderProfile.get(cameraId, profileId).videoFrameWidth >= 1080;
        }
        return false;
    }

    /**
     * Test video snapshot for each  available CamcorderProfile for a given camera.
     *
     * <p>
     * Preview size is set to the video size. For the burst test, frame drop and jittering
     * is not checked.
     * </p>
     *
     * @param burstTest Perform burst capture or single capture. For burst capture
     *                  {@value #BURST_VIDEO_SNAPSHOT_NUM} capture requests will be sent.
     */
    private void videoSnapshotTestByCamera(boolean burstTest)
            throws Exception {
        final int NUM_SINGLE_SHOT_TEST = 5;
        final int FRAMEDROP_TOLERANCE = 8;
        final int FRAME_SIZE_15M = 15000000;
        final float FRAME_DROP_TOLERENCE_FACTOR = 1.5f;
        int kFrameDrop_Tolerence = FRAMEDROP_TOLERANCE;

        for (int profileId : mCamcorderProfileList) {
            int cameraId = Integer.valueOf(mCamera.getId());
            if (!CamcorderProfile.hasProfile(cameraId, profileId) ||
                    allowedUnsupported(cameraId, profileId)) {
                continue;
            }

            CamcorderProfile profile = CamcorderProfile.get(cameraId, profileId);
            Size QCIF = new Size(176, 144);
            Size FULL_HD = new Size(1920, 1080);
            Size videoSz = new Size(profile.videoFrameWidth, profile.videoFrameHeight);
            Size maxPreviewSize = mOrderedPreviewSizes.get(0);

            if (mStaticInfo.isHardwareLevelLegacy() &&
                    (videoSz.getWidth() > maxPreviewSize.getWidth() ||
                     videoSz.getHeight() > maxPreviewSize.getHeight())) {
                // Skip. Legacy mode can only do recording up to max preview size
                continue;
            }

            if (!mSupportedVideoSizes.contains(videoSz)) {
                mCollector.addMessage(""Video size "" + videoSz.toString() + "" for profile ID "" +
                        profileId + "" must be one of the camera device supported video size!"");
                continue;
            }

            // For LEGACY, find closest supported smaller or equal JPEG size to the current video
            // size; if no size is smaller than the video, pick the smallest JPEG size.  The assert
            // for video size above guarantees that for LIMITED or FULL, we select videoSz here.
            // Also check for minFrameDuration here to make sure jpeg stream won't slow down
            // video capture
            Size videoSnapshotSz = mOrderedStillSizes.get(mOrderedStillSizes.size() - 1);
            // Allow a bit tolerance so we don't fail for a few nano seconds of difference
            final float FRAME_DURATION_TOLERANCE = 0.01f;
            long videoFrameDuration = (long) (1e9 / profile.videoFrameRate *
                    (1.0 + FRAME_DURATION_TOLERANCE));
            HashMap<Size, Long> minFrameDurationMap = mStaticInfo.
                    getAvailableMinFrameDurationsForFormatChecked(ImageFormat.JPEG);
            for (int i = mOrderedStillSizes.size() - 2; i >= 0; i--) {
                Size candidateSize = mOrderedStillSizes.get(i);
                if (mStaticInfo.isHardwareLevelLegacy()) {
                    // Legacy level doesn't report min frame duration
                    if (candidateSize.getWidth() <= videoSz.getWidth() &&
                            candidateSize.getHeight() <= videoSz.getHeight()) {
                        videoSnapshotSz = candidateSize;
                    }
                } else {
                    Long jpegFrameDuration = minFrameDurationMap.get(candidateSize);
                    assertTrue(""Cannot find minimum frame duration for jpeg size "" + candidateSize,
                            jpegFrameDuration != null);
                    if (candidateSize.getWidth() <= videoSz.getWidth() &&
                            candidateSize.getHeight() <= videoSz.getHeight() &&
                            jpegFrameDuration <= videoFrameDuration) {
                        videoSnapshotSz = candidateSize;
                    }
                }
            }
            Size defaultvideoSnapshotSz = videoSnapshotSz;

            /**
             * Only test full res snapshot when below conditions are all true.
             * 1. Camera is at least a LIMITED device.
             * 2. video size is up to max preview size, which will be bounded by 1080p.
             * 3. Full resolution jpeg stream can keep up to video stream speed.
             *    When full res jpeg stream cannot keep up to video stream speed, search
             *    the largest jpeg size that can susptain video speed instead.
             */
            if (mStaticInfo.isHardwareLevelAtLeastLimited() &&
                    videoSz.getWidth() <= maxPreviewSize.getWidth() &&
                    videoSz.getHeight() <= maxPreviewSize.getHeight()) {
                for (Size jpegSize : mOrderedStillSizes) {
                    Long jpegFrameDuration = minFrameDurationMap.get(jpegSize);
                    assertTrue(""Cannot find minimum frame duration for jpeg size "" + jpegSize,
                            jpegFrameDuration != null);
                    if (jpegFrameDuration <= videoFrameDuration) {
                        videoSnapshotSz = jpegSize;
                        break;
                    }
                    if (jpegSize.equals(videoSz)) {
                        throw new AssertionFailedError(
                                ""Cannot find adequate video snapshot size for video size"" +
                                        videoSz);
                    }
                }
            }

            if (videoSnapshotSz.getWidth() * videoSnapshotSz.getHeight() > FRAME_SIZE_15M)
                kFrameDrop_Tolerence = (int)(FRAMEDROP_TOLERANCE * FRAME_DROP_TOLERENCE_FACTOR);

            createImageReader(
                    videoSnapshotSz, ImageFormat.JPEG,
                    MAX_VIDEO_SNAPSHOT_IMAGES, /*listener*/null);

            // Full or better devices should support whatever video snapshot size calculated above.
            // Limited devices may only be able to support the default one.
            if (mStaticInfo.isHardwareLevelLimited()) {
                List<Surface> outputs = new ArrayList<Surface>();
                outputs.add(mPreviewSurface);
                outputs.add(mRecordingSurface);
                outputs.add(mReaderSurface);
                boolean isSupported = isStreamConfigurationSupported(
                        mCamera, outputs, mSessionListener, mHandler);
                if (!isSupported) {
                    videoSnapshotSz = defaultvideoSnapshotSz;
                    createImageReader(
                            videoSnapshotSz, ImageFormat.JPEG,
                            MAX_VIDEO_SNAPSHOT_IMAGES, /*listener*/null);
                }
            }

            if (videoSz.equals(QCIF) &&
                    ((videoSnapshotSz.getWidth() > FULL_HD.getWidth()) ||
                     (videoSnapshotSz.getHeight() > FULL_HD.getHeight()))) {
                List<Surface> outputs = new ArrayList<Surface>();
                outputs.add(mPreviewSurface);
                outputs.add(mRecordingSurface);
                outputs.add(mReaderSurface);
                boolean isSupported = isStreamConfigurationSupported(
                        mCamera, outputs, mSessionListener, mHandler);
                if (!isSupported) {
                    videoSnapshotSz = defaultvideoSnapshotSz;
                    createImageReader(
                            videoSnapshotSz, ImageFormat.JPEG,
                            MAX_VIDEO_SNAPSHOT_IMAGES, /*listener*/null);
                }
            }

            Log.i(TAG, ""Testing video snapshot size "" + videoSnapshotSz +
                    "" for video size "" + videoSz);

            if (VERBOSE) {
                Log.v(TAG, ""Testing camera recording with video size "" + videoSz.toString());
            }

            // Configure preview and recording surfaces.
            mOutMediaFileName = mDebugFileNameBase + ""/test_video.mp4"";
            if (DEBUG_DUMP) {
                mOutMediaFileName = mDebugFileNameBase + ""/test_video_"" + cameraId + ""_""
                        + videoSz.toString() + "".mp4"";
            }

            int numTestIterations = burstTest ? 1 : NUM_SINGLE_SHOT_TEST;
            int totalDroppedFrames = 0;

            for (int numTested = 0; numTested < numTestIterations; numTested++) {
                prepareRecordingWithProfile(profile);

                // prepare video snapshot
                SimpleCaptureCallback resultListener = new SimpleCaptureCallback();
                SimpleImageReaderListener imageListener = new SimpleImageReaderListener();
                CaptureRequest.Builder videoSnapshotRequestBuilder =
                        mCamera.createCaptureRequest((mStaticInfo.isHardwareLevelLegacy()) ?
                                CameraDevice.TEMPLATE_RECORD :
                                CameraDevice.TEMPLATE_VIDEO_SNAPSHOT);

                // prepare preview surface by using video size.
                updatePreviewSurfaceWithVideo(videoSz, profile.videoFrameRate);

                prepareVideoSnapshot(videoSnapshotRequestBuilder, imageListener);
                Range<Integer> fpsRange = Range.create(profile.videoFrameRate,
                        profile.videoFrameRate);
                videoSnapshotRequestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE,
                        fpsRange);
                if (mStaticInfo.isVideoStabilizationSupported()) {
                    videoSnapshotRequestBuilder.set(CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE,
                            CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE_ON);
                }
                CaptureRequest request = videoSnapshotRequestBuilder.build();

                // Start recording
                startRecording(/* useMediaRecorder */true, resultListener,
                        /*useVideoStab*/mStaticInfo.isVideoStabilizationSupported());
                long startTime = SystemClock.elapsedRealtime();

                // Record certain duration.
                SystemClock.sleep(RECORDING_DURATION_MS / 2);

                // take video snapshot
                if (burstTest) {
                    List<CaptureRequest> requests =
                            new ArrayList<CaptureRequest>(BURST_VIDEO_SNAPSHOT_NUM);
                    for (int i = 0; i < BURST_VIDEO_SNAPSHOT_NUM; i++) {
                        requests.add(request);
                    }
                    mSession.captureBurst(requests, resultListener, mHandler);
                } else {
                    mSession.capture(request, resultListener, mHandler);
                }

                // make sure recording is still going after video snapshot
                SystemClock.sleep(RECORDING_DURATION_MS / 2);

                // Stop recording and preview
                float durationMs = (float) stopRecording(/* useMediaRecorder */true);
                // For non-burst test, use number of frames to also double check video frame rate.
                // Burst video snapshot is allowed to cause frame rate drop, so do not use number
                // of frames to estimate duration
                if (!burstTest) {
                    durationMs = resultListener.getTotalNumFrames() * 1000.0f /
                        profile.videoFrameRate;
                }

                float frameDurationMs = 1000.0f / profile.videoFrameRate;
                // Validation recorded video
                validateRecording(videoSz, durationMs,
                        frameDurationMs, VID_SNPSHT_FRMDRP_RATE_TOLERANCE);

                if (burstTest) {
                    for (int i = 0; i < BURST_VIDEO_SNAPSHOT_NUM; i++) {
                        Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                        validateVideoSnapshotCapture(image, videoSnapshotSz);
                        image.close();
                    }
                } else {
                    // validate video snapshot image
                    Image image = imageListener.getImage(CAPTURE_IMAGE_TIMEOUT_MS);
                    validateVideoSnapshotCapture(image, videoSnapshotSz);

                    // validate if there is framedrop around video snapshot
                    totalDroppedFrames +=  validateFrameDropAroundVideoSnapshot(
                            resultListener, image.getTimestamp());

                    //TODO: validate jittering. Should move to PTS
                    //validateJittering(resultListener);

                    image.close();
                }
            }

            if (!burstTest) {
                Log.w(TAG, String.format(""Camera %d Video size %s: Number of dropped frames "" +
                        ""detected in %d trials is %d frames."", cameraId, videoSz.toString(),
                        numTestIterations, totalDroppedFrames));
                mCollector.expectLessOrEqual(
                        String.format(
                                ""Camera %d Video size %s: Number of dropped frames %d must not""
                                + "" be larger than %d"",
                                cameraId, videoSz.toString(), totalDroppedFrames,
                                kFrameDrop_Tolerence),
                        kFrameDrop_Tolerence, totalDroppedFrames);
            }
            closeImageReader();
        }
    }

    /**
     * Configure video snapshot request according to the still capture size
     */
    private void prepareVideoSnapshot(
            CaptureRequest.Builder requestBuilder,
            ImageReader.OnImageAvailableListener imageListener)
            throws Exception {
        mReader.setOnImageAvailableListener(imageListener, mHandler);
        assertNotNull(""Recording surface must be non-null!"", mRecordingSurface);
        requestBuilder.addTarget(mRecordingSurface);
        assertNotNull(""Preview surface must be non-null!"", mPreviewSurface);
        requestBuilder.addTarget(mPreviewSurface);
        assertNotNull(""Reader surface must be non-null!"", mReaderSurface);
        requestBuilder.addTarget(mReaderSurface);
    }

    /**
     * Find compatible preview sizes for video size and framerate.
     *
     * <p>Preview size will be capped with max preview size.</p>
     *
     * @param videoSize The video size used for preview.
     * @param videoFrameRate The video frame rate
     */
    private List<Size> getPreviewSizesForVideo(Size videoSize, int videoFrameRate) {
        if (mOrderedPreviewSizes == null) {
            throw new IllegalStateException(""supported preview size list is not initialized yet"");
        }
        final float FRAME_DURATION_TOLERANCE = 0.01f;
        long videoFrameDuration = (long) (1e9 / videoFrameRate *
                (1.0 + FRAME_DURATION_TOLERANCE));
        HashMap<Size, Long> minFrameDurationMap = mStaticInfo.
                getAvailableMinFrameDurationsForFormatChecked(ImageFormat.PRIVATE);
        Size maxPreviewSize = mOrderedPreviewSizes.get(0);
        ArrayList<Size> previewSizes = new ArrayList<>();
        if (videoSize.getWidth() > maxPreviewSize.getWidth() ||
                videoSize.getHeight() > maxPreviewSize.getHeight()) {
            for (Size s : mOrderedPreviewSizes) {
                Long frameDuration = minFrameDurationMap.get(s);
                if (mStaticInfo.isHardwareLevelLegacy()) {
                    // Legacy doesn't report min frame duration
                    frameDuration = new Long(0);
                }
                assertTrue(""Cannot find minimum frame duration for private size"" + s,
                        frameDuration != null);
                if (frameDuration <= videoFrameDuration &&
                        s.getWidth() <= videoSize.getWidth() &&
                        s.getHeight() <= videoSize.getHeight()) {
                    Log.v(TAG, ""Add preview size "" + s.toString() + "" for video size "" +
                            videoSize.toString());
                    previewSizes.add(s);
                }
            }
        }

        if (previewSizes.isEmpty()) {
            previewSizes.add(videoSize);
        }

        return previewSizes;
    }

    /**
     * Update preview size with video size.
     *
     * <p>Preview size will be capped with max preview size.</p>
     *
     * @param videoSize The video size used for preview.
     * @param videoFrameRate The video frame rate
     *
     */
    private void updatePreviewSurfaceWithVideo(Size videoSize, int videoFrameRate) {
        List<Size> previewSizes = getPreviewSizesForVideo(videoSize, videoFrameRate);
        updatePreviewSurface(previewSizes.get(0));
    }

    private void prepareRecordingWithProfile(CamcorderProfile profile) throws Exception {
        prepareRecordingWithProfile(profile, false);
    }

    /**
     * Configure MediaRecorder recording session with CamcorderProfile, prepare
     * the recording surface.
     */
    private void prepareRecordingWithProfile(CamcorderProfile profile,
            boolean useIntermediateSurface) throws Exception {
        // Prepare MediaRecorder.
        setupMediaRecorder(profile);
        prepareRecording(useIntermediateSurface);
    }

    private void setupMediaRecorder(CamcorderProfile profile) throws Exception {
        // Set-up MediaRecorder.
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);
        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.SURFACE);
        mMediaRecorder.setProfile(profile);

        mVideoFrameRate = profile.videoFrameRate;
        mVideoSize = new Size(profile.videoFrameWidth, profile.videoFrameHeight);
    }

    private void setupMediaRecorder(
            EncoderProfiles profiles,
            EncoderProfiles.VideoProfile videoProfile,
            EncoderProfiles.AudioProfile audioProfile) throws Exception {
        // Set-up MediaRecorder.
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);
        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.SURFACE);
        mMediaRecorder.setOutputFormat(profiles.getRecommendedFileFormat());
        mMediaRecorder.setVideoProfile(videoProfile);
        if (audioProfile != null) {
            mMediaRecorder.setAudioProfile(audioProfile);
        }

        mVideoFrameRate = videoProfile.getFrameRate();
        mVideoSize = new Size(videoProfile.getWidth(), videoProfile.getHeight());
    }

    private void prepareRecording(boolean useIntermediateSurface) throws Exception {
        // Continue preparing MediaRecorder
        mMediaRecorder.setOutputFile(mOutMediaFileName);
        if (mPersistentSurface != null) {
            mMediaRecorder.setInputSurface(mPersistentSurface);
            mRecordingSurface = mPersistentSurface;
        }
        mMediaRecorder.prepare();
        if (mPersistentSurface == null) {
            mRecordingSurface = mMediaRecorder.getSurface();
        }
        assertNotNull(""Recording surface must be non-null!"", mRecordingSurface);

        if (useIntermediateSurface) {
            mIntermediateReader = ImageReader.newInstance(
                    mVideoSize.getWidth(), mVideoSize.getHeight(),
                    ImageFormat.PRIVATE, /*maxImages*/3, HardwareBuffer.USAGE_VIDEO_ENCODE);

            mIntermediateSurface = mIntermediateReader.getSurface();
            mIntermediateWriter = ImageWriter.newInstance(mRecordingSurface, /*maxImages*/3,
                    ImageFormat.PRIVATE);
            mQueuer = new ImageWriterQueuer(mIntermediateWriter);

            mIntermediateThread = new HandlerThread(TAG);
            mIntermediateThread.start();
            mIntermediateHandler = new Handler(mIntermediateThread.getLooper());
            mIntermediateReader.setOnImageAvailableListener(mQueuer, mIntermediateHandler);
        }
    }

    /**
     * Configure MediaRecorder recording session with CamcorderProfile, prepare
     * the recording surface. Use AVC for video compression, AAC for audio compression.
     * Both are required for android devices by android CDD.
     */
    private void prepareRecording(Size sz, int videoFrameRate, int captureRate)
            throws Exception {
        // Prepare MediaRecorder.
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);
        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.SURFACE);
        mMediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.THREE_GPP);
        mMediaRecorder.setOutputFile(mOutMediaFileName);
        mMediaRecorder.setVideoEncodingBitRate(getVideoBitRate(sz));
        mMediaRecorder.setVideoFrameRate(videoFrameRate);
        mMediaRecorder.setCaptureRate(captureRate);
        mMediaRecorder.setVideoSize(sz.getWidth(), sz.getHeight());
        mMediaRecorder.setVideoEncoder(MediaRecorder.VideoEncoder.H264);
        mMediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.AAC);
        if (mPersistentSurface != null) {
            mMediaRecorder.setInputSurface(mPersistentSurface);
            mRecordingSurface = mPersistentSurface;
        }
        mMediaRecorder.prepare();
        if (mPersistentSurface == null) {
            mRecordingSurface = mMediaRecorder.getSurface();
        }
        assertNotNull(""Recording surface must be non-null!"", mRecordingSurface);
        mVideoFrameRate = videoFrameRate;
        mVideoSize = sz;
    }

    private void startRecording(boolean useMediaRecorder,
            CameraCaptureSession.CaptureCallback listener, boolean useVideoStab) throws Exception {
        startRecording(useMediaRecorder, listener, useVideoStab, /*variableFpsRange*/null,
                /*useIntermediateSurface*/false);
    }

    private void startRecording(boolean useMediaRecorder,
            CameraCaptureSession.CaptureCallback listener, boolean useVideoStab,
            boolean useIntermediateSurface) throws Exception {
        startRecording(useMediaRecorder, listener, useVideoStab, /*variableFpsRange*/null,
                useIntermediateSurface);
    }

    private void startRecording(boolean useMediaRecorder,
            CameraCaptureSession.CaptureCallback listener, boolean useVideoStab,
            Range<Integer> variableFpsRange, boolean useIntermediateSurface) throws Exception {
        if (!mStaticInfo.isVideoStabilizationSupported() && useVideoStab) {
            throw new IllegalArgumentException(""Video stabilization is not supported"");
        }

        List<Surface> outputSurfaces = new ArrayList<Surface>(2);
        assertTrue(""Both preview and recording surfaces should be valid"",
                mPreviewSurface.isValid() && mRecordingSurface.isValid());
        outputSurfaces.add(mPreviewSurface);
        if (useIntermediateSurface) {
            outputSurfaces.add(mIntermediateSurface);
        } else {
            outputSurfaces.add(mRecordingSurface);
        }

        // Video snapshot surface
        if (mReaderSurface != null) {
            outputSurfaces.add(mReaderSurface);
        }
        mSessionListener = new BlockingSessionCallback();

        CaptureRequest.Builder recordingRequestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
        // Make sure camera output frame rate is set to correct value.
        Range<Integer> fpsRange = (variableFpsRange == null) ?
                Range.create(mVideoFrameRate, mVideoFrameRate) : variableFpsRange;

        recordingRequestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
        if (useVideoStab) {
            recordingRequestBuilder.set(CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE,
                    CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE_ON);
        }
        if (useIntermediateSurface) {
            recordingRequestBuilder.addTarget(mIntermediateSurface);
            if (mQueuer != null) {
                mQueuer.resetInvalidSurfaceFlag();
            }
        } else {
            recordingRequestBuilder.addTarget(mRecordingSurface);
        }
        recordingRequestBuilder.addTarget(mPreviewSurface);
        CaptureRequest recordingRequest = recordingRequestBuilder.build();
        mSession = configureCameraSessionWithParameters(mCamera, outputSurfaces, mSessionListener,
                mHandler, recordingRequest);
        mSession.setRepeatingRequest(recordingRequest, listener, mHandler);

        if (useMediaRecorder) {
            mMediaRecorder.start();
        } else {
            // TODO: need implement MediaCodec path.
        }
        mRecordingStartTime = SystemClock.elapsedRealtime();
    }

    /**
     * Start video recording with preview and video surfaces sharing the same
     * camera stream.
     *
     * @return true if success, false if sharing is not supported.
     */
    private boolean startSharedRecording(boolean useMediaRecorder,
            CameraCaptureSession.CaptureCallback listener, boolean useVideoStab,
            Range<Integer> variableFpsRange) throws Exception {
        if (!mStaticInfo.isVideoStabilizationSupported() && useVideoStab) {
            throw new IllegalArgumentException(""Video stabilization is not supported"");
        }

        List<OutputConfiguration> outputConfigs = new ArrayList<OutputConfiguration>(2);
        assertTrue(""Both preview and recording surfaces should be valid"",
                mPreviewSurface.isValid() && mRecordingSurface.isValid());
        OutputConfiguration sharedConfig = new OutputConfiguration(mPreviewSurface);
        sharedConfig.enableSurfaceSharing();
        sharedConfig.addSurface(mRecordingSurface);
        outputConfigs.add(sharedConfig);

        CaptureRequest.Builder recordingRequestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_RECORD);
        // Make sure camera output frame rate is set to correct value.
        Range<Integer> fpsRange = (variableFpsRange == null) ?
                Range.create(mVideoFrameRate, mVideoFrameRate) : variableFpsRange;
        recordingRequestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
        if (useVideoStab) {
            recordingRequestBuilder.set(CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE,
                    CaptureRequest.CONTROL_VIDEO_STABILIZATION_MODE_ON);
        }
        CaptureRequest recordingRequest = recordingRequestBuilder.build();

        mSessionListener = new BlockingSessionCallback();
        mSession = tryConfigureCameraSessionWithConfig(mCamera, outputConfigs, recordingRequest,
                mSessionListener, mHandler);

        if (mSession == null) {
            Log.i(TAG, ""Sharing between preview and video is not supported"");
            return false;
        }

        recordingRequestBuilder.addTarget(mRecordingSurface);
        recordingRequestBuilder.addTarget(mPreviewSurface);
        mSession.setRepeatingRequest(recordingRequestBuilder.build(), listener, mHandler);

        if (useMediaRecorder) {
            mMediaRecorder.start();
        } else {
            // TODO: need implement MediaCodec path.
        }
        mRecordingStartTime = SystemClock.elapsedRealtime();
        return true;
    }


    private void stopCameraStreaming() throws Exception {
        if (VERBOSE) {
            Log.v(TAG, ""Stopping camera streaming and waiting for idle"");
        }
        // Stop repeating, wait for captures to complete, and disconnect from
        // surfaces
        mSession.close();
        mSessionListener.getStateWaiter().waitForState(SESSION_CLOSED, SESSION_CLOSE_TIMEOUT_MS);
    }

    private int stopRecording(boolean useMediaRecorder) throws Exception {
        return stopRecording(useMediaRecorder, /*useIntermediateSurface*/false,
                /*stopStreaming*/true);
    }

    // Stop recording and return the estimated video duration in milliseconds.
    private int stopRecording(boolean useMediaRecorder, boolean useIntermediateSurface,
            boolean stopStreaming) throws Exception {
        long stopRecordingTime = SystemClock.elapsedRealtime();
        if (useMediaRecorder) {
            if (stopStreaming) {
                stopCameraStreaming();
            }
            if (useIntermediateSurface) {
                mIntermediateReader.setOnImageAvailableListener(null, null);
                mQueuer.expectInvalidSurface();
            }

            mMediaRecorder.stop();
            // Can reuse the MediaRecorder object after reset.
            mMediaRecorder.reset();
        } else {
            // TODO: need implement MediaCodec path.
        }

        if (useIntermediateSurface) {
            mIntermediateReader.close();
            mQueuer.close();
            mIntermediateWriter.close();
            mIntermediateSurface.release();
            mIntermediateReader = null;
            mIntermediateSurface = null;
            mIntermediateWriter = null;
            mIntermediateThread.quitSafely();
            mIntermediateHandler = null;
        }

        if (mPersistentSurface == null && mRecordingSurface != null) {
            mRecordingSurface.release();
            mRecordingSurface = null;
        }
        return (int) (stopRecordingTime - mRecordingStartTime);
    }

    private void releaseRecorder() {
        if (mMediaRecorder != null) {
            mMediaRecorder.release();
            mMediaRecorder = null;
        }
    }

    private void validateRecording(
            Size sz, float expectedDurationMs, float expectedFrameDurationMs,
            float frameDropTolerance) throws Exception {
        validateRecording(sz,
                expectedDurationMs,  /*fixed FPS recording*/0.f,
                expectedFrameDurationMs, /*fixed FPS recording*/0.f,
                frameDropTolerance);
    }

    private void validateRecording(
            Size sz,
            float expectedDurationMinMs,      // Min duration (maxFps)
            float expectedDurationMaxMs,      // Max duration (minFps). 0.f for fixed fps recording
            float expectedFrameDurationMinMs, // maxFps
            float expectedFrameDurationMaxMs, // minFps. 0.f for fixed fps recording
            float frameDropTolerance) throws Exception {
        File outFile = new File(mOutMediaFileName);
        assertTrue(""No video is recorded"", outFile.exists());
        float maxFrameDuration = expectedFrameDurationMinMs * (1.0f + FRAMEDURATION_MARGIN);
        if (expectedFrameDurationMaxMs > 0.f) {
            maxFrameDuration = expectedFrameDurationMaxMs * (1.0f + FRAMEDURATION_MARGIN);
        }

        if (expectedDurationMaxMs == 0.f) {
            expectedDurationMaxMs = expectedDurationMinMs;
        }

        MediaExtractor extractor = new MediaExtractor();
        try {
            extractor.setDataSource(mOutMediaFileName);
            long durationUs = 0;
            int width = -1, height = -1;
            int numTracks = extractor.getTrackCount();
            int selectedTrack = -1;
            final String VIDEO_MIME_TYPE = ""video"";
            for (int i = 0; i < numTracks; i++) {
                MediaFormat format = extractor.getTrackFormat(i);
                String mime = format.getString(MediaFormat.KEY_MIME);
                if (mime.contains(VIDEO_MIME_TYPE)) {
                    Log.i(TAG, ""video format is: "" + format.toString());
                    durationUs = format.getLong(MediaFormat.KEY_DURATION);
                    width = format.getInteger(MediaFormat.KEY_WIDTH);
                    height = format.getInteger(MediaFormat.KEY_HEIGHT);
                    selectedTrack = i;
                    extractor.selectTrack(i);
                    break;
                }
            }
            if (selectedTrack < 0) {
                throw new AssertionFailedError(
                        ""Cannot find video track!"");
            }

            Size videoSz = new Size(width, height);
            assertTrue(""Video size doesn't match, expected "" + sz.toString() +
                    "" got "" + videoSz.toString(), videoSz.equals(sz));
            float duration = (float) (durationUs / 1000);
            if (VERBOSE) {
                Log.v(TAG, String.format(""Video duration: recorded %fms, expected [%f,%f]ms"",
                                         duration, expectedDurationMinMs, expectedDurationMaxMs));
            }

            // Do rest of validation only for better-than-LEGACY devices
            if (mStaticInfo.isHardwareLevelLegacy()) return;

            // TODO: Don't skip this one for video snapshot on LEGACY
            assertTrue(String.format(
                    ""Camera %s: Video duration doesn't match: recorded %fms, expected [%f,%f]ms."",
                    mCamera.getId(), duration,
                    expectedDurationMinMs * (1.f - DURATION_MARGIN),
                    expectedDurationMaxMs * (1.f + DURATION_MARGIN)),
                    duration > expectedDurationMinMs * (1.f - DURATION_MARGIN) &&
                            duration < expectedDurationMaxMs * (1.f + DURATION_MARGIN));

            // Check for framedrop
            long lastSampleUs = 0;
            int frameDropCount = 0;
            int expectedFrameCount = (int) (expectedDurationMinMs / expectedFrameDurationMinMs);
            ArrayList<Long> timestamps = new ArrayList<Long>(expectedFrameCount);
            while (true) {
                timestamps.add(extractor.getSampleTime());
                if (!extractor.advance()) {
                    break;
                }
            }
            Collections.sort(timestamps);
            long prevSampleUs = timestamps.get(0);
            for (int i = 1; i < timestamps.size(); i++) {
                long currentSampleUs = timestamps.get(i);
                float frameDurationMs = (float) (currentSampleUs - prevSampleUs) / 1000;
                if (frameDurationMs > maxFrameDuration) {
                    Log.w(TAG, String.format(
                        ""Frame drop at %d: expectation %f, observed %f"",
                        i, expectedFrameDurationMinMs, frameDurationMs));
                    frameDropCount++;
                }
                prevSampleUs = currentSampleUs;
            }
            float frameDropRate = 100.f * frameDropCount / timestamps.size();
            Log.i(TAG, String.format(""Frame drop rate %d/%d (%f%%)"",
                frameDropCount, timestamps.size(), frameDropRate));
            assertTrue(String.format(
                    ""Camera %s: Video frame drop rate too high: %f%%, tolerance %f%%. "" +
                    ""Video size: %s, expectedDuration [%f,%f], expectedFrameDuration %f, "" +
                    ""frameDropCnt %d, frameCount %d"",
                    mCamera.getId(), frameDropRate, frameDropTolerance,
                    sz.toString(), expectedDurationMinMs, expectedDurationMaxMs,
                    expectedFrameDurationMinMs, frameDropCount, timestamps.size()),
                    frameDropRate < frameDropTolerance);
        } finally {
            extractor.release();
            if (!DEBUG_DUMP) {
                outFile.delete();
            }
        }
    }

    /**
     * Validate video snapshot capture image object validity and test.
     *
     * <p> Check for size, format and jpeg decoding</p>
     *
     * @param image The JPEG image to be verified.
     * @param size The JPEG capture size to be verified against.
     */
    private void validateVideoSnapshotCapture(Image image, Size size) {
        CameraTestUtils.validateImage(image, size.getWidth(), size.getHeight(),
                ImageFormat.JPEG, /*filePath*/null);
    }

    /**
     * Validate if video snapshot causes frame drop.
     * Here frame drop is defined as frame duration >= 2 * expected frame duration.
     * Return the estimated number of frames dropped during video snapshot
     */
    private int validateFrameDropAroundVideoSnapshot(
            SimpleCaptureCallback resultListener, long imageTimeStamp) {
        double expectedDurationMs = 1000.0 / mVideoFrameRate;
        CaptureResult prevResult = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
        long prevTS = getValueNotNull(prevResult, CaptureResult.SENSOR_TIMESTAMP);
        while (resultListener.hasMoreResults()) {
            CaptureResult currentResult =
                    resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            long currentTS = getValueNotNull(currentResult, CaptureResult.SENSOR_TIMESTAMP);
            if (currentTS == imageTimeStamp) {
                // validate the timestamp before and after, then return
                CaptureResult nextResult =
                        resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
                long nextTS = getValueNotNull(nextResult, CaptureResult.SENSOR_TIMESTAMP);
                double durationMs = (currentTS - prevTS) / 1000000.0;
                int totalFramesDropped = 0;

                // Snapshots in legacy mode pause the preview briefly.  Skip the duration
                // requirements for legacy mode unless this is fixed.
                if (!mStaticInfo.isHardwareLevelLegacy()) {
                    mCollector.expectTrue(
                            String.format(
                                    ""Video %dx%d Frame drop detected before video snapshot: "" +
                                            ""duration %.2fms (expected %.2fms)"",
                                    mVideoSize.getWidth(), mVideoSize.getHeight(),
                                    durationMs, expectedDurationMs
                            ),
                            durationMs <= (expectedDurationMs * MAX_NUM_FRAME_DROP_INTERVAL_ALLOWED)
                    );
                    // Log a warning is there is any frame drop detected.
                    if (durationMs >= expectedDurationMs * 2) {
                        Log.w(TAG, String.format(
                                ""Video %dx%d Frame drop detected before video snapshot: "" +
                                        ""duration %.2fms (expected %.2fms)"",
                                mVideoSize.getWidth(), mVideoSize.getHeight(),
                                durationMs, expectedDurationMs
                        ));
                    }

                    durationMs = (nextTS - currentTS) / 1000000.0;
                    mCollector.expectTrue(
                            String.format(
                                    ""Video %dx%d Frame drop detected after video snapshot: "" +
                                            ""duration %.2fms (expected %.2fms)"",
                                    mVideoSize.getWidth(), mVideoSize.getHeight(),
                                    durationMs, expectedDurationMs
                            ),
                            durationMs <= (expectedDurationMs * MAX_NUM_FRAME_DROP_INTERVAL_ALLOWED)
                    );
                    // Log a warning is there is any frame drop detected.
                    if (durationMs >= expectedDurationMs * 2) {
                        Log.w(TAG, String.format(
                                ""Video %dx%d Frame drop detected after video snapshot: "" +
                                        ""duration %fms (expected %fms)"",
                                mVideoSize.getWidth(), mVideoSize.getHeight(),
                                durationMs, expectedDurationMs
                        ));
                    }

                    double totalDurationMs = (nextTS - prevTS) / 1000000.0;
                    // Minus 2 for the expected 2 frames interval
                    totalFramesDropped = (int) (totalDurationMs / expectedDurationMs) - 2;
                    if (totalFramesDropped < 0) {
                        Log.w(TAG, ""totalFrameDropped is "" + totalFramesDropped +
                                "". Video frame rate might be too fast."");
                    }
                    totalFramesDropped = Math.max(0, totalFramesDropped);
                }
                return totalFramesDropped;
            }
            prevTS = currentTS;
        }
        throw new AssertionFailedError(
                ""Video snapshot timestamp does not match any of capture results!"");
    }

    /**
     * Validate frame jittering from the input simple listener's buffered results
     */
    private void validateJittering(SimpleCaptureCallback resultListener) {
        double expectedDurationMs = 1000.0 / mVideoFrameRate;
        CaptureResult prevResult = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
        long prevTS = getValueNotNull(prevResult, CaptureResult.SENSOR_TIMESTAMP);
        while (resultListener.hasMoreResults()) {
            CaptureResult currentResult =
                    resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            long currentTS = getValueNotNull(currentResult, CaptureResult.SENSOR_TIMESTAMP);
            double durationMs = (currentTS - prevTS) / 1000000.0;
            double durationError = Math.abs(durationMs - expectedDurationMs);
            long frameNumber = currentResult.getFrameNumber();
            mCollector.expectTrue(
                    String.format(
                            ""Resolution %dx%d Frame %d: jittering (%.2fms) exceeds bound [%.2fms,%.2fms]"",
                            mVideoSize.getWidth(), mVideoSize.getHeight(),
                            frameNumber, durationMs,
                            expectedDurationMs - FRAME_DURATION_ERROR_TOLERANCE_MS,
                            expectedDurationMs + FRAME_DURATION_ERROR_TOLERANCE_MS),
                    durationError <= FRAME_DURATION_ERROR_TOLERANCE_MS);
            prevTS = currentTS;
        }
    }

    /**
     * Calculate a video bit rate based on the size. The bit rate is scaled
     * based on ratio of video size to 1080p size.
     */
    private int getVideoBitRate(Size sz) {
        int rate = BIT_RATE_1080P;
        float scaleFactor = sz.getHeight() * sz.getWidth() / (float)(1920 * 1080);
        rate = (int)(rate * scaleFactor);

        // Clamp to the MIN, MAX range.
        return Math.max(BIT_RATE_MIN, Math.min(BIT_RATE_MAX, rate));
    }

    /**
     * Check if the encoder and camera are able to support this size and frame rate.
     * Assume the video compression format is AVC.
     */
    private boolean isSupported(Size sz, int captureRate, int encodingRate) throws Exception {
        // Check camera capability.
        if (!isSupportedByCamera(sz, captureRate)) {
            return false;
        }

        // Check encode capability.
        if (!isSupportedByAVCEncoder(sz, encodingRate)){
            return false;
        }

        if(VERBOSE) {
            Log.v(TAG, ""Both encoder and camera support "" + sz.toString() + ""@"" + encodingRate + ""@""
                    + getVideoBitRate(sz) / 1000 + ""Kbps"");
        }

        return true;
    }

    private boolean isSupportedByCamera(Size sz, int frameRate) {
        // Check if camera can support this sz and frame rate combination.
        StreamConfigurationMap config = mStaticInfo.
                getValueFromKeyNonNull(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);

        long minDuration = config.getOutputMinFrameDuration(MediaRecorder.class, sz);
        if (minDuration == 0) {
            return false;
        }

        int maxFrameRate = (int) (1e9f / minDuration);
        return maxFrameRate >= frameRate;
    }

    /**
     * Check if encoder can support this size and frame rate combination by querying
     * MediaCodec capability. Check is based on size and frame rate. Ignore the bit rate
     * as the bit rates targeted in this test are well below the bit rate max value specified
     * by AVC specification for certain level.
     */
    private static boolean isSupportedByAVCEncoder(Size sz, int frameRate) {
        MediaFormat format = MediaFormat.createVideoFormat(
                MediaFormat.MIMETYPE_VIDEO_AVC, sz.getWidth(), sz.getHeight());
        format.setInteger(MediaFormat.KEY_FRAME_RATE, frameRate);
        MediaCodecList mcl = new MediaCodecList(MediaCodecList.REGULAR_CODECS);
        return mcl.findEncoderForFormat(format) != null;
    }

    private static class ImageWriterQueuer implements ImageReader.OnImageAvailableListener {
        public ImageWriterQueuer(ImageWriter writer) {
            mWriter = writer;
        }

        public void resetInvalidSurfaceFlag() {
            synchronized (mLock) {
                mExpectInvalidSurface = false;
            }
        }

        // Indicate that the writer surface is about to get released
        // and become invalid.
        public void expectInvalidSurface() {
            // If we sync on 'mLock', we risk a possible deadlock
            // during 'mWriter.queueInputImage(image)' which is
            // called while the lock is held.
            mExpectInvalidSurface = true;
        }

        @Override
        public void onImageAvailable(ImageReader reader) {
            Image image = null;
            try {
                image = reader.acquireNextImage();
            } finally {
                synchronized (mLock) {
                    if (image != null && mWriter != null) {
                        try {
                            mWriter.queueInputImage(image);
                            mQueuedCount++;
                        } catch (IllegalStateException e) {
                            // Per API documentation ISE are possible
                            // in case the writer surface is not valid.
                            // Re-throw in case we have some other
                            // unexpected ISE.
                            if (mExpectInvalidSurface) {
                                Log.d(TAG, ""Invalid writer surface"");
                                image.close();
                            } else {
                                throw e;
                            }
                        }
                    } else if (image != null) {
                        image.close();
                    }
                }
            }
        }

        public int getQueuedCount() {
            synchronized (mLock) {
                return mQueuedCount;
            }
        }

        public void close() {
            synchronized (mLock) {
                mWriter = null;
            }
        }

        private Object      mLock = new Object();
        private ImageWriter mWriter = null;
        private int         mQueuedCount = 0;
        private boolean     mExpectInvalidSurface = false;
    }
}"	""	""	"JPEG 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.RecordingTest"	"isColorOutputSupported"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/RecordingTest.java"	""	"/*
 *.
 */

package android.hardware.camera2.cts;

import static android.hardware.camera2.cts.CameraTestUtils.*;
import static com.android.ex.camera2.blocking.BlockingSessionCallback.*;

import android.graphics.ImageFormat;
import android.graphics.SurfaceTexture;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CameraConstrainedHighSpeedCaptureSession;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.cts.helpers.StaticMetadata;
import android.hardware.camera2.params.OutputConfiguration;
import android.hardware.camera2.params.SessionConfiguration;
import android.hardware.camera2.params.StreamConfigurationMap;
import android.hardware.HardwareBuffer;
import android.util.Size;
import android.hardware.camera2.cts.testcases.Camera2SurfaceViewTestCase;
import android.media.CamcorderProfile;
import android.media.EncoderProfiles;
import android.media.MediaCodec;
import android.media.MediaCodecInfo;
import android.media.MediaCodecInfo.CodecCapabilities;
import android.media.MediaCodecInfo.CodecProfileLevel;
import android.media.Image;
import android.media.ImageReader;
import android.media.ImageWriter;
import android.media.MediaCodecList;
import android.media.MediaExtractor;
import android.media.MediaFormat;
import android.media.MediaRecorder;
import android.os.Environment;
import android.os.Handler;
import android.os.HandlerThread;
import android.os.SystemClock;
import android.test.suitebuilder.annotation.LargeTest;
import android.util.Log;
import android.util.Range;
import android.view.Surface;

import com.android.compatibility.common.util.MediaUtils;
import com.android.ex.camera2.blocking.BlockingSessionCallback;

import junit.framework.AssertionFailedError;

import org.junit.runners.Parameterized;
import org.junit.runner.RunWith;
import org.junit.Test;

import java.io.File;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;
import java.util.HashMap;

/**
 * CameraDevice video recording use case tests by using MediaRecorder and
 * MediaCodec.
 */
@LargeTest
@RunWith(Parameterized.class)
public class RecordingTest extends Camera2SurfaceViewTestCase {
    private static final String TAG = ""RecordingTest"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);
    private static final boolean DEBUG_DUMP = Log.isLoggable(TAG, Log.DEBUG);
    private static final int RECORDING_DURATION_MS = 3000;
    private static final int PREVIEW_DURATION_MS = 3000;
    private static final float DURATION_MARGIN = 0.2f;
    private static final double FRAME_DURATION_ERROR_TOLERANCE_MS = 3.0;
    private static final float FRAMEDURATION_MARGIN = 0.2f;
    private static final float VID_SNPSHT_FRMDRP_RATE_TOLERANCE = 10.0f;
    private static final float FRMDRP_RATE_TOLERANCE = 5.0f;
    private static final int BIT_RATE_1080P = 16000000;
    private static final int BIT_RATE_MIN = 64000;
    private static final int BIT_RATE_MAX = 40000000;
    private static final int VIDEO_FRAME_RATE = 30;
    private static final int[] mCamcorderProfileList = {
            CamcorderProfile.QUALITY_HIGH,
            CamcorderProfile.QUALITY_2160P,
            CamcorderProfile.QUALITY_1080P,
            CamcorderProfile.QUALITY_720P,
            CamcorderProfile.QUALITY_480P,
            CamcorderProfile.QUALITY_CIF,
            CamcorderProfile.QUALITY_QCIF,
            CamcorderProfile.QUALITY_QVGA,
            CamcorderProfile.QUALITY_LOW,
    };
    private static final int MAX_VIDEO_SNAPSHOT_IMAGES = 5;
    private static final int BURST_VIDEO_SNAPSHOT_NUM = 3;
    private static final int SLOWMO_SLOW_FACTOR = 4;
    private static final int MAX_NUM_FRAME_DROP_INTERVAL_ALLOWED = 4;
    private List<Size> mSupportedVideoSizes;
    private Surface mRecordingSurface;
    private Surface mPersistentSurface;
    private MediaRecorder mMediaRecorder;
    private String mOutMediaFileName;
    private int mVideoFrameRate;
    private Size mVideoSize;
    private long mRecordingStartTime;

    private Surface mIntermediateSurface;
    private ImageReader mIntermediateReader;
    private ImageWriter mIntermediateWriter;
    private ImageWriterQueuer mQueuer;
    private HandlerThread mIntermediateThread;
    private Handler mIntermediateHandler;

    @Override
    public void setUp() throws Exception {
        super.setUp();
    }

    @Override
    public void tearDown() throws Exception {
        super.tearDown();
    }

    private void doBasicRecording(boolean useVideoStab) throws Exception {
        doBasicRecording(useVideoStab, false);
    }

    private void doBasicRecording(boolean useVideoStab, boolean useIntermediateSurface)
            throws Exception {
        doBasicRecording(useVideoStab, useIntermediateSurface, false);
    }

    private void doBasicRecording(boolean useVideoStab, boolean useIntermediateSurface,
            boolean useEncoderProfiles) throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                Log.i(TAG, ""Testing basic recording for camera "" + mCameraIdsUnderTest[i]);
                StaticMetadata staticInfo = mAllStaticInfo.get(mCameraIdsUnderTest[i]);
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }

                // External camera doesn't support CamcorderProfile recording
                if (staticInfo.isExternalCamera()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support CamcorderProfile, skipping"");
                    continue;
                }

                if (!staticInfo.isVideoStabilizationSupported() && useVideoStab) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support video stabilization, skipping the stabilization""
                            + "" test"");
                    continue;
                }

                // Re-use the MediaRecorder object for the same camera device.
                mMediaRecorder = new MediaRecorder();
                openDevice(mCameraIdsUnderTest[i]);
                initSupportedVideoSize(mCameraIdsUnderTest[i]);

                basicRecordingTestByCamera(mCamcorderProfileList, useVideoStab,
                        useIntermediateSurface, useEncoderProfiles);
            } finally {
                closeDevice();
                releaseRecorder();
            }
        }
    }

    /**
     * <p>
     * Test basic video stabilitzation camera recording.
     * </p>
     * <p>
     * This test covers the typical basic use case of camera recording with video
     * stabilization is enabled, if video stabilization is supported.
     * MediaRecorder is used to record the audio and video, CamcorderProfile is
     * used to configure the MediaRecorder. It goes through the pre-defined
     * CamcorderProfile list, test each profile configuration and validate the
     * recorded video. Preview is set to the video size.
     * </p>
     */"	""	""	"1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.fov.PhotoCaptureActivity"	"OnClickListener"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/fov/PhotoCaptureActivity.java"	""	"public void test/*
 *.
 */

package com.android.cts.verifier.camera.fov;

import android.app.Activity;
import android.app.AlertDialog;
import android.app.Dialog;
import android.content.Context;
import android.content.DialogInterface;
import android.content.Intent;
import android.graphics.Color;
import android.hardware.Camera;
import android.hardware.Camera.PictureCallback;
import android.hardware.Camera.ShutterCallback;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraManager;
import android.os.Bundle;
import android.os.PowerManager;
import android.os.PowerManager.WakeLock;
import android.util.Log;
import android.view.Surface;
import android.view.SurfaceHolder;
import android.view.SurfaceView;
import android.view.View;
import android.view.View.OnClickListener;
import android.widget.AdapterView;
import android.widget.AdapterView.OnItemSelectedListener;
import android.widget.ArrayAdapter;
import android.widget.Button;
import android.widget.Spinner;
import android.widget.TextView;
import android.widget.Toast;

import com.android.cts.verifier.R;
import com.android.cts.verifier.TestResult;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

/**
 * An activity for showing the camera preview and taking a picture.
 */
public class PhotoCaptureActivity extends Activity
        implements PictureCallback, SurfaceHolder.Callback {
    private static final String TAG = PhotoCaptureActivity.class.getSimpleName();
    private static final int FOV_REQUEST_CODE = 1006;
    private static final String PICTURE_FILENAME = ""photo.jpg"";
    private static float mReportedFovDegrees = 0;
    private float mReportedFovPrePictureTaken = -1;

    private SurfaceView mPreview;
    private SurfaceHolder mSurfaceHolder;
    private Spinner mResolutionSpinner;
    private List<SelectableResolution> mSupportedResolutions;
    private ArrayAdapter<SelectableResolution> mAdapter;

    private SelectableResolution mSelectedResolution;
    private Camera mCamera;
    private Size mSurfaceSize;
    private boolean mCameraInitialized = false;
    private boolean mPreviewActive = false;
    private boolean mTakingPicture = false;
    private int mResolutionSpinnerIndex = -1;
    private WakeLock mWakeLock;
    private long shutterStartTime;
    private int mPreviewOrientation;
    private int mJpegOrientation;

    private ArrayList<Integer> mPreviewSizeCamerasToProcess = new ArrayList<Integer>();

    private Dialog mActiveDialog;

    /**
     * Selected preview size per camera. If null, preview size should be
     * automatically detected.
     */
    private Size[] mPreviewSizes = null;

    public static File getPictureFile(Context context) {
        return new File(context.getExternalCacheDir(), PICTURE_FILENAME);
    }

    public static float getReportedFovDegrees() {
        return mReportedFovDegrees;
    }

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.camera_fov_calibration_photo_capture);

        int cameraToBeTested = 0;
        for (int cameraId = 0; cameraId < Camera.getNumberOfCameras(); ++cameraId) {
            if (!isExternalCamera(cameraId)) {
                cameraToBeTested++;
            }
        }

        mPreview = (SurfaceView) findViewById(R.id.camera_fov_camera_preview);
        mSurfaceHolder = mPreview.getHolder();
        mSurfaceHolder.addCallback(this);

        // This is required for older versions of Android hardware.
        mSurfaceHolder.setType(SurfaceHolder.SURFACE_TYPE_PUSH_BUFFERS);

        TextView textView = (TextView) findViewById(R.id.camera_fov_tap_to_take_photo);
        textView.setTextColor(Color.WHITE);

        Button setupButton = (Button) findViewById(R.id.camera_fov_settings_button);
        setupButton.setOnClickListener(new OnClickListener() {

            @Override
            public void onClick(View v) {
                startActivity(new Intent(
                        PhotoCaptureActivity.this, CalibrationPreferenceActivity.class));
            }
        });

        Button changePreviewSizeButton = (Button) findViewById(
                R.id.camera_fov_change_preview_size_button);
        changePreviewSizeButton.setOnClickListener(new OnClickListener() {
            @Override
            public void onClick(View v) {
                // Stop camera until preview sizes have been obtained.
                if (mCamera != null) {
                    mCamera.stopPreview();
                    mCamera.release();
                    mCamera = null;
                }

                mPreviewSizeCamerasToProcess.clear();
                mPreviewSizes =  new Size[Camera.getNumberOfCameras()];
                for (int cameraId = 0; cameraId < Camera.getNumberOfCameras(); ++cameraId) {
                    if (!isExternalCamera(cameraId)) {
                        mPreviewSizeCamerasToProcess.add(cameraId);
                    }
                }
                showNextDialogToChoosePreviewSize();
            }
        });

        View previewView = findViewById(R.id.camera_fov_preview_overlay);
        previewView.setOnClickListener(new OnClickListener() {
            @Override
            public void onClick(View v) {
                if (mPreviewActive && !mTakingPicture) {
                    mTakingPicture = true;
                    shutterStartTime = System.currentTimeMillis();

                    mCamera.takePicture(new ShutterCallback() {
                        @Override
                        public void onShutter() {
                            long dT = System.currentTimeMillis() - shutterStartTime;
                            Log.d(""CTS"", ""Shutter Lag: "" + dT);
                        }
                    }, null, PhotoCaptureActivity.this);
                }
            }
        });

        mResolutionSpinner = (Spinner) findViewById(R.id.camera_fov_resolution_selector);
        mResolutionSpinner.setOnItemSelectedListener(new OnItemSelectedListener() {
            @Override
            public void onItemSelected(
                    AdapterView<?> parent, View view, int position, long id) {
                if (mSupportedResolutions != null) {
                    SelectableResolution resolution = mSupportedResolutions.get(position);
                    switchToCamera(resolution, false);

                    // It should be guaranteed that the FOV is correctly updated after setParameters().
                    mReportedFovPrePictureTaken = mCamera.getParameters().getHorizontalViewAngle();

                    mResolutionSpinnerIndex = position;
                    startPreview();
                }
            }

            @Override
            public void onNothingSelected(AdapterView<?> arg0) {}
        });

        if (cameraToBeTested == 0) {
            Log.i(TAG, ""No cameras needs to be tested. Setting test pass."");
            Toast.makeText(this, ""No cameras needs to be tested. Test pass."",
                    Toast.LENGTH_LONG).show();

            TestResult.setPassedResult(this, getClass().getName(),
                    ""All cameras are external, test skipped!"");
            finish();
        }
    }

    @Override
    protected void onResume() {
        super.onResume();
        // Keep the device from going to sleep.
        PowerManager pm = (PowerManager) getSystemService(Context.POWER_SERVICE);
        mWakeLock = pm.newWakeLock(PowerManager.FULL_WAKE_LOCK, TAG);
        mWakeLock.acquire();

        if (mSupportedResolutions == null) {
            mSupportedResolutions = new ArrayList<SelectableResolution>();
            int numCameras = Camera.getNumberOfCameras();
            for (int cameraId = 0; cameraId < numCameras; ++cameraId) {
                if (isExternalCamera(cameraId)) {
                    continue;
                }

                Camera camera = Camera.open(cameraId);

                // Get the supported picture sizes and fill the spinner.
                List<Camera.Size> supportedSizes =
                        camera.getParameters().getSupportedPictureSizes();
                for (Camera.Size size : supportedSizes) {
                    mSupportedResolutions.add(
                            new SelectableResolution(cameraId, size.width, size.height));
                }
                camera.release();
            }
        }

        // Find the first untested entry.
        for (mResolutionSpinnerIndex = 0;
                mResolutionSpinnerIndex < mSupportedResolutions.size();
                mResolutionSpinnerIndex++) {
            if (!mSupportedResolutions.get(mResolutionSpinnerIndex).tested) {
                break;
            }
        }

        mAdapter = new ArrayAdapter<SelectableResolution>(
                this, android.R.layout.simple_spinner_dropdown_item,
                mSupportedResolutions);
        mResolutionSpinner.setAdapter(mAdapter);

        mResolutionSpinner.setSelection(mResolutionSpinnerIndex);
        setResult(RESULT_CANCELED);
    }

    @Override
    public void onPause() {
        if (mCamera != null) {
            if (mPreviewActive) {
                mCamera.stopPreview();
            }

            mCamera.release();
            mCamera = null;
        }
        mPreviewActive = false;
        mWakeLock.release();
        super.onPause();
    }

    @Override
    public void onPictureTaken(byte[] data, Camera camera) {
        File pictureFile = getPictureFile(this);
        Camera.Parameters params = mCamera.getParameters();
        mReportedFovDegrees = params.getHorizontalViewAngle();

        // Show error if FOV does not match the value reported before takePicture().
        if (mReportedFovPrePictureTaken != mReportedFovDegrees) {
            mSupportedResolutions.get(mResolutionSpinnerIndex).tested = true;
            mSupportedResolutions.get(mResolutionSpinnerIndex).passed = false;

            AlertDialog.Builder dialogBuilder = new AlertDialog.Builder(this);
            dialogBuilder.setTitle(R.string.camera_fov_reported_fov_problem);
            dialogBuilder.setNeutralButton(
                    android.R.string.ok, new DialogInterface.OnClickListener() {
                @Override
                public void onClick(DialogInterface dialog, int which) {
                    if (mActiveDialog != null) {
                        mActiveDialog.dismiss();
                        mActiveDialog = null;
                        initializeCamera();
                    }
                }
            });

            String message  = getResources().getString(R.string.camera_fov_reported_fov_problem_message);
            dialogBuilder.setMessage(String.format(message, mReportedFovPrePictureTaken, mReportedFovDegrees));
            mActiveDialog = dialogBuilder.show();
            mTakingPicture = false;
            return;
        }

        try {
            FileOutputStream fos = new FileOutputStream(pictureFile);
            fos.write(data);
            fos.close();
            Log.d(TAG, ""File saved to "" + pictureFile.getAbsolutePath());

            // Start activity which will use the taken picture to determine the
            // FOV.
            startActivityForResult(new Intent(this, DetermineFovActivity.class),
                    FOV_REQUEST_CODE + mResolutionSpinnerIndex, null);
        } catch (IOException e) {
            Log.e(TAG, ""Could not save picture file."", e);
            Toast.makeText(this, ""Could not save picture file: "" + e.getMessage(),
                    Toast.LENGTH_LONG).show();
        }
        mTakingPicture = false;
    }

    @Override
    protected void onActivityResult(int requestCode, int resultCode, Intent data) {
        if (resultCode != RESULT_OK) {
            return;
        }
        int testIndex = requestCode - FOV_REQUEST_CODE;
        SelectableResolution res = mSupportedResolutions.get(testIndex);
        res.tested = true;
        float reportedFOV = CtsTestHelper.getReportedFOV(data);
        float measuredFOV = CtsTestHelper.getMeasuredFOV(data);
        res.measuredFOV = measuredFOV;
        if (CtsTestHelper.isResultPassed(reportedFOV, measuredFOV)) {
            res.passed = true;
        }

        boolean allTested = true;
        for (int i = 0; i < mSupportedResolutions.size(); i++) {
            if (!mSupportedResolutions.get(i).tested) {
                allTested = false;
                break;
            }
        }
        if (!allTested) {
            mAdapter.notifyDataSetChanged();
            return;
        }

        boolean allPassed = true;
        for (int i = 0; i < mSupportedResolutions.size(); i++) {
            if (!mSupportedResolutions.get(i).passed) {
                allPassed = false;
                break;
            }
        }
        if (allPassed) {
            TestResult.setPassedResult(this, getClass().getName(),
                    CtsTestHelper.getTestDetails(mSupportedResolutions));
        } else {
            TestResult.setFailedResult(this, getClass().getName(),
                    CtsTestHelper.getTestDetails(mSupportedResolutions));
        }
        finish();
    }

    @Override
    public void surfaceChanged(
            SurfaceHolder holder, int format, int width, int height) {
        mSurfaceSize = new Size(width, height);
        initializeCamera();
    }

    @Override
    public void surfaceCreated(SurfaceHolder holder) {
        // Nothing to do.
    }

    @Override
    public void surfaceDestroyed(SurfaceHolder holder) {
        // Nothing to do.
    }

    private void showNextDialogToChoosePreviewSize() {
        final int cameraId = mPreviewSizeCamerasToProcess.remove(0);

        Camera camera = Camera.open(cameraId);
        final List<Camera.Size> sizes = camera.getParameters()
                .getSupportedPreviewSizes();
        String[] choices = new String[sizes.size()];
        for (int i = 0; i < sizes.size(); ++i) {
            Camera.Size size = sizes.get(i);
            choices[i] = size.width + "" x "" + size.height;
        }

        final AlertDialog.Builder builder = new AlertDialog.Builder(this);
        String dialogTitle = String.format(
                getResources().getString(R.string.camera_fov_choose_preview_size_for_camera),
                cameraId);
        builder.setTitle(
                dialogTitle).
                setOnCancelListener(new DialogInterface.OnCancelListener() {
                    @Override
                    public void onCancel(DialogInterface arg0) {
                        // User cancelled preview size selection.
                        mPreviewSizes = null;
                        switchToCamera(mSelectedResolution, true);
                    }
                }).
                setSingleChoiceItems(choices, 0, new DialogInterface.OnClickListener() {
                    @Override
                    public void onClick(DialogInterface dialog, int which) {
                        Camera.Size size = sizes.get(which);
                        mPreviewSizes[cameraId] = new Size(
                                size.width, size.height);
                        dialog.dismiss();

                        if (mPreviewSizeCamerasToProcess.isEmpty()) {
                            // We're done, re-initialize camera.
                            switchToCamera(mSelectedResolution, true);
                        } else {
                            // Process other cameras.
                            showNextDialogToChoosePreviewSize();
                        }
                    }
                }).create().show();
        camera.release();
    }

    private void initializeCamera() {
        initializeCamera(true);
    }

    private void initializeCamera(boolean startPreviewAfterInit) {
        if (mCamera == null || mSurfaceHolder.getSurface() == null) {
            return;
        }

        try {
            mCamera.setPreviewDisplay(mSurfaceHolder);
        } catch (Throwable t) {
            Log.e(TAG, ""Could not set preview display"", t);
            Toast.makeText(this, t.getMessage(), Toast.LENGTH_LONG).show();
            return;
        }

        calculateOrientations(this, mSelectedResolution.cameraId, mCamera);
        Camera.Parameters params = setCameraParams(mCamera);

        // Either use chosen preview size for current camera or automatically
        // choose preview size based on view dimensions.
        Size selectedPreviewSize = null;
        if (mPreviewSizes != null) {
            selectedPreviewSize = mPreviewSizes[mSelectedResolution.cameraId];
        } else if (mSurfaceSize != null) {
            selectedPreviewSize = getBestPreviewSize(
                    mSurfaceSize.width, mSurfaceSize.height, params);
        }

        if (selectedPreviewSize != null) {
            params.setPreviewSize(selectedPreviewSize.width, selectedPreviewSize.height);
            mCamera.setParameters(params);
            mCameraInitialized = true;
        }

        if (startPreviewAfterInit) {
            if (selectedPreviewSize == null) {
                Log.w(TAG, ""Preview started without setting preview size"");
            }
            startPreview();
        }
    }

    private void startPreview() {
        if (mCameraInitialized && mCamera != null) {
            mCamera.setDisplayOrientation(mPreviewOrientation);
            mCamera.startPreview();
            mPreviewActive = true;
        }
    }

    private void switchToCamera(SelectableResolution resolution, boolean startPreview) {
        if (mCamera != null) {
            mCamera.stopPreview();
            mCamera.release();
        }

        mSelectedResolution = resolution;
        mCamera = Camera.open(mSelectedResolution.cameraId);

        initializeCamera(startPreview);
    }

    /**
     * Get the best supported focus mode.
     *
     * @param camera - Android camera object.
     * @return the best supported focus mode.
     */
    private static String getFocusMode(Camera camera) {
        List<String> modes = camera.getParameters().getSupportedFocusModes();
        if (modes != null) {
            if (modes.contains(Camera.Parameters.FOCUS_MODE_INFINITY)) {
                Log.v(TAG, ""Using Focus mode infinity"");
                return Camera.Parameters.FOCUS_MODE_INFINITY;
            }
            if (modes.contains(Camera.Parameters.FOCUS_MODE_FIXED)) {
                Log.v(TAG, ""Using Focus mode fixed"");
                return Camera.Parameters.FOCUS_MODE_FIXED;
            }
        }
        Log.v(TAG, ""Using Focus mode auto."");
        return Camera.Parameters.FOCUS_MODE_AUTO;
    }

    /**
     * Set the common camera parameters on the given camera and returns the
     * parameter object for further modification, if needed.
     */
    private Camera.Parameters setCameraParams(Camera camera) {
        // The picture size is taken and set from the spinner selection
        // callback.
        Camera.Parameters params = camera.getParameters();
        params.setJpegThumbnailSize(0, 0);
        params.setJpegQuality(100);
        params.setRotation(mJpegOrientation);
        params.setFocusMode(getFocusMode(camera));
        params.setZoom(0);
        params.setPictureSize(mSelectedResolution.width, mSelectedResolution.height);
        return params;
    }

    private Size getBestPreviewSize(
            int width, int height, Camera.Parameters parameters) {
        Size result = null;

        for (Camera.Size size : parameters.getSupportedPreviewSizes()) {
            if (size.width <= width && size.height <= height) {
                if (result == null) {
                    result = new Size(size.width, size.height);
                } else {
                    int resultArea = result.width * result.height;
                    int newArea = size.width * size.height;

                    if (newArea > resultArea) {
                        result = new Size(size.width, size.height);
                    }
                }
            }
        }
        return result;
    }

    private void calculateOrientations(Activity activity,
            int cameraId, android.hardware.Camera camera) {
        android.hardware.Camera.CameraInfo info =
                new android.hardware.Camera.CameraInfo();
        android.hardware.Camera.getCameraInfo(cameraId, info);
        int rotation = activity.getWindowManager().getDefaultDisplay()
                .getRotation();
        int degrees = 0;
        switch (rotation) {
            case Surface.ROTATION_0: degrees = 0; break;
            case Surface.ROTATION_90: degrees = 90; break;
            case Surface.ROTATION_180: degrees = 180; break;
            case Surface.ROTATION_270: degrees = 270; break;
        }

        if (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {
            mJpegOrientation = (info.orientation + degrees) % 360;
            mPreviewOrientation = (360 - mJpegOrientation) % 360;  // compensate the mirror
        } else {  // back-facing
            mJpegOrientation = (info.orientation - degrees + 360) % 360;
            mPreviewOrientation = mJpegOrientation;
        }
    }

    private boolean isExternalCamera(int cameraId) {
        CameraManager manager = (CameraManager) this.getSystemService(Context.CAMERA_SERVICE);
        try {
            String cameraIdStr = manager.getCameraIdList()[cameraId];
            CameraCharacteristics characteristics =
                    manager.getCameraCharacteristics(cameraIdStr);

            if (characteristics.get(CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL) ==
                            CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL) {
                // External camera doesn't support FOV informations
                return true;
            }
        } catch (CameraAccessException e) {
            Toast.makeText(this, ""Could not access camera "" + cameraId +
                    "": "" + e.getMessage(), Toast.LENGTH_LONG).show();
        }
        return false;
    }
}"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.PerformanceTest"	"testCameraLaunch"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/PerformanceTest.java"	""	"public void testCameraLaunch() throws Exception {
        double[] avgCameraLaunchTimes = new double[mTestRule.getCameraIdsUnderTest().length];

        int counter = 0;
        for (String id : mTestRule.getCameraIdsUnderTest()) {
            // Do NOT move these variables to outer scope
            // They will be passed to DeviceReportLog and their references will be stored
            String streamName = ""test_camera_launch"";
            mReportLog = new DeviceReportLog(REPORT_LOG_NAME, streamName);
            mReportLog.addValue(""camera_id"", id, ResultType.NEUTRAL, ResultUnit.NONE);
            double[] cameraOpenTimes = new double[NUM_TEST_LOOPS];
            double[] configureStreamTimes = new double[NUM_TEST_LOOPS];
            double[] startPreviewTimes = new double[NUM_TEST_LOOPS];
            double[] stopPreviewTimes = new double[NUM_TEST_LOOPS];
            double[] cameraCloseTimes = new double[NUM_TEST_LOOPS];
            double[] cameraLaunchTimes = new double[NUM_TEST_LOOPS];
            try {
                CameraCharacteristics ch =
                        mTestRule.getCameraManager().getCameraCharacteristics(id);
                mTestRule.setStaticInfo(new StaticMetadata(ch));
                boolean isColorOutputSupported = mTestRule.getStaticInfo().isColorOutputSupported();
                if (isColorOutputSupported) {
                    initializeImageReader(id, ImageFormat.YUV_420_888);
                } else {
                    assertTrue(""Depth output must be supported if regular output isn't!"",
                            mTestRule.getStaticInfo().isDepthOutputSupported());
                    initializeImageReader(id, ImageFormat.DEPTH16);
                }

                SimpleImageListener imageListener = null;
                long startTimeMs, openTimeMs, configureTimeMs, previewStartedTimeMs;
                for (int i = 0; i < NUM_TEST_LOOPS; i++) {
                    try {
                        // Need create a new listener every iteration to be able to wait
                        // for the first image comes out.
                        imageListener = new SimpleImageListener();
                        mTestRule.getReader().setOnImageAvailableListener(
                                imageListener, mTestRule.getHandler());
                        startTimeMs = SystemClock.elapsedRealtime();

                        // Blocking open camera
                        simpleOpenCamera(id);
                        openTimeMs = SystemClock.elapsedRealtime();
                        cameraOpenTimes[i] = openTimeMs - startTimeMs;

                        // Blocking configure outputs.
                        CaptureRequest previewRequest =
                                configureReaderAndPreviewOutputs(id, isColorOutputSupported);
                        configureTimeMs = SystemClock.elapsedRealtime();
                        configureStreamTimes[i] = configureTimeMs - openTimeMs;

                        // Blocking start preview (start preview to first image arrives)
                        SimpleCaptureCallback resultListener =
                                new SimpleCaptureCallback();
                        blockingStartPreview(id, resultListener, previewRequest, imageListener);
                        previewStartedTimeMs = SystemClock.elapsedRealtime();
                        startPreviewTimes[i] = previewStartedTimeMs - configureTimeMs;
                        cameraLaunchTimes[i] = previewStartedTimeMs - startTimeMs;

                        // Let preview on for a couple of frames
                        CameraTestUtils.waitForNumResults(resultListener, NUM_RESULTS_WAIT,
                                WAIT_FOR_RESULT_TIMEOUT_MS);

                        // Blocking stop preview
                        startTimeMs = SystemClock.elapsedRealtime();
                        blockingStopRepeating();
                        stopPreviewTimes[i] = SystemClock.elapsedRealtime() - startTimeMs;
                    }
                    finally {
                        // Blocking camera close
                        startTimeMs = SystemClock.elapsedRealtime();
                        mTestRule.closeDevice(id);
                        cameraCloseTimes[i] = SystemClock.elapsedRealtime() - startTimeMs;
                    }
                }

                avgCameraLaunchTimes[counter] = Stat.getAverage(cameraLaunchTimes);
                // Finish the data collection, report the KPIs.
                // ReportLog keys have to be lowercase underscored format.
                mReportLog.addValues(""camera_open_time"", cameraOpenTimes, ResultType.LOWER_BETTER,
                        ResultUnit.MS);
                mReportLog.addValues(""camera_configure_stream_time"", configureStreamTimes,
                        ResultType.LOWER_BETTER, ResultUnit.MS);
                mReportLog.addValues(""camera_start_preview_time"", startPreviewTimes,
                        ResultType.LOWER_BETTER, ResultUnit.MS);
                mReportLog.addValues(""camera_camera_stop_preview"", stopPreviewTimes,
                        ResultType.LOWER_BETTER, ResultUnit.MS);
                mReportLog.addValues(""camera_camera_close_time"", cameraCloseTimes,
                        ResultType.LOWER_BETTER, ResultUnit.MS);
                mReportLog.addValues(""camera_launch_time"", cameraLaunchTimes,
                        ResultType.LOWER_BETTER, ResultUnit.MS);
            }
            finally {
                mTestRule.closeDefaultImageReader();
                closePreviewSurface();
            }
            counter++;
            mReportLog.submit(mInstrumentation);

            if (VERBOSE) {
                Log.v(TAG, ""Camera "" + id + "" device open times(ms): ""
                        + Arrays.toString(cameraOpenTimes)
                        + "". Average(ms): "" + Stat.getAverage(cameraOpenTimes)
                        + "". Min(ms): "" + Stat.getMin(cameraOpenTimes)
                        + "". Max(ms): "" + Stat.getMax(cameraOpenTimes));
                Log.v(TAG, ""Camera "" + id + "" configure stream times(ms): ""
                        + Arrays.toString(configureStreamTimes)
                        + "". Average(ms): "" + Stat.getAverage(configureStreamTimes)
                        + "". Min(ms): "" + Stat.getMin(configureStreamTimes)
                        + "". Max(ms): "" + Stat.getMax(configureStreamTimes));
                Log.v(TAG, ""Camera "" + id + "" start preview times(ms): ""
                        + Arrays.toString(startPreviewTimes)
                        + "". Average(ms): "" + Stat.getAverage(startPreviewTimes)
                        + "". Min(ms): "" + Stat.getMin(startPreviewTimes)
                        + "". Max(ms): "" + Stat.getMax(startPreviewTimes));
                Log.v(TAG, ""Camera "" + id + "" stop preview times(ms): ""
                        + Arrays.toString(stopPreviewTimes)
                        + "". Average(ms): "" + Stat.getAverage(stopPreviewTimes)
                        + "". nMin(ms): "" + Stat.getMin(stopPreviewTimes)
                        + "". nMax(ms): "" + Stat.getMax(stopPreviewTimes));
                Log.v(TAG, ""Camera "" + id + "" device close times(ms): ""
                        + Arrays.toString(cameraCloseTimes)
                        + "". Average(ms): "" + Stat.getAverage(cameraCloseTimes)
                        + "". Min(ms): "" + Stat.getMin(cameraCloseTimes)
                        + "". Max(ms): "" + Stat.getMax(cameraCloseTimes));
                Log.v(TAG, ""Camera "" + id + "" camera launch times(ms): ""
                        + Arrays.toString(cameraLaunchTimes)
                        + "". Average(ms): "" + Stat.getAverage(cameraLaunchTimes)
                        + "". Min(ms): "" + Stat.getMin(cameraLaunchTimes)
                        + "". Max(ms): "" + Stat.getMax(cameraLaunchTimes));
            }
        }
        if (mTestRule.getCameraIdsUnderTest().length != 0) {
            String streamName = ""test_camera_launch_average"";
            mReportLog = new DeviceReportLog(REPORT_LOG_NAME, streamName);
            mReportLog.setSummary(""camera_launch_average_time_for_all_cameras"",
                    Stat.getAverage(avgCameraLaunchTimes), ResultType.LOWER_BETTER, ResultUnit.MS);
            mReportLog.submit(mInstrumentation);
        }
    }

    /**
     * Test camera capture KPI for YUV_420_888, PRIVATE, JPEG, RAW and RAW+JPEG
     * formats: the time duration between sending out a single image capture request
     * and receiving image data and capture result.
     * <p>
     * It enumerates the following metrics: capture latency, computed by
     * measuring the time between sending out the capture request and getting
     * the image data; partial result latency, computed by measuring the time
     * between sending out the capture request and getting the partial result;
     * capture result latency, computed by measuring the time between sending
     * out the capture request and getting the full capture result.
     * </p>
     */"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.PerformanceTest"	"testSingleCapture"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/PerformanceTest.java"	""	"public void testSingleCapture() throws Exception {
        int[] JPEG_FORMAT = {ImageFormat.JPEG};
        testSingleCaptureForFormat(JPEG_FORMAT, ""jpeg"", /*addPreviewDelay*/ true);
        if (!mTestRule.isPerfMeasure()) {
            int[] YUV_FORMAT = {ImageFormat.YUV_420_888};
            testSingleCaptureForFormat(YUV_FORMAT, null, /*addPreviewDelay*/ false);
            int[] PRIVATE_FORMAT = {ImageFormat.PRIVATE};
            testSingleCaptureForFormat(PRIVATE_FORMAT, ""private"", /*addPreviewDelay*/ true);
            int[] RAW_FORMAT = {ImageFormat.RAW_SENSOR};
            testSingleCaptureForFormat(RAW_FORMAT, ""raw"", /*addPreviewDelay*/ true);
            int[] RAW_JPEG_FORMATS = {ImageFormat.RAW_SENSOR, ImageFormat.JPEG};
            testSingleCaptureForFormat(RAW_JPEG_FORMATS, ""raw_jpeg"", /*addPreviewDelay*/ true);
        }
    }

    private String appendFormatDescription(String message, String formatDescription) {
        if (message == null) {
            return null;
        }

        String ret = message;
        if (formatDescription != null) {
            ret = String.format(ret + ""_%s"", formatDescription);
        }

        return ret;
    }

    private void testSingleCaptureForFormat(int[] formats, String formatDescription,
            boolean addPreviewDelay) throws Exception {
        double[] avgResultTimes = new double[mTestRule.getCameraIdsUnderTest().length];
        double[] avgCaptureTimes = new double[mTestRule.getCameraIdsUnderTest().length];

        int counter = 0;
        for (String id : mTestRule.getCameraIdsUnderTest()) {
            // Do NOT move these variables to outer scope
            // They will be passed to DeviceReportLog and their references will be stored
            String streamName = appendFormatDescription(""test_single_capture"", formatDescription);
            mReportLog = new DeviceReportLog(REPORT_LOG_NAME, streamName);
            mReportLog.addValue(""camera_id"", id, ResultType.NEUTRAL, ResultUnit.NONE);
            double[] captureTimes = new double[NUM_TEST_LOOPS];
            double[] getPartialTimes = new double[NUM_TEST_LOOPS];
            double[] getResultTimes = new double[NUM_TEST_LOOPS];
            ImageReader[] readers = null;
            try {
                if (!mTestRule.getAllStaticInfo().get(id).isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + id + "" does not support color outputs, skipping"");
                    continue;
                }

                StreamConfigurationMap configMap = mTestRule.getAllStaticInfo().get(
                        id).getCharacteristics().get(
                        CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
                boolean formatsSupported = true;
                for (int format : formats) {
                    if (!configMap.isOutputSupportedFor(format)) {
                        Log.i(TAG, ""Camera "" + id + "" does not support output format: "" + format +
                                "" skipping"");
                        formatsSupported = false;
                        break;
                    }
                }
                if (!formatsSupported) {
                    continue;
                }

                mTestRule.openDevice(id);

                boolean partialsExpected = mTestRule.getStaticInfo().getPartialResultCount() > 1;
                long startTimeMs;
                boolean isPartialTimingValid = partialsExpected;
                for (int i = 0; i < NUM_TEST_LOOPS; i++) {

                    // setup builders and listeners
                    CaptureRequest.Builder previewBuilder =
                            mTestRule.getCamera().createCaptureRequest(
                                    CameraDevice.TEMPLATE_PREVIEW);
                    CaptureRequest.Builder captureBuilder =
                            mTestRule.getCamera().createCaptureRequest(
                                    CameraDevice.TEMPLATE_STILL_CAPTURE);
                    SimpleCaptureCallback previewResultListener =
                            new SimpleCaptureCallback();
                    SimpleTimingResultListener captureResultListener =
                            new SimpleTimingResultListener();
                    SimpleImageListener[] imageListeners = new SimpleImageListener[formats.length];
                    Size[] imageSizes = new Size[formats.length];
                    for (int j = 0; j < formats.length; j++) {
                        Size sizeBound = mTestRule.isPerfClassTest() ? new Size(1920, 1080) : null;
                        imageSizes[j] = CameraTestUtils.getSortedSizesForFormat(
                                id,
                                mTestRule.getCameraManager(),
                                formats[j],
                                sizeBound).get(0);
                        imageListeners[j] = new SimpleImageListener();
                    }

                    readers = prepareStillCaptureAndStartPreview(id, previewBuilder, captureBuilder,
                            mTestRule.getOrderedPreviewSizes().get(0), imageSizes, formats,
                            previewResultListener, NUM_MAX_IMAGES, imageListeners,
                            false /*isHeic*/);

                    if (addPreviewDelay) {
                        Thread.sleep(500);
                    }

                    // Capture an image and get image data
                    startTimeMs = SystemClock.elapsedRealtime();
                    CaptureRequest request = captureBuilder.build();
                    mTestRule.getCameraSession().capture(
                            request, captureResultListener, mTestRule.getHandler());

                    Pair<CaptureResult, Long> partialResultNTime = null;
                    if (partialsExpected) {
                        partialResultNTime = captureResultListener.getPartialResultNTimeForRequest(
                                request, NUM_RESULTS_WAIT);
                        // Even if maxPartials > 1, may not see partials for some devices
                        if (partialResultNTime == null) {
                            partialsExpected = false;
                            isPartialTimingValid = false;
                        }
                    }
                    Pair<CaptureResult, Long> captureResultNTime =
                            captureResultListener.getCaptureResultNTimeForRequest(
                                    request, NUM_RESULTS_WAIT);

                    double [] imageTimes = new double[formats.length];
                    for (int j = 0; j < formats.length; j++) {
                        imageListeners[j].waitForImageAvailable(
                                CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS);
                        imageTimes[j] = imageListeners[j].getTimeReceivedImage();
                    }

                    captureTimes[i] = Stat.getAverage(imageTimes) - startTimeMs;
                    if (partialsExpected) {
                        getPartialTimes[i] = partialResultNTime.second - startTimeMs;
                        if (getPartialTimes[i] < 0) {
                            isPartialTimingValid = false;
                        }
                    }
                    getResultTimes[i] = captureResultNTime.second - startTimeMs;

                    // simulate real scenario (preview runs a bit)
                    CameraTestUtils.waitForNumResults(previewResultListener, NUM_RESULTS_WAIT,
                            WAIT_FOR_RESULT_TIMEOUT_MS);

                    blockingStopRepeating();

                    CameraTestUtils.closeImageReaders(readers);
                    readers = null;
                }
                String message = appendFormatDescription(""camera_capture_latency"",
                        formatDescription);
                mReportLog.addValues(message, captureTimes, ResultType.LOWER_BETTER, ResultUnit.MS);
                // If any of the partial results do not contain AE and AF state, then no report
                if (isPartialTimingValid) {
                    message = appendFormatDescription(""camera_partial_result_latency"",
                            formatDescription);
                    mReportLog.addValues(message, getPartialTimes, ResultType.LOWER_BETTER,
                            ResultUnit.MS);
                }
                message = appendFormatDescription(""camera_capture_result_latency"",
                        formatDescription);
                mReportLog.addValues(message, getResultTimes, ResultType.LOWER_BETTER,
                        ResultUnit.MS);

                avgResultTimes[counter] = Stat.getAverage(getResultTimes);
                avgCaptureTimes[counter] = Stat.getAverage(captureTimes);
            }
            finally {
                CameraTestUtils.closeImageReaders(readers);
                readers = null;
                mTestRule.closeDevice(id);
                closePreviewSurface();
            }
            counter++;
            mReportLog.submit(mInstrumentation);
        }

        // Result will not be reported in CTS report if no summary is printed.
        if (mTestRule.getCameraIdsUnderTest().length != 0) {
            String streamName = appendFormatDescription(""test_single_capture_average"",
                    formatDescription);
            mReportLog = new DeviceReportLog(REPORT_LOG_NAME, streamName);
            // In performance measurement mode, capture the buffer latency rather than result
            // latency.
            if (mTestRule.isPerfMeasure()) {
                String message = appendFormatDescription(
                        ""camera_capture_average_latency_for_all_cameras"", formatDescription);
                mReportLog.setSummary(message, Stat.getAverage(avgCaptureTimes),
                        ResultType.LOWER_BETTER, ResultUnit.MS);
            } else {
                String message = appendFormatDescription(
                        ""camera_capture_result_average_latency_for_all_cameras"", formatDescription);
                mReportLog.setSummary(message, Stat.getAverage(avgResultTimes),
                        ResultType.LOWER_BETTER, ResultUnit.MS);
            }
            mReportLog.submit(mInstrumentation);
        }
    }

    /**
     * Test multiple capture KPI for YUV_420_888 format: the average time duration
     * between sending out image capture requests and receiving capture results.
     * <p>
     * It measures capture latency, which is the time between sending out the capture
     * request and getting the full capture result, and the frame duration, which is the timestamp
     * gap between results.
     * </p>
     */"	""	""	"JPEG 1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.PerformanceTest"	"testReprocessingCaptureStall"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/PerformanceTest.java"	""	"public void testReprocessingCaptureStall() throws Exception {
        for (String id : mTestRule.getCameraIdsUnderTest()) {
            for (int format : REPROCESS_FORMATS) {
                if (!isReprocessSupported(id, format)) {
                    continue;
                }

                try {
                    mTestRule.openDevice(id);
                    String streamName = ""test_reprocessing_capture_stall"";
                    mReportLog = new DeviceReportLog(REPORT_LOG_NAME, streamName);
                    mReportLog.addValue(""camera_id"", id, ResultType.NEUTRAL, ResultUnit.NONE);
                    mReportLog.addValue(""format"", format, ResultType.NEUTRAL, ResultUnit.NONE);
                    reprocessingCaptureStallTestByCamera(format);
                } finally {
                    closeReaderWriters();
                    mTestRule.closeDevice(id);
                    closePreviewSurface();
                    mReportLog.submit(mInstrumentation);
                }
            }
        }
    }

    private void reprocessingCaptureStallTestByCamera(int reprocessInputFormat) throws Exception {
        prepareReprocessCapture(reprocessInputFormat);

        // Let it stream for a while before reprocessing
        startZslStreaming();
        waitForFrames(NUM_RESULTS_WAIT);

        final int NUM_REPROCESS_TESTED = MAX_REPROCESS_IMAGES / 2;
        // Prepare several reprocessing request
        Image[] inputImages = new Image[NUM_REPROCESS_TESTED];
        CaptureRequest.Builder[] reprocessReqs = new CaptureRequest.Builder[MAX_REPROCESS_IMAGES];
        for (int i = 0; i < NUM_REPROCESS_TESTED; i++) {
            inputImages[i] =
                    mCameraZslImageListener.getImage(CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS);
            TotalCaptureResult zslResult =
                    mZslResultListener.getCaptureResult(
                            WAIT_FOR_RESULT_TIMEOUT_MS, inputImages[i].getTimestamp());
            reprocessReqs[i] = mTestRule.getCamera().createReprocessCaptureRequest(zslResult);
            reprocessReqs[i].addTarget(mJpegReader.getSurface());
            reprocessReqs[i].set(CaptureRequest.NOISE_REDUCTION_MODE,
                    CaptureRequest.NOISE_REDUCTION_MODE_HIGH_QUALITY);
            reprocessReqs[i].set(CaptureRequest.EDGE_MODE,
                    CaptureRequest.EDGE_MODE_HIGH_QUALITY);
            mWriter.queueInputImage(inputImages[i]);
        }

        double[] maxCaptureGapsMs = new double[NUM_REPROCESS_TESTED];
        double[] averageFrameDurationMs = new double[NUM_REPROCESS_TESTED];
        Arrays.fill(averageFrameDurationMs, 0.0);
        final int MAX_REPROCESS_RETURN_FRAME_COUNT = 20;
        SimpleCaptureCallback reprocessResultListener = new SimpleCaptureCallback();
        for (int i = 0; i < NUM_REPROCESS_TESTED; i++) {
            mZslResultListener.drain();
            CaptureRequest reprocessRequest = reprocessReqs[i].build();
            mTestRule.getCameraSession().capture(
                    reprocessRequest, reprocessResultListener, mTestRule.getHandler());
            // Wait for reprocess output jpeg and result come back.
            reprocessResultListener.getCaptureResultForRequest(reprocessRequest,
                    CameraTestUtils.CAPTURE_RESULT_TIMEOUT_MS);
            mJpegListener.getImage(CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS).close();
            long numFramesMaybeStalled = mZslResultListener.getTotalNumFrames();
            assertTrue(""Reprocess capture result should be returned in ""
                            + MAX_REPROCESS_RETURN_FRAME_COUNT + "" frames"",
                    numFramesMaybeStalled <= MAX_REPROCESS_RETURN_FRAME_COUNT);

            // Need look longer time, as the stutter could happen after the reprocessing
            // output frame is received.
            long[] timestampGap = new long[MAX_REPROCESS_RETURN_FRAME_COUNT + 1];
            Arrays.fill(timestampGap, 0);
            CaptureResult[] results = new CaptureResult[timestampGap.length];
            long[] frameDurationsNs = new long[timestampGap.length];
            for (int j = 0; j < results.length; j++) {
                results[j] = mZslResultListener.getCaptureResult(
                        CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS);
                if (j > 0) {
                    timestampGap[j] = results[j].get(CaptureResult.SENSOR_TIMESTAMP) -
                            results[j - 1].get(CaptureResult.SENSOR_TIMESTAMP);
                    assertTrue(""Time stamp should be monotonically increasing"",
                            timestampGap[j] > 0);
                }
                frameDurationsNs[j] = results[j].get(CaptureResult.SENSOR_FRAME_DURATION);
            }

            if (VERBOSE) {
                Log.i(TAG, ""timestampGap: "" + Arrays.toString(timestampGap));
                Log.i(TAG, ""frameDurationsNs: "" + Arrays.toString(frameDurationsNs));
            }

            // Get the number of candidate results, calculate the average frame duration
            // and max timestamp gap.
            Arrays.sort(timestampGap);
            double maxTimestampGapMs = timestampGap[timestampGap.length - 1] / 1000000.0;
            for (int m = 0; m < frameDurationsNs.length; m++) {
                averageFrameDurationMs[i] += (frameDurationsNs[m] / 1000000.0);
            }
            averageFrameDurationMs[i] /= frameDurationsNs.length;

            maxCaptureGapsMs[i] = maxTimestampGapMs;
        }

        blockingStopRepeating();

        String reprocessType = ""YUV reprocessing"";
        if (reprocessInputFormat == ImageFormat.PRIVATE) {
            reprocessType = ""opaque reprocessing"";
        }
        mReportLog.addValue(""reprocess_type"", reprocessType, ResultType.NEUTRAL, ResultUnit.NONE);
        mReportLog.addValues(""max_capture_timestamp_gaps"", maxCaptureGapsMs,
                ResultType.LOWER_BETTER, ResultUnit.MS);
        mReportLog.addValues(""capture_average_frame_duration"", averageFrameDurationMs,
                ResultType.LOWER_BETTER, ResultUnit.MS);
        mReportLog.setSummary(""camera_reprocessing_average_max_capture_timestamp_gaps"",
                Stat.getAverage(maxCaptureGapsMs), ResultType.LOWER_BETTER, ResultUnit.MS);

        // The max timestamp gap should be less than (captureStall + 1) x average frame
        // duration * (1 + error margin).
        int maxCaptureStallFrames = mTestRule.getStaticInfo().getMaxCaptureStallOrDefault();
        for (int i = 0; i < maxCaptureGapsMs.length; i++) {
            double stallDurationBound = averageFrameDurationMs[i] *
                    (maxCaptureStallFrames + 1) * (1 + REPROCESS_STALL_MARGIN);
            assertTrue(""max capture stall duration should be no larger than "" + stallDurationBound,
                    maxCaptureGapsMs[i] <= stallDurationBound);
        }
    }

    private void reprocessingPerformanceTestByCamera(int reprocessInputFormat, boolean asyncMode,
            boolean requireHighQuality)
            throws Exception {
        // Prepare the reprocessing capture
        prepareReprocessCapture(reprocessInputFormat);

        // Start ZSL streaming
        startZslStreaming();
        waitForFrames(NUM_RESULTS_WAIT);

        CaptureRequest.Builder[] reprocessReqs = new CaptureRequest.Builder[MAX_REPROCESS_IMAGES];
        Image[] inputImages = new Image[MAX_REPROCESS_IMAGES];
        double[] getImageLatenciesMs = new double[MAX_REPROCESS_IMAGES];
        long startTimeMs;
        for (int i = 0; i < MAX_REPROCESS_IMAGES; i++) {
            inputImages[i] =
                    mCameraZslImageListener.getImage(CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS);
            TotalCaptureResult zslResult =
                    mZslResultListener.getCaptureResult(
                            WAIT_FOR_RESULT_TIMEOUT_MS, inputImages[i].getTimestamp());
            reprocessReqs[i] = mTestRule.getCamera().createReprocessCaptureRequest(zslResult);
            if (requireHighQuality) {
                // Reprocessing should support high quality for NR and edge modes.
                reprocessReqs[i].set(CaptureRequest.NOISE_REDUCTION_MODE,
                        CaptureRequest.NOISE_REDUCTION_MODE_HIGH_QUALITY);
                reprocessReqs[i].set(CaptureRequest.EDGE_MODE,
                        CaptureRequest.EDGE_MODE_HIGH_QUALITY);
            }
            reprocessReqs[i].addTarget(mJpegReader.getSurface());
        }

        if (asyncMode) {
            // async capture: issue all the reprocess requests as quick as possible, then
            // check the throughput of the output jpegs.
            for (int i = 0; i < MAX_REPROCESS_IMAGES; i++) {
                // Could be slow for YUV reprocessing, do it in advance.
                mWriter.queueInputImage(inputImages[i]);
            }

            // Submit the requests
            for (int i = 0; i < MAX_REPROCESS_IMAGES; i++) {
                mTestRule.getCameraSession().capture(reprocessReqs[i].build(), null, null);
            }

            // Get images
            startTimeMs = SystemClock.elapsedRealtime();
            Image jpegImages[] = new Image[MAX_REPROCESS_IMAGES];
            for (int i = 0; i < MAX_REPROCESS_IMAGES; i++) {
                jpegImages[i] = mJpegListener.getImage(CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS);
                getImageLatenciesMs[i] = SystemClock.elapsedRealtime() - startTimeMs;
                startTimeMs = SystemClock.elapsedRealtime();
            }
            for (Image i : jpegImages) {
                i.close();
            }
        } else {
            // sync capture: issue reprocess request one by one, only submit next one when
            // the previous capture image is returned. This is to test the back to back capture
            // performance.
            Image jpegImages[] = new Image[MAX_REPROCESS_IMAGES];
            for (int i = 0; i < MAX_REPROCESS_IMAGES; i++) {
                startTimeMs = SystemClock.elapsedRealtime();
                mWriter.queueInputImage(inputImages[i]);
                mTestRule.getCameraSession().capture(reprocessReqs[i].build(), null, null);
                jpegImages[i] = mJpegListener.getImage(CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS);
                getImageLatenciesMs[i] = SystemClock.elapsedRealtime() - startTimeMs;
            }
            for (Image i : jpegImages) {
                i.close();
            }
        }

        blockingStopRepeating();

        String reprocessType = ""YUV reprocessing"";
        if (reprocessInputFormat == ImageFormat.PRIVATE) {
            reprocessType = ""opaque reprocessing"";
        }

        // Report the performance data
        String captureMsg;
        if (asyncMode) {
            captureMsg = ""capture latency"";
            if (requireHighQuality) {
                captureMsg += "" for High Quality noise reduction and edge modes"";
            }
            mReportLog.addValue(""reprocess_type"", reprocessType, ResultType.NEUTRAL,
                    ResultUnit.NONE);
            mReportLog.addValue(""capture_message"", captureMsg, ResultType.NEUTRAL,
                    ResultUnit.NONE);
            mReportLog.addValues(""latency"", getImageLatenciesMs, ResultType.LOWER_BETTER,
                    ResultUnit.MS);
            mReportLog.setSummary(""camera_reprocessing_average_latency"",
                    Stat.getAverage(getImageLatenciesMs), ResultType.LOWER_BETTER, ResultUnit.MS);
        } else {
            captureMsg = ""shot to shot latency"";
            if (requireHighQuality) {
                captureMsg += "" for High Quality noise reduction and edge modes"";
            }
            mReportLog.addValue(""reprocess_type"", reprocessType, ResultType.NEUTRAL,
                    ResultUnit.NONE);
            mReportLog.addValue(""capture_message"", captureMsg, ResultType.NEUTRAL,
                    ResultUnit.NONE);
            mReportLog.addValues(""latency"", getImageLatenciesMs, ResultType.LOWER_BETTER,
                    ResultUnit.MS);
            mReportLog.setSummary(""camera_reprocessing_shot_to_shot_average_latency"",
                    Stat.getAverage(getImageLatenciesMs), ResultType.LOWER_BETTER, ResultUnit.MS);
        }
    }

    /**
     * Start preview and ZSL streaming
     */
    private void startZslStreaming() throws Exception {
        CaptureRequest.Builder zslBuilder =
                mTestRule.getCamera().createCaptureRequest(CameraDevice.TEMPLATE_ZERO_SHUTTER_LAG);
        zslBuilder.addTarget(mPreviewSurface);
        zslBuilder.addTarget(mCameraZslReader.getSurface());
        mTestRule.getCameraSession().setRepeatingRequest(
                zslBuilder.build(), mZslResultListener, mTestRule.getHandler());
    }

    /**
     * Wait for a certain number of frames, the images and results will be drained from the
     * listeners to make sure that next reprocessing can get matched results and images.
     *
     * @param numFrameWait The number of frames to wait before return, 0 means that
     *      this call returns immediately after streaming on.
     */
    private void waitForFrames(int numFrameWait) throws Exception {
        if (numFrameWait < 0) {
            throw new IllegalArgumentException(""numFrameWait "" + numFrameWait +
                    "" should be non-negative"");
        }

        for (int i = 0; i < numFrameWait; i++) {
            mCameraZslImageListener.getImage(CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS).close();
        }
    }

    private void closeReaderWriters() {
        mCameraZslImageListener.drain();
        CameraTestUtils.closeImageReader(mCameraZslReader);
        mCameraZslReader = null;
        mJpegListener.drain();
        CameraTestUtils.closeImageReader(mJpegReader);
        mJpegReader = null;
        CameraTestUtils.closeImageWriter(mWriter);
        mWriter = null;
    }

    private void prepareReprocessCapture(int inputFormat)
            throws CameraAccessException {
        // 1. Find the right preview and capture sizes.
        Size maxPreviewSize = mTestRule.getOrderedPreviewSizes().get(0);
        Size[] supportedInputSizes =
                mTestRule.getStaticInfo().getAvailableSizesForFormatChecked(inputFormat,
                        StaticMetadata.StreamDirection.Input);
        Size maxInputSize = CameraTestUtils.getMaxSize(supportedInputSizes);
        Size maxJpegSize = mTestRule.getOrderedStillSizes().get(0);
        updatePreviewSurface(maxPreviewSize);
        mZslResultListener = new SimpleCaptureCallback();

        // 2. Create camera output ImageReaders.
        // YUV/Opaque output, camera should support output with input size/format
        mCameraZslImageListener = new SimpleImageReaderListener(
                /*asyncMode*/true, MAX_ZSL_IMAGES - MAX_REPROCESS_IMAGES);
        mCameraZslReader = CameraTestUtils.makeImageReader(
                maxInputSize, inputFormat, MAX_ZSL_IMAGES,
                mCameraZslImageListener, mTestRule.getHandler());
        // Jpeg reprocess output
        mJpegListener = new SimpleImageReaderListener();
        mJpegReader = CameraTestUtils.makeImageReader(
                maxJpegSize, ImageFormat.JPEG, MAX_JPEG_IMAGES,
                mJpegListener, mTestRule.getHandler());

        // create camera reprocess session
        List<Surface> outSurfaces = new ArrayList<Surface>();
        outSurfaces.add(mPreviewSurface);
        outSurfaces.add(mCameraZslReader.getSurface());
        outSurfaces.add(mJpegReader.getSurface());
        InputConfiguration inputConfig = new InputConfiguration(maxInputSize.getWidth(),
                maxInputSize.getHeight(), inputFormat);
        mTestRule.setCameraSessionListener(new BlockingSessionCallback());
        mTestRule.setCameraSession(CameraTestUtils.configureReprocessableCameraSession(
                mTestRule.getCamera(), inputConfig, outSurfaces,
                mTestRule.getCameraSessionListener(), mTestRule.getHandler()));

        // 3. Create ImageWriter for input
        mWriter = CameraTestUtils.makeImageWriter(
                mTestRule.getCameraSession().getInputSurface(), MAX_INPUT_IMAGES,
                /*listener*/null, /*handler*/null);
    }

    /**
     * Stop repeating requests for current camera and waiting for it to go back to idle, resulting
     * in an idle device.
     */
    private void blockingStopRepeating() throws Exception {
        stopRepeating();
        mTestRule.getCameraSessionListener().getStateWaiter().waitForState(
                BlockingSessionCallback.SESSION_READY, CameraTestUtils.CAMERA_IDLE_TIMEOUT_MS);
    }

    private void blockingStartPreview(String id, CaptureCallback listener,
            CaptureRequest previewRequest, SimpleImageListener imageListener)
            throws Exception {
        mTestRule.getCameraSession().setRepeatingRequest(
                previewRequest, listener, mTestRule.getHandler());
        imageListener.waitForImageAvailable(CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS);
    }

    /**
     * Setup still capture configuration and start preview.
     *
     * @param id The camera id under test
     * @param previewBuilder The capture request builder to be used for preview
     * @param stillBuilder The capture request builder to be used for still capture
     * @param previewSz Preview size
     * @param captureSizes Still capture sizes
     * @param formats The single capture image formats
     * @param resultListener Capture result listener
     * @param maxNumImages The max number of images set to the image reader
     * @param imageListeners The single capture capture image listeners
     * @param isHeic Capture HEIC image if true, JPEG image if false
     */
    private ImageReader[] prepareStillCaptureAndStartPreview(String id,
            CaptureRequest.Builder previewBuilder, CaptureRequest.Builder stillBuilder,
            Size previewSz, Size[] captureSizes, int[] formats, CaptureCallback resultListener,
            int maxNumImages, ImageReader.OnImageAvailableListener[] imageListeners,
            boolean isHeic)
            throws Exception {

        if ((captureSizes == null) || (formats == null) || (imageListeners == null) &&
                (captureSizes.length != formats.length) ||
                (formats.length != imageListeners.length)) {
            throw new IllegalArgumentException(""Invalid capture sizes/formats or image listeners!"");
        }

        if (VERBOSE) {
            Log.v(TAG, String.format(""Prepare still capture and preview (%s)"",
                    previewSz.toString()));
        }

        // Update preview size.
        updatePreviewSurface(previewSz);

        ImageReader[] readers = new ImageReader[captureSizes.length];
        List<Surface> outputSurfaces = new ArrayList<Surface>();
        outputSurfaces.add(mPreviewSurface);
        for (int i = 0; i < captureSizes.length; i++) {
            readers[i] = CameraTestUtils.makeImageReader(captureSizes[i], formats[i], maxNumImages,
                    imageListeners[i], mTestRule.getHandler());
            outputSurfaces.add(readers[i].getSurface());
        }

        // Configure the requests.
        previewBuilder.addTarget(mPreviewSurface);
        stillBuilder.addTarget(mPreviewSurface);
        for (int i = 0; i < readers.length; i++) {
            stillBuilder.addTarget(readers[i].getSurface());
        }

        // Update target fps based on the min frame duration of preview.
        CameraCharacteristics ch = mTestRule.getStaticInfo().getCharacteristics();
        StreamConfigurationMap config = ch.get(
                CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        long minFrameDuration = Math.max(FRAME_DURATION_NS_30FPS, config.getOutputMinFrameDuration(
                SurfaceTexture.class, previewSz));
        Range<Integer> targetRange =
                CameraTestUtils.getSuitableFpsRangeForDuration(id,
                minFrameDuration, mTestRule.getStaticInfo());
        previewBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, targetRange);
        stillBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, targetRange);

        CaptureRequest previewRequest = previewBuilder.build();
        mTestRule.setCameraSessionListener(new BlockingSessionCallback());
        boolean useSessionKeys = isFpsRangeASessionKey(ch);
        configureAndSetCameraSession(outputSurfaces, useSessionKeys, previewRequest);

        // Start preview.
        mTestRule.getCameraSession().setRepeatingRequest(
                previewRequest, resultListener, mTestRule.getHandler());

        return readers;
    }

    /**
     * Helper function to check if TARGET_FPS_RANGE is a session parameter
     */
    private boolean isFpsRangeASessionKey(CameraCharacteristics ch) {
        List<CaptureRequest.Key<?>> sessionKeys = ch.getAvailableSessionKeys();
        return sessionKeys != null &&
                sessionKeys.contains(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE);
    }

    /**
     * Helper function to configure camera session using parameters provided.
     */
    private void configureAndSetCameraSession(List<Surface> surfaces,
            boolean useInitialRequest, CaptureRequest initialRequest)
            throws CameraAccessException {
        CameraCaptureSession cameraSession;
        if (useInitialRequest) {
            cameraSession = CameraTestUtils.configureCameraSessionWithParameters(
                mTestRule.getCamera(), surfaces,
                mTestRule.getCameraSessionListener(), mTestRule.getHandler(),
                initialRequest);
        } else {
            cameraSession = CameraTestUtils.configureCameraSession(
                mTestRule.getCamera(), surfaces,
                mTestRule.getCameraSessionListener(), mTestRule.getHandler());
        }
        mTestRule.setCameraSession(cameraSession);
    }

    /**
     * Setup single capture configuration and start preview.
     *
     * @param previewBuilder The capture request builder to be used for preview
     * @param stillBuilder The capture request builder to be used for still capture
     * @param previewSz Preview size
     * @param captureSz Still capture size
     * @param format The single capture image format
     * @param resultListener Capture result listener
     * @param sessionListener Session listener
     * @param maxNumImages The max number of images set to the image reader
     * @param imageListener The single capture capture image listener
     * @param useSessionKeys Create capture session using session keys from previewRequest
     */
    private void prepareCaptureAndStartPreview(CaptureRequest.Builder previewBuilder,
            CaptureRequest.Builder stillBuilder, Size previewSz, Size captureSz, int format,
            CaptureCallback resultListener, CameraCaptureSession.StateCallback sessionListener,
            int maxNumImages, ImageReader.OnImageAvailableListener imageListener,
            boolean  useSessionKeys) throws Exception {
        if ((captureSz == null) || (imageListener == null)) {
            throw new IllegalArgumentException(""Invalid capture size or image listener!"");
        }

        if (VERBOSE) {
            Log.v(TAG, String.format(""Prepare single capture (%s) and preview (%s)"",
                    captureSz.toString(), previewSz.toString()));
        }

        // Update preview size.
        updatePreviewSurface(previewSz);

        // Create ImageReader.
        mTestRule.createDefaultImageReader(captureSz, format, maxNumImages, imageListener);

        // Configure output streams with preview and jpeg streams.
        List<Surface> outputSurfaces = new ArrayList<Surface>();
        outputSurfaces.add(mPreviewSurface);
        outputSurfaces.add(mTestRule.getReaderSurface());
        if (sessionListener == null) {
            mTestRule.setCameraSessionListener(new BlockingSessionCallback());
        } else {
            mTestRule.setCameraSessionListener(new BlockingSessionCallback(sessionListener));
        }

        // Configure the requests.
        previewBuilder.addTarget(mPreviewSurface);
        stillBuilder.addTarget(mPreviewSurface);
        stillBuilder.addTarget(mTestRule.getReaderSurface());
        CaptureRequest previewRequest = previewBuilder.build();

        configureAndSetCameraSession(outputSurfaces, useSessionKeys, previewRequest);

        // Start preview.
        mTestRule.getCameraSession().setRepeatingRequest(
                previewRequest, resultListener, mTestRule.getHandler());
    }

    /**
     * Update the preview surface size.
     *
     * @param size The preview size to be updated.
     */
    private void updatePreviewSurface(Size size) {
        if ((mPreviewSurfaceTexture != null ) || (mPreviewSurface != null)) {
            closePreviewSurface();
        }

        mPreviewSurfaceTexture = new SurfaceTexture(/*random int*/ 1);
        mPreviewSurfaceTexture.setDefaultBufferSize(size.getWidth(), size.getHeight());
        mPreviewSurface = new Surface(mPreviewSurfaceTexture);
    }

    /**
     * Release preview surface and corresponding surface texture.
     */
    private void closePreviewSurface() {
        if (mPreviewSurface != null) {
            mPreviewSurface.release();
            mPreviewSurface = null;
        }

        if (mPreviewSurfaceTexture != null) {
            mPreviewSurfaceTexture.release();
            mPreviewSurfaceTexture = null;
        }
    }

    private boolean isReprocessSupported(String cameraId, int format)
            throws CameraAccessException {
        if (format != ImageFormat.YUV_420_888 && format != ImageFormat.PRIVATE) {
            throw new IllegalArgumentException(
                    ""format "" + format + "" is not supported for reprocessing"");
        }

        StaticMetadata info = new StaticMetadata(
                mTestRule.getCameraManager().getCameraCharacteristics(cameraId), CheckLevel.ASSERT,
                /*collector*/ null);
        int cap = CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_YUV_REPROCESSING;
        if (format == ImageFormat.PRIVATE) {
            cap = CameraCharacteristics.REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING;
        }
        return info.isCapabilitySupported(cap);
    }

    /**
     * Stop the repeating requests of current camera.
     * Does _not_ wait for the device to go idle
     */
    private void stopRepeating() throws Exception {
        // Stop repeat, wait for captures to complete, and disconnect from surfaces
        if (mTestRule.getCameraSession() != null) {
            if (VERBOSE) Log.v(TAG, ""Stopping preview"");
            mTestRule.getCameraSession().stopRepeating();
        }
    }

    /**
     * Configure reader and preview outputs and wait until done.
     *
     * @return The preview capture request
     */
    private CaptureRequest configureReaderAndPreviewOutputs(
            String id, boolean isColorOutputSupported)
            throws Exception {
        if (mPreviewSurface == null || mTestRule.getReaderSurface() == null) {
            throw new IllegalStateException(""preview and reader surface must be initilized first"");
        }

        // Create previewBuilder
        CaptureRequest.Builder previewBuilder =
                mTestRule.getCamera().createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        if (isColorOutputSupported) {
            previewBuilder.addTarget(mPreviewSurface);
        }
        previewBuilder.addTarget(mTestRule.getReaderSurface());


        // Figure out constant target FPS range no larger than 30fps
        CameraCharacteristics ch = mTestRule.getStaticInfo().getCharacteristics();
        StreamConfigurationMap config =
                ch.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
        long minFrameDuration = Math.max(FRAME_DURATION_NS_30FPS,
                config.getOutputMinFrameDuration(mImageReaderFormat, mPreviewSize));

        List<Surface> outputSurfaces = new ArrayList<>();
        outputSurfaces.add(mTestRule.getReaderSurface());
        if (isColorOutputSupported) {
            outputSurfaces.add(mPreviewSurface);
            minFrameDuration = Math.max(minFrameDuration,
                    config.getOutputMinFrameDuration(SurfaceTexture.class, mPreviewSize));
        }
        Range<Integer> targetRange =
                CameraTestUtils.getSuitableFpsRangeForDuration(id,
                        minFrameDuration, mTestRule.getStaticInfo());
        previewBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, targetRange);

        // Create capture session
        boolean useSessionKeys = isFpsRangeASessionKey(ch);
        CaptureRequest previewRequest = previewBuilder.build();
        mTestRule.setCameraSessionListener(new BlockingSessionCallback());
        configureAndSetCameraSession(outputSurfaces, useSessionKeys, previewRequest);

        return previewRequest;
    }

    /**
     * Initialize the ImageReader instance and preview surface.
     * @param cameraId The camera to be opened.
     * @param format The format used to create ImageReader instance.
     */
    private void initializeImageReader(String cameraId, int format) throws Exception {
        mTestRule.setOrderedPreviewSizes(CameraTestUtils.getSortedSizesForFormat(
                cameraId, mTestRule.getCameraManager(), format,
                CameraTestUtils.getPreviewSizeBound(mTestRule.getWindowManager(),
                        CameraTestUtils.PREVIEW_SIZE_BOUND)));
        mPreviewSize = mTestRule.getOrderedPreviewSizes().get(0);
        mImageReaderFormat = format;
        mTestRule.createDefaultImageReader(
                mPreviewSize, format, NUM_MAX_IMAGES, /*listener*/null);
        updatePreviewSurface(mPreviewSize);
    }

    private void simpleOpenCamera(String cameraId) throws Exception {
        mTestRule.setCamera(CameraTestUtils.openCamera(
                mTestRule.getCameraManager(), cameraId,
                mTestRule.getCameraListener(), mTestRule.getHandler()));
        mTestRule.getCollector().setCameraId(cameraId);
        mTestRule.setStaticInfo(new StaticMetadata(
                mTestRule.getCameraManager().getCameraCharacteristics(cameraId),
                CheckLevel.ASSERT, /*collector*/null));
    }

    /**
     * Simple image listener that can be used to time the availability of first image.
     *
     */
    private static class SimpleImageListener implements ImageReader.OnImageAvailableListener {
        private ConditionVariable imageAvailable = new ConditionVariable();
        private boolean imageReceived = false;
        private long mTimeReceivedImage = 0;

        @Override
        public void onImageAvailable(ImageReader reader) {
            Image image = null;
            if (!imageReceived) {
                if (VERBOSE) {
                    Log.v(TAG, ""First image arrives"");
                }
                imageReceived = true;
                mTimeReceivedImage = SystemClock.elapsedRealtime();
                imageAvailable.open();
            }
            image = reader.acquireNextImage();
            if (image != null) {
                image.close();
            }
        }

        /**
         * Wait for image available, return immediately if the image was already
         * received, otherwise wait until an image arrives.
         */
        public void waitForImageAvailable(long timeout) {
            if (imageReceived) {
                imageReceived = false;
                return;
            }

            if (imageAvailable.block(timeout)) {
                imageAvailable.close();
                imageReceived = true;
            } else {
                throw new TimeoutRuntimeException(""Unable to get the first image after ""
                        + CameraTestUtils.CAPTURE_IMAGE_TIMEOUT_MS + ""ms"");
            }
        }

        public long getTimeReceivedImage() {
            return mTimeReceivedImage;
        }
    }

    private static class SimpleTimingResultListener
            extends CameraCaptureSession.CaptureCallback {
        private final LinkedBlockingQueue<Pair<CaptureResult, Long> > mPartialResultQueue =
                new LinkedBlockingQueue<Pair<CaptureResult, Long> >();
        private final LinkedBlockingQueue<Pair<CaptureResult, Long> > mResultQueue =
                new LinkedBlockingQueue<Pair<CaptureResult, Long> > ();

        @Override
        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request,
                TotalCaptureResult result) {
            try {
                Long time = SystemClock.elapsedRealtime();
                mResultQueue.put(new Pair<CaptureResult, Long>(result, time));
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureCompleted"");
            }
        }

        @Override
        public void onCaptureProgressed(CameraCaptureSession session, CaptureRequest request,
                CaptureResult partialResult) {
            try {
                // check if AE and AF state exists
                Long time = -1L;
                if (partialResult.get(CaptureResult.CONTROL_AE_STATE) != null &&
                        partialResult.get(CaptureResult.CONTROL_AF_STATE) != null) {
                    time = SystemClock.elapsedRealtime();
                }
                mPartialResultQueue.put(new Pair<CaptureResult, Long>(partialResult, time));
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(
                        ""Can't handle InterruptedException in onCaptureProgressed"");
            }
        }

        public Pair<CaptureResult, Long> getPartialResultNTime(long timeout) {
            try {
                Pair<CaptureResult, Long> result =
                        mPartialResultQueue.poll(timeout, TimeUnit.MILLISECONDS);
                return result;
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }
        }

        public Pair<CaptureResult, Long> getCaptureResultNTime(long timeout) {
            try {
                Pair<CaptureResult, Long> result =
                        mResultQueue.poll(timeout, TimeUnit.MILLISECONDS);
                assertNotNull(""Wait for a capture result timed out in "" + timeout + ""ms"", result);
                return result;
            } catch (InterruptedException e) {
                throw new UnsupportedOperationException(""Unhandled interrupted exception"", e);
            }
        }

        public Pair<CaptureResult, Long> getPartialResultNTimeForRequest(CaptureRequest myRequest,
                int numResultsWait) {
            if (numResultsWait < 0) {
                throw new IllegalArgumentException(""numResultsWait must be no less than 0"");
            }

            Pair<CaptureResult, Long> result;
            int i = 0;
            do {
                result = getPartialResultNTime(CameraTestUtils.CAPTURE_RESULT_TIMEOUT_MS);
                // The result may be null if no partials are produced on this particular path, so
                // stop trying
                if (result == null) break;
                if (result.first.getRequest().equals(myRequest)) {
                    return result;
                }
            } while (i++ < numResultsWait);

            // No partials produced - this may not be an error, since a given device may not
            // produce any partials on this testing path
            return null;
        }

        public Pair<CaptureResult, Long> getCaptureResultNTimeForRequest(CaptureRequest myRequest,
                int numResultsWait) {
            if (numResultsWait < 0) {
                throw new IllegalArgumentException(""numResultsWait must be no less than 0"");
            }

            Pair<CaptureResult, Long> result;
            int i = 0;
            do {
                result = getCaptureResultNTime(CameraTestUtils.CAPTURE_RESULT_TIMEOUT_MS);
                if (result.first.getRequest().equals(myRequest)) {
                    return result;
                }
            } while (i++ < numResultsWait);

            throw new TimeoutRuntimeException(""Unable to get the expected capture result after ""
                    + ""waiting for "" + numResultsWait + "" results"");
        }

    }
}"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.NativeImageReaderTest"	"testJpeg"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/NativeImageReaderTest.java"	""	"public void testJpeg() {
        assertTrue(""testJpeg fail, see log for details"",
                testJpegNative(mDebugFileNameBase, mOverrideCameraId));
    }"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.NativeImageReaderTest"	"testDepthJpeg"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/NativeImageReaderTest.java"	""	"public void testDepthJpeg() {
        assertTrue(""testDepthJpeg fail, see log for details"",
                testDepthJpegNative(mDebugFileNameBase, mOverrideCameraId));
    }"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.NativeImageReaderTest"	"testImageReaderCloseAcquiredImages"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/NativeImageReaderTest.java"	""	"public void testImageReaderCloseAcquiredImages() {
        assertTrue(""testImageReaderClose fail, see log for details"",
                testImageReaderCloseAcquiredImagesNative(mOverrideCameraId));
    }

    private static native boolean testJpegNative(String filePath, String overrideCameraId);
    private static native boolean testY8Native(String filePath, String overrideCameraId);
    private static native boolean testHeicNative(String filePath, String overrideCameraId);
    private static native boolean testDepthJpegNative(String filePath, String overrideCameraId);
    private static native boolean testImageReaderCloseAcquiredImagesNative(String overrideCameraId);
}"	""	""	"JPEG"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"com.android.cts.verifier.camera.video.CameraVideoActivity"	"exists"	""	"/home/gpoor/cts-12-source/cts/apps/CtsVerifier/src/com/android/cts/verifier/camera/video/CameraVideoActivity.java"	""	"public void test/*
 *.
 */
package com.android.cts.verifier.camera.video;

import android.app.AlertDialog;
import android.content.Context;
import android.content.DialogInterface;
import android.graphics.Matrix;
import android.graphics.SurfaceTexture;
import android.hardware.Camera;
import android.hardware.Camera.CameraInfo;
import android.hardware.Camera.Size;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraCharacteristics;
import android.hardware.camera2.CameraManager;
import android.hardware.cts.helpers.CameraUtils;
import android.media.CamcorderProfile;
import android.media.MediaPlayer;
import android.media.MediaRecorder;
import android.os.Bundle;
import android.os.Environment;
import android.os.Handler;
import android.text.method.ScrollingMovementMethod;
import android.util.Log;
import android.view.Surface;
import android.view.TextureView;
import android.view.View;
import android.widget.AdapterView;
import android.widget.ArrayAdapter;
import android.widget.Button;
import android.widget.ImageButton;
import android.widget.Spinner;
import android.widget.TextView;
import android.widget.Toast;
import android.widget.VideoView;

import com.android.cts.verifier.PassFailButtons;
import com.android.cts.verifier.R;

import java.io.File;
import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.Date;
import java.util.List;
import java.util.Optional;
import java.util.TreeSet;


/**
 * Tests for manual verification of camera video capture
 */
public class CameraVideoActivity extends PassFailButtons.Activity
        implements TextureView.SurfaceTextureListener {

    private static final String TAG = ""CtsCameraVideo"";
    private static final boolean VERBOSE = Log.isLoggable(TAG, Log.VERBOSE);
    private static final int MEDIA_TYPE_IMAGE = 1;
    private static final int MEDIA_TYPE_VIDEO = 2;
    private static final int VIDEO_LENGTH = 3000; // in ms

    private TextureView mPreviewView;
    private SurfaceTexture mPreviewTexture;
    private int mPreviewTexWidth;
    private int mPreviewTexHeight;
    private int mPreviewRotation;
    private int mVideoRotation;

    private VideoView mPlaybackView;

    private Spinner mCameraSpinner;
    private Spinner mResolutionSpinner;

    private int mCurrentCameraId = -1;
    private Camera mCamera;
    private boolean mIsExternalCamera;
    private int mVideoFrameRate = -1;

    private MediaRecorder mMediaRecorder;

    private List<Size> mPreviewSizes;
    private Size mNextPreviewSize;
    private Size mPreviewSize;
    private List<Integer> mVideoSizeIds;
    private List<String> mVideoSizeNames;
    private int mCurrentVideoSizeId;
    private String mCurrentVideoSizeName;

    private boolean isRecording = false;
    private boolean isPlayingBack = false;
    private Button captureButton;
    private ImageButton mPassButton;
    private ImageButton mFailButton;

    private TextView mStatusLabel;

    private TreeSet<CameraCombination> mTestedCombinations = new TreeSet<>(COMPARATOR);
    private TreeSet<CameraCombination> mUntestedCombinations = new TreeSet<>(COMPARATOR);
    private TreeSet<String> mUntestedCameras = new TreeSet<>();

    private File outputVideoFile;

    private class CameraCombination {
        private final int mCameraIndex;
        private final int mVideoSizeIdIndex;
        private final String mVideoSizeName;

        private CameraCombination(
            int cameraIndex, int videoSizeIdIndex, String videoSizeName) {
            this.mCameraIndex = cameraIndex;
            this.mVideoSizeIdIndex = videoSizeIdIndex;
            this.mVideoSizeName = videoSizeName;
        }

        @Override
        public String toString() {
            return String.format(""Camera %d, %s"", mCameraIndex, mVideoSizeName);
        }
    }

    private static final Comparator<CameraCombination> COMPARATOR =
        Comparator.<CameraCombination, Integer>comparing(c -> c.mCameraIndex)
            .thenComparing(c -> c.mVideoSizeIdIndex);

    /**
     * @see #MEDIA_TYPE_IMAGE
     * @see #MEDIA_TYPE_VIDEO
     */
    private File getOutputMediaFile(int type) {
        File mediaStorageDir = new File(getExternalFilesDir(null), TAG);
        if (mediaStorageDir == null) {
            Log.e(TAG, ""failed to retrieve external files directory"");
            return null;
        }

        if (!mediaStorageDir.exists()) {
            if (!mediaStorageDir.mkdirs()) {
                Log.d(TAG, ""failed to create directory"");
                return null;
            }
        }

        String timeStamp = new SimpleDateFormat(""yyyyMMdd_HHmmss"").format(new Date());
        File mediaFile;
        if (type == MEDIA_TYPE_IMAGE) {
            mediaFile = new File(mediaStorageDir.getPath() + File.separator +
                    ""IMG_"" + timeStamp + "".jpg"");
        } else if (type == MEDIA_TYPE_VIDEO) {
            mediaFile = new File(mediaStorageDir.getPath() + File.separator +
                    ""VID_"" + timeStamp + "".mp4"");
            if (VERBOSE) {
                Log.v(TAG, ""getOutputMediaFile: output file "" + mediaFile.getPath());
            }
        } else {
            return null;
        }

        return mediaFile;
    }

    private static final int BIT_RATE_720P = 8000000;
    private static final int BIT_RATE_MIN = 64000;
    private static final int BIT_RATE_MAX = BIT_RATE_720P;

    private int getVideoBitRate(Camera.Size sz) {
        int rate = BIT_RATE_720P;
        float scaleFactor = sz.height * sz.width / (float)(1280 * 720);
        rate = (int)(rate * scaleFactor);

        // Clamp to the MIN, MAX range.
        return Math.max(BIT_RATE_MIN, Math.min(BIT_RATE_MAX, rate));
    }

    private int getVideoFrameRate() {
        return mVideoFrameRate;
    }

    private void setVideoFrameRate(int videoFrameRate) {
        mVideoFrameRate = videoFrameRate;
    }

    private boolean prepareVideoRecorder() {

        mMediaRecorder = new MediaRecorder();

        // Step 1: unlock and set camera to MediaRecorder
        mCamera.unlock();
        mMediaRecorder.setCamera(mCamera);

        // Step 2: set sources
        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);
        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);

        // Step 3: set a CamcorderProfile
        if (mIsExternalCamera) {
            Camera.Size recordSize = null;
            switch (mCurrentVideoSizeId) {
                case CamcorderProfile.QUALITY_QCIF:
                    recordSize = mCamera.new Size(176, 144);
                break;
                case CamcorderProfile.QUALITY_QVGA:
                    recordSize = mCamera.new Size(320, 240);
                break;
                case CamcorderProfile.QUALITY_CIF:
                    recordSize = mCamera.new Size(352, 288);
                break;
                case CamcorderProfile.QUALITY_480P:
                    recordSize = mCamera.new Size(720, 480);
                break;
                case CamcorderProfile.QUALITY_720P:
                    recordSize = mCamera.new Size(1280, 720);
                break;
                default:
                    String msg = ""Unknown CamcorderProfile: "" + mCurrentVideoSizeId;
                    Log.e(TAG, msg);
                    releaseMediaRecorder();
                    throw new AssertionError(msg);
            }

            mMediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.DEFAULT);
            mMediaRecorder.setVideoEncoder(MediaRecorder.VideoEncoder.DEFAULT);
            mMediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.DEFAULT);
            mMediaRecorder.setVideoEncodingBitRate(getVideoBitRate(recordSize));
            mMediaRecorder.setVideoSize(recordSize.width, recordSize.height);
            mMediaRecorder.setVideoFrameRate(getVideoFrameRate());
        } else {
            mMediaRecorder.setProfile(CamcorderProfile.get(mCurrentCameraId, mCurrentVideoSizeId));
        }

        // Step 4: set output file
        outputVideoFile = getOutputMediaFile(MEDIA_TYPE_VIDEO);
        mMediaRecorder.setOutputFile(outputVideoFile.toString());

        // Step 5: set preview output
        // This is not necessary since preview has been taken care of

        // Step 6: set orientation hint
        mMediaRecorder.setOrientationHint(mVideoRotation);

        // Step 7: prepare configured MediaRecorder
        try {
            mMediaRecorder.prepare();
        } catch (IOException e) {
            Log.e(TAG, ""IOException preparing MediaRecorder: "", e);
            releaseMediaRecorder();
            throw new AssertionError(e);
        }

        mMediaRecorder.setOnErrorListener(
                new MediaRecorder.OnErrorListener() {
                    @Override
                    public void onError(MediaRecorder mr, int what, int extra) {
                        if (what == MediaRecorder.MEDIA_RECORDER_ERROR_UNKNOWN) {
                            Log.e(TAG, ""unknown error in media recorder, error: "" + extra);
                        } else {
                            Log.e(TAG, ""media recorder server died, error: "" + extra);
                        }

                        failTest(""Media recorder error."");
                    }
                });

        if (VERBOSE) {
            Log.v(TAG, ""prepareVideoRecorder: prepared configured MediaRecorder"");
        }

        return true;
    }

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        setContentView(R.layout.camera_video);
        setPassFailButtonClickListeners();
        setInfoResources(R.string.camera_video, R.string.video_info, /*viewId*/-1);

        mPreviewView = (TextureView) findViewById(R.id.video_capture);
        mPlaybackView = (VideoView) findViewById(R.id.video_playback);
        mPlaybackView.setOnCompletionListener(mPlaybackViewListener);

        captureButton = (Button) findViewById(R.id.record_button);
        mPassButton = (ImageButton) findViewById(R.id.pass_button);
        mFailButton = (ImageButton) findViewById(R.id.fail_button);
        mPassButton.setEnabled(false);
        mFailButton.setEnabled(true);

        mPreviewView.setSurfaceTextureListener(this);

        int numCameras = Camera.getNumberOfCameras();
        String[] cameraNames = new String[numCameras];
        for (int i = 0; i < numCameras; i++) {
            cameraNames[i] = ""Camera "" + i;
            mUntestedCameras.add(""All combinations for Camera "" + i + ""\n"");
        }
        if (VERBOSE) {
            Log.v(TAG, ""onCreate: number of cameras="" + numCameras);
        }
        mCameraSpinner = (Spinner) findViewById(R.id.cameras_selection);
        mCameraSpinner.setAdapter(
            new ArrayAdapter<String>(
                this, R.layout.camera_list_item, cameraNames));
        mCameraSpinner.setOnItemSelectedListener(mCameraSpinnerListener);

        mResolutionSpinner = (Spinner) findViewById(R.id.resolution_selection);
        mResolutionSpinner.setOnItemSelectedListener(mResolutionSelectedListener);

        mStatusLabel = (TextView) findViewById(R.id.status_label);

        Button mNextButton = (Button) findViewById(R.id.next_button);
        mNextButton.setOnClickListener(v -> {
            setUntestedCombination();
            if (VERBOSE) {
                Log.v(TAG, ""onClick: mCurrentVideoSizeId = "" +
                    mCurrentVideoSizeId + "" "" + mCurrentVideoSizeName);
                Log.v(TAG, ""onClick: setting preview size ""
                    + mNextPreviewSize.width + ""x"" + mNextPreviewSize.height);
            }

            startPreview();
            if (VERBOSE) {
                Log.v(TAG, ""onClick: started new preview"");
            }
            captureButton.performClick();
        });
    }

    /**
     * Set an untested combination of the current camera and video size.
     * Triggered by next button click.
     */
    private void setUntestedCombination() {
        Optional<CameraCombination> combination = mUntestedCombinations.stream().filter(
            c -> c.mCameraIndex == mCurrentCameraId).findFirst();
        if (!combination.isPresent()) {
            Toast.makeText(this, ""All Camera "" + mCurrentCameraId + "" tests are done."",
                Toast.LENGTH_SHORT).show();
            return;
        }

        // There is untested combination for the current camera, set the next untested combination.
        int mNextVideoSizeIdIndex = combination.get().mVideoSizeIdIndex;

        mCurrentVideoSizeId = mVideoSizeIds.get(mNextVideoSizeIdIndex);
        mCurrentVideoSizeName = mVideoSizeNames.get(mNextVideoSizeIdIndex);
        mNextPreviewSize = matchPreviewRecordSize();
        mResolutionSpinner.setSelection(mNextVideoSizeIdIndex);
    }

    @Override
    public void onResume() {
        super.onResume();

        setUpCamera(mCameraSpinner.getSelectedItemPosition());
        if (VERBOSE) {
            Log.v(TAG, ""onResume: camera has been setup"");
        }

        setUpCaptureButton();
        if (VERBOSE) {
            Log.v(TAG, ""onResume: captureButton has been setup"");
        }

    }

    @Override
    public void onPause() {
        super.onPause();

        releaseMediaRecorder();
        shutdownCamera();
        mPreviewTexture = null;
    }

    private MediaPlayer.OnCompletionListener mPlaybackViewListener =
            new MediaPlayer.OnCompletionListener() {

                @Override
                public void onCompletion(MediaPlayer mp) {
                    isPlayingBack = false;
                    mPlaybackView.stopPlayback();
                    captureButton.setEnabled(true);

                    mStatusLabel.setMovementMethod(new ScrollingMovementMethod());
                    StringBuilder progress = new StringBuilder();
                    progress.append(getResources().getString(R.string.status_ready));
                    progress.append(""\n---- Progress ----\n"");
                    progress.append(getTestDetails());
                    mStatusLabel.setText(progress.toString());
                }

    };

    private void releaseMediaRecorder() {
        if (mMediaRecorder != null) {
            mMediaRecorder.reset();
            mMediaRecorder.release();
            mMediaRecorder = null;
            mCamera.lock(); // check here, lock camera for later use
        }
    }

    @Override
    public String getTestDetails() {
        StringBuilder reportBuilder = new StringBuilder();
        reportBuilder.append(""Tested combinations:\n"");
        for (CameraCombination combination: mTestedCombinations) {
            reportBuilder.append(combination);
            reportBuilder.append(""\n"");
        }
        reportBuilder.append(""Untested combinations:\n"");
        for (String untestedCam : mUntestedCameras) {
            reportBuilder.append(untestedCam);
        }
        for (CameraCombination combination: mUntestedCombinations) {
            reportBuilder.append(combination);
            reportBuilder.append(""\n"");
        }
        return reportBuilder.toString();
    }

    @Override
    public void onSurfaceTextureAvailable(SurfaceTexture surface,
            int width, int height) {
        mPreviewTexture = surface;
        mPreviewTexWidth = width;
        mPreviewTexHeight = height;
        if (mCamera != null) {
            startPreview();
        }
    }

    @Override
    public void onSurfaceTextureSizeChanged(SurfaceTexture surface, int width, int height) {
        // Ignored, Camera does all the work for us
    }

    @Override
    public boolean onSurfaceTextureDestroyed(SurfaceTexture surface) {
        return true;
    }


    @Override
    public void onSurfaceTextureUpdated(SurfaceTexture surface) {
        // Invoked every time there's a new Camera preview frame
    }

    private AdapterView.OnItemSelectedListener mCameraSpinnerListener =
            new AdapterView.OnItemSelectedListener() {
                @Override
                public void onItemSelected(AdapterView<?> parent,
                        View view, int pos, long id) {
                    if (mCurrentCameraId != pos) {
                        setUpCamera(pos);
                    }
                }

                @Override
                public void onNothingSelected(AdapterView<?> parent) {
                    // Intentionally left blank
                }

            };

    private AdapterView.OnItemSelectedListener mResolutionSelectedListener =
            new AdapterView.OnItemSelectedListener() {
                @Override
                public void onItemSelected(AdapterView<?> parent,
                        View view, int position, long id) {
                    if (mVideoSizeIds.get(position) != mCurrentVideoSizeId) {
                        mCurrentVideoSizeId = mVideoSizeIds.get(position);
                        mCurrentVideoSizeName = mVideoSizeNames.get(position);
                        if (VERBOSE) {
                            Log.v(TAG, ""onItemSelected: mCurrentVideoSizeId = "" +
                                    mCurrentVideoSizeId + "" "" + mCurrentVideoSizeName);
                        }
                        mNextPreviewSize = matchPreviewRecordSize();
                        if (VERBOSE) {
                            Log.v(TAG, ""onItemSelected: setting preview size ""
                                    + mNextPreviewSize.width + ""x"" + mNextPreviewSize.height);
                        }

                        startPreview();
                        if (VERBOSE) {
                            Log.v(TAG, ""onItemSelected: started new preview"");
                        }
                    }
                }

                @Override
                public void onNothingSelected(AdapterView<?> parent) {
                    // Intentionally left blank
                }

            };


    private void setUpCaptureButton() {
        captureButton.setOnClickListener (
                new View.OnClickListener() {
                    @Override
                    public void onClick(View V) {
                        if ((!isRecording) && (!isPlayingBack)) {
                            if (prepareVideoRecorder()) {
                                mMediaRecorder.start();
                                if (VERBOSE) {
                                    Log.v(TAG, ""onClick: started mMediaRecorder"");
                                }
                                isRecording = true;
                                captureButton.setEnabled(false);
                                mStatusLabel.setText(getResources()
                                        .getString(R.string.status_recording));
                            } else {
                                releaseMediaRecorder();
                                Log.e(TAG, ""media recorder cannot be set up"");
                                failTest(""Unable to set up media recorder."");
                            }
                            Handler h = new Handler();
                            Runnable mDelayedPreview = new Runnable() {
                                @Override
                                public void run() {
                                    mMediaRecorder.stop();
                                    releaseMediaRecorder();

                                    mPlaybackView.setVideoPath(outputVideoFile.getPath());
                                    mPlaybackView.start();
                                    isRecording = false;
                                    isPlayingBack = true;
                                    mStatusLabel.setText(getResources()
                                            .getString(R.string.status_playback));

                                    int resIdx = mResolutionSpinner.getSelectedItemPosition();
                                    CameraCombination combination = new CameraCombination(
                                            mCurrentCameraId, resIdx,
                                            mVideoSizeNames.get(resIdx));

                                    mUntestedCombinations.remove(combination);
                                    mTestedCombinations.add(combination);

                                    if (mUntestedCombinations.isEmpty() &&
                                            mUntestedCameras.isEmpty()) {
                                        mPassButton.setEnabled(true);
                                        if (VERBOSE) {
                                            Log.v(TAG, ""run: test success"");
                                        }
                                    }
                                }
                            };
                            h.postDelayed(mDelayedPreview, VIDEO_LENGTH);
                        }

                    }
                }
        );
    }

    private class VideoSizeNamePair {
        private int sizeId;
        private String sizeName;

        public VideoSizeNamePair(int id, String name) {
            sizeId = id;
            sizeName = name;
        }

        public int getSizeId() {
            return sizeId;
        }

        public String getSizeName() {
            return sizeName;
        }
    }

    private ArrayList<VideoSizeNamePair> getVideoSizeNamePairs(int cameraId) {
        int[] qualityArray = {
                CamcorderProfile.QUALITY_LOW,
                CamcorderProfile.QUALITY_HIGH,
                CamcorderProfile.QUALITY_QCIF,  // 176x144
                CamcorderProfile.QUALITY_QVGA,  // 320x240
                CamcorderProfile.QUALITY_CIF,   // 352x288
                CamcorderProfile.QUALITY_480P,  // 720x480
                CamcorderProfile.QUALITY_720P,  // 1280x720
                CamcorderProfile.QUALITY_1080P, // 1920x1080 or 1920x1088
                CamcorderProfile.QUALITY_2160P
        };

        final Camera.Size skip = mCamera.new Size(-1, -1);
        Camera.Size[] videoSizeArray = {
                skip,
                skip,
                mCamera.new Size(176, 144),
                mCamera.new Size(320, 240),
                mCamera.new Size(352, 288),
                mCamera.new Size(720, 480),
                mCamera.new Size(1280, 720),
                skip,
                skip
        };

        String[] nameArray = {
                ""LOW"",
                ""HIGH"",
                ""QCIF"",
                ""QVGA"",
                ""CIF"",
                ""480P"",
                ""720P"",
                ""1080P"",
                ""2160P""
        };

        ArrayList<VideoSizeNamePair> availableSizes =
                new ArrayList<VideoSizeNamePair> ();

        Camera.Parameters p = mCamera.getParameters();
        List<Camera.Size> supportedVideoSizes = p.getSupportedVideoSizes();
        for (int i = 0; i < qualityArray.length; i++) {
            if (mIsExternalCamera) {
                Camera.Size videoSz = videoSizeArray[i];
                if (videoSz.equals(skip)) {
                    continue;
                }
                if (supportedVideoSizes.contains(videoSz)) {
                    VideoSizeNamePair pair = new VideoSizeNamePair(qualityArray[i], nameArray[i]);
                    availableSizes.add(pair);
                }
            } else {
                if (CamcorderProfile.hasProfile(cameraId, qualityArray[i])) {
                    VideoSizeNamePair pair = new VideoSizeNamePair(qualityArray[i], nameArray[i]);
                    availableSizes.add(pair);
                }
            }
        }
        return availableSizes;
    }

    static class ResolutionQuality {
        private int videoSizeId;
        private int width;
        private int height;

        public ResolutionQuality() {
            // intentionally left blank
        }
        public ResolutionQuality(int newSizeId, int newWidth, int newHeight) {
            videoSizeId = newSizeId;
            width = newWidth;
            height = newHeight;
        }
    }

    private Size findRecordSize(int cameraId) {
        int[] possibleQuality = {
                CamcorderProfile.QUALITY_LOW,
                CamcorderProfile.QUALITY_HIGH,
                CamcorderProfile.QUALITY_QCIF,
                CamcorderProfile.QUALITY_QVGA,
                CamcorderProfile.QUALITY_CIF,
                CamcorderProfile.QUALITY_480P,
                CamcorderProfile.QUALITY_720P,
                CamcorderProfile.QUALITY_1080P,
                CamcorderProfile.QUALITY_2160P
        };

        final Camera.Size skip = mCamera.new Size(-1, -1);
        Camera.Size[] videoSizeArray = {
                skip,
                skip,
                mCamera.new Size(176, 144),
                mCamera.new Size(320, 240),
                mCamera.new Size(352, 288),
                mCamera.new Size(720, 480),
                mCamera.new Size(1280, 720),
                skip,
                skip
        };

        ArrayList<ResolutionQuality> qualityList = new ArrayList<ResolutionQuality>();
        Camera.Parameters p = mCamera.getParameters();
        List<Camera.Size> supportedVideoSizes = p.getSupportedVideoSizes();
        for (int i = 0; i < possibleQuality.length; i++) {
            if (mIsExternalCamera) {
                Camera.Size videoSz = videoSizeArray[i];
                if (videoSz.equals(skip)) {
                    continue;
                }
                if (supportedVideoSizes.contains(videoSz)) {
                    qualityList.add(new ResolutionQuality(possibleQuality[i],
                            videoSz.width, videoSz.height));
                }
            } else {
                if (CamcorderProfile.hasProfile(cameraId, possibleQuality[i])) {
                    CamcorderProfile profile = CamcorderProfile.get(cameraId, possibleQuality[i]);
                    qualityList.add(new ResolutionQuality(possibleQuality[i],
                            profile.videoFrameWidth, profile.videoFrameHeight));
                }
            }
        }

        Size recordSize = null;
        for (int i = 0; i < qualityList.size(); i++) {
            if (mCurrentVideoSizeId == qualityList.get(i).videoSizeId) {
                recordSize = mCamera.new Size(qualityList.get(i).width,
                        qualityList.get(i).height);
                break;
            }
        }

        if (recordSize == null) {
            Log.e(TAG, ""findRecordSize: did not find a match"");
            failTest(""Cannot find video size"");
        }
        return recordSize;
    }

    // Match preview size with current recording size mCurrentVideoSizeId
    private Size matchPreviewRecordSize() {
        Size recordSize = findRecordSize(mCurrentCameraId);

        Size matchedSize = null;
        // First try to find exact match in size
        for (int i = 0; i < mPreviewSizes.size(); i++) {
            if (mPreviewSizes.get(i).equals(recordSize)) {
                matchedSize = mCamera.new Size(recordSize.width, recordSize.height);
                break;
            }
        }
        // Second try to find same ratio in size
        if (matchedSize == null) {
            for (int i = mPreviewSizes.size() - 1; i >= 0; i--) {
                if (mPreviewSizes.get(i).width * recordSize.height ==
                        mPreviewSizes.get(i).height * recordSize.width) {
                    matchedSize = mCamera.new Size(mPreviewSizes.get(i).width,
                            mPreviewSizes.get(i).height);
                    break;
                }
            }
        }
        //Third try to find one with similar if not the same apect ratio
        if (matchedSize == null) {
            for (int i = mPreviewSizes.size() - 1; i >= 0; i--) {
                if (Math.abs((float)mPreviewSizes.get(i).width * recordSize.height /
                        mPreviewSizes.get(i).height / recordSize.width - 1) < 0.12) {
                    matchedSize = mCamera.new Size(mPreviewSizes.get(i).width,
                            mPreviewSizes.get(i).height);
                    break;
                }
            }
        }
        // Last resort, just use the first preview size
        if (matchedSize == null) {
            matchedSize = mCamera.new Size(mPreviewSizes.get(0).width,
                    mPreviewSizes.get(0).height);
        }

        if (VERBOSE) {
            Log.v(TAG, ""matchPreviewRecordSize "" + matchedSize.width + ""x"" + matchedSize.height);
        }

        return matchedSize;
    }

    private void setUpCamera(int id) {
        shutdownCamera();

        mCurrentCameraId = id;
        mIsExternalCamera = isExternalCamera(id);
        try {
            mCamera = Camera.open(id);
        }
        catch (Exception e) {
            Log.e(TAG, ""camera is not available"", e);
            failTest(""camera not available"" + e.getMessage());
            return;
        }

        Camera.Parameters p = mCamera.getParameters();
        if (VERBOSE) {
            Log.v(TAG, ""setUpCamera: setUpCamera got camera parameters"");
        }

        // Get preview resolutions
        List<Size> unsortedSizes = p.getSupportedPreviewSizes();

        class SizeCompare implements Comparator<Size> {
            @Override
            public int compare(Size lhs, Size rhs) {
                if (lhs.width < rhs.width) return -1;
                if (lhs.width > rhs.width) return 1;
                if (lhs.height < rhs.height) return -1;
                if (lhs.height > rhs.height) return 1;
                return 0;
            }
        };

        if (mIsExternalCamera) {
            setVideoFrameRate(p.getPreviewFrameRate());
        }

        SizeCompare s = new SizeCompare();
        TreeSet<Size> sortedResolutions = new TreeSet<Size>(s);
        sortedResolutions.addAll(unsortedSizes);

        mPreviewSizes = new ArrayList<Size>(sortedResolutions);

        ArrayList<VideoSizeNamePair> availableVideoSizes = getVideoSizeNamePairs(id);
        String[] availableVideoSizeNames = new String[availableVideoSizes.size()];
        mVideoSizeIds = new ArrayList<Integer>();
        mVideoSizeNames = new ArrayList<String>();
        for (int i = 0; i < availableVideoSizes.size(); i++) {
            availableVideoSizeNames[i] = availableVideoSizes.get(i).getSizeName();
            mVideoSizeIds.add(availableVideoSizes.get(i).getSizeId());
            mVideoSizeNames.add(availableVideoSizeNames[i]);
        }

        mResolutionSpinner.setAdapter(
            new ArrayAdapter<String>(
                this, R.layout.camera_list_item, availableVideoSizeNames));

        // Update untested
        mUntestedCameras.remove(""All combinations for Camera "" + id + ""\n"");

        for (int videoSizeIdIndex = 0;
                videoSizeIdIndex < mVideoSizeIds.size(); videoSizeIdIndex++) {
            CameraCombination combination = new CameraCombination(
                id, videoSizeIdIndex, mVideoSizeNames.get(videoSizeIdIndex));

            if (!mTestedCombinations.contains(combination)) {
                mUntestedCombinations.add(combination);
            }
        }

        // Set initial values
        mCurrentVideoSizeId = mVideoSizeIds.get(0);
        mCurrentVideoSizeName = mVideoSizeNames.get(0);
        mNextPreviewSize = matchPreviewRecordSize();
        mResolutionSpinner.setSelection(0);

        // Set up correct display orientation
        CameraInfo info = new CameraInfo();
        Camera.getCameraInfo(id, info);
        int rotation = getWindowManager().getDefaultDisplay().getRotation();
        int degrees = 0;
        switch (rotation) {
            case Surface.ROTATION_0: degrees = 0; break;
            case Surface.ROTATION_90: degrees = 90; break;
            case Surface.ROTATION_180: degrees = 180; break;
            case Surface.ROTATION_270: degrees = 270; break;
        }

        if (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {
            mVideoRotation = (info.orientation + degrees) % 360;
            mPreviewRotation = (360 - mVideoRotation) % 360;  // compensate the mirror
        } else {  // back-facing
            mVideoRotation = (info.orientation - degrees + 360) % 360;
            mPreviewRotation = mVideoRotation;
        }
        if (mPreviewRotation != 0 && mPreviewRotation != 180) {
            Log.w(TAG,
                ""Display orientation correction is not 0 or 180, as expected!"");
        }

        mCamera.setDisplayOrientation(mPreviewRotation);

        // Start up preview if display is ready
        if (mPreviewTexture != null) {
            startPreview();
        }
    }

    private void shutdownCamera() {
        if (mCamera != null) {
            mCamera.setPreviewCallback(null);
            mCamera.stopPreview();
            mCamera.release();
            mCamera = null;
        }
    }

    /**
     * starts capturing and drawing frames on screen
     */
    private void startPreview() {

        mCamera.stopPreview();

        Matrix transform = new Matrix();
        float widthRatio = mNextPreviewSize.width / (float)mPreviewTexWidth;
        float heightRatio = mNextPreviewSize.height / (float)mPreviewTexHeight;
        if (VERBOSE) {
            Log.v(TAG, ""startPreview: widthRatio="" + widthRatio + "" "" + ""heightRatio="" +
                    heightRatio);
        }

        if (heightRatio < widthRatio) {
            transform.setScale(1, heightRatio / widthRatio);
            transform.postTranslate(0,
                    mPreviewTexHeight * (1 - heightRatio / widthRatio) / 2);
            if (VERBOSE) {
                Log.v(TAG, ""startPreview: shrink vertical by "" + heightRatio / widthRatio);
            }
        } else {
            transform.setScale(widthRatio / heightRatio, 1);
            transform.postTranslate(mPreviewTexWidth * (1 - widthRatio / heightRatio) / 2, 0);
            if (VERBOSE) {
                Log.v(TAG, ""startPreview: shrink horizontal by "" + widthRatio / heightRatio);
            }
        }

        mPreviewView.setTransform(transform);

        mPreviewSize = mNextPreviewSize;

        Camera.Parameters p = mCamera.getParameters();
        p.setPreviewSize(mPreviewSize.width, mPreviewSize.height);
        mCamera.setParameters(p);

        try {
            mCamera.setPreviewTexture(mPreviewTexture);
            if (mPreviewTexture == null) {
                Log.e(TAG, ""preview texture is null."");
            }
            if (VERBOSE) {
                Log.v(TAG, ""startPreview: set preview texture in startPreview"");
            }
            mCamera.startPreview();
            if (VERBOSE) {
                Log.v(TAG, ""startPreview: started preview in startPreview"");
            }
        } catch (IOException ioe) {
            Log.e(TAG, ""Unable to start up preview"", ioe);
            // Show a dialog box to tell user test failed
            failTest(""Unable to start preview."");
        }
    }

    private void failTest(String failMessage) {
        DialogInterface.OnClickListener dialogClickListener =
                new DialogInterface.OnClickListener() {
                    @Override
                    public void onClick(DialogInterface dialog, int which) {
                        switch (which) {
                            case DialogInterface.BUTTON_POSITIVE:
                                setTestResultAndFinish(/* passed */false);
                                break;
                            case DialogInterface.BUTTON_NEGATIVE:
                                break;
                        }
                    }
                };

        AlertDialog.Builder builder = new AlertDialog.Builder(CameraVideoActivity.this);
        builder.setMessage(getString(R.string.dialog_fail_test) + "". "" + failMessage)
                .setPositiveButton(R.string.fail_quit, dialogClickListener)
                .setNegativeButton(R.string.cancel, dialogClickListener)
                .show();
    }

    private boolean isExternalCamera(int cameraId) {
        try {
            return CameraUtils.isExternal(this, cameraId);
        } catch (Exception e) {
            Toast.makeText(this, ""Could not access camera "" + cameraId +
                    "": "" + e.getMessage(), Toast.LENGTH_LONG).show();
        }
        return false;
    }

}"	""	""	"1080"	""	""	""	""	""	""	""	""	""	""
"2.2.7.2  . Camera"	"7.5"	"H-1-7"	"7.5/H-1-7"	"07050000.720107"	"""[7.5/H-1-7] For apps targeting API level 31 or higher, the camera device MUST NOT support JPEG capture resolutions smaller than 1080p for both primary cameras. """	""	""	"JPEG MEDIA_PERFORMANCE_CLASS 1080"	""	""	""	""	""	""	""	""	"android.hardware.camera2.cts.SurfaceViewPreviewTest"	"testDeferredSurfaces"	"CtsCameraTestCases"	"/home/gpoor/cts-12-source/cts/tests/camera/src/android/hardware/camera2/cts/SurfaceViewPreviewTest.java"	""	"public void testDeferredSurfaces() throws Exception {
        for (int i = 0; i < mCameraIdsUnderTest.length; i++) {
            try {
                StaticMetadata staticInfo = mAllStaticInfo.get(mCameraIdsUnderTest[i]);
                if (staticInfo.isHardwareLevelLegacy()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] + "" is legacy, skipping"");
                    continue;
                }
                if (!staticInfo.isColorOutputSupported()) {
                    Log.i(TAG, ""Camera "" + mCameraIdsUnderTest[i] +
                            "" does not support color outputs, skipping"");
                    continue;
                }

                openDevice(mCameraIdsUnderTest[i]);
                testDeferredSurfacesByCamera(mCameraIdsUnderTest[i]);
            }
            finally {
                closeDevice();
            }
        }
    }

    private void testDeferredSurfacesByCamera(String cameraId) throws Exception {
        Size maxPreviewSize = m1080pBoundedOrderedPreviewSizes.get(0);

        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();

        // Create a SurfaceTexture for a second output
        SurfaceTexture sharedOutputTexture = new SurfaceTexture(/*random texture ID*/ 5);
        sharedOutputTexture.setDefaultBufferSize(maxPreviewSize.getWidth(),
                maxPreviewSize.getHeight());
        Surface sharedOutputSurface1 = new Surface(sharedOutputTexture);

        class TextureAvailableListener implements SurfaceTexture.OnFrameAvailableListener {
            @Override
            public void onFrameAvailable(SurfaceTexture t) {
                mGotFrame = true;
            }
            public boolean gotFrame() { return mGotFrame; }

            private volatile boolean mGotFrame = false;
        }
        TextureAvailableListener textureAvailableListener = new TextureAvailableListener();

        sharedOutputTexture.setOnFrameAvailableListener(textureAvailableListener, mHandler);

        updatePreviewSurface(maxPreviewSize);

        // Create deferred outputs for surface view and surface texture
        OutputConfiguration surfaceViewOutput = new OutputConfiguration(maxPreviewSize,
                SurfaceHolder.class);
        OutputConfiguration surfaceTextureOutput = new OutputConfiguration(maxPreviewSize,
                SurfaceTexture.class);

        List<OutputConfiguration> outputSurfaces = new ArrayList<>();
        outputSurfaces.add(surfaceViewOutput);
        outputSurfaces.add(surfaceTextureOutput);

        // Create non-deferred ImageReader output (JPEG for LIMITED-level compatibility)
        ImageDropperListener imageListener = new ImageDropperListener();
        createImageReader(mOrderedStillSizes.get(0), ImageFormat.JPEG, /*maxImages*/ 3,
                imageListener);
        OutputConfiguration jpegOutput =
                new OutputConfiguration(OutputConfiguration.SURFACE_GROUP_ID_NONE, mReaderSurface);
        outputSurfaces.add(jpegOutput);

        // Confirm that other surface types aren't supported for OutputConfiguration
        Class[] unsupportedClasses =
                {android.media.ImageReader.class, android.media.MediaCodec.class,
                 android.renderscript.Allocation.class, android.media.MediaRecorder.class};

        for (Class klass : unsupportedClasses) {
            try {
                OutputConfiguration bad = new OutputConfiguration(maxPreviewSize, klass);
                fail(""OutputConfiguration allowed use of unsupported class "" + klass);
            } catch (IllegalArgumentException e) {
                // expected
            }
        }

        // Confirm that zero surface size isn't supported for OutputConfiguration
        Size[] sizeZeros = { new Size(0, 0), new Size(1, 0), new Size(0, 1) };
        for (Size size : sizeZeros) {
            try {
                OutputConfiguration bad = new OutputConfiguration(size, SurfaceHolder.class);
                fail(""OutputConfiguration allowed use of zero surfaceSize"");
            } catch (IllegalArgumentException e) {
                //expected
            }
        }

        // Check whether session configuration is supported
        CameraTestUtils.checkSessionConfigurationSupported(mCamera, mHandler, outputSurfaces,
                /*inputConfig*/ null, SessionConfiguration.SESSION_REGULAR,
                /*defaultSupport*/ true, ""Deferred session configuration query failed"");

        // Create session

        BlockingSessionCallback sessionListener =
                new BlockingSessionCallback();

        mSession = configureCameraSessionWithConfig(mCamera, outputSurfaces, sessionListener,
                mHandler);
        sessionListener.getStateWaiter().waitForState(BlockingSessionCallback.SESSION_READY,
                SESSION_CONFIGURE_TIMEOUT_MS);

        // Submit JPEG requests

        CaptureRequest.Builder request = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        request.addTarget(mReaderSurface);

        final int SOME_FRAMES = 10;
        for (int i = 0; i < SOME_FRAMES; i++) {
            mSession.capture(request.build(), resultListener, mHandler);
        }

        // Wait to get some frames out to ensure we can operate just the one expected surface
        waitForNumResults(resultListener, SOME_FRAMES);
        assertTrue(""No images received"", imageListener.getImageCount() > 0);

        // Ensure we can't use the deferred surfaces yet
        request.addTarget(sharedOutputSurface1);
        try {
            mSession.capture(request.build(), resultListener, mHandler);
            fail(""Should have received IAE for trying to use a deferred target "" +
                    ""that's not yet configured"");
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Add deferred surfaces to their configurations
        surfaceViewOutput.addSurface(mPreviewSurface);
        surfaceTextureOutput.addSurface(sharedOutputSurface1);

        // Verify bad inputs to addSurface
        try {
            surfaceViewOutput.addSurface(null);
            fail(""No error from setting a null deferred surface"");
        } catch (NullPointerException e) {
            // expected
        }
        try {
            surfaceViewOutput.addSurface(mPreviewSurface);
            fail(""Shouldn't be able to set deferred surface twice"");
        } catch (IllegalStateException e) {
            // expected
        }

        // Add first deferred surface to session
        List<OutputConfiguration> deferredSurfaces = new ArrayList<>();
        deferredSurfaces.add(surfaceTextureOutput);

        mSession.finalizeOutputConfigurations(deferredSurfaces);

        // Try a second time, this should error

        try {
            mSession.finalizeOutputConfigurations(deferredSurfaces);
            fail(""Should have received ISE for trying to finish a deferred output twice"");
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Use new deferred surface for a bit
        imageListener.resetImageCount();
        for (int i = 0; i < SOME_FRAMES; i++) {
            mSession.capture(request.build(), resultListener, mHandler);
        }
        waitForNumResults(resultListener, SOME_FRAMES);
        assertTrue(""No images received"", imageListener.getImageCount() > 0);
        assertTrue(""No texture update received"", textureAvailableListener.gotFrame());

        // Ensure we can't use the last deferred surface yet
        request.addTarget(mPreviewSurface);
        try {
            mSession.capture(request.build(), resultListener, mHandler);
            fail(""Should have received IAE for trying to use a deferred target that's"" +
                    "" not yet configured"");
        } catch (IllegalArgumentException e) {
            // expected
        }

        // Add final deferred surface
        deferredSurfaces.clear();
        deferredSurfaces.add(surfaceViewOutput);

        mSession.finalizeOutputConfigurations(deferredSurfaces);

        // Use final deferred surface for a bit
        imageListener.resetImageCount();
        for (int i = 0; i < SOME_FRAMES; i++) {
            mSession.capture(request.build(), resultListener, mHandler);
        }
        waitForNumResults(resultListener, SOME_FRAMES);
        assertTrue(""No images received"", imageListener.getImageCount() > 0);
        // Can't check GL output since we don't have a context to call updateTexImage on, and
        // the callback only fires once per updateTexImage call.
        // And there's no way to verify data is going to a SurfaceView

        // Check for invalid output configurations being handed to a session
        OutputConfiguration badConfig =
                new OutputConfiguration(maxPreviewSize, SurfaceTexture.class);
        deferredSurfaces.clear();
        try {
            mSession.finalizeOutputConfigurations(deferredSurfaces);
            fail(""No error for empty list passed to finalizeOutputConfigurations"");
        } catch (IllegalArgumentException e) {
            // expected
        }

        deferredSurfaces.add(badConfig);
        try {
            mSession.finalizeOutputConfigurations(deferredSurfaces);
            fail(""No error for invalid output config being passed to finalizeOutputConfigurations"");
        } catch (IllegalArgumentException e) {
            // expected
        }

    }

    /**
     * Measure the inter-frame interval based on SENSOR_TIMESTAMP for frameCount frames from the
     * provided capture listener.  If prevTimestamp is positive, it is used for the first interval
     * calculation; otherwise, the first result is used to establish the starting time.
     *
     * Returns the mean interval in the first pair entry, and the largest interval in the second
     * pair entry
     */
    Pair<Long, Long> measureMeanFrameInterval(SimpleCaptureCallback resultListener, int frameCount,
            long prevTimestamp) throws Exception {
        long summedIntervals = 0;
        long maxInterval = 0;
        int measurementCount = frameCount - ((prevTimestamp > 0) ? 0 : 1);

        for (int i = 0; i < frameCount; i++) {
            CaptureResult result = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
            long timestamp = result.get(CaptureResult.SENSOR_TIMESTAMP);
            if (prevTimestamp > 0) {
                long interval = timestamp - prevTimestamp;
                if (interval > maxInterval) maxInterval = interval;
                summedIntervals += interval;
            }
            prevTimestamp = timestamp;
        }
        return new Pair<Long, Long>(summedIntervals / measurementCount, maxInterval);
    }


    /**
     * Test preview fps range for all supported ranges. The exposure time are frame duration are
     * validated.
     */
    private void previewFpsRangeTestByCamera() throws Exception {
        Size maxPreviewSz;
        Range<Integer>[] fpsRanges = getDescendingTargetFpsRanges(mStaticInfo);
        boolean antiBandingOffIsSupported = mStaticInfo.isAntiBandingOffModeSupported();
        Range<Integer> fpsRange;
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        SimpleCaptureCallback resultListener = new SimpleCaptureCallback();

        for (int i = 0; i < fpsRanges.length; i += 1) {
            fpsRange = fpsRanges[i];
            if (mStaticInfo.isHardwareLevelLegacy()) {
                // Legacy devices don't report minimum frame duration for preview sizes. The FPS
                // range should be valid for any supported preview size.
                maxPreviewSz = mOrderedPreviewSizes.get(0);
            } else {
                maxPreviewSz = getMaxPreviewSizeForFpsRange(fpsRange);
            }

            requestBuilder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);
            // Turn off auto antibanding to avoid exposure time and frame duration interference
            // from antibanding algorithm.
            if (antiBandingOffIsSupported) {
                requestBuilder.set(CaptureRequest.CONTROL_AE_ANTIBANDING_MODE,
                        CaptureRequest.CONTROL_AE_ANTIBANDING_MODE_OFF);
            } else {
                // The device doesn't implement the OFF mode, test continues. It need make sure
                // that the antibanding algorithm doesn't interfere with the fps range control.
                Log.i(TAG, ""OFF antibanding mode is not supported, the camera device output must"" +
                        "" satisfy the specified fps range regardless of its current antibanding"" +
                        "" mode"");
            }

            startPreview(requestBuilder, maxPreviewSz, resultListener);
            resultListener = new SimpleCaptureCallback();
            mSession.setRepeatingRequest(requestBuilder.build(), resultListener, mHandler);

            waitForSettingsApplied(resultListener, NUM_FRAMES_WAITED_FOR_UNKNOWN_LATENCY);

            verifyPreviewTargetFpsRange(resultListener, NUM_FRAMES_VERIFIED, fpsRange,
                    maxPreviewSz);
            stopPreview();
            resultListener.drain();
        }
    }

    private void verifyPreviewTargetFpsRange(SimpleCaptureCallback resultListener,
            int numFramesVerified, Range<Integer> fpsRange, Size previewSz) {
        CaptureResult result = resultListener.getCaptureResult(WAIT_FOR_RESULT_TIMEOUT_MS);
        List<Integer> capabilities = mStaticInfo.getAvailableCapabilitiesChecked();

        if (capabilities.contains(CaptureRequest.REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR)) {
            long frameDuration = getValueNotNull(result, CaptureResult.SENSOR_FRAME_DURATION);
            long[] frameDurationRange =
                    new long[]{(long) (1e9 / fpsRange.getUpper()), (long) (1e9 / fpsRange.getLower())};
            mCollector.expectInRange(
                    ""Frame duration must be in the range of "" + Arrays.toString(frameDurationRange),
                    frameDuration, (long) (frameDurationRange[0] * (1 - FRAME_DURATION_ERROR_MARGIN)),
                    (long) (frameDurationRange[1] * (1 + FRAME_DURATION_ERROR_MARGIN)));
            long expTime = getValueNotNull(result, CaptureResult.SENSOR_EXPOSURE_TIME);
            mCollector.expectTrue(String.format(""Exposure time %d must be no larger than frame""
                    + ""duration %d"", expTime, frameDuration), expTime <= frameDuration);

            Long minFrameDuration = mMinPreviewFrameDurationMap.get(previewSz);
            boolean findDuration = mCollector.expectTrue(""Unable to find minFrameDuration for size ""
                    + previewSz.toString(), minFrameDuration != null);
            if (findDuration) {
                mCollector.expectTrue(""Frame duration "" + frameDuration + "" must be no smaller than""
                        + "" minFrameDuration "" + minFrameDuration, frameDuration >= minFrameDuration);
            }
        } else {
            Log.i(TAG, ""verifyPreviewTargetFpsRange - MANUAL_SENSOR control is not supported,"" +
                    "" skipping duration and exposure time check."");
        }
    }

    /**
     * Test all supported preview sizes for a camera device
     *
     * @throws Exception
     */
    private void previewTestByCamera() throws Exception {
        List<Size> previewSizes = getSupportedPreviewSizes(
                mCamera.getId(), mCameraManager, PREVIEW_SIZE_BOUND);

        for (final Size sz : previewSizes) {
            if (VERBOSE) {
                Log.v(TAG, ""Testing camera preview size: "" + sz.toString());
            }

            // TODO: vary the different settings like crop region to cover more cases.
            CaptureRequest.Builder requestBuilder =
                    mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
            CaptureCallback mockCaptureCallback =
                    mock(CameraCaptureSession.CaptureCallback.class);

            startPreview(requestBuilder, sz, mockCaptureCallback);
            verifyCaptureResults(mSession, mockCaptureCallback, NUM_FRAMES_VERIFIED,
                    NUM_FRAMES_VERIFIED * FRAME_TIMEOUT_MS);
            stopPreview();
        }
    }

    private void previewTestPatternTestByCamera() throws Exception {
        Size maxPreviewSize = mOrderedPreviewSizes.get(0);
        int[] testPatternModes = mStaticInfo.getAvailableTestPatternModesChecked();
        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        CaptureCallback mockCaptureCallback;

        final int[] TEST_PATTERN_DATA = {0, 0xFFFFFFFF, 0xFFFFFFFF, 0}; // G:100%, RB:0.
        for (int mode : testPatternModes) {
            if (VERBOSE) {
                Log.v(TAG, ""Test pattern mode: "" + mode);
            }
            requestBuilder.set(CaptureRequest.SENSOR_TEST_PATTERN_MODE, mode);
            if (mode == CaptureRequest.SENSOR_TEST_PATTERN_MODE_SOLID_COLOR) {
                // Assign color pattern to SENSOR_TEST_PATTERN_MODE_DATA
                requestBuilder.set(CaptureRequest.SENSOR_TEST_PATTERN_DATA, TEST_PATTERN_DATA);
            }
            mockCaptureCallback = mock(CaptureCallback.class);
            startPreview(requestBuilder, maxPreviewSize, mockCaptureCallback);
            verifyCaptureResults(mSession, mockCaptureCallback, NUM_TEST_PATTERN_FRAMES_VERIFIED,
                    NUM_TEST_PATTERN_FRAMES_VERIFIED * FRAME_TIMEOUT_MS);
        }

        stopPreview();
    }

    private void surfaceSetTestByCamera(String cameraId) throws Exception {
        final int MAX_SURFACE_GROUP_ID = 10;
        Size maxPreviewSz = mOrderedPreviewSizes.get(0);
        Size yuvSizeBound = maxPreviewSz; // Default case: legacy device
        if (mStaticInfo.isHardwareLevelLimited()) {
            yuvSizeBound = mOrderedVideoSizes.get(0);
        } else if (mStaticInfo.isHardwareLevelAtLeastFull()) {
            yuvSizeBound = null;
        }
        Size maxYuvSize = getSupportedPreviewSizes(cameraId, mCameraManager, yuvSizeBound).get(0);

        CaptureRequest.Builder requestBuilder =
                mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
        ImageDropperListener imageListener = new ImageDropperListener();

        updatePreviewSurface(maxPreviewSz);
        createImageReader(maxYuvSize, ImageFormat.YUV_420_888, MAX_READER_IMAGES, imageListener);
        List<OutputConfiguration> outputConfigs = new ArrayList<OutputConfiguration>();
        OutputConfiguration previewConfig = new OutputConfiguration(mPreviewSurface);
        OutputConfiguration yuvConfig = new OutputConfiguration(mReaderSurface);
        assertEquals(OutputConfiguration.SURFACE_GROUP_ID_NONE, previewConfig.getSurfaceGroupId());
        assertEquals(OutputConfiguration.SURFACE_GROUP_ID_NONE, yuvConfig.getSurfaceGroupId());
        assertEquals(mPreviewSurface, previewConfig.getSurface());
        assertEquals(mReaderSurface, yuvConfig.getSurface());
        outputConfigs.add(previewConfig);
        outputConfigs.add(yuvConfig);
        requestBuilder.addTarget(mPreviewSurface);
        requestBuilder.addTarget(mReaderSurface);

        // Test different stream set ID.
        for (int surfaceGroupId = OutputConfiguration.SURFACE_GROUP_ID_NONE;
                surfaceGroupId < MAX_SURFACE_GROUP_ID; surfaceGroupId++) {
            if (VERBOSE) {
                Log.v(TAG, ""test preview with surface group id: "");
            }

            previewConfig = new OutputConfiguration(surfaceGroupId, mPreviewSurface);
            yuvConfig = new OutputConfiguration(surfaceGroupId, mReaderSurface);
            outputConfigs.clear();
            outputConfigs.add(previewConfig);
            outputConfigs.add(yuvConfig);

            for (OutputConfiguration config : outputConfigs) {
                assertEquals(surfaceGroupId, config.getSurfaceGroupId());
            }

            CameraCaptureSession.StateCallback mockSessionListener =
                    mock(CameraCaptureSession.StateCallback.class);

            mSession = configureCameraSessionWithConfig(mCamera, outputConfigs,
                    mockSessionListener, mHandler);


            mSession.prepare(mPreviewSurface);
            verify(mockSessionListener,
                    timeout(PREPARE_TIMEOUT_MS).times(1)).
                    onSurfacePrepared(eq(mSession), eq(mPreviewSurface));

            mSession.prepare(mReaderSurface);
            verify(mockSessionListener,
                    timeout(PREPARE_TIMEOUT_MS).times(1)).
                    onSurfacePrepared(eq(mSession), eq(mReaderSurface));

            CaptureRequest request = requestBuilder.build();
            CaptureCallback mockCaptureCallback =
                    mock(CameraCaptureSession.CaptureCallback.class);
            mSession.setRepeatingRequest(request, mockCaptureCallback, mHandler);
            verifyCaptureResults(mSession, mockCaptureCallback, NUM_FRAMES_VERIFIED,
                    NUM_FRAMES_VERIFIED * FRAME_TIMEOUT_MS);
        }
    }

    private class IsCaptureResultValid implements ArgumentMatcher<TotalCaptureResult> {
        @Override
        public boolean matches(TotalCaptureResult obj) {
            TotalCaptureResult result = obj;
            Long timeStamp = result.get(CaptureResult.SENSOR_TIMESTAMP);
            if (timeStamp != null && timeStamp.longValue() > 0L) {
                return true;
            }
            return false;
        }
    }

    private void verifyCaptureResults(
            CameraCaptureSession session,
            CaptureCallback mockListener,
            int expectResultCount,
            int timeOutMs) {
        // Should receive expected number of onCaptureStarted callbacks.
        ArgumentCaptor<Long> timestamps = ArgumentCaptor.forClass(Long.class);
        ArgumentCaptor<Long> frameNumbers = ArgumentCaptor.forClass(Long.class);
        verify(mockListener,
                timeout(timeOutMs).atLeast(expectResultCount))
                        .onCaptureStarted(
                                eq(session),
                                isA(CaptureRequest.class),
                                timestamps.capture(),
                                frameNumbers.capture());

        // Validate timestamps: all timestamps should be larger than 0 and monotonically increase.
        long timestamp = 0;
        for (Long nextTimestamp : timestamps.getAllValues()) {
            assertNotNull(""Next timestamp is null!"", nextTimestamp);
            assertTrue(""Captures are out of order"", timestamp < nextTimestamp);
            timestamp = nextTimestamp;
        }

        // Validate framenumbers: all framenumbers should be consecutive and positive
        long frameNumber = -1;
        for (Long nextFrameNumber : frameNumbers.getAllValues()) {
            assertNotNull(""Next frame number is null!"", nextFrameNumber);
            assertTrue(""Captures are out of order"",
                    (frameNumber == -1) || (frameNumber + 1 == nextFrameNumber));
            frameNumber = nextFrameNumber;
        }

        // Should receive expected number of capture results.
        verify(mockListener,
                timeout(timeOutMs).atLeast(expectResultCount))
                        .onCaptureCompleted(
                                eq(session),
                                isA(CaptureRequest.class),
                                argThat(new IsCaptureResultValid()));

        // Should not receive any capture failed callbacks.
        verify(mockListener, never())
                        .onCaptureFailed(
                                eq(session),
                                isA(CaptureRequest.class),
                                isA(CaptureFailure.class));
    }

}"	""	""	"JPEG 1080 1080"	""	""	""	""	""	""	""	""	""	""
